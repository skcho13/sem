[
  {
    "objectID": "contents/baser.html",
    "href": "contents/baser.html",
    "title": "Base R",
    "section": "",
    "text": "90년대에 통계 분석을 위해 개발된 R 언어와 대비하여, 좀 더 직관적이고 효율적인 데이터 분석을 위해 새로운 문법이 R내의 패키지 형태로 구현되었는데 이 새로운 생태계 안의 패키지들의 모임이 Tidyverse라는 이름하에 발전하고 있음: Tidyverse\n이 패키지들은 design philosophy, grammar, data structures를 공유하며 유기적으로 작동됨.\n기존 R의 문법과는 상당한 차이가 있어 단점도 지적되고 있고, 소위 base-R을 고수하는 사람들과 tidyverse를 기본으로 사용하는 사람들이 나뉘어 있다고 알려져 있음.\n아마도 빠르게 발전하고 있는 tidyverse/tidymodel 생태계의 언어들이 기본으로 자리잡지 않을까 함.\n본 강의에서는 주로 tidyverse의 언어로만 분석하고자 함.",
    "crumbs": [
      "R tutorial",
      "Base R"
    ]
  },
  {
    "objectID": "contents/baser.html#r의-데이터-구조와-변수-타입",
    "href": "contents/baser.html#r의-데이터-구조와-변수-타입",
    "title": "Base R",
    "section": "R의 데이터 구조와 변수 타입",
    "text": "R의 데이터 구조와 변수 타입\n주로 vector (벡터)와 data frame (데이터프레임)을 다룸\n\nSource: R in Action by Rob Kabacoff\nData frame의 예\n\n각 column이 하나의 variable (변수)를 구성하고, 한가지 타입의 데이터로 이루어짐\n\n각 Row가 하나의 observation (관측치)을 구성함.\n\n이러한 형태를 갖춘 데이터를 tidy라고도 부르며, 이를 벗어난 형태의 경우 가공이 필요함.\nex. “m23”: male이고 23세임을 나타내는 표기도 있음\n\n\nlibrary(tidyverse)\n\ncps &lt;- mosaicData::CPS85 # mosaicData package의 CPS85 데이터셋\nhead(cps) |&gt; print() # print()는 생략할 것!\n\n  wage educ race sex hispanic south married exper union age   sector\n1  9.0   10    W   M       NH    NS Married    27   Not  43    const\n2  5.5   12    W   M       NH    NS Married    20   Not  38    sales\n3  3.8   12    W   F       NH    NS  Single     4   Not  22    sales\n4 10.5   12    W   F       NH    NS Married    29   Not  47 clerical\n5 15.0   12    W   M       NH    NS Married    40 Union  58    const\n6  9.0   16    W   F       NH    NS Married    27   Not  49 clerical\n\n\n\ncps &lt;- as_tibble(cps) # tibble vs. data.frame\nhead(cps) |&gt; print()\n\n# A tibble: 6 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1   9      10 W     M     NH       NS    Married    27 Not      43 const   \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales   \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales   \n4  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n5  15      12 W     M     NH       NS    Married    40 Union    58 const   \n6   9      16 W     F     NH       NS    Married    27 Not      49 clerical\n\n\n\n# Dataset의 설명\nhelp(CPS85, package=\"mosaicData\") # 또는\n?mosaicData::CPS85",
    "crumbs": [
      "R tutorial",
      "Base R"
    ]
  },
  {
    "objectID": "contents/baser.html#vector",
    "href": "contents/baser.html#vector",
    "title": "Base R",
    "section": "Vector",
    "text": "Vector\n한 가지 타입으로만 구성: 숫자 (numeric), 문자 (character), 논리형 (logical), factor, etc\n\nvar &lt;- c(1, 2, 5, 3, 6, -2, 4) # 변수에 assign: '=' 대신 '&lt;-'\nnm &lt;- c(\"one\", \"two\", \"three\")\ntf &lt;- c(TRUE, TRUE, TRUE, FALSE, TRUE, FALSE)\n\n# 타입/클래스 확인\nclass(var)\n## [1] \"double\"\n\nclass(nm)\n## [1] \"character\"\n\nclass(tf)\n## [1] \"logical\"\n\n\n원소의 추출 및 대체\n다음은 원소를 추출, 대체하는 R의 native한 방식임\n수업에서는 뒤에서 다룰 tidyverse 문법을 주로 활용할 것임\nVector의 경우\n\nvar\n## [1]  1  2  5  3  6 -2  4\n\nvar[3]\n## [1] 5\n\nvar[c(1, 3, 5)]\n## [1] 1 5 6\n\nvar[2:6] # \":\"\" slicing: c(2, 3, 4, 5, 6)\n## [1]  2  5  3  6 -2\n\nvar[c(1, 3:5)] # 혼합\n## [1] 1 5 3 6\n\nvar[-c(1, 3)] # \"-\"는 제외라는 의미\n## [1]  2  3  6 -2  4\n\nc(10, var, 100, 101) # 추가\n##  [1]  10   1   2   5   3   6  -2   4 100 101\n\nvar[2] &lt;- 55 # 대체\n## var\n## [1]  1 55  5  3  6 -2  4\n\nvar[c(2, 5)] &lt;- c(200, 500) # 대체\n## var\n## [1]   1 200   5   3 500  -2   4\n\n# numeric 벡터의 연산: recycling rule\n1:5 * 2\n## [1]  2  4  6  8 10\n\nc(1, 3, 5) - 5\n## [1] -4 -2  0\n\nc(2, 4, 6) / 2\n## [1] 1 2 3\n\nc(1, 3) * c(2, 4)\n## [1]  2 12\n\nc(1, 3) - c(2, 4)\n## [1] -1 -1",
    "crumbs": [
      "R tutorial",
      "Base R"
    ]
  },
  {
    "objectID": "contents/baser.html#factor",
    "href": "contents/baser.html#factor",
    "title": "Base R",
    "section": "Factor",
    "text": "Factor\nVector로서 명목변수(카테고리)를 다룸\npatientID &lt;- c(1, 2, 1, 3)\ndiabetes &lt;- c(\"Type1\", \"Type2\", \"Type1\", \"Type1\")\nstatus &lt;- c(\"Poor\", \"Improved\", \"Excellent\", \"Poor\")\n\n# factor로 변환: 알파벳 순서로 levels의 순서가 정해짐\nfactor(patientID)\n## [1] 1 2 1 3\n## Levels: 1 2 3\n\nfactor(diabetes)\n## [1] Type1 Type2 Type1 Type1\n## Levels: Type1 Type2\n\nfactor(status, order = TRUE) # order를 표시\n## [1] Poor      Improved  Excellent Poor     \n## Levels: Excellent &lt; Improved &lt; Poor\n\n# 구체적으로 표시하는 것을 추천: 지정한 성분 순서대로 levels의 순서가 정해짐\nfactor(status, levels = c(\"Poor\", \"Improved\", \"Excellent\"),\n                                         order = TRUE)\n## [1] Poor      Improved  Excellent Poor     \n## Levels: Poor &lt; Improved &lt; Excellent\n\n# order가 없을시\nfactor(status, levels = c(\"Poor\", \"Improved\", \"Excellent\"))\n## [1] Poor      Improved  Excellent Poor     \n## Levels: Poor Improved Excellent\n\n# 대표적으로 성별을 코딩할 때: 숫자대신 레이블로 표시\nsex &lt;- c(1, 2, 1, 1, 1, 2, 2, 1)\nfactor(sex, levels = c(1, 2), labels = c(\"Male\", \"Female\"))\n## [1] Male   Female Male   Male   Male   Female Female Male  \n## Levels: Male Female\n\nsex_fct &lt;- factor(sex, levels = c(1, 2), labels = c(\"Male\", \"Female\"))\n\nlevels(sex) # 레벨 확인\n## NULL\nlevels(sex_fct) # 레벨 확인\n## [1] \"Male\"   \"Female\"\n\nsex\n## [1] 1 2 1 1 1 2 2 1\nsex_fct\n## [1] Male   Female Male   Male   Male   Female Female Male  \n## Levels: Male Female",
    "crumbs": [
      "R tutorial",
      "Base R"
    ]
  },
  {
    "objectID": "contents/baser.html#data-frame",
    "href": "contents/baser.html#data-frame",
    "title": "Base R",
    "section": "Data Frame",
    "text": "Data Frame\n\n데이터 프레임의 구성\n# 벡터들로부터 데이터 프레임 구성\npatientID &lt;- c(1, 2, 3, 4)\nage &lt;- c(25, 34, 28, 52)\ndiabetes &lt;- c(\"Type1\", \"Type2\", \"Type1\", \"Type1\")\nstatus &lt;- c(\"Poor\", \"Improved\", \"Excellent\", \"Poor\")\n\npatientdata &lt;- data.frame(patientID, age, diabetes, status)\n\npatientdata\n##   patientID age diabetes    status\n## 1         1  25    Type1      Poor\n## 2         2  34    Type2  Improved\n## 3         3  28    Type1 Excellent\n## 4         4  52    Type1      Poor\n\nmidterm &lt;- data.frame(english = c(90, 80, 60, 70),\n                      math = c(50, 60, 100, 20),\n                      class = c(1, 1, 2, 2))\nmidterm\n##   english math class\n## 1      90   50     1\n## 2      80   60     1\n## 3      60  100     2\n## 4      70   20     2\n\n\n원소의 추출 및 대체\n# 원소의 추출\npatientdata[1:2] # 변수의 열을 지정\n##   patientID age\n## 1         1  25\n## 2         2  34\n## 3         3  28\n## 4         4  52\n\npatientdata[c(\"diabetes\", \"status\")] # 열 이름을 지정\n##   diabetes    status\n## 1    Type1      Poor\n## 2    Type2  Improved\n## 3    Type1 Excellent\n## 4    Type1      Poor\n\npatientdata[c(1, 3), c(\"age\", \"status\")] # 행과 열을 모두 지정\n##   age    status\n## 1  25      Poor\n## 3  28 Excellent\n\npatientdata[c(1, 3), c(2, 4)]\n##   age    status\n## 1  25      Poor\n## 3  28 Excellent\n\npatientdata[, 1:2] # patientdata[1:2]과 동일, 빈칸은 모든 행을 의미\n##   patientID age\n## 1         1  25\n## 2         2  34\n## 3         3  28\n## 4         4  52\n\npatientdata[1:2, ] # 빈칸은 모든 열을 의미\n##   patientID age diabetes   status\n## 1         1  25    Type1     Poor\n## 2         2  34    Type2 Improved\n\npatientdata[-1] # 열 제외\n##   age diabetes    status\n## 1  25    Type1      Poor\n## 2  34    Type2  Improved\n## 3  28    Type1 Excellent\n## 4  52    Type1      Poor\n\npatientdata[-c(1, 3)] # 열 제외\n##   age    status\n## 1  25      Poor\n## 2  34  Improved\n## 3  28 Excellent\n## 4  52      Poor\n\npatientdata[-c(1:2), 2:4] # 행 제외 & 열 선택\n##   age diabetes    status\n## 3  28    Type1 Excellent\n## 4  52    Type1      Poor\n\n\n# 변수/열의 성분을 벡터로 추출: $ 또는 [[ ]]을 이용\npatientdata$age # $를 이용\n## [1] 25 34 28 52\n\nclass(patientdata$age) # numeric vector임을 확인\n## [1] \"numeric\"\n\npatientdata[[\"age\"]] # patientdata$age과 동일, [[ ]] doule bracket을 이용해 벡터로 추출\n## [1] 25 34 28 52\n\npatientdata[[2]] # 열의 위치를 이용해도 동일한 추출\n## [1] 25 34 28 52\n\npatientdata[\"age\"] # [ ] single bracket은 열을 선택하는 것으로 데이터 프레임으로 추출\n##   age\n## 1  25\n## 2  34\n## 3  28\n## 4  52\n\npatientdata[2] # 2번째 열을 추출; patientdata[\"age\"]과 동일\n##   age\n## 1  25\n## 2  34\n## 3  28\n## 4  52\n\n\n데이터의 추가 및 대체\n# 데이터 추가\npatientdata$gender &lt;- c(1, 1, 2, 2) \n\npatientdata\n##   patientID age diabetes    status gender\n## 1         1  25    Type1      Poor      1\n## 2         2  34    Type2  Improved      1\n## 3         3  28    Type1 Excellent      2\n## 4         4  52    Type1      Poor      2\n\n# 데이터 대체\npatientdata[c(1,3), \"age\"] # 혼동: 원칙적으로 데이터프레임으로 추출되어야하나 벡터로 추출됨\n## [1] 25 28\n\npatientdata[c(1,3), \"age\"] &lt;- c(88, 99)\npatientdata\n##   patientID age diabetes    status gender\n## 1         1  88    Type1      Poor      1\n## 2         2  34    Type2  Improved      1\n## 3         3  99    Type1 Excellent      2\n## 4         4  52    Type1      Poor      2\n\n# 참고\nrow.names(patientdata) # 데이터 프레임의 행 이름\n## [1] \"1\" \"2\" \"3\" \"4\"\n\nrow.names(patientdata) &lt;- c(\"a\", \"b\", \"c\", \"d\")\npatientdata\n##   patientID age diabetes    status gender\n## a         1  88    Type1      Poor      1\n## b         2  34    Type2  Improved      1\n## c         3  99    Type1 Excellent      2\n## d         4  52    Type1      Poor      2",
    "crumbs": [
      "R tutorial",
      "Base R"
    ]
  },
  {
    "objectID": "contents/baser.html#tibble",
    "href": "contents/baser.html#tibble",
    "title": "Base R",
    "section": "Tibble",
    "text": "Tibble\n기존 data.frame의 단점을 보안한 tidyverse에서 기본이 되는 데이터 형식\n\nData frame vs. tibble\nPrinting의 차이\ncps &lt;- mosaicData::CPS85 # data.frame\ncps\n#   wage educ race sex hispanic south married exper union age   sector\n# 1  9.0   10    W   M       NH    NS Married    27   Not  43    const\n# 2  5.5   12    W   M       NH    NS Married    20   Not  38    sales\n# 3  3.8   12    W   F       NH    NS  Single     4   Not  22    sales\n# 4 10.5   12    W   F       NH    NS Married    29   Not  47 clerical\n# 5 15.0   12    W   M       NH    NS Married    40 Union  58    const\n# 6  9.0   16    W   F       NH    NS Married    27   Not  49 clerical\n...\n\ncps_tibble &lt;- as_tibble(cps)\ncps_tibble\n# # A tibble: 534 × 11\n#    wage  educ race  sex   hispanic south married exper union   age sector  \n#   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n# 1   9      10 W     M     NH       NS    Married    27 Not      43 const   \n# 2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales   \n# 3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales   \n# 4  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n# 5  15      12 W     M     NH       NS    Married    40 Union    58 const   \n# 6   9      16 W     F     NH       NS    Married    27 Not      49 clerical\n# # … with 528 more rows\n그 외의 차이는 R for Data Science/10.3 Tibbles vs. data.frame을 참고",
    "crumbs": [
      "R tutorial",
      "Base R"
    ]
  },
  {
    "objectID": "contents/notice.html#중간시험-대체-과제",
    "href": "contents/notice.html#중간시험-대체-과제",
    "title": "Notice",
    "section": "중간시험 대체 과제",
    "text": "중간시험 대체 과제",
    "crumbs": [
      "Notice"
    ]
  },
  {
    "objectID": "contents/notice.html#기말시험",
    "href": "contents/notice.html#기말시험",
    "title": "Notice",
    "section": "기말시험",
    "text": "기말시험",
    "crumbs": [
      "Notice"
    ]
  },
  {
    "objectID": "contents/DAS-II.html",
    "href": "contents/DAS-II.html",
    "title": "The Differential Ability Scales—Second Edition",
    "section": "",
    "text": "Elliott, Colin D. (2012). “Chapter 13: The Differential Ability Scales—Second Edition”. In Flanagan, Dawn P.; Harrison, Patti L. (eds.). Contemporary Intellectual Assessment: Theories, tests, and issues (Third ed.)."
  },
  {
    "objectID": "contents/visualize.html",
    "href": "contents/visualize.html",
    "title": "Visualize",
    "section": "",
    "text": "데이터 시각화는 탐색적 분석에 더 초점이 맞춰져 있음.\n\n소위 data mining이라고 부르는 데이터 내의 숨겨진 패턴을 찾고 분석하는 탐색적 분석은 전통적인 통계에서 discouraging되어 왔음.\n\n확률에 근거한 통계 이론은 데이터를 수집하기 전에 가설을 세우고 그 가설을 confirm하는 방식을 취함.\n\n논란의 여지가 있지만, 원칙적으로 가설에 근거해 수집한 자료가 가설과 일치하는지를 확인하는 작업에서는 자료를 두 번 이상 들여다 보지 않아야 함.\n\n그럼에도 불구하고, 탐색적 분석은 behind doors에서 이루어지거나 새로운 가설을 세우기 위한 방편으로 이용되었음.\n\n또한, 매우 엄격한 잣대를 적용하는 상황에서도 통계 이론의 특성으로 인해 기본적인 탐색적 분석은 반드시 선행되어야 함.\n\n연구 가설의 진위를 탐구할 때, 탐색적 분석에서 쉽게 빠질 수 있는 편향성(bias)는 항상 조심할 필요가 있고, 확신을 위해서는 새로이 자료를 수집해서 가설을 재검증할 필요가 있음.\n\n탐색적 분석을 위해서는 다양한 시각화 기술이 요하나, 일반적인 통계 분석을 위해서 필요로하는 최소한으로 제한하고자 함.\n또한, 복잡한 통계치를 살펴볼 때, 직접 시각화를 하기보다는 패키지가 알아서 시각화를 해주기 때문에 자세히 알지 못해도 무방함.\n좀 더 상세한 내용에 대해서는\n\nR for Data Science/Visualize\nggplot2 book\nggplot2 extensions\n통계치 표현: ggstatsplot, ggpubr\nData Visualization with R by Rob Kabacoff : 적절한 밸런스\nggplot2 cheatsheet : pdf 다운로드\n\n\n\n\n\n\n\nNote\n\n\n\n충분히 큰 데이터의 경우, 일정량의 데이터 가령 1/4을 따로 떼어놓고, 3/4만으로 탐색적 분석을 통해 모델을 만든 후, 따로 떼어놓은 1/4로 (가설)검증을 하는 cross-validation 방법이 있는데, machine leanring분야에서는 기본적인 process.\nCross-validation 방식에는 여러 변형들이 있음; e.g. 데이터를 4등분하여 각각 4번 위의 방식을 반복하여 합치는 방식, 3가지 (training, validation, test sets)로 나누어 분석",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#exploratory-vs.-confirmatory-analysis",
    "href": "contents/visualize.html#exploratory-vs.-confirmatory-analysis",
    "title": "Visualize",
    "section": "",
    "text": "데이터 시각화는 탐색적 분석에 더 초점이 맞춰져 있음.\n\n소위 data mining이라고 부르는 데이터 내의 숨겨진 패턴을 찾고 분석하는 탐색적 분석은 전통적인 통계에서 discouraging되어 왔음.\n\n확률에 근거한 통계 이론은 데이터를 수집하기 전에 가설을 세우고 그 가설을 confirm하는 방식을 취함.\n\n논란의 여지가 있지만, 원칙적으로 가설에 근거해 수집한 자료가 가설과 일치하는지를 확인하는 작업에서는 자료를 두 번 이상 들여다 보지 않아야 함.\n\n그럼에도 불구하고, 탐색적 분석은 behind doors에서 이루어지거나 새로운 가설을 세우기 위한 방편으로 이용되었음.\n\n또한, 매우 엄격한 잣대를 적용하는 상황에서도 통계 이론의 특성으로 인해 기본적인 탐색적 분석은 반드시 선행되어야 함.\n\n연구 가설의 진위를 탐구할 때, 탐색적 분석에서 쉽게 빠질 수 있는 편향성(bias)는 항상 조심할 필요가 있고, 확신을 위해서는 새로이 자료를 수집해서 가설을 재검증할 필요가 있음.\n\n탐색적 분석을 위해서는 다양한 시각화 기술이 요하나, 일반적인 통계 분석을 위해서 필요로하는 최소한으로 제한하고자 함.\n또한, 복잡한 통계치를 살펴볼 때, 직접 시각화를 하기보다는 패키지가 알아서 시각화를 해주기 때문에 자세히 알지 못해도 무방함.\n좀 더 상세한 내용에 대해서는\n\nR for Data Science/Visualize\nggplot2 book\nggplot2 extensions\n통계치 표현: ggstatsplot, ggpubr\nData Visualization with R by Rob Kabacoff : 적절한 밸런스\nggplot2 cheatsheet : pdf 다운로드\n\n\n\n\n\n\n\nNote\n\n\n\n충분히 큰 데이터의 경우, 일정량의 데이터 가령 1/4을 따로 떼어놓고, 3/4만으로 탐색적 분석을 통해 모델을 만든 후, 따로 떼어놓은 1/4로 (가설)검증을 하는 cross-validation 방법이 있는데, machine leanring분야에서는 기본적인 process.\nCross-validation 방식에는 여러 변형들이 있음; e.g. 데이터를 4등분하여 각각 4번 위의 방식을 반복하여 합치는 방식, 3가지 (training, validation, test sets)로 나누어 분석",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#basics",
    "href": "contents/visualize.html#basics",
    "title": "Visualize",
    "section": "Basics",
    "text": "Basics\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\n\npenguins |&gt;\n    print() # 무시\n\n# A tibble: 344 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA NA     2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# … with 338 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nVariabels:\n\nspecies: a penguin’s species (Adelie, Chinstrap, or Gentoo).\n\nflipper_length_mm: length of a penguin’s flipper, in millimeters.\n\nbody_mass_g: body mass of a penguin, in grams.\n\n더 자세한 사항은 ?penguins\nggplot을 이용한 시각화는 주로 3가지 성분으로 나뉨\n\ndata: 사용할 데이터\n\nmapping: data의 변수들을 어떤 특성에 mapping할 것인지 specify\n\ngeom: 어떤 시각화 개체(graphical objects)로 데이터를 표현할 것인지 specify\n\n\n# x, y축에 변수를 mapping\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n)\n\n\n\n\n\n\n\n\n\n# point로 데이터를 표시: scatterplot\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n#&gt; Warning: Removed 2 rows containing missing values (`geom_point()`).\n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n실제로 is.na()함수를 이용해 missing을 확인해보면,\npenguins |&gt;\n  select(species, flipper_length_mm, body_mass_g) |&gt;\n  filter(is.na(body_mass_g) | is.na(flipper_length_mm))  # true, false의 boolean type\n#&gt; # A tibble: 2 × 3\n#&gt;   species flipper_length_mm body_mass_g\n#&gt;   &lt;fct&gt;               &lt;int&gt;       &lt;int&gt;\n#&gt; 1 Adelie                 NA          NA\n#&gt; 2 Gentoo                 NA          NA\n\n\n\nAdding aesthetics and layers\n\n# spcies에 color (aesthetics)를 mapping\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point()\n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n위에서 species마다 다른 색을 입혀서 다른 패턴이 나타나는지 확인해 볼 수 있음\nggplot2는 + 기호로 연결하여 계속 layer를 추가할 수 있음.\n다음은 trendline 혹은 fitted line이라고 부르는 경향성을 확인해 볼 수 있는 라인의 layer를 추가함\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_smooth()`).”\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nData에 fitted curve를 구하는 방식에는 여러 방법이 있음\n\nLinear fit: 1차 함수형태인 직선으로 fit\nSmoothing fit\n\nPolynominal fit: n차 다항함수형태로 fit\nLoess/lowess: locally estimated/weighted scatterplot smoothing\nGAM: generalized additive model\nSpine: piece-wise polynominal regression\n\n\n나중에 좀 더 자세히 알아봄\n\n\nggplot2는 플랏의 대상에 다음과 같은 속성을 부여할 수 있음\ncolor, size, shape, fill, alpha\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species, shape = island)\n) +\n  geom_point() \n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n어떤 속성을 어떤 변수에 할당하는 것이 적절한지를 선택하는 것이 기술\n\n\n\n\nCategorical vs. continuous\ncolor와 같은 속성은 카테고리 변수가 좀 더 적절하나, 연속변수에서도 적용될 수 있음\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = bill_length_mm)\n) +\n  geom_point() \n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n반대로, x, y에 카테고리 변수를 mapping하여 scatterplot을 그리면 다음과 같은 overploting의 문제가 생김\n\nggplot(\n  data = penguins,\n  mapping = aes(x = species, y = body_mass_g, color = sex)\n) +\n  geom_point() \n\nWarning message:\n“Removed 11 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\n\nOverplotting\nOverplotting의 문제를 해결하는 방식은 주로\n\nalpha(투명도)를 조정하거나 랜덤하게 흐뜨려그리는 geom_jitter()를 사용\n\n애초에 겹치지 않게 그리는 방법도 있음: e.g. beeswarm plot\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = species, y = body_mass_g, color = sex)\n) +\n  geom_jitter(width = .2) # jitter의 정도: width, height\n\nWarning message:\n“Removed 11 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = species, y = body_mass_g, color = sex)\n) +\n  geom_jitter(width = .2, alpha = .5) # alpha: 투명도 0 ~ 1\n\nWarning message:\n“Removed 11 rows containing missing values (`geom_point()`).”",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#geometric-objects",
    "href": "contents/visualize.html#geometric-objects",
    "title": "Visualize",
    "section": "Geometric objects",
    "text": "Geometric objects\nggplot2는 40가지 넘는 geom objects를 제공함.\n주로 통계를 위해 쓰일 geom들은\n\ngeom_point, geom_smooth()\ngeom_boxplot()\ngeom_histogram(), geom_freqploy(), geom_density()\n\nGlobal vs. local mapping\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species)) + # color mapping은 geom_point에만 적용\n  geom_smooth() # 맨 위의 mapping에 있는 global mapping을 inherit\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_smooth()`).”\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_smooth(mapping = aes(linetype = sex), se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_smooth()`).”\n\n\n\n\n\n\n\n\n\n\nggplot(\n    data = penguins,\n    mapping = aes(x = bill_depth_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species)) +  # color mapping은 geom_point에만 \n  geom_smooth(method = lm)  # 맨 위의 mapping에 있는 global mapping을 inherit, method: fitted line의 종류\n\n\n\n\n\n\n\n\n\nggplot(\n    data = penguins,\n    mapping = aes(x = bill_depth_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species)) +\n  geom_smooth(method = lm) +  # 맨 위의 mapping에 있는 global mapping을 inherit\n  geom_smooth(mapping = aes(color = species), method = lm) # color mapping 추가\n\n\n\n\n\n\n\n\naes() 내부, 외부에서의 mapping\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n    geom_point(mapping = aes(color = species)) # aesthetic color에 변수를 mapping\n\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n    geom_point(color = \"skyblue\") + # geom의 color 속성에 색을 지정\n    geom_smooth(color = \"orangered\")",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#statistical-transformations",
    "href": "contents/visualize.html#statistical-transformations",
    "title": "Visualize",
    "section": "Statistical transformations",
    "text": "Statistical transformations\nggplot2는 편의를 위해 통계치를 구해 표시해주는데,\n경우에 따라 직접 통계치를 계산 후 새로 얻는 데이터로 그리는 것이 유리함\n\nDistribution\ngeom_histogram(), geom_freqploy(), geom_density()\n\n# y축에 표시되는 통계치들이 계산됨\nggplot(data = penguins, mapping = aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 100) # binwidth vs. bins\n\nggplot(data = penguins, mapping = aes(x = body_mass_g, colour = sex)) +\n  geom_freqpoly(binwidth = 100)\n\nggplot(data = penguins, mapping = aes(x = body_mass_g, colour = sex)) +\n  geom_density(bw = 100) # bw: band width\n\n\n\n\n\n\n\n\n\n\n\n\nBoxplot\nBoxplot은 분포에 대한 정보은 줄어드나, 카테고리별로 간결하게 비교되는 장점\nboxplot()\n\n\n\nggplot(data = penguins, mapping = aes(x = species, y = body_mass_g)) +\n    geom_boxplot()\n\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_boxplot()`).”\n\n\n\n\n\n\n\n\n\n\nggplot(data = penguins, mapping = aes(x = species, y = body_mass_g)) +\n    geom_boxplot() +\n    geom_jitter(alpha = .6)\n\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_boxplot()`).”\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\ncps &lt;- as_tibble(mosaicData::CPS85)\ncps |&gt;\n    filter(wage &lt; 30) |&gt; \n    ggplot(aes(x = as.factor(educ), y = wage)) +  # as.factor(): numeric을 factor로 변환\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\nggplot(data = penguins, mapping = aes(x = species, y = body_mass_g, fill = sex)) + # color는 box의 테두리 색, fill은 내부색\n  geom_boxplot()\n\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_boxplot()`).”\n\n\n\n\n\n\n\n\n\n\n\nBarplot\nBarplot은 여러방식으로 쓸 수 있는데, 문법이 조금 복잡하고, 수업에서 거의 사용하지 않을 예정이므로 웹사이트를 참조\nR for Data Science/Layers/Statistical transformations\n\nggplot(data = penguins) + \n  geom_bar(mapping = aes(x = species)) # 개수\n\n\n\n\n\n\n\n\n\n\nDiscretize\n연속 변수를 임의의 구간으로 나누어 카테고리처럼 적용하기 할 수 있음\ncut_width(), cut_number(), cut_interval()\n\ncut_width(): 구간의 길이를 정함\ncut_number(): 동일한 갯수의 관측값을 갖는 n개의 그룹\ncut_interval(): 동일한 길이의 n개의 그룹\n\n\nggplot(\n  data = penguins,\n  mapping = aes(\n      x = bill_length_mm, y = bill_depth_mm,\n      color = cut_interval(body_mass_g, 3) # body_mass_g의 값을 3개의 동일한 길이의 구간으로 나눔\n  )\n) +\n  geom_point() +\n  geom_smooth(se = FALSE, span = 1) # span: smoothing 정도 조절\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_smooth()`).”\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#facets",
    "href": "contents/visualize.html#facets",
    "title": "Visualize",
    "section": "Facets",
    "text": "Facets\n카테고리 변수들이 지니는 카테고리들(레벨)로 나누어 그리기\nfacet_wrap(), facet_grid()\n\nggplot(data = penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n  geom_point() +\n  facet_wrap(~species) # species의 레벨로 나뉘어짐\n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\nfacet_wrap()은 레벨이 많아지면 다음의 facet_grid()와는 다르게 화면크기에 맞춰 다음 줄로 넘어감\n\nggplot(data = penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n  geom_point() +\n  facet_grid(sex ~ species)  # 행과 열에 각각 sex, species\n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\nggplot(\n  data = penguins, \n  aes(x = body_mass_g, y = flipper_length_mm, color = sex) # color 추가\n) +\n  geom_point(alpha = .6) +\n  facet_grid(island ~ species)  # 행과 열에 각각 sex, species\n\nWarning message:\n“Removed 11 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFacet과 color 중 어떤 방식으로 표현하는 것이 유리한가? 밸런스를 잘 선택!\n\n\nggplot(data = penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n  geom_point() +\n  facet_wrap(~species)\n\nggplot(data = penguins, aes(x = body_mass_g, y = flipper_length_mm, color = species)) +\n  geom_point()\n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#labels",
    "href": "contents/visualize.html#labels",
    "title": "Visualize",
    "section": "Labels",
    "text": "Labels\nlabs() 안에 각 요소별로 지정\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = island)) +\n  geom_smooth() +\n  labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n    color = \"Species\", shape = \"Island\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n앞으로는 pipe operator와 함께, 축약 형태로\n\ndata = 대신 첫번째 argument 위치에 data frame이 위치\nmapping = 은 두번째 argument 위치에 aes()을 위치\n\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n은 다음과 같이\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n\n\nPipe operator로 다음과 연결될 수 있음\n\npenguins |&gt;\n    filter(!is.na(sex) & island != \"Torgersen\") |&gt;  # 성별이 missing이 아니고, Torgersen섬은 제외\n    ggplot(aes(x = body_mass_g, y = flipper_length_mm, color = sex)) +\n    geom_point() +\n    geom_smooth() +\n    facet_wrap(~island)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#examples",
    "href": "contents/visualize.html#examples",
    "title": "Visualize",
    "section": "Examples",
    "text": "Examples\n이전에 다뤘던 CPS85 데이터로 보면,\n\ncps &lt;- as_tibble(mosaicData::CPS85) # mosaicData package의 CPS85 데이터셋\ncps |&gt;\n   print() # 생략!\n\n# A tibble: 534 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1   9      10 W     M     NH       NS    Married    27 Not      43 const   \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales   \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales   \n4  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n5  15      12 W     M     NH       NS    Married    40 Union    58 const   \n6   9      16 W     F     NH       NS    Married    27 Not      49 clerical\n# … with 528 more rows\n\n\n\ncps |&gt;\n    ggplot(aes(x = wage, color = married)) +\n    geom_freqpoly(binwidth=1)\n\n\n\n\n\n\n\n\n\ncps |&gt;\n    ggplot(aes(x = wage, color = married)) +\n    geom_freqpoly(binwidth = 1) +\n    facet_wrap(~sex)\n\n\n\n\n\n\n\n\n\ncps |&gt;\n  ggplot(aes(x = married, y = wage)) +\n  geom_boxplot(width = .2) +\n  geom_jitter(width = .2, alpha = .2, color = \"red\") +\n  scale_y_continuous(label = scales::label_dollar())  # y축 scale의 변경\n\n\n\n\n\n\n\n\n\ncps |&gt;\n  ggplot(aes(x = married, y = wage, fill = sex)) +\n  geom_boxplot()\n  \n\n\n\n\n\n\n\n\n\ncps |&gt;\n    filter(wage &lt; 30) |&gt; \n    ggplot(aes(x = sector, y = wage, fill = sex)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\ncps |&gt;\n    filter(wage &lt; 30) |&gt;\n    ggplot(aes(x = sector, y = wage, fill = sex)) +\n    geom_boxplot() +\n    facet_grid(married ~ .) \n\n\n\n\n\n\n\n\n\nplot &lt;- cps |&gt;\n  filter(wage &lt; 30) |&gt;\n  ggplot(aes(x = age, y = wage)) +\n  geom_point(alpha = .6) +\n  geom_smooth()\nplot\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n확대, 축소 혹은 제한된 범위에서 보려면 다음 2가지를 구분해야 함\ncoord_cartesian() vs. xlim() or ylim()\n\n\n\nplot + coord_cartesian(xlim = c(18, 40)) # zoom in\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nplot + xlim(18, 40) # data crop\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\nWarning message:\n“Removed 181 rows containing non-finite values (`stat_smooth()`).”\nWarning message:\n“Removed 181 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\ncps |&gt;\n    filter(wage &lt; 30 & sector %in% c(\"manag\", \"manuf\", \"prof\", \"sales\")) |&gt;\n    ggplot(aes(x = age, y = wage, color = sex)) +\n    geom_point() +\n    geom_smooth(se = FALSE, span = 1) +\n    facet_wrap(~sector)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/setup2.html",
    "href": "contents/setup2.html",
    "title": "Packages for SEM",
    "section": "",
    "text": "당분간 lavaan 버전 0.6-17을 사용\n\n# remove lavaan\nremove.packages(\"lavaan\")\n\n# install lavaan 0.6-17\ninstall.packages(\"https://cran.r-project.org/src/contrib/Archive/lavaan/lavaan_0.6-17.tar.gz\", repo = NULL, type = \"source\")\n\nsemTools\nmanymome\ntidySEM: graph_sem()\nlavaanExtra: nice_tidySEM()\nsemPlot: semPaths()",
    "crumbs": [
      "Packages for SEM"
    ]
  },
  {
    "objectID": "contents/ml.html",
    "href": "contents/ml.html",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)\nsim1.csv 파일\nsim1 &lt;- read_csv(\"data/sim1.csv\")\nsim1 |&gt; print(n = 5)\n\n# A tibble: 30 x 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1  4.20\n2     1  7.51\n3     1  2.13\n4     2  8.99\n5     2 10.2 \n# i 25 more rows\nScatter plot of sim1 data\nsim1 |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Scatter plot of sim1 data\")",
    "crumbs": [
      "ML Estimation"
    ]
  },
  {
    "objectID": "contents/ml.html#ols-ordinary-least-squares-추정방식",
    "href": "contents/ml.html#ols-ordinary-least-squares-추정방식",
    "title": "Maximum Likelihood Estimation",
    "section": "OLS (Ordinary Least Squares) 추정방식",
    "text": "OLS (Ordinary Least Squares) 추정방식\n선형 모델 family인 \\(\\hat{y} = b_0 + b_1 x\\)을 세운 후\n잔차 \\(e_i = y_i - \\hat{y_i}\\)의 제곱의 합, 즉 \\(\\sum e_i^2\\)이 최소가 되도록하는 \\(b_0, b_1\\)을 추정하는 방법\n\nmod &lt;- lm(y ~ x, data = sim1) \nmod |&gt; coef() |&gt; print()\n\n(Intercept)           x \n   4.220822    2.051533 \n\n\n즉, OLS 방식에서 최선의 모형은 \\(\\hat{y} = 4.22 + 2.05x\\)\n이 모형의 예측값과 잔차를 보면,\n\n\nAdd predictions and residuals to sim1 data\nlibrary(modelr)\nsim1 &lt;- sim1 |&gt; \n  add_predictions(mod) |&gt; \n  add_residuals(mod)\nsim1 |&gt; print(n = 7)\n\n\n# A tibble: 30 x 4\n      x     y  pred  resid\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1  4.20  6.27 -2.07 \n2     1  7.51  6.27  1.24 \n3     1  2.13  6.27 -4.15 \n4     2  8.99  8.32  0.665\n5     2 10.2   8.32  1.92 \n6     2 11.3   8.32  2.97 \n7     3  7.36 10.4  -3.02 \n# i 23 more rows",
    "crumbs": [
      "ML Estimation"
    ]
  },
  {
    "objectID": "contents/ml.html#maximum-likelihood-estimation",
    "href": "contents/ml.html#maximum-likelihood-estimation",
    "title": "Maximum Likelihood Estimation",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\n데이터가 발생된 것으로 가정하는 분포를 고려했을 때,\n어떨때 주어진 데이터가 관측될 확률/가능도(likelihood)가 최대가 되겠는가로 접근하는 방식으로,\nX, Y의 관계와 확률분포를 함께 고려함.\n\n선형관계라면, 즉 \\(E(Y|X=x_i) = \\beta_0 + \\beta_1x_i\\)   (\\(E\\): expected value, 기대값)\n분포가 Gaussian이라면, 즉 \\(Y|(X=x_i) \\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2)\\)   (\\(\\sigma\\): 표준편차)\n\n\nLikelihood \\(L = \\displaystyle\\prod_{i=1}^{n}{P_i}\\)   (관측치들 독립일 때, product rule에 의해)\n분포가 Gaussian이라면(평균: \\(\\mu\\), 표준편차: \\(\\sigma\\)), 즉 \\(f(t) = \\displaystyle\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{(t-\\mu)^2}{2\\sigma^2}\\right)\\)라면\n\\(L = \\displaystyle\\prod_{i=1}^{n}{f(y_i, x_i)} = \\displaystyle\\prod_{i=1}^{n}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}\\right)}\\)\n이 때, 이 likelihood를 최대화하는 \\(\\beta_0, \\beta_1, \\sigma\\)를 찾는 것이 목표이며,\n이처럼 분포가 Gaussian라면, OLS estimation과 동일한 값을 얻음. (단, \\(\\sigma\\)는 bias가 존재)\n다른 분포를 가지더라도 동일하게 적용할 수 있음!\n\n즉, likelihood의 관점에서 주어진 데이터에 가장 근접하도록(likelihood가 최대가 되는) “분포의 구조”를 얻는 과정\n\n여러 편의를 위해, log likelihood를 최대화함.\n\n\n\n\n\n\nLog likelihood\n\n\n\n다음 두가지를 고려하면,\n\\(log(x*y) = log(x) + log(y)\\)\n\\(e^x * e^y = e^{x+y}\\)\n\\(log(L) = \\displaystyle\\sum_{i=1}^{n}{log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}\\right)\\right)} = \\displaystyle\\sum_{i=1}^{n}{-log(\\sqrt{2\\pi\\sigma^2}) - \\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}}\\)\n두 번째 항이 앞서 정의한 squared error와 동일함\n\n\n잔차에 대한 간단한 정보들\n\nlm(y ~ x, data = sim1) |&gt; summary() |&gt; print()\n\n\nCall:\nlm(formula = y ~ x, data = sim1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1469 -1.5197  0.1331  1.4670  4.6516 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.2208     0.8688   4.858 4.09e-05 ***\nx             2.0515     0.1400  14.651 1.17e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.203 on 28 degrees of freedom\nMultiple R-squared:  0.8846,    Adjusted R-squared:  0.8805 \nF-statistic: 214.7 on 1 and 28 DF,  p-value: 1.173e-14\n\n\n\n\n# 위의 모형의 잔차들에 대해 정규분포 함수의 값을 구하면,\nsigma &lt;- sd(sim1$resid)  # 잔차의 표준편차: 2.16\n# 좀 더 정확히는 샘플 수로 나누지 않고, 자유도로 나누어야 함\nsim1 &lt;- sim1 |&gt; \n  mutate(norm = dnorm(resid, mean = 0, sd = sigma))\nsim1 |&gt; print(n = 5)\n\n# A tibble: 30 x 5\n      x     y  pred  resid   norm\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1  4.20  6.27 -2.07  0.117 \n2     1  7.51  6.27  1.24  0.156 \n3     1  2.13  6.27 -4.15  0.0294\n4     2  8.99  8.32  0.665 0.176 \n5     2 10.2   8.32  1.92  0.124 \n# i 25 more rows\n\n\n\n\n\nGaussin distribution with mean 0 and sd 2.16\nx &lt;- seq(-10, 10, length = 100)\ny &lt;- dnorm(x, mean = 0, sd = sigma)\ntibble(x = x, y = y) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  labs(\n    title = \"Gaussian distribution with mean 0 and sd 2.16\",\n    x = \"Residuals\", y = \"Density\"\n  ) +\n  geom_bar(data = sim1, aes(x = resid, y = norm), stat = \"identity\", color = \"black\")\n\n\n\n\n\n\n\n\n\n즉, 위 그림에서 높이를 모두 곱하면, likelihood를 얻을 수 있으며,\nLog-likelihood는 log를 취해 더하면 되므로,\n\nlog(sim1$norm) |&gt; sum() |&gt; print()\n\n[1] -65.2347\n\n\n이 값은 lavaan 결과에서 보여지며, 모형 적합도 계산을 위한 기본적인 값으로 사용됨.\n\nfit &lt;- sem('y ~ x', data = sim1)\nsummary(fit, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         2\n\n  Number of observations                            30\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nModel Test Baseline Model:\n\n  Test statistic                                64.784\n  Degrees of freedom                                 1\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.000\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)                -65.226\n  Loglikelihood unrestricted model (H1)        -65.226\n                                                      \n  Akaike (AIC)                                 134.452\n  Bayesian (BIC)                               137.255\n  Sample-size adjusted Bayesian (SABIC)        131.028\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: RMSEA &lt;= 0.050                       NA\n  P-value H_0: RMSEA &gt;= 0.080                       NA\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  y ~                                                 \n    x                 2.052    0.135   15.166    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y                 4.529    1.169    3.873    0.000\n\n\n\nMaximum likelihood estimation의 결과는 분포가 Gaussian이라면 OLS와 동일하며,\n다른 분포를 가지는 데이터에 대해서도 동일한 원리로 (즉, likelihood를 최대로 하도록) 파라미터를 추정할 수 있음.\n대표적인 예가 logistic regression이며, 이 경우의 분포는 이항분포 또는 베르누이 분포를 가정하여 likelihood를 계산함.",
    "crumbs": [
      "ML Estimation"
    ]
  },
  {
    "objectID": "contents/ml.html#구조방정식-모형에서-분포에-대한-가정들",
    "href": "contents/ml.html#구조방정식-모형에서-분포에-대한-가정들",
    "title": "Maximum Likelihood Estimation",
    "section": "구조방정식 모형에서 분포에 대한 가정들",
    "text": "구조방정식 모형에서 분포에 대한 가정들\n관찰된 변수(measured variable)들이 multivariate normal distribution을 따른다는 가정을 함.\n이 때, 각 변수들은 정규분포를 따르게 되지만, 그 반대는 아님.\n\n\nSource: p. 150, Introduction to Statistical Learning with Applications in Python by G. James, D. Witten, T. Hastie, R. Tibshirani\n\n\n\n\n\n\n\nMultivariate Gaussian for 2-dimensional data\n\n\n\nMultivariate Gaussian 분포: \\(\\displaystyle f(\\mathbf{x}) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mu)^T\\Sigma^{-1}(\\mathbf{x}-\\mu)\\right)\\)\n\n\\(\\displaystyle \\mathbf{x}= \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\)\n\\(\\mu\\): \\(X_1\\)과 \\(X_2\\)의 평균 벡터: \\(\\displaystyle \\begin{bmatrix} \\mu_{1} \\\\ \\mu_{2} \\end{bmatrix}\\)\n\\(\\Sigma\\): 공분산 행렬(covariance matrix): \\(\\displaystyle \\begin{bmatrix} \\sigma_{1}^2 & \\sigma_{12} \\\\ \\sigma_{21} & \\sigma_{2}^2 \\end{bmatrix}\\)\n\n\n\n각 잠재변수(latent variable)들은 정규분포를 따른다고 가정\n\n정규성에 대한 검증이 필요하며,\n정규성을 따르지 않는 경우, robust estimation이나 bootstrap 방법을 사용할 수 있음.\n\n분포 뿐만 아니라 회귀에서의 모든 가정들이 SEM에서 동일하게 적용됨!",
    "crumbs": [
      "ML Estimation"
    ]
  },
  {
    "objectID": "contents/import.html",
    "href": "contents/import.html",
    "title": "Import",
    "section": "",
    "text": "자세한 데이터 import에 대해서는 링크",
    "crumbs": [
      "R tutorial",
      "Import"
    ]
  },
  {
    "objectID": "contents/import.html#text-files-csv",
    "href": "contents/import.html#text-files-csv",
    "title": "Import",
    "section": "Text files: csv",
    "text": "Text files: csv\nreadr 패키지(tidyverse에 포함)\nread_csv(), write_csv()\n\nR 기본 함수 read.csv()를 개선\n다양한 옵션은 ?read_csv, ?write_csv 참고\n\n\ncsv 파일 읽기\naltruism.csv 파일 링크\n\nlibrary(tidyverse)\n\nhelping &lt;- read_csv(\"data/altruism.csv\")\nhelping |&gt; print()\n\n# A tibble: 120 × 12\n      id pho_1 pho_2 pho_3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1   250    95    95    95     1  2004      80      NA      80      80      70\n 2    32    58    62    NA     0  2003      62      58      59      57      56\n 3   109   100    50    50    NA  2003      90      51      51      51      52\n 4   209    77    77    64     1  2004      66      72      88      82      67\n 5    94    77    50    77     1  2003     100     100     100      51      78\n 6   260   100    75   100     0  2004     100      60      70      55      70\n 7   258    77    94    86     1  2004      91      93      85      91      73\n 8   244    90    68    20     0  2004      67      66      31      67      63\n 9   180   100    79    77     0  2003      61      51      30      51      51\n10   182    75    50    64     1  2003      80      80      70      65      70\n# … with 110 more rows, and 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nread_csv()의 자주 사용되는 옵션\nread_csv(\"data/file.csv\", skip = 2) # 첫 2절 스킵\nread_csv(\"data/file.csv\", na = \".\") # 결측치가 .으로 기록된 파일\n\n\n\n\ncsv 파일 쓰기\nwrite_csv(): 단, 쓰기를 하면서 변수 타입 소멸\n\nwrite_csv(helping, file=\"data/helping_new.csv\")",
    "crumbs": [
      "R tutorial",
      "Import"
    ]
  },
  {
    "objectID": "contents/import.html#excel-spreadsheets",
    "href": "contents/import.html#excel-spreadsheets",
    "title": "Import",
    "section": "Excel spreadsheets",
    "text": "Excel spreadsheets\nreadxl package\nread_excel(), read_xlsx(), read_xls()\n\n엑셀 파일 읽기\nstduents.xlsx 파일 링크\n\nlibrary(readxl) # install.packages(\"readxl\")\n\nstud &lt;- read_xlsx(\"data/students.xlsx\")\nstud |&gt; print()\n\n# A tibble: 1,000 × 93\n  stu_id sch_id sstratid   sex  race ethnic bys42a bys42b bys44a bys44b bys44c\n   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 124966   1249        1     2     4      1      3      4      2      4      4\n2 124972   1249        1     1     4      1      4      5      1      3      3\n3 175551   1755        1     2     3      0     NA      3      2      3      3\n4 180660   1806        1     1     4      1      2     NA      1      4      4\n5 180672   1806        1     2     4      1      2      3      1      4      3\n6 298885   2988        2     1     3      0      5      4      2      3      3\n# … with 994 more rows, and 82 more variables: bys44d &lt;dbl&gt;, bys44e &lt;dbl&gt;,\n#   bys44f &lt;dbl&gt;, bys44g &lt;dbl&gt;, bys44h &lt;dbl&gt;, bys44i &lt;dbl&gt;, bys44j &lt;dbl&gt;,\n#   bys44k &lt;dbl&gt;, bys44l &lt;dbl&gt;, bys44m &lt;dbl&gt;, bys48a &lt;dbl&gt;, bys48b &lt;dbl&gt;,\n#   bys79a &lt;dbl&gt;, byfamsiz &lt;dbl&gt;, famcomp &lt;dbl&gt;, bygrads &lt;dbl&gt;, byses &lt;dbl&gt;,\n#   byfaminc &lt;dbl&gt;, parocc &lt;dbl&gt;, bytxrstd &lt;dbl&gt;, bytxmstd &lt;dbl&gt;,\n#   bytxsstd &lt;dbl&gt;, bytxhstd &lt;dbl&gt;, bypared &lt;dbl&gt;, bytests &lt;dbl&gt;,\n#   par_inv &lt;dbl&gt;, f1s36a1 &lt;dbl&gt;, f1s36a2 &lt;dbl&gt;, f1s36b1 &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nNote\n\n\n\nSpecify sheet either by position or by name\nread_excel(\"salaries.xlsx\", sheet = 2) # The default is sheet = 1\nread_excel(\"salaries.xlsx\", sheet = \"personnel\")",
    "crumbs": [
      "R tutorial",
      "Import"
    ]
  },
  {
    "objectID": "contents/import.html#statistical-packages",
    "href": "contents/import.html#statistical-packages",
    "title": "Import",
    "section": "Statistical packages",
    "text": "Statistical packages\nSPSS의 데이터: read_sav()\nstudents-shorter.sav 파일 링크\n\nlibrary(haven) # install.packages(\"haven\")\n\nstud_spss &lt;- read_sav(\"data/students-shorter.sav\")\nstud_spss |&gt; print()\n\n# A tibble: 1,000 x 93\n   stu_id    sch_id   sstratid sex     race    ethnic  bys42a   bys42b   bys44a \n   &lt;dbl+lbl&gt; &lt;dbl+lb&gt; &lt;dbl+lb&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt; &lt;dbl+lb&gt; &lt;dbl+l&gt;\n 1 124966    1249     1        2 [Fem~ 4 [Whi~ 1 [whi~  3 [2-3~  4 [3-4~ 2 [Agr~\n 2 124972    1249     1        1 [Mal~ 4 [Whi~ 1 [whi~  4 [3-4~  5 [4-5~ 1 [Str~\n 3 175551    1755     1        2 [Fem~ 3 [Bla~ 0 [blk~ NA        3 [2-3~ 2 [Agr~\n 4 180660    1806     1        1 [Mal~ 4 [Whi~ 1 [whi~  2 [1-2~ NA       1 [Str~\n 5 180672    1806     1        2 [Fem~ 4 [Whi~ 1 [whi~  2 [1-2~  3 [2-3~ 1 [Str~\n 6 298885    2988     2        1 [Mal~ 3 [Bla~ 0 [blk~  5 [4-5~  4 [3-4~ 2 [Agr~\n 7 604419    6044     6        2 [Fem~ 4 [Whi~ 1 [whi~  4 [3-4~  5 [4-5~ 2 [Agr~\n 8 605355    6053     6        2 [Fem~ 4 [Whi~ 1 [whi~  2 [1-2~  3 [2-3~ 1 [Str~\n 9 605377    6053     6        2 [Fem~ 4 [Whi~ 1 [whi~  3 [2-3~  5 [4-5~ 2 [Agr~\n10 637529    6375     6        1 [Mal~ 3 [Bla~ 0 [blk~  5 [4-5~  6 [Ove~ 2 [Agr~\n# i 990 more rows\n# i 84 more variables: bys44b &lt;dbl+lbl&gt;, bys44c &lt;dbl+lbl&gt;, bys44d &lt;dbl+lbl&gt;,\n#   bys44e &lt;dbl+lbl&gt;, bys44f &lt;dbl+lbl&gt;, bys44g &lt;dbl+lbl&gt;, bys44h &lt;dbl+lbl&gt;,\n#   bys44i &lt;dbl+lbl&gt;, bys44j &lt;dbl+lbl&gt;, bys44k &lt;dbl+lbl&gt;, bys44l &lt;dbl+lbl&gt;,\n#   bys44m &lt;dbl+lbl&gt;, bys48a &lt;dbl+lbl&gt;, bys48b &lt;dbl+lbl&gt;, bys79a &lt;dbl+lbl&gt;,\n#   byfamsiz &lt;dbl+lbl&gt;, famcomp &lt;dbl+lbl&gt;, bygrads &lt;dbl+lbl&gt;, byses &lt;dbl+lbl&gt;,\n#   byfaminc &lt;dbl+lbl&gt;, parocc &lt;dbl&gt;, bytxrstd &lt;dbl+lbl&gt;, ...\n\n\n\nstud_spss |&gt;\n    select(ethnic) |&gt;\n    print()\n\n# A tibble: 1,000 x 1\n  ethnic            \n  &lt;dbl+lbl&gt;         \n1 1 [white-asian]   \n2 1 [white-asian]   \n3 0 [blk,namer,hisp]\n4 1 [white-asian]   \n5 1 [white-asian]   \n6 0 [blk,namer,hisp]\n# i 994 more rows\n\n\nlabelled 데이터 참고\n\ninstall.packages(\"labelled\")\nlibrary(labelled)\n\n\n# labelled 변수를 factor로 변환\nstud_spss |&gt;\n    unlabelled() |&gt;\n    print()\n\n# A tibble: 1,000 x 93\n  stu_id sch_id sstratid sex    race   ethnic bys42a bys42b bys44a bys44b bys44c\n   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; \n1 124966   1249        1 Female White~ white~ 2-3 h~ 3-4 h~ Agree  Stron~ Stron~\n2 124972   1249        1 Male   White~ white~ 3-4 h~ 4-5 h~ Stron~ Disag~ Disag~\n3 175551   1755        1 Female Black~ blk,n~ NA     2-3 h~ Agree  Disag~ Disag~\n4 180660   1806        1 Male   White~ white~ 1-2 h~ NA     Stron~ Stron~ Stron~\n5 180672   1806        1 Female White~ white~ 1-2 h~ 2-3 h~ Stron~ Stron~ Disag~\n6 298885   2988        2 Male   Black~ blk,n~ 4-5 h~ 3-4 h~ Agree  Disag~ Disag~\n# i 994 more rows\n# i 82 more variables: bys44d &lt;fct&gt;, bys44e &lt;fct&gt;, bys44f &lt;fct&gt;, bys44g &lt;fct&gt;,\n#   bys44h &lt;fct&gt;, bys44i &lt;fct&gt;, bys44j &lt;fct&gt;, bys44k &lt;fct&gt;, bys44l &lt;fct&gt;,\n#   bys44m &lt;fct&gt;, bys48a &lt;fct&gt;, bys48b &lt;fct&gt;, bys79a &lt;fct&gt;, byfamsiz &lt;fct&gt;,\n#   famcomp &lt;fct&gt;, bygrads &lt;dbl&gt;, byses &lt;dbl&gt;, byfaminc &lt;fct&gt;, parocc &lt;dbl&gt;,\n#   bytxrstd &lt;dbl&gt;, bytxmstd &lt;dbl&gt;, bytxsstd &lt;dbl&gt;, bytxhstd &lt;dbl&gt;,\n#   bypared &lt;fct&gt;, bytests &lt;dbl&gt;, par_inv &lt;dbl&gt;, f1s36a1 &lt;fct&gt;, ...\n\n\nLabels 제거하기\n\nstud &lt;- stud_spss |&gt;\n    remove_val_labels()\nstud |&gt; print()\n\n# A tibble: 1,000 x 93\n  stu_id sch_id sstratid   sex  race ethnic bys42a bys42b bys44a bys44b bys44c\n   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 124966   1249        1     2     4      1      3      4      2      4      4\n2 124972   1249        1     1     4      1      4      5      1      3      3\n3 175551   1755        1     2     3      0     NA      3      2      3      3\n4 180660   1806        1     1     4      1      2     NA      1      4      4\n5 180672   1806        1     2     4      1      2      3      1      4      3\n6 298885   2988        2     1     3      0      5      4      2      3      3\n# i 994 more rows\n# i 82 more variables: bys44d &lt;dbl&gt;, bys44e &lt;dbl&gt;, bys44f &lt;dbl&gt;, bys44g &lt;dbl&gt;,\n#   bys44h &lt;dbl&gt;, bys44i &lt;dbl&gt;, bys44j &lt;dbl&gt;, bys44k &lt;dbl&gt;, bys44l &lt;dbl&gt;,\n#   bys44m &lt;dbl&gt;, bys48a &lt;dbl&gt;, bys48b &lt;dbl&gt;, bys79a &lt;dbl&gt;, byfamsiz &lt;dbl&gt;,\n#   famcomp &lt;dbl&gt;, bygrads &lt;dbl&gt;, byses &lt;dbl&gt;, byfaminc &lt;dbl&gt;, parocc &lt;dbl&gt;,\n#   bytxrstd &lt;dbl&gt;, bytxmstd &lt;dbl&gt;, bytxsstd &lt;dbl&gt;, bytxhstd &lt;dbl&gt;,\n#   bypared &lt;dbl&gt;, bytests &lt;dbl&gt;, par_inv &lt;dbl&gt;, f1s36a1 &lt;dbl&gt;, ...",
    "crumbs": [
      "R tutorial",
      "Import"
    ]
  },
  {
    "objectID": "contents/diagnostics.html",
    "href": "contents/diagnostics.html",
    "title": "Diagnostics",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)",
    "crumbs": [
      "Keith's",
      "Diagnostics"
    ]
  },
  {
    "objectID": "contents/diagnostics.html#multivariate-test",
    "href": "contents/diagnostics.html#multivariate-test",
    "title": "Diagnostics",
    "section": "Multivariate test",
    "text": "Multivariate test\n\nmardia for Mardia’s test,\nhz for Henze-Zirkler’s test, # default\nroyston for Royston’s test,\ndh for Doornik-Hansen’s test,\nenergy for E-statistic\n\n\nlibrary(MVN)\n\n\n# multivariate normality\nmvn(data = df, mvnTest = \"hz\")$multivariateNormality |&gt; print()\n\n           Test       HZ p value MVN\n1 Henze-Zirkler 26.31374       0  NO\n\n\n\n# multivariate normality plot\npar(mfrow = c(1, 1))\nresults &lt;- mvn(data = df, multivariatePlot = \"qq\")",
    "crumbs": [
      "Keith's",
      "Diagnostics"
    ]
  },
  {
    "objectID": "contents/diagnostics.html#univariate-test",
    "href": "contents/diagnostics.html#univariate-test",
    "title": "Diagnostics",
    "section": "Univariate test",
    "text": "Univariate test\n\nShapiro-Wilk (SW),\nCramer-von Mises (CVM),\nLilliefors (Lillie),\nShapiro-Francia (SF),\nAnderson-Darling (AD) # default\n\n\nmvn(data = df, univariateTest = \"AD\")$univariateNormality |&gt; print()\n\n              Test  Variable Statistic   p value Normality\n1 Anderson-Darling    X1        0.2219  0.8297      YES   \n2 Anderson-Darling    X2       62.9376  &lt;0.001      NO    \n3 Anderson-Darling    X3        4.1598  &lt;0.001      NO    \n\n\n\n# univariate normality histogram\nresults &lt;- mvn(data = df, univariatePlot = \"histogram\")\n\n\n\n\n\n\n\n\n\n# univariate normality scatterplot\nresults &lt;- mvn(data = df, univariatePlot = \"scatter\")",
    "crumbs": [
      "Keith's",
      "Diagnostics"
    ]
  },
  {
    "objectID": "contents/diagnostics.html#outliers",
    "href": "contents/diagnostics.html#outliers",
    "title": "Diagnostics",
    "section": "Outliers",
    "text": "Outliers\n\n# outliers\nresults &lt;- mvn(data = df, mvnTest = \"hz\", multivariateOutlierMethod = \"adj\")",
    "crumbs": [
      "Keith's",
      "Diagnostics"
    ]
  },
  {
    "objectID": "contents/diagnostics.html#homework-dataset",
    "href": "contents/diagnostics.html#homework-dataset",
    "title": "Diagnostics",
    "section": "Homework Dataset",
    "text": "Homework Dataset\n\nhw_mean &lt;- haven::read_sav(\"data/chap 19 latent means/Homework means.sav\")\nhw_mean &lt;- hw_mean |&gt; \n  select(-ethnic, -Female, hw10 = f1s36a2, hw12 = f2s25f2) |&gt; \n  na.omit()\nhw_mean |&gt; print()\n\n# A tibble: 798 × 13\n   bytxrstd  bytxmstd  bytxsstd  bytxhstd  parocc hw10    hw12    eng92 math92 sci92 soc92\n   &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;  &lt;dbl&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 44.2      52.6      44.5      53.9        27.4 1 [1 H… 2 [1-3…  8      4.4   8.5   5.37\n 2 39.9      56.0      55.7      56.4        70.6 4 [7-9… 5 [10-…  5.5    4.62  5.5   4.83\n 3 52.7      47.9      44.8      41.8        56.3 3 [4-6… 5 [10-…  8.37   3.5   8     6.29\n 4 42.3      61.9      53.5      47.0        70.6 3 [4-6… 1 [LES…  8.33   5     7     7.67\n 5 63.2      56.1      64.7      62.6        27.4 3 [4-6… 5 [10-…  6      4.92  3.11  7.5 \n 6 59.0      40.7      42.1      50.4        70.6 2 [2-3… 5 [10-…  6.75   6.43  7.2   6.83\n 7 52.6      60.9      62.4      54          61.3 2 [2-3… 2 [1-3…  8.67   3.25  5.85  9   \n 8 59.9      67.7      51.2      39.1        66.1 3 [4-6… 5 [10-…  9.8    7.25  8     9   \n 9 35.7      47.9      49.2      51.3        66.2 0 [NON… 3 [4-6…  1.6    2.7   3.5   4   \n10 41.8      53.8      55.5      50.4        49.7 1 [1 H… 2 [1-3…  8      5.14  7.1   7.33\n# ℹ 788 more rows\n# ℹ 2 more variables: byfaminc &lt;dbl&gt;, bypared &lt;dbl+lbl&gt;\n\n\n\n# multivariate & univariate normality\nmvn(data = hw_mean)$multivariateNormality |&gt; print()\nmvn(data = hw_mean)$univariateNormality |&gt; print()\n\n           Test       HZ p value MVN\n1 Henze-Zirkler 1.057538       0  NO\n               Test  Variable Statistic   p value Normality\n1  Anderson-Darling bytxrstd     7.6118  &lt;0.001      NO    \n2  Anderson-Darling bytxmstd     5.7422  &lt;0.001      NO    \n3  Anderson-Darling bytxsstd     1.2679  0.0027      NO    \n4  Anderson-Darling bytxhstd     2.7309  &lt;0.001      NO    \n5  Anderson-Darling  parocc     53.6441  &lt;0.001      NO    \n6  Anderson-Darling   hw10      29.5673  &lt;0.001      NO    \n7  Anderson-Darling   hw12      16.6879  &lt;0.001      NO    \n8  Anderson-Darling   eng92      4.3695  &lt;0.001      NO    \n9  Anderson-Darling  math92      2.0612  &lt;0.001      NO    \n10 Anderson-Darling   sci92      2.2486  &lt;0.001      NO    \n11 Anderson-Darling   soc92      3.5983  &lt;0.001      NO    \n12 Anderson-Darling byfaminc    12.5805  &lt;0.001      NO    \n13 Anderson-Darling  bypared    31.9085  &lt;0.001      NO    \n\n\n\n# multi-variate normality\nresults &lt;- mvn(data = hw_mean, multivariatePlot = \"qq\")\n\n\n\n\n\n\n\n\n\n# univariate normality\nresults &lt;- mvn(data = hw_mean, univariatePlot = \"histogram\")\n\n\n\n\n\n\n\n\n\nresults &lt;- mvn(data = hw_mean, univariatePlot = \"box\")\n\n\n\n\n\n\n\n\n\n# outliers\nresults &lt;- mvn(data = hw_mean, mvnTest = \"hz\", multivariateOutlierMethod = \"adj\", showOutliers = TRUE, showNewData = TRUE)  # show options\n\n\n\n\n\n\n\n\n이상치들\n\nresults$multivariateOutliers |&gt; as_tibble() |&gt; print()\n\n# A tibble: 214 × 3\n   Observation `Mahalanobis Distance` Outlier\n   &lt;chr&gt;                        &lt;dbl&gt; &lt;chr&gt;  \n 1 1                             86.4 TRUE   \n 2 2                             85.0 TRUE   \n 3 3                             79.4 TRUE   \n 4 4                             77.5 TRUE   \n 5 5                             77.0 TRUE   \n 6 6                             76.1 TRUE   \n 7 7                             76.0 TRUE   \n 8 8                             74.9 TRUE   \n 9 9                             74.7 TRUE   \n10 10                            74.2 TRUE   \n# ℹ 204 more rows\n\n\n이상치가 제거된 데이터셋\nresults$newData",
    "crumbs": [
      "Keith's",
      "Diagnostics"
    ]
  },
  {
    "objectID": "contents/diagnostics.html#non-normality에-대한-대응",
    "href": "contents/diagnostics.html#non-normality에-대한-대응",
    "title": "Diagnostics",
    "section": "Non-normality에 대한 대응",
    "text": "Non-normality에 대한 대응\n\n\n\n\n\n\nNon-normality에 대한 대응\n\n\n\n\nRobust ML\nBootstrapping: Bollen-Stine bootstrap\n분포에 대한 가정이 없는 방법: WLS(fully weighted least squares)\n\nML 파라미터 추정치는 일반적으로 robust하나, 표준오차는 문제가 될 수 있음.\n다음과 같은 ML에 대한 robust estimation들은 표준오차에 대한 보정을 제공함.\n\nOption “MLM” is for complete data sets only and generates the mean-adjusted Satorra-Bentler scaled chi-square.\nOption “MLR” can be applied in complete or incomplete data sets, and it generates a mean adjusted chi-square based on the Yuan-Bentler T2 statistic. Because this method accommodates missing data, it is the most flexible option listed here.\nOption “MLMV” is for complete data sets only and computes a mean- and variance-adjusted scaled chi-square.\nOption “MLMVS” generates a mean- and variance adjusted chi-square with a correction for heteroscedasticity by Satterthwaite (1941). Model degrees of freedom are estimated, and the method is for complete data sets.\n\n\nSource: p. 137, 163, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n\n\n\n\n\nLoad data\nhw_mean &lt;- haven::read_sav(\"data/chap 19 latent means/Homework means.sav\")\nhw_mean &lt;- hw_mean |&gt; \n  rename(hw10 = f1s36a2, hw12 = f2s25f2)\n\n\n\n# missing data\nVIM::aggr(hw_mean, numbers = TRUE, sortVars = TRUE)\n\n\n Variables sorted by number of missings: \n Variable Count\n     hw12 0.096\n     hw10 0.046\n byfaminc 0.042\n bytxhstd 0.032\n bytxsstd 0.031\n bytxmstd 0.030\n bytxrstd 0.029\n    eng92 0.020\n   math92 0.019\n    sci92 0.014\n    soc92 0.011\n   parocc 0.009\n   ethnic 0.009\n  bypared 0.001\n   Female 0.000\n\n\n\n\n\n\n\n\n\n\nRobust ML\n결측치가 포함된 경우: estimator = \"MLR\"\n\n결측치에 대해 FIML(Full Information Maximum Likelihood) 적용\n‘Huber-White’ robust standard errors\n\n결측치가 없는 경우: estimator = \"MLM\"\n\n결측치가 있다면 listwise deletion\nclassic robust standard errors\n\n\nhw_model &lt;- \"\n  famback =~ parocc + byfaminc + bypared\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw12 + hw10\n  grades =~ eng92 + math92 + sci92 + soc92\n\n  bytxrstd ~~ eng92\n  bytxmstd ~~ math92\n  bytxsstd ~~ sci92\n  bytxhstd ~~ soc92\n\n  prevach ~ famback\n  grades ~ prevach + hw\n  hw ~ prevach + famback\n\"\nsem_fit_robust &lt;- sem(hw_model, data = hw_mean, estimator = \"MLR\")\nsummary(sem_fit_robust, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 141 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        35\n\n                                                  Used       Total\n  Number of observations                           798        1000\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                91.530      89.681\n  Degrees of freedom                                56          56\n  P-value (Chi-square)                           0.002       0.003\n  Scaling correction factor                                  1.021\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              5965.249    5618.146\n  Degrees of freedom                                78          78\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.062\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.994       0.994\n  Tucker-Lewis Index (TLI)                       0.992       0.992\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.994\n  Robust Tucker-Lewis Index (TLI)                            0.992\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -26118.080  -26118.080\n  Scaling correction factor                                  1.029\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)             NA          NA\n  Scaling correction factor                                  1.024\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               52306.160   52306.160\n  Bayesian (BIC)                             52470.033   52470.033\n  Sample-size adjusted Bayesian (SABIC)      52358.889   52358.889\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.028       0.027\n  90 Percent confidence interval - lower         0.017       0.016\n  90 Percent confidence interval - upper         0.038       0.038\n  P-value H_0: RMSEA &lt;= 0.050                    1.000       1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.028\n  90 Percent confidence interval - lower                     0.016\n  90 Percent confidence interval - upper                     0.038\n  P-value H_0: Robust RMSEA &lt;= 0.050                         1.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.023       0.023\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              14.926    0.720\n    byfaminc          0.116    0.006   18.977    0.000    1.731    0.714\n    bypared           0.071    0.003   20.360    0.000    1.060    0.846\n  prevach =~                                                            \n    bytxrstd          1.000                               8.047    0.840\n    bytxmstd          1.010    0.034   29.773    0.000    8.126    0.844\n    bytxsstd          0.972    0.035   27.545    0.000    7.822    0.806\n    bytxhstd          0.930    0.033   28.448    0.000    7.487    0.812\n  hw =~                                                                 \n    hw12              1.000                               1.117    0.579\n    hw10              1.061    0.123    8.638    0.000    1.185    0.705\n  grades =~                                                             \n    eng92             1.000                               2.190    0.901\n    math92            0.870    0.029   29.960    0.000    1.905    0.776\n    sci92             0.987    0.026   37.349    0.000    2.162    0.876\n    soc92             1.038    0.026   40.350    0.000    2.273    0.894\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.289    0.022   13.027    0.000    0.536    0.536\n  grades ~                                                              \n    prevach           0.136    0.012   11.785    0.000    0.500    0.500\n    hw                0.524    0.105    5.000    0.000    0.267    0.267\n  hw ~                                                                  \n    prevach           0.048    0.008    5.615    0.000    0.342    0.342\n    famback           0.015    0.005    3.033    0.002    0.194    0.194\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng92             0.357    0.273    1.306    0.191    0.357    0.065\n .bytxmstd ~~                                                           \n   .math92            1.648    0.353    4.667    0.000    1.648    0.206\n .bytxsstd ~~                                                           \n   .sci92             0.422    0.336    1.257    0.209    0.422    0.062\n .bytxhstd ~~                                                           \n   .soc92             0.265    0.309    0.858    0.391    0.265    0.043\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          206.883   13.049   15.854    0.000  206.883    0.481\n   .byfaminc          2.888    0.243   11.881    0.000    2.888    0.491\n   .bypared           0.445    0.046    9.579    0.000    0.445    0.284\n   .bytxrstd         27.069    1.975   13.703    0.000   27.069    0.295\n   .bytxmstd         26.756    1.863   14.360    0.000   26.756    0.288\n   .bytxsstd         32.884    2.044   16.087    0.000   32.884    0.350\n   .bytxhstd         29.043    2.016   14.407    0.000   29.043    0.341\n   .hw12              2.468    0.190   12.975    0.000    2.468    0.664\n   .hw10              1.420    0.186    7.629    0.000    1.420    0.503\n   .eng92             1.111    0.092   12.135    0.000    1.111    0.188\n   .math92            2.393    0.133   18.028    0.000    2.393    0.397\n   .sci92             1.419    0.106   13.371    0.000    1.419    0.233\n   .soc92             1.295    0.110   11.820    0.000    1.295    0.200\n    famback         222.792   18.016   12.366    0.000    1.000    1.000\n   .prevach          46.180    3.332   13.858    0.000    0.713    0.713\n   .hw                0.965    0.158    6.123    0.000    0.774    0.774\n   .grades            2.683    0.203   13.235    0.000    0.559    0.559\n\n\n\n\n\nBootstrapping\nBollen-Stine bootstrap\n\n결측치는 listwise deletion\n파라미터는 ML로 추정\nstandard errer: se = \"bootstrap\"\nchi-square test: test = \"bootstrap\"\n\n\nset.seed(123)  # set seed for reproducibility\nsem_fit_boot &lt;- sem(hw_model, data = hw_mean, test = \"bootstrap\", se = \"bootstrap\")\nsummary(sem_fit_boot, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 141 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        35\n\n                                                  Used       Total\n  Number of observations                           798        1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                91.530\n  Degrees of freedom                                56\n  P-value (Chi-square)                           0.002\n                                                      \n  Test statistic                                91.530\n  Degrees of freedom                                56\n  P-value (Bollen-Stine bootstrap)               0.005\n\nModel Test Baseline Model:\n\n  Test statistic                              5965.249\n  Degrees of freedom                                78\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.994\n  Tucker-Lewis Index (TLI)                       0.992\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -26118.080\n  Loglikelihood unrestricted model (H1)             NA\n                                                      \n  Akaike (AIC)                               52306.160\n  Bayesian (BIC)                             52470.033\n  Sample-size adjusted Bayesian (SABIC)      52358.889\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.028\n  90 Percent confidence interval - lower         0.017\n  90 Percent confidence interval - upper         0.038\n  P-value H_0: RMSEA &lt;= 0.050                    1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.023\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws             1000\n  Number of successful bootstrap draws            1000\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              14.926    0.720\n    byfaminc          0.116    0.006   18.949    0.000    1.731    0.714\n    bypared           0.071    0.004   20.265    0.000    1.060    0.846\n  prevach =~                                                            \n    bytxrstd          1.000                               8.047    0.840\n    bytxmstd          1.010    0.033   30.379    0.000    8.126    0.844\n    bytxsstd          0.972    0.034   28.502    0.000    7.822    0.806\n    bytxhstd          0.930    0.034   27.195    0.000    7.487    0.812\n  hw =~                                                                 \n    hw12              1.000                               1.117    0.579\n    hw10              1.061    0.130    8.141    0.000    1.185    0.705\n  grades =~                                                             \n    eng92             1.000                               2.190    0.901\n    math92            0.870    0.028   30.702    0.000    1.905    0.776\n    sci92             0.987    0.026   38.382    0.000    2.162    0.876\n    soc92             1.038    0.026   39.719    0.000    2.273    0.894\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.289    0.022   13.189    0.000    0.536    0.536\n  grades ~                                                              \n    prevach           0.136    0.012   11.377    0.000    0.500    0.500\n    hw                0.524    0.110    4.740    0.000    0.267    0.267\n  hw ~                                                                  \n    prevach           0.048    0.009    5.391    0.000    0.342    0.342\n    famback           0.015    0.005    3.110    0.002    0.194    0.194\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng92             0.357    0.271    1.318    0.188    0.357    0.065\n .bytxmstd ~~                                                           \n   .math92            1.648    0.356    4.629    0.000    1.648    0.206\n .bytxsstd ~~                                                           \n   .sci92             0.422    0.337    1.253    0.210    0.422    0.062\n .bytxhstd ~~                                                           \n   .soc92             0.265    0.309    0.858    0.391    0.265    0.043\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          206.883   13.110   15.781    0.000  206.883    0.481\n   .byfaminc          2.888    0.235   12.300    0.000    2.888    0.491\n   .bypared           0.445    0.047    9.521    0.000    0.445    0.284\n   .bytxrstd         27.069    1.978   13.686    0.000   27.069    0.295\n   .bytxmstd         26.756    1.868   14.322    0.000   26.756    0.288\n   .bytxsstd         32.884    2.105   15.618    0.000   32.884    0.350\n   .bytxhstd         29.043    2.033   14.286    0.000   29.043    0.341\n   .hw12              2.468    0.191   12.905    0.000    2.468    0.664\n   .hw10              1.420    0.193    7.369    0.000    1.420    0.503\n   .eng92             1.111    0.091   12.241    0.000    1.111    0.188\n   .math92            2.393    0.129   18.542    0.000    2.393    0.397\n   .sci92             1.419    0.101   14.096    0.000    1.419    0.233\n   .soc92             1.295    0.106   12.236    0.000    1.295    0.200\n    famback         222.792   18.013   12.368    0.000    1.000    1.000\n   .prevach          46.180    3.283   14.066    0.000    0.713    0.713\n   .hw                0.965    0.156    6.178    0.000    0.774    0.774\n   .grades            2.683    0.196   13.714    0.000    0.559    0.559\n\n\n\n\n\n분포에 대한 가정이 없는 방법\nWLS (fully weighted least squares); estimator = \"WLS\"\nalso called ADF (Asymptotically Distribution-Free) estimator\n\nsem_fit &lt;- sem(hw_model, data = hw_mean, estimator = \"WLS\")  # listwise deletion\nsummary(sem_fit, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 203 iterations\n\n  Estimator                                        WLS\n  Optimization method                           NLMINB\n  Number of model parameters                        35\n\n                                                  Used       Total\n  Number of observations                           798        1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                92.368\n  Degrees of freedom                                56\n  P-value (Chi-square)                           0.002\n\nModel Test Baseline Model:\n\n  Test statistic                              1426.169\n  Degrees of freedom                                78\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.973\n  Tucker-Lewis Index (TLI)                       0.962\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.029\n  90 Percent confidence interval - lower         0.018\n  90 Percent confidence interval - upper         0.039\n  P-value H_0: RMSEA &lt;= 0.050                    1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.028\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              14.979    0.721\n    byfaminc          0.112    0.006   19.911    0.000    1.672    0.709\n    bypared           0.070    0.003   21.695    0.000    1.056    0.853\n  prevach =~                                                            \n    bytxrstd          1.000                               7.978    0.838\n    bytxmstd          1.024    0.032   31.887    0.000    8.171    0.854\n    bytxsstd          0.981    0.033   29.308    0.000    7.828    0.814\n    bytxhstd          0.937    0.031   29.802    0.000    7.477    0.812\n  hw =~                                                                 \n    hw12              1.000                               1.120    0.581\n    hw10              1.077    0.124    8.652    0.000    1.206    0.720\n  grades =~                                                             \n    eng92             1.000                               2.193    0.913\n    math92            0.881    0.028   31.987    0.000    1.931    0.786\n    sci92             0.984    0.025   40.086    0.000    2.158    0.879\n    soc92             1.029    0.025   41.273    0.000    2.257    0.896\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.304    0.021   14.656    0.000    0.570    0.570\n  grades ~                                                              \n    prevach           0.155    0.011   14.428    0.000    0.563    0.563\n    hw                0.400    0.092    4.359    0.000    0.204    0.204\n  hw ~                                                                  \n    prevach           0.052    0.009    5.705    0.000    0.370    0.370\n    famback           0.012    0.005    2.496    0.013    0.155    0.155\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng92             0.196    0.253    0.775    0.438    0.196    0.038\n .bytxmstd ~~                                                           \n   .math92            1.548    0.326    4.749    0.000    1.548    0.205\n .bytxsstd ~~                                                           \n   .sci92             0.105    0.302    0.347    0.729    0.105    0.016\n .bytxhstd ~~                                                           \n   .soc92             0.191    0.293    0.652    0.514    0.191    0.032\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          207.654   12.372   16.784    0.000  207.654    0.481\n   .byfaminc          2.765    0.215   12.858    0.000    2.765    0.497\n   .bypared           0.419    0.043    9.698    0.000    0.419    0.273\n   .bytxrstd         26.949    1.857   14.512    0.000   26.949    0.297\n   .bytxmstd         24.859    1.718   14.469    0.000   24.859    0.271\n   .bytxsstd         31.132    1.887   16.497    0.000   31.132    0.337\n   .bytxhstd         28.937    1.852   15.627    0.000   28.937    0.341\n   .hw12              2.461    0.188   13.088    0.000    2.461    0.662\n   .hw10              1.353    0.182    7.443    0.000    1.353    0.482\n   .eng92             0.964    0.085   11.394    0.000    0.964    0.167\n   .math92            2.305    0.126   18.318    0.000    2.305    0.382\n   .sci92             1.372    0.094   14.673    0.000    1.372    0.228\n   .soc92             1.247    0.100   12.469    0.000    1.247    0.197\n    famback         224.377   17.172   13.067    0.000    1.000    1.000\n   .prevach          42.929    3.117   13.771    0.000    0.675    0.675\n   .hw                0.971    0.151    6.411    0.000    0.774    0.774\n   .grades            2.576    0.178   14.508    0.000    0.536    0.536",
    "crumbs": [
      "Keith's",
      "Diagnostics"
    ]
  },
  {
    "objectID": "contents/chap22.html",
    "href": "contents/chap22.html",
    "title": "Chapter 22. Latent Interactions",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)",
    "crumbs": [
      "Keith's",
      "Latent Interactions"
    ]
  },
  {
    "objectID": "contents/chap22.html#latent-variable-interactions",
    "href": "contents/chap22.html#latent-variable-interactions",
    "title": "Chapter 22. Latent Interactions",
    "section": "Latent Variable Interactions",
    "text": "Latent Variable Interactions\n잠재변수의 상호작용 효과\n카테고리 변수와 상호작용한다면, 앞서 multi-group SEM을 이용\n두 연속 잠재변수간의 상호작용: 다양한 방식이 시도되며 여전히 연구 중; 요인의 값을 계산할 수 없어 상호작용항도 생성할 수 없음\n\nProduct Indicator Approach: 관찰 변수들의 곱으로 이루어진 새로운 잠재변수를 구성\n\n여러 방식 중에 double mean centering이 여러 이점을 갖춤\n\nDistribution Analytic (DA) Approaches\n\nThe Latent Moderated Structural equations (LMS): ML estimation maximizing the log-likelihood of the joint distribution of indicators.\nThe Quasi Maximum Likelihood approach (QML): LMS에 비해 계산이 간단하며, non-normal 데이터에 대해 robust.\nMarkov chain Monte Carlo (MCMC) estimation: 베이지안 접근방식으로 posterior distribution로부터 직접 샘플링하여 곱의 항을 계산\nMplus는 기본적으로 LMS로 추정, MCMC도 가능; XWITH 명령어로 간단히 구현 가능\nR에서는 modsem package의 methods 참고\n\n\n이전 예에서, 학생의 이전 성취 수준에 따라 과제의 효과가 달라질 수 있는가?\n즉, 성취도가 낮은 학생에게는 숙제에 더 많은 시간을 할애하는 것이 더 효과적일 수 있는가?\n\n\n# Load the data\ncols &lt;- c(\"bytxrstd\", \"bytxmstd\", \"bytxsstd\", \"bytxhstd\", \"parocc\", \"hw10\", \"hw12\", \"eng12\", \"math12\", \"sci12\", \"ss12\", \"female\", \"byfaminc\", \"bypared\", \"minority\")\n\nhw &lt;- read_delim(\"data/chap 22 latent var interactions and MLM/Latent Interactions/HW latent 8-12.dat\", delim = \" \", col_names = cols, na = \"-999\")\nhw &lt;- hw |&gt; na.omit()\n\n\nhw_model &lt;- '\n  famback =~ parocc + bypared + byfaminc\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw10 + hw12\n  grades =~ eng12 + math12 + sci12 + ss12\n  \n  bytxrstd ~~ eng12\n  bytxmstd ~~ math12\n  bytxsstd ~~ sci12\n  bytxhstd ~~ ss12\n  \n  prevach ~ famback\n  hw ~ prevach + famback\n  grades ~ prevach + hw + prevach:hw  # \":\" interaction\n'\nlibrary(modsem)\n# double mean centering\nhw_dblcent &lt;- modsem(hw_model, data = hw, method = \"dblcent\")  # default\nsummary(hw_dblcent) |&gt; print()\n\nmodsem (version 1.0.4, approach = dblcent):\nlavaan 0.6-19 ended normally after 385 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        69\n\n  Number of observations                           794\n\nModel Test User Model:\n                                                      \n  Test statistic                               256.142\n  Degrees of freedom                               162\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  famback =~                                          \n    parocc            1.000                           \n    bypared           0.071    0.004   18.999    0.000\n    byfaminc          0.116    0.007   17.752    0.000\n  prevach =~                                          \n    bytxrstd          1.000                           \n    bytxmstd          1.012    0.036   28.237    0.000\n    bytxsstd          0.973    0.037   26.473    0.000\n    bytxhstd          0.933    0.035   26.603    0.000\n  hw =~                                               \n    hw10              1.000                           \n    hw12              0.942    0.109    8.658    0.000\n  grades =~                                           \n    eng12             1.000                           \n    math12            0.873    0.031   28.616    0.000\n    sci12             0.988    0.028   35.818    0.000\n    ss12              1.041    0.028   37.304    0.000\n  prevachhw =~                                        \n    bytxrstdhw10      1.000                           \n    bytxmstdhw10      0.951    0.077   12.424    0.000\n    bytxsstdhw10      1.071    0.084   12.797    0.000\n    bytxhstdhw10      0.942    0.074   12.642    0.000\n    bytxrstdhw12      2.038    2.345    0.869    0.385\n    bytxmstdhw12      1.922    2.214    0.868    0.385\n    bytxsstdhw12      2.216    2.551    0.869    0.385\n    bytxhstdhw12      1.967    2.265    0.868    0.385\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  prevach ~                                           \n    famback           0.289    0.023   12.441    0.000\n  hw ~                                                \n    prevach           0.052    0.009    5.955    0.000\n    famback           0.015    0.005    3.227    0.001\n  grades ~                                            \n    prevach           0.136    0.011   12.154    0.000\n    hw                0.489    0.095    5.141    0.000\n    prevachhw         0.008    0.011    0.706    0.480\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .bytxrstd ~~                                         \n   .eng12             0.364    0.268    1.356    0.175\n .bytxmstd ~~                                         \n   .math12            1.632    0.350    4.668    0.000\n .bytxsstd ~~                                         \n   .sci12             0.470    0.305    1.542    0.123\n .bytxhstd ~~                                         \n   .ss12              0.269    0.287    0.936    0.349\n .bytxhstdhw10 ~~                                     \n   .bytxmstdhw12      0.000                           \n   .bytxrstdhw12      0.000                           \n   .bytxsstdhw12      0.000                           \n .bytxmstdhw10 ~~                                     \n   .bytxhstdhw12      0.000                           \n .bytxrstdhw10 ~~                                     \n   .bytxhstdhw12      0.000                           \n .bytxsstdhw10 ~~                                     \n   .bytxhstdhw12      0.000                           \n .bytxmstdhw10 ~~                                     \n   .bytxrstdhw12      0.000                           \n   .bytxsstdhw12      0.000                           \n .bytxrstdhw10 ~~                                     \n   .bytxmstdhw12      0.000                           \n .bytxsstdhw10 ~~                                     \n   .bytxmstdhw12      0.000                           \n .bytxrstdhw10 ~~                                     \n   .bytxsstdhw12      0.000                           \n .bytxsstdhw10 ~~                                     \n   .bytxrstdhw12      0.000                           \n .bytxhstdhw10 ~~                                     \n   .bytxhstdhw12     34.892    4.394    7.941    0.000\n .bytxmstdhw10 ~~                                     \n   .bytxhstdhw10    130.191   48.822    2.667    0.008\n .bytxrstdhw10 ~~                                     \n   .bytxhstdhw10    133.182   51.278    2.597    0.009\n .bytxsstdhw10 ~~                                     \n   .bytxhstdhw10    140.596   54.854    2.563    0.010\n .bytxmstdhw12 ~~                                     \n   .bytxhstdhw12     47.378  203.348    0.233    0.816\n .bytxrstdhw12 ~~                                     \n   .bytxhstdhw12     40.099  215.462    0.186    0.852\n .bytxsstdhw12 ~~                                     \n   .bytxhstdhw12     33.141  234.203    0.142    0.887\n .bytxmstdhw10 ~~                                     \n   .bytxmstdhw12     44.296    4.800    9.229    0.000\n .bytxrstdhw10 ~~                                     \n   .bytxmstdhw10    138.200   51.847    2.666    0.008\n .bytxmstdhw10 ~~                                     \n   .bytxsstdhw10    142.652   55.446    2.573    0.010\n .bytxrstdhw12 ~~                                     \n   .bytxmstdhw12     56.033  210.678    0.266    0.790\n .bytxmstdhw12 ~~                                     \n   .bytxsstdhw12     49.128  228.992    0.215    0.830\n .bytxrstdhw10 ~~                                     \n   .bytxrstdhw12     32.388    4.748    6.821    0.000\n   .bytxsstdhw10    144.756   58.237    2.486    0.013\n .bytxrstdhw12 ~~                                     \n   .bytxsstdhw12     34.132  242.613    0.141    0.888\n .bytxsstdhw10 ~~                                     \n   .bytxsstdhw12     37.987    5.304    7.162    0.000\n  famback ~~                                          \n    prevachhw         5.625    7.572    0.743    0.458\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .parocc          205.248   13.657   15.029    0.000\n   .bypared           0.447    0.048    9.241    0.000\n   .byfaminc          2.887    0.189   15.242    0.000\n   .bytxrstd         27.188    1.847   14.724    0.000\n   .bytxmstd         26.775    1.843   14.525    0.000\n   .bytxsstd         32.415    2.049   15.822    0.000\n   .bytxhstd         29.092    1.852   15.710    0.000\n   .hw10              1.425    0.170    8.360    0.000\n   .hw12              2.471    0.185   13.334    0.000\n   .eng12             1.103    0.083   13.218    0.000\n   .math12            2.366    0.134   17.629    0.000\n   .sci12             1.409    0.095   14.834    0.000\n   .ss12              1.300    0.095   13.746    0.000\n   .bytxrstdhw10    221.253   54.852    4.034    0.000\n   .bytxmstdhw10    218.645   49.808    4.390    0.000\n   .bytxsstdhw10    233.675   62.691    3.727    0.000\n   .bytxhstdhw10    198.329   48.656    4.076    0.000\n   .bytxrstdhw12    148.114  223.711    0.662    0.508\n   .bytxmstdhw12    171.117  199.381    0.858    0.391\n   .bytxsstdhw12    147.265  264.304    0.557    0.577\n   .bytxhstdhw12    136.239  208.458    0.654    0.513\n    famback         223.156   20.894   10.680    0.000\n   .prevach          45.846    3.501   13.094    0.000\n   .hw                1.086    0.170    6.389    0.000\n   .grades            2.666    0.188   14.199    0.000\n    prevachhw        46.766   54.276    0.862    0.389\n\n\n\nStandardized estimates:\n\nstandardized_estimates(hw_dblcent) |&gt; print()\n\n            lhs op          rhs est.std    se       z pvalue ci.lower ci.upper\n1       famback =~       parocc   0.722 0.022  32.299  0.000    0.678    0.766\n2       famback =~      bypared   0.846 0.019  44.594  0.000    0.809    0.883\n3       famback =~     byfaminc   0.715 0.023  31.687  0.000    0.671    0.759\n4       prevach =~     bytxrstd   0.839 0.013  63.164  0.000    0.813    0.865\n5       prevach =~     bytxmstd   0.843 0.013  64.948  0.000    0.818    0.869\n6       prevach =~     bytxsstd   0.808 0.015  54.557  0.000    0.779    0.837\n7       prevach =~     bytxhstd   0.811 0.015  55.336  0.000    0.783    0.840\n8            hw =~         hw10   0.705 0.043  16.552  0.000    0.622    0.789\n9            hw =~         hw12   0.580 0.040  14.617  0.000    0.502    0.657\n10       grades =~        eng12   0.901 0.009 101.552  0.000    0.884    0.919\n11       grades =~       math12   0.779 0.015  50.495  0.000    0.749    0.809\n12       grades =~        sci12   0.876 0.010  86.301  0.000    0.856    0.896\n13       grades =~         ss12   0.894 0.009  96.737  0.000    0.876    0.912\n14     bytxrstd ~~        eng12   0.066 0.048   1.372  0.170   -0.028    0.161\n15     bytxmstd ~~       math12   0.205 0.041   4.948  0.000    0.124    0.286\n16     bytxsstd ~~        sci12   0.070 0.045   1.559  0.119   -0.018    0.157\n17     bytxhstd ~~         ss12   0.044 0.046   0.942  0.346   -0.047    0.135\n18      prevach  ~      famback   0.537 0.031  17.146  0.000    0.476    0.599\n19           hw  ~      prevach   0.349 0.055   6.294  0.000    0.240    0.458\n20           hw  ~      famback   0.191 0.058   3.303  0.001    0.078    0.304\n21       grades  ~      prevach   0.500 0.036  13.859  0.000    0.429    0.571\n22       grades  ~           hw   0.266 0.045   5.902  0.000    0.178    0.354\n23       grades  ~    prevachhw   0.024 0.036   0.669  0.504   -0.046    0.094\n24    prevachhw =~ bytxrstdhw10   0.418 0.242   1.728  0.084   -0.056    0.892\n25    prevachhw =~ bytxmstdhw10   0.403 0.233   1.726  0.084   -0.055    0.860\n26    prevachhw =~ bytxsstdhw10   0.432 0.250   1.729  0.084   -0.058    0.922\n27    prevachhw =~ bytxhstdhw10   0.416 0.241   1.728  0.084   -0.056    0.888\n28    prevachhw =~ bytxrstdhw12   0.753 0.434   1.736  0.083   -0.097    1.604\n29    prevachhw =~ bytxmstdhw12   0.709 0.409   1.733  0.083   -0.093    1.510\n30    prevachhw =~ bytxsstdhw12   0.781 0.449   1.738  0.082   -0.100    1.661\n31    prevachhw =~ bytxhstdhw12   0.755 0.435   1.736  0.083   -0.097    1.608\n32 bytxhstdhw10 ~~ bytxmstdhw12   0.000 0.000      NA     NA    0.000    0.000\n33 bytxhstdhw10 ~~ bytxrstdhw12   0.000 0.000      NA     NA    0.000    0.000\n34 bytxhstdhw10 ~~ bytxsstdhw12   0.000 0.000      NA     NA    0.000    0.000\n35 bytxmstdhw10 ~~ bytxhstdhw12   0.000 0.000      NA     NA    0.000    0.000\n36 bytxrstdhw10 ~~ bytxhstdhw12   0.000 0.000      NA     NA    0.000    0.000\n37 bytxsstdhw10 ~~ bytxhstdhw12   0.000 0.000      NA     NA    0.000    0.000\n38 bytxmstdhw10 ~~ bytxrstdhw12   0.000 0.000      NA     NA    0.000    0.000\n39 bytxmstdhw10 ~~ bytxsstdhw12   0.000 0.000      NA     NA    0.000    0.000\n40 bytxrstdhw10 ~~ bytxmstdhw12   0.000 0.000      NA     NA    0.000    0.000\n41 bytxsstdhw10 ~~ bytxmstdhw12   0.000 0.000      NA     NA    0.000    0.000\n42 bytxrstdhw10 ~~ bytxsstdhw12   0.000 0.000      NA     NA    0.000    0.000\n43 bytxsstdhw10 ~~ bytxrstdhw12   0.000 0.000      NA     NA    0.000    0.000\n44 bytxhstdhw10 ~~ bytxhstdhw12   0.212 0.139   1.532  0.125   -0.059    0.484\n45 bytxmstdhw10 ~~ bytxhstdhw10   0.625 0.089   7.037  0.000    0.451    0.799\n46 bytxrstdhw10 ~~ bytxhstdhw10   0.636 0.090   7.048  0.000    0.459    0.813\n47 bytxsstdhw10 ~~ bytxhstdhw10   0.653 0.089   7.322  0.000    0.478    0.828\n48 bytxmstdhw12 ~~ bytxhstdhw12   0.310 0.915   0.339  0.734   -1.483    2.103\n49 bytxrstdhw12 ~~ bytxhstdhw12   0.282 1.089   0.259  0.795   -1.852    2.416\n50 bytxsstdhw12 ~~ bytxhstdhw12   0.234 1.265   0.185  0.853   -2.246    2.714\n51 bytxmstdhw10 ~~ bytxmstdhw12   0.229 0.110   2.086  0.037    0.014    0.444\n52 bytxrstdhw10 ~~ bytxmstdhw10   0.628 0.089   7.095  0.000    0.455    0.802\n53 bytxmstdhw10 ~~ bytxsstdhw10   0.631 0.091   6.936  0.000    0.453    0.809\n54 bytxrstdhw12 ~~ bytxmstdhw12   0.352 0.854   0.412  0.680   -1.321    2.025\n55 bytxmstdhw12 ~~ bytxsstdhw12   0.309 0.986   0.314  0.754   -1.623    2.241\n56 bytxrstdhw10 ~~ bytxrstdhw12   0.179 0.115   1.549  0.121   -0.047    0.405\n57 bytxrstdhw10 ~~ bytxsstdhw10   0.637 0.094   6.777  0.000    0.452    0.821\n58 bytxrstdhw12 ~~ bytxsstdhw12   0.231 1.262   0.183  0.855   -2.242    2.704\n59 bytxsstdhw10 ~~ bytxsstdhw12   0.205 0.158   1.294  0.196   -0.105    0.515\n60       parocc ~~       parocc   0.479 0.032  14.853  0.000    0.416    0.542\n61      bypared ~~      bypared   0.284 0.032   8.839  0.000    0.221    0.347\n62     byfaminc ~~     byfaminc   0.489 0.032  15.142  0.000    0.425    0.552\n63     bytxrstd ~~     bytxrstd   0.297 0.022  13.326  0.000    0.253    0.340\n64     bytxmstd ~~     bytxmstd   0.289 0.022  13.171  0.000    0.246    0.331\n65     bytxsstd ~~     bytxsstd   0.347 0.024  14.494  0.000    0.300    0.394\n66     bytxhstd ~~     bytxhstd   0.342 0.024  14.365  0.000    0.295    0.388\n67         hw10 ~~         hw10   0.503 0.060   8.363  0.000    0.385    0.620\n68         hw12 ~~         hw12   0.664 0.046  14.435  0.000    0.574    0.754\n69        eng12 ~~        eng12   0.187 0.016  11.710  0.000    0.156    0.219\n70       math12 ~~       math12   0.393 0.024  16.380  0.000    0.346    0.441\n71        sci12 ~~        sci12   0.232 0.018  13.031  0.000    0.197    0.267\n72         ss12 ~~         ss12   0.200 0.017  12.129  0.000    0.168    0.233\n73 bytxrstdhw10 ~~ bytxrstdhw10   0.826 0.202   4.087  0.000    0.430    1.221\n74 bytxmstdhw10 ~~ bytxmstdhw10   0.838 0.188   4.463  0.000    0.470    1.206\n75 bytxsstdhw10 ~~ bytxsstdhw10   0.813 0.216   3.766  0.000    0.390    1.237\n76 bytxhstdhw10 ~~ bytxhstdhw10   0.827 0.200   4.131  0.000    0.435    1.219\n77 bytxrstdhw12 ~~ bytxrstdhw12   0.433 0.654   0.662  0.508   -0.848    1.714\n78 bytxmstdhw12 ~~ bytxmstdhw12   0.498 0.580   0.858  0.391   -0.639    1.634\n79 bytxsstdhw12 ~~ bytxsstdhw12   0.391 0.701   0.557  0.577   -0.984    1.765\n80 bytxhstdhw12 ~~ bytxhstdhw12   0.429 0.657   0.653  0.513   -0.859    1.718\n81      famback ~~      famback   1.000 0.000      NA     NA    1.000    1.000\n82      prevach ~~      prevach   0.712 0.034  21.144  0.000    0.646    0.777\n83           hw ~~           hw   0.770 0.040  19.143  0.000    0.691    0.849\n84       grades ~~       grades   0.558 0.032  17.317  0.000    0.495    0.621\n85    prevachhw ~~    prevachhw   1.000 0.000      NA     NA    1.000    1.000\n86      famback ~~    prevachhw   0.055 0.052   1.052  0.293   -0.048    0.158\n\n\nThe Quasi Maximum Likelihood approach (QML)\n내생변수 간의 상호작용의 경우, 우회해서 적용: modsem 글 참고\n\nhw_linear &lt;- '\n  prevach ~ famback\n  hw ~ prevach + famback\n'\nhw_nonlinear &lt;- '\n  famback =~ parocc + bypared + byfaminc\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw10 + hw12\n  grades =~ eng12 + math12 + sci12 + ss12\n  \n  bytxrstd ~~ eng12\n  bytxmstd ~~ math12\n  bytxsstd ~~ sci12\n  bytxhstd ~~ ss12\n  \n  grades ~ prevach + hw + prevach:hw\n'\nest_qml &lt;- modsem(hw_nonlinear, data = hw, cov.syntax = hw_linear, method = \"qml\")\nsummary(est_qml)\n\n\nmodsem (version 1.0.4):\n  Estimator                                         QML\n  Optimization method                            NLMINB\n  Number of observations                            794\n  Number of iterations                               63\n  Loglikelihood                               -25991.01\n  Akaike (AIC)                                 52072.01\n  Bayesian (BIC)                               52282.48\n \nFit Measures for H0:\n  Loglikelihood                                  -25991\n  Akaike (AIC)                                 52070.02\n  Bayesian (BIC)                               52275.81\n  Chi-square                                     123.95\n  Degrees of Freedom (Chi-square)                    60\n  P-value (Chi-square)                            0.000\n  RMSEA                                           0.011\n \nComparative fit to H0 (no interaction effect)\n  Loglikelihood change                             0.00\n  Difference test (D)                              0.01\n  Degrees of freedom (D)                              1\n  P-value (D)                                     0.934\n \nR-Squared:\n  grades                                          0.450\n  prevach                                         0.287\n  hw                                              0.229\nR-Squared Null-Model (H0):\n  grades                                          0.450\n  prevach                                         0.288\n  hw                                              0.229\nR-Squared Change:\n  grades                                          0.000\n  prevach                                         0.000\n  hw                                              0.000\n\nParameter Estimates:\n  Coefficients                           unstandardized\n  Information                                  observed\n  Standard errors                              standard\n \nLatent Variables:\n                  Estimate  Std.Error  z.value  P(&gt;|z|)\n  prevach =~ \n    bytxrstd         1.000                             \n    bytxmstd         1.015      0.036    27.84    0.000\n    bytxsstd         0.975      0.037    26.36    0.000\n    bytxhstd         0.934      0.035    26.73    0.000\n  famback =~ \n    parocc           1.000                             \n    bypared          0.071      0.004    19.13    0.000\n    byfaminc         0.116      0.007    17.78    0.000\n  hw =~ \n    hw10             1.000                             \n    hw12             0.943      0.107     8.79    0.000\n  grades =~ \n    eng12            1.000                             \n    math12           0.883      0.031    28.66    0.000\n    sci12            0.985      0.028    35.53    0.000\n    ss12             1.040      0.028    37.64    0.000\n\nRegressions:\n                  Estimate  Std.Error  z.value  P(&gt;|z|)\n  grades ~ \n    prevach          0.139      0.011    12.21    0.000\n    hw               0.491      0.100     4.93    0.000\n    prevach:hw      -0.001      0.007    -0.12    0.905\n  prevach ~ \n    famback          0.288      0.023    12.37    0.000\n  hw ~ \n    famback          0.015      0.005     3.19    0.001\n    prevach          0.052      0.009     5.90    0.000\n\nIntercepts:\n                  Estimate  Std.Error  z.value  P(&gt;|z|)\n    bytxrstd        52.482      0.340   154.56    0.000\n    bytxmstd        52.628      0.342   153.75    0.000\n    bytxsstd        52.269      0.343   152.27    0.000\n    bytxhstd        52.495      0.328   160.21    0.000\n    parocc          53.212      0.735    72.44    0.000\n    bypared          3.250      0.045    72.99    0.000\n    byfaminc        10.127      0.086   117.44    0.000\n    hw10             2.587      0.060    43.30    0.000\n    hw12             3.395      0.068    49.59    0.000\n    eng12            6.535      0.092    71.35    0.000\n    math12           5.865      0.092    63.95    0.000\n    sci12            6.188      0.092    66.91    0.000\n    ss12             6.689      0.096    69.76    0.000\n    grades           0.000                             \n    prevach          0.000                             \n    famback          0.000                             \n    hw               0.000                             \n\nVariances:\n                  Estimate  Std.Error  z.value  P(&gt;|z|)\n    bytxrstd        27.189      1.844    14.74    0.000\n    bytxmstd        26.770      1.863    14.37    0.000\n    bytxsstd        32.415      2.045    15.85    0.000\n    bytxhstd        29.093      1.845    15.77    0.000\n    parocc         205.247     13.630    15.06    0.000\n    bypared          0.448      0.049     9.21    0.000\n    byfaminc         2.889      0.191    15.11    0.000\n    hw10             1.425      0.169     8.42    0.000\n    hw12             2.470      0.184    13.45    0.000\n    eng12            1.111      0.083    13.40    0.000\n    math12           2.352      0.135    17.48    0.000\n    sci12            1.406      0.095    14.77    0.000\n    ss12             1.300      0.094    13.90    0.000\n    grades           2.638      0.190    13.87    0.000\n    famback        223.156     20.808    10.72    0.000\n    prevach         45.844      3.510    13.06    0.000\n    hw               1.087      0.169     6.42    0.000",
    "crumbs": [
      "Keith's",
      "Latent Interactions"
    ]
  },
  {
    "objectID": "contents/chap22.html#testing-curvilinear-effects",
    "href": "contents/chap22.html#testing-curvilinear-effects",
    "title": "Chapter 22. Latent Interactions",
    "section": "Testing Curvilinear Effects",
    "text": "Testing Curvilinear Effects\n2차항을 추가하여 곡선적 효과를 검증; 즉, X와 X의 상호작용으로 이루어진 새로운 변수를 추가\n\nhw_model_sq &lt;- '\n  famback =~ parocc + bypared + byfaminc\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw10 + hw12\n  grades =~ eng12 + math12 + sci12 + ss12\n  \n  bytxrstd ~~ eng12\n  bytxmstd ~~ math12\n  bytxsstd ~~ sci12\n  bytxhstd ~~ ss12\n  \n  prevach ~ famback\n  hw ~ prevach + famback\n  grades ~ prevach + hw + hw:hw\n'\nhw_dblcent_sq &lt;- modsem(hw_model_sq, data = hw, method = \"dblcent\")  # default\nsummary(hw_dblcent_sq) |&gt; print()\n\nmodsem (version 1.0.4, approach = dblcent):\nlavaan 0.6-19 ended normally after 249 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        45\n\n  Number of observations                           794\n\nModel Test User Model:\n                                                      \n  Test statistic                               594.603\n  Degrees of freedom                                91\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  famback =~                                          \n    parocc            1.000                           \n    bypared           0.070    0.004   19.016    0.000\n    byfaminc          0.116    0.007   17.757    0.000\n  prevach =~                                          \n    bytxrstd          1.000                           \n    bytxmstd          1.011    0.036   28.223    0.000\n    bytxsstd          0.973    0.037   26.480    0.000\n    bytxhstd          0.933    0.035   26.628    0.000\n  hw =~                                               \n    hw10              1.000                           \n    hw12              0.897    0.094    9.533    0.000\n  grades =~                                           \n    eng12             1.000                           \n    math12            0.872    0.030   29.360    0.000\n    sci12             0.987    0.027   36.817    0.000\n    ss12              1.042    0.027   38.483    0.000\n  hwhw =~                                             \n    hw10hw10          1.000                           \n    hw12hw10          0.320    0.133    2.413    0.016\n    hw12hw12          0.559    0.213    2.620    0.009\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  prevach ~                                           \n    famback           0.291    0.023   12.521    0.000\n  hw ~                                                \n    prevach           0.049    0.009    5.666    0.000\n    famback           0.018    0.005    3.886    0.000\n  grades ~                                            \n    prevach           0.132    0.012   11.402    0.000\n    hw                0.691    0.102    6.747    0.000\n    hwhw             -0.178    0.071   -2.515    0.012\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .bytxrstd ~~                                         \n   .eng12             0.368    0.268    1.371    0.170\n .bytxmstd ~~                                         \n   .math12            1.622    0.350    4.632    0.000\n .bytxsstd ~~                                         \n   .sci12             0.466    0.305    1.527    0.127\n .bytxhstd ~~                                         \n   .ss12              0.280    0.287    0.975    0.329\n .hw10hw10 ~~                                         \n   .hw12hw12          0.000                           \n   .hw12hw10          5.255    1.064    4.941    0.000\n .hw12hw10 ~~                                         \n   .hw12hw12          6.124    0.730    8.383    0.000\n  famback ~~                                          \n    hwhw              8.352    2.347    3.559    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .parocc          204.369   13.635   14.988    0.000\n   .bypared           0.464    0.048    9.650    0.000\n   .byfaminc          2.873    0.189   15.195    0.000\n   .bytxrstd         27.158    1.846   14.713    0.000\n   .bytxmstd         26.850    1.846   14.547    0.000\n   .bytxsstd         32.414    2.049   15.820    0.000\n   .bytxhstd         29.043    1.850   15.698    0.000\n   .hw10              1.363    0.159    8.573    0.000\n   .hw12              2.540    0.173   14.713    0.000\n   .eng12             1.101    0.083   13.259    0.000\n   .math12            2.372    0.134   17.656    0.000\n   .sci12             1.414    0.095   14.899    0.000\n   .ss12              1.291    0.094   13.744    0.000\n   .hw10hw10         10.622    2.641    4.023    0.000\n   .hw12hw10         11.705    0.777   15.059    0.000\n   .hw12hw12         22.084    1.385   15.940    0.000\n    famback         224.030   20.923   10.707    0.000\n   .prevach          45.550    3.487   13.062    0.000\n   .hw                1.121    0.164    6.820    0.000\n   .grades            2.370    0.206   11.509    0.000\n    hwhw              6.756    2.677    2.524    0.012\n\n\n\nThe Quasi Maximum Likelihood approach (QML)\n\nhw_linear &lt;- '\n  prevach ~ famback\n  hw ~ prevach + famback\n'\nhw_nonlinear_sq &lt;- '\n  famback =~ parocc + bypared + byfaminc\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw10 + hw12\n  grades =~ eng12 + math12 + sci12 + ss12\n  \n  bytxrstd ~~ eng12\n  bytxmstd ~~ math12\n  bytxsstd ~~ sci12\n  bytxhstd ~~ ss12\n  \n  grades ~ prevach + hw + hw:hw\n'\nest_qml_sq &lt;- modsem(hw_nonlinear_sq, data = hw, cov.syntax = hw_linear, method = \"qml\")\nsummary(est_qml_sq)\n\n\nmodsem (version 1.0.4):\n  Estimator                                         QML\n  Optimization method                            NLMINB\n  Number of observations                            794\n  Number of iterations                              117\n  Loglikelihood                               -25987.85\n  Akaike (AIC)                                 52065.69\n  Bayesian (BIC)                               52276.16\n \nFit Measures for H0:\n  Loglikelihood                                  -25991\n  Akaike (AIC)                                 52070.02\n  Bayesian (BIC)                               52275.81\n  Chi-square                                     123.95\n  Degrees of Freedom (Chi-square)                    60\n  P-value (Chi-square)                            0.000\n  RMSEA                                           0.011\n \nComparative fit to H0 (no interaction effect)\n  Loglikelihood change                             3.16\n  Difference test (D)                              6.32\n  Degrees of freedom (D)                              1\n  P-value (D)                                     0.012\n \nR-Squared:\n  grades                                          0.471\n  prevach                                         0.290\n  hw                                              0.214\nR-Squared Null-Model (H0):\n  grades                                          0.450\n  prevach                                         0.288\n  hw                                              0.229\nR-Squared Change:\n  grades                                          0.021\n  prevach                                         0.002\n  hw                                             -0.015\n\nParameter Estimates:\n  Coefficients                           unstandardized\n  Information                                  observed\n  Standard errors                              standard\n \nLatent Variables:\n                  Estimate  Std.Error  z.value  P(&gt;|z|)\n  hw =~ \n    hw10             1.000                             \n    hw12             0.887      0.111     8.00    0.000\n  famback =~ \n    parocc           1.000                             \n    bypared          0.071      0.004    19.14    0.000\n    byfaminc         0.116      0.007    17.78    0.000\n  prevach =~ \n    bytxrstd         1.000                             \n    bytxmstd         1.015      0.037    27.80    0.000\n    bytxsstd         0.977      0.037    26.34    0.000\n    bytxhstd         0.937      0.035    26.72    0.000\n  grades =~ \n    eng12            1.000                             \n    math12           0.883      0.031    28.64    0.000\n    sci12            0.986      0.028    35.50    0.000\n    ss12             1.041      0.028    37.65    0.000\n\nRegressions:\n                  Estimate  Std.Error  z.value  P(&gt;|z|)\n  grades ~ \n    hw               0.549      0.096     5.74    0.000\n    prevach          0.138      0.011    12.45    0.000\n    hw:hw           -0.127      0.050    -2.54    0.011\n  prevach ~ \n    famback          0.288      0.023    12.39    0.000\n  hw ~ \n    famback          0.015      0.005     3.22    0.001\n    prevach          0.051      0.009     5.80    0.000\n\nIntercepts:\n                  Estimate  Std.Error  z.value  P(&gt;|z|)\n    hw10             2.587      0.060    43.29    0.000\n    hw12             3.395      0.068    49.58    0.000\n    parocc          53.208      0.735    72.43    0.000\n    bypared          3.251      0.045    72.95    0.000\n    byfaminc        10.127      0.086   117.39    0.000\n    bytxrstd        52.479      0.339   154.73    0.000\n    bytxmstd        52.624      0.342   153.82    0.000\n    bytxsstd        52.265      0.343   152.24    0.000\n    bytxhstd        52.491      0.328   160.12    0.000\n    eng12            6.721      0.116    58.17    0.000\n    math12           6.029      0.111    54.37    0.000\n    sci12            6.371      0.116    55.08    0.000\n    ss12             6.882      0.121    56.98    0.000\n    grades           0.000                             \n    hw               0.000                             \n    famback          0.000                             \n    prevach          0.000                             \n\nVariances:\n                  Estimate  Std.Error  z.value  P(&gt;|z|)\n    hw10             1.320      0.193     6.84    0.000\n    hw12             2.531      0.190    13.36    0.000\n    parocc         204.376     13.561    15.07    0.000\n    bypared          0.450      0.049     9.26    0.000\n    byfaminc         2.889      0.191    15.12    0.000\n    bytxrstd        27.190      1.841    14.77    0.000\n    bytxmstd        26.809      1.863    14.39    0.000\n    bytxsstd        32.411      2.046    15.84    0.000\n    bytxhstd        29.059      1.846    15.74    0.000\n    eng12            1.115      0.083    13.43    0.000\n    math12           2.356      0.134    17.52    0.000\n    sci12            1.406      0.095    14.79    0.000\n    ss12             1.292      0.093    13.86    0.000\n    grades           2.519      0.191    13.16    0.000\n    famback        224.038     20.905    10.72    0.000\n    prevach         45.557      3.482    13.08    0.000\n    hw               1.191      0.196     6.06    0.000",
    "crumbs": [
      "Keith's",
      "Latent Interactions"
    ]
  },
  {
    "objectID": "contents/chap19.html",
    "href": "contents/chap19.html",
    "title": "Chapter 19. Latent Means in SEM",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)",
    "crumbs": [
      "Keith's",
      "Latent Means"
    ]
  },
  {
    "objectID": "contents/chap19.html#multigroup-analysis",
    "href": "contents/chap19.html#multigroup-analysis",
    "title": "Chapter 19. Latent Means in SEM",
    "section": "Multigroup Analysis",
    "text": "Multigroup Analysis\nMG-MACS (Multiple Group Mean and Covariance Structure Analysis) Approach\n\nInitial Model\n측정 모형에 대해서:\n\n요인부하량이 두 집단이 동일하다고 가정함: 잠재변수가 동일한 metric을 가진 것으로 제약 (집단 간 요인구조에 대한 weak measurement invariance 가정)\n요인에 대해서 intercept가 두 집단이 동일하다고 가정함: 잠재변수의 원점(0)이 동일하다고 제약 (strong measurement invariance 가정)\n\n구조 모형에 대해서:\n\nPretest 잠재변수에 대해 두 집단의 평균이 모두 0으로 동일하다고 가정; 즉 무선할당이 적절히 이루어졌다고 가정함; test 가능\nPosttest 잠재변수에 대해서\n\n통제집단에서 intercept:0 고정, 치료집단에서 intercept는 추정함; default\n이를 통해 |치료집단 - 통제집단| 차이가 intercept에서 나타남\n\n\n\n\n# initial model\nhotflash_model_mg &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\n  hf_pre ~ 0*1  # default로 group2에서 intercept를 0으로 고정하지 않음!\n  \n  ### Defaults\n  # hf_post ~ c(a1, a2)*1  # 첫번째 집단의 intercept:0, 두번째 집단 intercept는 추정\n  # a1 == 0\n\"\nhotflash_fit_mg &lt;- sem(hotflash_model_mg,\n  data = hotflash,\n  meanstructure = TRUE, # mean structure\n  group = \"g\", # factor type\n  group.equal = c(\"loadings\", \"intercepts\")\n)\nsummary(hotflash_fit_mg, \n  standardized = TRUE, \n  fit.measures = TRUE,\n  remove.unused = FALSE, # keep the unused parameters\n) |&gt; print()\n\nlavaan 0.6-19 ended normally after 326 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        31\n  Number of equality constraints                     6\n\n  Number of observations per group:                   \n    control                                         48\n    treatment                                       48\n\nModel Test User Model:\n                                                      \n  Test statistic                                 5.399\n  Degrees of freedom                                 3\n  P-value (Chi-square)                           0.145\n  Test statistic for each group:\n    control                                      2.169\n    treatment                                    3.229\n\nModel Test Baseline Model:\n\n  Test statistic                               182.941\n  Degrees of freedom                                12\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.986\n  Tucker-Lewis Index (TLI)                       0.944\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1425.733\n  Loglikelihood unrestricted model (H1)      -1423.033\n                                                      \n  Akaike (AIC)                                2901.465\n  Bayesian (BIC)                              2965.574\n  Sample-size adjusted Bayesian (SABIC)       2886.638\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.129\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.302\n  P-value H_0: RMSEA &lt;= 0.050                    0.185\n  P-value H_0: RMSEA &gt;= 0.080                    0.751\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.088\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [control]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              16.945    0.792\n    HF1     (.p2.)    0.388    0.115    3.388    0.001    6.573    0.607\n  hf_post =~                                                            \n    int2              1.000                              21.010    0.959\n    HF2     (.p4.)    0.312    0.037    8.433    0.000    6.559    0.587\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre            1.139    0.274    4.156    0.000    0.919    0.919\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              67.058   18.003    3.725    0.000   67.058    0.862\n .int1 ~~                                                               \n   .int2            -28.916   98.630   -0.293    0.769  -28.916   -0.355\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.15.)   42.049    2.027   20.742    0.000   42.049    1.966\n   .HF1     (.16.)   15.473    1.089   14.206    0.000   15.473    1.429\n   .int2    (.17.)   39.131    2.690   14.548    0.000   39.131    1.785\n   .HF2     (.18.)   14.044    1.248   11.256    0.000   14.044    1.258\n   .hf_post           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            170.287  124.805    1.364    0.172  170.287    0.372\n   .HF1              74.107   19.536    3.793    0.000   74.107    0.632\n   .int2             39.030  107.473    0.363    0.716   39.030    0.081\n   .HF2              81.674   19.666    4.153    0.000   81.674    0.655\n    hf_pre          287.136  148.917    1.928    0.054    1.000    1.000\n   .hf_post          68.950   49.956    1.380    0.168    0.156    0.156\n\n\nGroup 2 [treatment]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              10.689    0.567\n    HF1     (.p2.)    0.388    0.115    3.388    0.001    4.146    0.384\n  hf_post =~                                                            \n    int2              1.000                               5.158    0.496\n    HF2     (.p4.)    0.312    0.037    8.433    0.000    1.610    0.308\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre           -0.456    0.459   -0.993    0.321   -0.944   -0.944\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              47.191   10.578    4.461    0.000   47.191    0.950\n .int1 ~~                                                               \n   .int2             44.760   37.540    1.192    0.233   44.760    0.319\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.15.)   42.049    2.027   20.742    0.000   42.049    2.230\n   .HF1     (.16.)   15.473    1.089   14.206    0.000   15.473    1.432\n   .int2    (.17.)   39.131    2.690   14.548    0.000   39.131    3.764\n   .HF2     (.18.)   14.044    1.248   11.256    0.000   14.044    2.684\n   .hf_post         -28.439    3.135   -9.071    0.000   -5.513   -5.513\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            241.229   74.588    3.234    0.001  241.229    0.679\n   .HF1              99.551   23.596    4.219    0.000   99.551    0.853\n   .int2             81.492   25.448    3.202    0.001   81.492    0.754\n   .HF2              24.780    5.610    4.417    0.000   24.780    0.905\n    hf_pre          114.255   70.087    1.630    0.103    1.000    1.000\n   .hf_post           2.901   52.283    0.055    0.956    0.109    0.109\n\n\n\n\nparameterEstimates(hotflash_fit_mg) |&gt; subset(op == \"~1\") |&gt; print()\n\n       lhs op rhs block group label     est    se      z pvalue ci.lower ci.upper\n8   hf_pre ~1         1     1         0.000 0.000     NA     NA    0.000    0.000\n15    int1 ~1         1     1 .p15.  42.049 2.027 20.742      0   38.075   46.022\n16     HF1 ~1         1     1 .p16.  15.473 1.089 14.206      0   13.339   17.608\n17    int2 ~1         1     1 .p17.  39.131 2.690 14.548      0   33.859   44.403\n18     HF2 ~1         1     1 .p18.  14.044 1.248 11.256      0   11.598   16.489\n19 hf_post ~1         1     1         0.000 0.000     NA     NA    0.000    0.000\n27  hf_pre ~1         2     2         0.000 0.000     NA     NA    0.000    0.000\n34    int1 ~1         2     2 .p15.  42.049 2.027 20.742      0   38.075   46.022\n35     HF1 ~1         2     2 .p16.  15.473 1.089 14.206      0   13.339   17.608\n36    int2 ~1         2     2 .p17.  39.131 2.690 14.548      0   33.859   44.403\n37     HF2 ~1         2     2 .p18.  14.044 1.248 11.256      0   11.598   16.489\n38 hf_post ~1         2     2       -28.439 3.135 -9.071      0  -34.584  -22.295\n\n\n\n# Free parameters 확인\nparTable(hotflash_fit_mg) |&gt; print()\n\n   id     lhs op     rhs user block group free ustart exo label plabel   start     est\n1   1  hf_pre =~    int1    1     1     1    0      1   0         .p1.   1.000   1.000\n2   2  hf_pre =~     HF1    1     1     1    1     NA   0  .p2.   .p2.   0.388   0.388\n3   3 hf_post =~    int2    1     1     1    0      1   0         .p3.   1.000   1.000\n4   4 hf_post =~     HF2    1     1     1    2     NA   0  .p4.   .p4.   0.448   0.312\n5   5 hf_post  ~  hf_pre    1     1     1    3     NA   0         .p5.   0.000   1.139\n6   6     HF1 ~~     HF2    1     1     1    4     NA   0         .p6.   0.000  67.058\n7   7    int1 ~~    int2    1     1     1    5     NA   0         .p7.   0.000 -28.916\n8   8  hf_pre ~1            1     1     1    0      0   0         .p8.   0.000   0.000\n9   9    int1 ~~    int1    0     1     1    6     NA   0         .p9. 224.087 170.287\n10 10     HF1 ~~     HF1    0     1     1    7     NA   0        .p10.  57.344  74.107\n11 11    int2 ~~    int2    0     1     1    8     NA   0        .p11. 233.573  39.030\n12 12     HF2 ~~     HF2    0     1     1    9     NA   0        .p12.  61.474  81.674\n13 13  hf_pre ~~  hf_pre    0     1     1   10     NA   0        .p13.   0.050 287.136\n14 14 hf_post ~~ hf_post    0     1     1   11     NA   0        .p14.   0.050  68.950\n15 15    int1 ~1            0     1     1   12     NA   0 .p15.  .p15.  46.312  42.049\n16 16     HF1 ~1            0     1     1   13     NA   0 .p16.  .p16.  17.077  15.473\n17 17    int2 ~1            0     1     1   14     NA   0 .p17.  .p17.  42.250  39.131\n18 18     HF2 ~1            0     1     1   15     NA   0 .p18.  .p18.  15.508  14.044\n19 19 hf_post ~1            0     1     1    0      0   0        .p19.   0.000   0.000\n20 20  hf_pre =~    int1    1     2     2    0      1   0        .p20.   1.000   1.000\n21 21  hf_pre =~     HF1    1     2     2   16     NA   0  .p2.  .p21.   0.275   0.388\n22 22 hf_post =~    int2    1     2     2    0      1   0        .p22.   1.000   1.000\n23 23 hf_post =~     HF2    1     2     2   17     NA   0  .p4.  .p23.   0.044   0.312\n24 24 hf_post  ~  hf_pre    1     2     2   18     NA   0        .p24.   0.000  -0.456\n25 25     HF1 ~~     HF2    1     2     2   19     NA   0        .p25.   0.000  47.191\n26 26    int1 ~~    int2    1     2     2   20     NA   0        .p26.   0.000  44.760\n27 27  hf_pre ~1            1     2     2    0      0   0        .p27.   0.000   0.000\n28 28    int1 ~~    int1    0     2     2   21     NA   0        .p28. 165.271 241.229\n29 29     HF1 ~~     HF1    0     2     2   22     NA   0        .p29.  63.038  99.551\n30 30    int2 ~~    int2    0     2     2   23     NA   0        .p30.  56.430  81.492\n31 31     HF2 ~~     HF2    0     2     2   24     NA   0        .p31.  12.662  24.780\n32 32  hf_pre ~~  hf_pre    0     2     2   25     NA   0        .p32.   0.050 114.255\n33 33 hf_post ~~ hf_post    0     2     2   26     NA   0        .p33.   0.050   2.901\n34 34    int1 ~1            0     2     2   27     NA   0 .p15.  .p34.  39.000  42.049\n35 35     HF1 ~1            0     2     2   28     NA   0 .p16.  .p35.  14.396  15.473\n36 36    int2 ~1            0     2     2   29     NA   0 .p17.  .p36.  10.875  39.131\n37 37     HF2 ~1            0     2     2   30     NA   0 .p18.  .p37.   5.036  14.044\n38 38 hf_post ~1            0     2     2   31     NA   0        .p38.   0.000 -28.439\n39 39    .p2. ==   .p21.    2     0     0    0     NA   0                0.000   0.000\n40 40    .p4. ==   .p23.    2     0     0    0     NA   0                0.000   0.000\n41 41   .p15. ==   .p34.    2     0     0    0     NA   0                0.000   0.000\n42 42   .p16. ==   .p35.    2     0     0    0     NA   0                0.000   0.000\n43 43   .p17. ==   .p36.    2     0     0    0     NA   0                0.000   0.000\n44 44   .p18. ==   .p37.    2     0     0    0     NA   0                0.000   0.000\n        se\n1    0.000\n2    0.115\n3    0.000\n4    0.037\n5    0.274\n6   18.003\n7   98.630\n8    0.000\n9  124.805\n10  19.536\n11 107.473\n12  19.666\n13 148.917\n14  49.956\n15   2.027\n16   1.089\n17   2.690\n18   1.248\n19   0.000\n20   0.000\n21   0.115\n22   0.000\n23   0.037\n24   0.459\n25  10.578\n26  37.540\n27   0.000\n28  74.588\n29  23.596\n30  25.448\n31   5.610\n32  70.087\n33  52.283\n34   2.027\n35   1.089\n36   2.690\n37   1.248\n38   3.135\n39   0.000\n40   0.000\n41   0.000\n42   0.000\n43   0.000\n44   0.000\n\n\n실제 평균과 비교해보면,\n\n추정된 관찰변수의 평균(intercept)는 잠재변수의 평균(0)에 대한 예측값임.\n\n위에서는 두 집단 간에 pre-test 잠재변수의 값이 동일하게 0이라고 가정했음.\n\n\nhotflash |&gt;\n  group_by(g) |&gt;\n  summarise(across(HF1:int2, mean)) |&gt; print()\n\n# A tibble: 2 × 5\n  g           HF1   HF2  int1  int2\n  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 control    17.1 15.5   46.3  42.2\n2 treatment  14.4  5.04  39    10.9\n\n\n\n모형에 대한 여러 가정들과 효과들을 검증\n\n\n\n\n\n\n\nKeith’s Table 19.2에서 RMSEA값은 조정되지 않은 값으로 \\(\\sqrt{2}\\)를 곱해야 lavaan/Mplus 결과값과 동일함.\n\n\n\n\n\n무선 할당의 적절성 (Pretests Differ)\n사전 검사에서 두 집단 간의 평균(intercept)이 같다는 가정을 하지 않았을 때,\n즉, free the estimate of the intercepts for the pretest latent variables\n\n# free pretest intercepts\nhotflash_model_mg2 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\n  # hf_pre ~ 0*1 # 제거\n\"\nhotflash_fit_mg2 &lt;- sem(hotflash_model_mg2,\n  data = hotflash,\n  meanstructure = TRUE,\n  group = \"g\", # factor type\n  group.equal = c(\"loadings\", \"intercepts\")\n)\n\n\nsummary(hotflash_fit_mg2, estimates = FALSE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 316 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        32\n  Number of equality constraints                     6\n\n  Number of observations per group:                   \n    control                                         48\n    treatment                                       48\n\nModel Test User Model:\n                                                      \n  Test statistic                                 1.827\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.401\n  Test statistic for each group:\n    control                                      0.069\n    treatment                                    1.758\n\n\n\nparameterEstimates(hotflash_fit_mg2) |&gt; subset(op == \"~1\") |&gt; print()\n\n       lhs op rhs block group label     est    se      z pvalue ci.lower ci.upper\n14    int1 ~1         1     1 .p14.  46.160 2.931 15.749  0.000   40.415   51.904\n15     HF1 ~1         1     1 .p15.  17.176 1.460 11.763  0.000   14.315   20.038\n16    int2 ~1         1     1 .p16.  42.265 3.128 13.513  0.000   36.135   48.395\n17     HF2 ~1         1     1 .p17.  15.587 1.547 10.077  0.000   12.555   18.619\n18  hf_pre ~1         1     1         0.000 0.000     NA     NA    0.000    0.000\n19 hf_post ~1         1     1         0.000 0.000     NA     NA    0.000    0.000\n33    int1 ~1         2     2 .p14.  46.160 2.931 15.749  0.000   40.415   51.904\n34     HF1 ~1         2     2 .p15.  17.176 1.460 11.763  0.000   14.315   20.038\n35    int2 ~1         2     2 .p16.  42.265 3.128 13.513  0.000   36.135   48.395\n36     HF2 ~1         2     2 .p17.  15.587 1.547 10.077  0.000   12.555   18.619\n37  hf_pre ~1         2     2        -6.970 3.653 -1.908  0.056  -14.131    0.190\n38 hf_post ~1         2     2       -34.716 5.690 -6.101  0.000  -45.869  -23.564\n\n\n\n\ncompareFit(hotflash_fit_mg, hotflash_fit_mg2) |&gt; summary() |&gt; print()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n                 Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)  \nhotflash_fit_mg2  2 2899.9 2966.6 1.8271                                        \nhotflash_fit_mg   3 2901.5 2965.6 5.3986     3.5715 0.23146       1    0.05878 .\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n####################### Model Fit Indices ###########################\n                  chisq df pvalue rmsea    cfi    tli  srmr       aic       bic\nhotflash_fit_mg2 1.827†  2   .401 .000† 1.000† 1.006† .041† 2899.894† 2966.567 \nhotflash_fit_mg  5.399   3   .145 .129   .986   .944  .088  2901.465  2965.574†\n\n################## Differences in Fit Indices #######################\n                                   df rmsea    cfi    tli  srmr   aic    bic\nhotflash_fit_mg - hotflash_fit_mg2  1 0.129 -0.014 -0.062 0.047 1.572 -0.993\n\nThe following lavaan models were compared:\n    hotflash_fit_mg2\n    hotflash_fit_mg\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.\n\n\n\n\n두 방식의 비교 (Test Assumptions)\n더미 변수 방식과 MG-MACS 방식의 비교\n더미 변수 방식의 추정에서 숨은 가정들\n\n두 집단 간에 모든 구조가 동일함: factor loading, intercept, residual variance/covariance\nPre-test가 post-test주는 효과도 동일함 (ANCOVA의 가정)\n\n이전 더미 변수 방식의 경우:\n\n\n# Test assumptions\nhotflash_model_mg3 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ c(a, a)*hf_pre  # 사전 검사가 사후 검사에 동일하게 영향을 미친다고 가정\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\n  hf_pre ~ 0*1  # pretest 동일성 가정\n\"\nhotflash_fit_mg3 &lt;- sem(hotflash_model_mg3,\n  data = hotflash,\n  meanstructure = TRUE,\n  group = \"g\", # factor type\n  group.equal = c(\n    \"loadings\", \"intercepts\",\n    \"residuals\", \"lv.variances\",\n    \"residual.covariances\"\n  )\n)\n\n\nparameterEstimates(hotflash_fit_mg3) |&gt; subset(op == \"~\") |&gt; print()\n\n       lhs op    rhs block group label   est    se   z pvalue ci.lower ci.upper\n5  hf_post  ~ hf_pre     1     1     a 0.487 0.195 2.5  0.012    0.105    0.868\n24 hf_post  ~ hf_pre     2     2     a 0.487 0.195 2.5  0.012    0.105    0.868\n\n\n\nparameterEstimates(hotflash_fit_mg3) |&gt; subset(op == \"~~\") |&gt; print()\n\n       lhs op     rhs block group label     est      se      z pvalue ci.lower ci.upper\n6      HF1 ~~     HF2     1     1  .p6.  60.277  10.820  5.571  0.000   39.071   81.484\n7     int1 ~~    int2     1     1  .p7. -24.643  55.046 -0.448  0.654 -132.531   83.246\n9     int1 ~~    int1     1     1  .p9.  52.755 179.207  0.294  0.768 -298.483  403.993\n10     HF1 ~~     HF1     1     1 .p10.  98.604  17.851  5.524  0.000   63.617  133.592\n11    int2 ~~    int2     1     1 .p11.  70.300  51.321  1.370  0.171  -30.286  170.887\n12     HF2 ~~     HF2     1     1 .p12.  53.415   9.107  5.865  0.000   35.566   71.265\n13  hf_pre ~~  hf_pre     1     1 .p13. 349.971 190.043  1.842  0.066  -22.507  722.449\n14 hf_post ~~ hf_post     1     1 .p14. 138.752  35.569  3.901  0.000   69.039  208.465\n25     HF1 ~~     HF2     2     2  .p6.  60.277  10.820  5.571  0.000   39.071   81.484\n26    int1 ~~    int2     2     2  .p7. -24.643  55.046 -0.448  0.654 -132.531   83.246\n28    int1 ~~    int1     2     2  .p9.  52.755 179.207  0.294  0.768 -298.483  403.993\n29     HF1 ~~     HF1     2     2 .p10.  98.604  17.851  5.524  0.000   63.617  133.592\n30    int2 ~~    int2     2     2 .p11.  70.300  51.321  1.370  0.171  -30.286  170.887\n31     HF2 ~~     HF2     2     2 .p12.  53.415   9.107  5.865  0.000   35.566   71.265\n32  hf_pre ~~  hf_pre     2     2 .p13. 349.971 190.043  1.842  0.066  -22.507  722.449\n33 hf_post ~~ hf_post     2     2 .p14. 138.752  35.569  3.901  0.000   69.039  208.465\n\n\n\nparameterEstimates(hotflash_fit_mg3) |&gt; subset(op == \"~1\") |&gt; print()\n\n       lhs op rhs block group label     est    se      z pvalue ci.lower ci.upper\n8   hf_pre ~1         1     1         0.000 0.000     NA     NA    0.000    0.000\n15    int1 ~1         1     1 .p15.  42.656 2.048 20.826      0   38.642   46.671\n16     HF1 ~1         1     1 .p16.  15.737 1.128 13.949      0   13.525   17.948\n17    int2 ~1         1     1 .p17.  40.870 2.349 17.396      0   36.266   45.475\n18     HF2 ~1         1     1 .p18.  14.713 1.044 14.088      0   12.666   16.760\n19 hf_post ~1         1     1         0.000 0.000     NA     NA    0.000    0.000\n27  hf_pre ~1         2     2         0.000 0.000     NA     NA    0.000    0.000\n34    int1 ~1         2     2 .p15.  42.656 2.048 20.826      0   38.642   46.671\n35     HF1 ~1         2     2 .p16.  15.737 1.128 13.949      0   13.525   17.948\n36    int2 ~1         2     2 .p17.  40.870 2.349 17.396      0   36.266   45.475\n37     HF2 ~1         2     2 .p18.  14.713 1.044 14.088      0   12.666   16.760\n38 hf_post ~1         2     2       -28.615 3.149 -9.087      0  -34.787  -22.444\n\n\n앞서 더미 변수 방식의 결과와 달리 모형 적합도가 매우 좋지 않음!\n\nMulti-group 방식에서는 가정들에 대한 검증도 동시에 이루어지고 있음.\n\n즉, 여기서는 더미 변수 방식의 가정들이 매우 맞지 않음을 확인할 수 있음.\n\n\n# Dummy 방식과의 비교\ncompareFit(hotflash_fit, hotflash_fit_mg3, nested = FALSE) |&gt; summary() |&gt; print()\n\n####################### Model Fit Indices ###########################\n                   chisq df pvalue rmsea   cfi   tli  srmr       aic       bic\nhotflash_fit      3.501†  2   .174 .088† .994† .968† .082† 2956.482† 2987.254†\nhotflash_fit_mg3 86.416  12   .000 .359  .565  .565  .534  2964.482  3005.512 \n\nThe following lavaan models were compared:\n    hotflash_fit\n    hotflash_fit_mg3\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.\n\n\n\n# Initial model과의 비교\ncompareFit(hotflash_fit_mg, hotflash_fit_mg3) |&gt; summary()|&gt; print()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n                 Df    AIC    BIC   Chisq Chisq diff  RMSEA Df diff Pr(&gt;Chisq)    \nhotflash_fit_mg   3 2901.5 2965.6  5.3986                                         \nhotflash_fit_mg3 12 2964.5 3005.5 86.4155     81.017 0.4083       9  1.015e-13 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n####################### Model Fit Indices ###########################\n                   chisq df pvalue rmsea   cfi   tli  srmr       aic       bic\nhotflash_fit_mg   5.399†  3   .145 .129† .986† .944† .088† 2901.465† 2965.574†\nhotflash_fit_mg3 86.416  12   .000 .359  .565  .565  .534  2964.482  3005.512 \n\n################## Differences in Fit Indices #######################\n                                   df rmsea    cfi    tli  srmr    aic    bic\nhotflash_fit_mg3 - hotflash_fit_mg  9  0.23 -0.421 -0.379 0.446 63.017 39.938\n\nThe following lavaan models were compared:\n    hotflash_fit_mg\n    hotflash_fit_mg3\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.\n\n\n\nSlopes Vary\n앞의 모형에서 ANCOVA의 가정인 두 집단 간에 pre-test와 post-test의 관계가 동일하다는 가정을 검증함.\n즉, 앞의 모든 가정을 포함한 모형에서, 집단 별로 기울기만 다르게 추정하면,\n\n# Slopes Vary\nhotflash_model_mg4 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ c(a, b)*hf_pre\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\n  hf_pre ~ 0*1\n\"\nhotflash_fit_mg4 &lt;- sem(hotflash_model_mg4,\n  data = hotflash,\n  meanstructure = TRUE,\n  group = \"g\", # factor type\n  group.equal = c(\n    \"loadings\", \"intercepts\",\n    \"residuals\", \"lv.variances\",\n    \"residual.covariances\"\n  )\n)\n\n\ncompareFit(hotflash_fit_mg3, hotflash_fit_mg4) |&gt; summary() |&gt; print()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n                 Df    AIC    BIC  Chisq Chisq diff  RMSEA Df diff Pr(&gt;Chisq)    \nhotflash_fit_mg4 11 2925.3 2968.8 45.194                                         \nhotflash_fit_mg3 12 2964.5 3005.5 86.415     41.222 0.9154       1  1.359e-10 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n####################### Model Fit Indices ###########################\n                   chisq df pvalue rmsea   cfi   tli  srmr       aic       bic\nhotflash_fit_mg4 45.194† 11   .000 .254† .800† .782† .209† 2925.260† 2968.854†\nhotflash_fit_mg3 86.416  12   .000 .359  .565  .565  .534  2964.482  3005.512 \n\n################## Differences in Fit Indices #######################\n                                    df rmsea    cfi    tli  srmr    aic    bic\nhotflash_fit_mg3 - hotflash_fit_mg4  1 0.105 -0.235 -0.217 0.325 39.222 36.658\n\nThe following lavaan models were compared:\n    hotflash_fit_mg4\n    hotflash_fit_mg3\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.\n\n\n\n\n\nInitial Model revised\nInitial 모형의 적합도가 낮은 이유로 인해 더 나은 모형으로 개선\n두 집단에서 모두 Cov(int1, int2)가 통계적으로 유의하지 않으므로 0으로 제약\nChi-square가 약간 증가하나 추가 자유도 2 확보\n\nparameterEstimates(hotflash_fit_mg) |&gt; subset(op == \"~~\") |&gt; print()\n\n       lhs op     rhs block group label     est      se      z pvalue ci.lower ci.upper\n6      HF1 ~~     HF2     1     1        67.058  18.003  3.725  0.000   31.773  102.344\n7     int1 ~~    int2     1     1       -28.916  98.630 -0.293  0.769 -222.227  164.396\n9     int1 ~~    int1     1     1       170.287 124.805  1.364  0.172  -74.326  414.900\n10     HF1 ~~     HF1     1     1        74.107  19.536  3.793  0.000   35.817  112.397\n11    int2 ~~    int2     1     1        39.030 107.473  0.363  0.716 -171.614  249.674\n12     HF2 ~~     HF2     1     1        81.674  19.666  4.153  0.000   43.128  120.220\n13  hf_pre ~~  hf_pre     1     1       287.136 148.917  1.928  0.054   -4.735  579.008\n14 hf_post ~~ hf_post     1     1        68.950  49.956  1.380  0.168  -28.962  166.863\n25     HF1 ~~     HF2     2     2        47.191  10.578  4.461  0.000   26.460   67.923\n26    int1 ~~    int2     2     2        44.760  37.540  1.192  0.233  -28.817  118.337\n28    int1 ~~    int1     2     2       241.229  74.588  3.234  0.001   95.039  387.419\n29     HF1 ~~     HF1     2     2        99.551  23.596  4.219  0.000   53.304  145.798\n30    int2 ~~    int2     2     2        81.492  25.448  3.202  0.001   31.615  131.368\n31     HF2 ~~     HF2     2     2        24.780   5.610  4.417  0.000   13.785   35.775\n32  hf_pre ~~  hf_pre     2     2       114.255  70.087  1.630  0.103  -23.113  251.622\n33 hf_post ~~ hf_post     2     2         2.901  52.283  0.055  0.956  -99.571  105.373\n\n\n\n# Initial 2: remove covariances between int1, int2\nhotflash_model_mg5 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre\n\n  HF1 ~~ HF2\n  int1 ~~ 0*int2  # fix the covariance to 0\n\n  hf_pre ~ 0*1\n\"\nhotflash_fit_mg5 &lt;- sem(hotflash_model_mg5,\n  data = hotflash,\n  meanstructure = TRUE,\n  group = \"g\", # factor type\n  group.equal = c(\"loadings\", \"intercepts\")\n)\n\n\ncompareFit(hotflash_fit_mg, hotflash_fit_mg5) |&gt; summary() |&gt; print()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n                 Df    AIC    BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nhotflash_fit_mg   3 2901.5 2965.6 5.3986                                    \nhotflash_fit_mg5  5 2899.1 2958.1 7.0745     1.6759     0       2     0.4326\n\n####################### Model Fit Indices ###########################\n                  chisq df pvalue rmsea   cfi   tli  srmr       aic       bic\nhotflash_fit_mg  5.399†  3   .145 .129  .986  .944  .088† 2901.465  2965.574 \nhotflash_fit_mg5 7.074   5   .215 .093† .988† .971† .095  2899.141† 2958.121†\n\n################## Differences in Fit Indices #######################\n                                   df  rmsea   cfi   tli  srmr    aic    bic\nhotflash_fit_mg5 - hotflash_fit_mg  2 -0.036 0.002 0.027 0.006 -2.324 -7.453\n\nThe following lavaan models were compared:\n    hotflash_fit_mg\n    hotflash_fit_mg5\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.\n\n\n\n\n효과에 대한 재검증\n\nNo Main Effect\n치료의 효과가 없다는 가정: post-test 잠재변수의 intercept를 0으로 제약\n(pre-test 잠재변수의 intercept는 0으로 제약되어 있음)\n\n# No main effect\nhotflash_model_mg6 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre\n\n  HF1 ~~ HF2\n  int1 ~~ 0*int2  # fix the covariance to 0\n\n  hf_pre ~ 0*1\n  hf_post ~ 0*1  # fix the intercepts to 0 (아래 group.equal에서 means을 동일하게 제약하는 것과 같은 결과)\n\"\nhotflash_fit_mg6 &lt;- sem(hotflash_model_mg6,\n  data = hotflash,\n  meanstructure = TRUE,\n  group = \"g\", # factor type\n  group.equal = c(\"loadings\", \"intercepts\", \n                  \"means\") # fix the means to 0\n)\n\n\ncompareFit(hotflash_fit_mg5, hotflash_fit_mg6) |&gt; summary() |&gt; print()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n                 Df    AIC    BIC   Chisq Chisq diff  RMSEA Df diff Pr(&gt;Chisq)    \nhotflash_fit_mg5  5 2899.1 2958.1  7.0745                                         \nhotflash_fit_mg6  6 2958.8 3015.2 68.7447      61.67 1.1243       1  4.061e-15 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n####################### Model Fit Indices ###########################\n                   chisq df pvalue rmsea   cfi   tli  srmr       aic       bic\nhotflash_fit_mg5  7.074†  5   .215 .093† .988† .971† .095† 2899.141† 2958.121†\nhotflash_fit_mg6 68.745   6   .000 .467  .633  .266  .524  2958.811  3015.227 \n\n################## Differences in Fit Indices #######################\n                                    df rmsea    cfi    tli  srmr   aic    bic\nhotflash_fit_mg6 - hotflash_fit_mg5  1 0.374 -0.355 -0.705 0.429 59.67 57.106\n\nThe following lavaan models were compared:\n    hotflash_fit_mg5\n    hotflash_fit_mg6\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.\n\n\n\nparameterEstimates(hotflash_fit_mg5, standardized = \"std.all\") |&gt; subset(op == \"~1\" & lhs == \"hf_post\") |&gt; print()\n\n       lhs op rhs block group label     est    se      z pvalue ci.lower ci.upper std.all\n19 hf_post ~1         1     1         0.000 0.000     NA     NA    0.000    0.000    0.00\n38 hf_post ~1         2     2       -28.308 3.143 -9.006      0  -34.468  -22.147   -4.25\n\n\n\n\n\n\n\n\nNote\n\n\n\n두 모형의 큰 차이는 chi-square difference test에서나 post-test의 intercept에 대한 유의성 테스트에서나 동일하게 나타남.\n치료 효과의 크기에 대해서는 Int 변수의 metric으로 23.8점 감소한다고 해석할 수 있으나, 표준화하려면 Int1과 Int2의 pooled standard deviation을 사용해야 함.\n즉, cohend’s d = -28.3 / pooled SD = \\(\\displaystyle \\frac{28.3}{\\sqrt{\\frac{200.8 + 206.2}{2}}} = -1.96\\)\n앞서 더미 변수 방식에서는 Group(Hypnosis) → Post의 회귀계수는 -28.6(비표준화), -1.39(표준화)\n\n\n\n# int의 분산 확인\nparameterEstimates(hotflash_fit_mg5) |&gt; subset(op == \"~~\" & (lhs == \"int1\" | lhs == \"int2\")) |&gt; print()\n\n    lhs op  rhs block group label     est     se     z pvalue ci.lower ci.upper\n7  int1 ~~ int2     1     1         0.000  0.000    NA     NA    0.000    0.000\n9  int1 ~~ int1     1     1       200.813 65.273 3.076  0.002   72.880  328.747\n11 int2 ~~ int2     1     1        62.914 75.243 0.836  0.403  -84.560  210.388\n26 int1 ~~ int2     2     2         0.000  0.000    NA     NA    0.000    0.000\n28 int1 ~~ int1     2     2       206.182 65.952 3.126  0.002   76.918  335.447\n30 int2 ~~ int2     2     2        68.308 20.877 3.272  0.001   27.389  109.227\n\n\n\nparameterEstimates(hotflash_fit_mg5) |&gt; subset(op == \"~~\") |&gt; print()\n\n       lhs op     rhs block group label     est     se     z pvalue ci.lower ci.upper\n6      HF1 ~~     HF2     1     1        64.439 15.850 4.066  0.000   33.373   95.505\n7     int1 ~~    int2     1     1         0.000  0.000    NA     NA    0.000    0.000\n9     int1 ~~    int1     1     1       200.813 65.273 3.076  0.002   72.880  328.747\n10     HF1 ~~     HF1     1     1        70.829 16.758 4.226  0.000   37.983  103.674\n11    int2 ~~    int2     1     1        62.914 75.243 0.836  0.403  -84.560  210.388\n12     HF2 ~~     HF2     1     1        79.187 17.862 4.433  0.000   44.179  114.196\n13  hf_pre ~~  hf_pre     1     1       255.650 97.491 2.622  0.009   64.572  446.727\n14 hf_post ~~ hf_post     1     1        70.303 49.163 1.430  0.153  -26.054  166.660\n25     HF1 ~~     HF2     2     2        46.821 10.332 4.532  0.000   26.571   67.072\n26    int1 ~~    int2     2     2         0.000  0.000    NA     NA    0.000    0.000\n28    int1 ~~    int1     2     2       206.182 65.952 3.126  0.002   76.918  335.447\n29     HF1 ~~     HF1     2     2        96.979 23.606 4.108  0.000   50.712  143.247\n30    int2 ~~    int2     2     2        68.308 20.877 3.272  0.001   27.389  109.227\n31     HF2 ~~     HF2     2     2        23.690  5.445 4.351  0.000   13.018   34.362\n32  hf_pre ~~  hf_pre     2     2       143.736 60.058 2.393  0.017   26.024  261.449\n33 hf_post ~~ hf_post     2     2        39.097 22.874 1.709  0.087   -5.735   83.930\n\n\n\n\nNo Interaction Effect\n앞서 Slopes differs에서처럼 pretest → posttest의 관계가 집단에 따라 동일하다고 가정하면,\n즉, no slope difference를 가정\n\n# No slope difference: interaction\nhotflash_model_mg7 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ c(a, a)*hf_pre\n\n  HF1 ~~ HF2\n  int1 ~~ 0*int2  # fix the covariance to 0\n\n  hf_pre ~ 0*1\n\"\nhotflash_fit_mg7 &lt;- sem(hotflash_model_mg7,\n  data = hotflash,\n  meanstructure = TRUE,\n  group = \"g\", # factor type\n  group.equal = c(\"loadings\", \"intercepts\")\n)\n\n\ncompareFit(hotflash_fit_mg5, hotflash_fit_mg7) |&gt; summary() |&gt; print()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n                 Df    AIC    BIC   Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nhotflash_fit_mg5  5 2899.1 2958.1  7.0745                                          \nhotflash_fit_mg7  6 2913.0 2969.5 22.9747       15.9 0.55716       1  6.677e-05 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n####################### Model Fit Indices ###########################\n                   chisq df pvalue rmsea   cfi   tli  srmr       aic       bic\nhotflash_fit_mg5  7.074†  5   .215 .093† .988† .971† .095† 2899.141† 2958.121†\nhotflash_fit_mg7 22.975   6   .001 .243  .901  .801  .113  2913.041  2969.457 \n\n################## Differences in Fit Indices #######################\n                                    df rmsea    cfi    tli  srmr  aic    bic\nhotflash_fit_mg7 - hotflash_fit_mg5  1  0.15 -0.087 -0.169 0.018 13.9 11.336\n\nThe following lavaan models were compared:\n    hotflash_fit_mg5\n    hotflash_fit_mg7\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.\n\n\n\nparameterEstimates(hotflash_fit_mg5, standardized=\"std.all\") |&gt; subset(op == \"~\") |&gt; print()\n\n       lhs op    rhs block group label    est    se      z pvalue ci.lower ci.upper\n5  hf_post  ~ hf_pre     1     1        1.169 0.271  4.315  0.000    0.638    1.700\n24 hf_post  ~ hf_pre     2     2       -0.191 0.197 -0.972  0.331   -0.577    0.195\n   std.all\n5    0.912\n24  -0.345\n\n\n\n\n\n\n\n\nNote\n\n\n\n통제집단에서만 사전검사에서 높은 열감을 보고한 여성이 사후검사도 높은 열감을 보이는 상관관계(0.912)를 보임.\n치료집단에서는 사전검사 수치가 사후검사에 영향을 주지 않는 것으로 보임.\n즉, 사전검사와 어느 집단에 속하는지가 서로 상호작용하여 사후검사에 효과가 나타났음; interaction effect",
    "crumbs": [
      "Keith's",
      "Latent Means"
    ]
  },
  {
    "objectID": "contents/chap17.html",
    "href": "contents/chap17.html",
    "title": "Chapter 17. Putting it all together",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)",
    "crumbs": [
      "Keith's",
      "Latent Variable SEM"
    ]
  },
  {
    "objectID": "contents/chap17.html#latent-variable-sem",
    "href": "contents/chap17.html#latent-variable-sem",
    "title": "Chapter 17. Putting it all together",
    "section": "Latent Variable SEM",
    "text": "Latent Variable SEM\nA latent variable path model 또는 structural regression (SR) model\n\nMeasurement model (측정 모형): 잠재변수와 관찰된 변수들 간의 관계을 나타내는 모델\nStructural model (구조 모형): 잠재변수들 간의 관계를 나타내는 모델\n\n먼저 측정 모형에 대한 적합도/적절성를 평가해 모형의 misfit을 파악한 후, 구조 모형에 대한 적합도/적절성를 평가하는 2-step 방식이 권장됨.\n\nStep 1: 잠재변수들만으로 이루어진 CFA를 구성하면, 모든 잠재변수들 간의 관계는 correlated되는 것으로 가정\n\nGlobal/local 적합도를 살펴보고 문제점을 파악; 수정/보완\n이후 structural part가 고려되면 적합도는 더 나빠지게 됨; 일종의 제약이 추가되는 형태\n\nStep 2: 구조 모형을 추가하여 적합도를 평가\n\n만약 구조 모형을 수정하는데 측정 모형에 대한 factor loading이 크게 변한다면, 측정 모형에 대한 심각한 문제가 있다고 봐야 함; 잠재변수에 대한 의미에 대한 교란 문제\n\n\n적합도 지표는 보통 측정 모형의 적합도가 훨씬 큰 비중을 차지하게 됨; Anderson and Gerbing (1988)\n즉, 잠재변수 간의 관계에 대한 평가에는 부적절하기 쉬워짐\n\na little-recognized drawback of these goodness-of-fit indices is that they are usually heavily influenced by the goodness of fit of the measurement model portion of the overall (composite) model and only reflect, to a much lesser degree, the goodness of fit of the causal (path) model portion of the overall model. (p. 440, Mulaik et al. (1989))\n\n\n측정 모형을 적당히 수정해서 전체 적합도를 높이는 시도는 주의!\n반면, 측정 모형을 무리하게 향상시켰다고 해도 구조 부분의 회귀계수에 대한 편향이 크게 발생하지는 않는 것으로 파악됨 (p.267, Kline(2023) 참고)\n구조 모형 부분을 분리해서 계산하는 방법이 제안됨; Anderson and Gerbing (1988)\n이에 기반한 구조 부분에 대한 RMSEA인 RMSEA-Path 적합도 지수 by O’Boyle and Williams (2011); calculator 링크\n앞선 연구에서 구조 부분(인과 구조)에 대한 테스트에 대한 제안들 중 다음과 같은 측정모형과의 chi-square difference tests를 제안했음 (Condition 10) (James, L. R., Mulaik, S. A., & Brett, J. M. (1982). Causal analysis.)\n\n측정 모형: 잠재변수 간의 경로가 모두 free로 설정된 correlated 된 모형\n연구자의 복합 모형: 측정 모형으로부터 잠재변수 간의 경로 중 일부가 제약된 모형\n이 둘 간의 chi-square difference test는 경로에 대한 완전한 모형과의 차이를 통계적으로 검증하는 것으로 해석됨\n\n\n\nIn 63% of the studies the RMSEA-P was greater than the .08 standard that is typically used and supported by Williams and O’Boyle (2010), and in nearly half (47%) of these 43 studies the value for the RMSEA-P failed to meet the least stringent criterion of .10 associated with mediocre fit. Moreover, many of the RMSEA-P values greatly exceeded .10, in that eight were above .15, and two were greater than .20. As for the RMSEA-P CI results, in 34 cases (79%) the studies failed to meet one or both of Chen et al.’s (2008) criteria. (O’Boyle, & Williams, 2013)\n\n\n예제: 또래 거부의 영향\n\n\n\n\n\n\nBuhs, E. S., & Ladd, G. W. (2001). Peer rejection as antecedent of young children’s school adjustment: An examination of mediating processes. Developmental psychology, 37(4), 550. Buhs & Ladd, 2001; 링크\n\nRejection\n\nAverage ratings(평균 평가): 학급의 다른 아이들이 각 아동에게 준 평균 사회성 평가\nNegative Nominations(부정적 지목): 각 아동이 부정적으로 지목된 횟수(다른 아이들이 함께 놀고 싶지 않은 사람)\n\nClassroom Participation (Change): 이전 평가에서 교실 참여도의 변화\n\n협력적 참여(예: 책임을 받아들임)\n자율적 참여(예: 자기 주도적)에 대한 교사 평가에서 추정\n\nAchievement: 적응의 한 측면\n\n표준화된 학교 준비도 검사의 언어\n정량적 하위 검사에서 추정\n\nEmotional adjustment(정서적 적응)\n\n외로움\n학교를 피하고 싶은 욕구(학교 회피)에 대한 자가 평가\n\n\n\n\n\n\nMeasurement part; 서로 상관 | Structural part; 거의 포화\n\n\n\n\n\n\n\n\n\nLoad the data\nrejection &lt;- haven::read_sav(\"data/chap 17 intro latent var SEM/buhs & ladd data.sav\")\n\n\n\nrejection |&gt; print()\n\n# A tibble: 399 × 8\n   ave_rats neg_nom    coop    auto quant  lang  lone schavoid\n      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1   -1.33  -1.09    1.19    0.691  7.47   6.30 2.09     2.48 \n 2    1.32   0.551  -0.128  -0.0710 2.72   2.76 1.42     2.16 \n 3   -0.639 -1.09   -0.293  -1.26   6.40   5.39 1.59     1.38 \n 4    1.42  -0.365  -0.190  -0.563  0.989  1.05 0.940    2.13 \n 5    0.584 -0.0127 -0.360  -0.126  2.80   3.56 0.357    2.20 \n 6   -1.20  -1.51    0.0370  0.0676 7.07   7.79 1.37     2.03 \n 7    0.420  0.390  -0.252   0.398  3.68   3.47 2.08     3.00 \n 8   -0.404 -0.806   0.775   1.03   7.03   4.94 2.03     2.61 \n 9    1.99   1.89   -0.454  -0.664  1.51   5.08 0.660    0.992\n10   -0.383 -1.76    0.638   0.0292 6.68   4.95 2.27     2.34 \n# ℹ 389 more rows\n\n\n\npsych::lowerCor(rejection)\n\n         av_rt ng_nm coop  auto  quant lang  lone  schvd\nave_rats  1.00                                          \nneg_nom   0.76  1.00                                    \ncoop     -0.31 -0.30  1.00                              \nauto     -0.18 -0.17  0.37  1.00                        \nquant    -0.48 -0.38  0.27  0.25  1.00                  \nlang     -0.38 -0.31  0.20  0.08  0.69  1.00            \nlone     -0.27 -0.19  0.18  0.10  0.34  0.30  1.00      \nschavoid -0.25 -0.25  0.19  0.15  0.22  0.20  0.31  1.00\n\n\n\nmod_reject &lt;- '\n  # Measurement model\n  Reject =~ ave_rats + neg_nom\n  ClassPart =~ coop + auto\n  Achieve =~ quant + lang\n  EmotAdj =~ lone + schavoid\n  \n  # Structural model\n  EmotAdj ~ ClassPart + Reject\n  Achieve ~ ClassPart + Reject\n  ClassPart ~ Reject\n\n  # Residual covariances\n  EmotAdj ~~ 0*Achieve\n'\n\nfit_reject &lt;- sem(mod_reject, data = rejection)\nsummary(fit_reject, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 51 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n  Number of observations                           399\n\nModel Test User Model:\n                                                      \n  Test statistic                                41.974\n  Degrees of freedom                                15\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               974.475\n  Degrees of freedom                                28\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.972\n  Tucker-Lewis Index (TLI)                       0.947\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3713.422\n  Loglikelihood unrestricted model (H1)      -3692.435\n                                                      \n  Akaike (AIC)                                7468.844\n  Bayesian (BIC)                              7552.613\n  Sample-size adjusted Bayesian (SABIC)       7485.978\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.067\n  90 Percent confidence interval - lower         0.044\n  90 Percent confidence interval - upper         0.092\n  P-value H_0: RMSEA &lt;= 0.050                    0.109\n  P-value H_0: RMSEA &gt;= 0.080                    0.207\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.046\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Reject =~                                                             \n    ave_rats          1.000                               0.901    0.949\n    neg_nom           0.802    0.057   14.175    0.000    0.722    0.803\n  ClassPart =~                                                          \n    coop              1.000                               0.409    0.682\n    auto              0.788    0.141    5.603    0.000    0.322    0.520\n  Achieve =~                                                            \n    quant             1.000                               1.892    0.957\n    lang              0.683    0.063   10.915    0.000    1.291    0.726\n  EmotAdj =~                                                            \n    lone              1.000                               0.317    0.567\n    schavoid          1.140    0.223    5.110    0.000    0.361    0.540\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  EmotAdj ~                                                             \n    ClassPart         0.289    0.098    2.947    0.003    0.372    0.372\n    Reject           -0.118    0.034   -3.434    0.001   -0.335   -0.335\n  Achieve ~                                                             \n    ClassPart         1.298    0.386    3.363    0.001    0.281    0.281\n    Reject           -0.847    0.136   -6.220    0.000   -0.403   -0.403\n  ClassPart ~                                                           \n    Reject           -0.205    0.034   -6.063    0.000   -0.451   -0.451\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .Achieve ~~                                                            \n   .EmotAdj           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .ave_rats          0.089    0.048    1.852    0.064    0.089    0.099\n   .neg_nom           0.287    0.037    7.777    0.000    0.287    0.355\n   .coop              0.192    0.032    6.056    0.000    0.192    0.535\n   .auto              0.280    0.027   10.447    0.000    0.280    0.730\n   .quant             0.333    0.281    1.184    0.237    0.333    0.085\n   .lang              1.494    0.168    8.894    0.000    1.494    0.473\n   .lone              0.212    0.025    8.477    0.000    0.212    0.679\n   .schavoid          0.317    0.034    9.222    0.000    0.317    0.708\n    Reject            0.811    0.079   10.212    0.000    1.000    1.000\n   .ClassPart         0.133    0.032    4.165    0.000    0.797    0.797\n   .Achieve           2.350    0.335    7.009    0.000    0.657    0.657\n   .EmotAdj           0.064    0.020    3.183    0.001    0.637    0.637\n\nR-Square:\n                   Estimate\n    ave_rats          0.901\n    neg_nom           0.645\n    coop              0.465\n    auto              0.270\n    quant             0.915\n    lang              0.527\n    lone              0.321\n    schavoid          0.292\n    ClassPart         0.203\n    Achieve           0.343\n    EmotAdj           0.363\n\n\n\n\nmodindices(fit_reject, sort = T) |&gt; subset(mi &gt; 3) |&gt; print()\n\n        lhs op     rhs     mi    epc sepc.lv sepc.all sepc.nox\n43  Achieve =~    lone 17.657  0.080   0.152    0.271    0.271\n51 ave_rats ~~ neg_nom 13.461  1.120   1.120    7.003    7.003\n85  Achieve  ~ EmotAdj 13.461  2.180   0.365    0.365    0.365\n64     coop ~~    auto 13.461  0.293   0.293    1.266    1.266\n84  EmotAdj  ~ Achieve 13.461  0.059   0.354    0.354    0.354\n14  Achieve ~~ EmotAdj 13.461  0.139   0.360    0.360    0.360\n70     auto ~~    lang 10.688 -0.121  -0.121   -0.187   -0.187\n69     auto ~~   quant  8.863  0.125   0.125    0.408    0.408\n41  Achieve =~    coop  6.368 -0.094  -0.178   -0.297   -0.297\n74    quant ~~    lone  4.734  0.075   0.075    0.284    0.284\n62  neg_nom ~~    lone  4.150  0.032   0.032    0.129    0.129\n65     coop ~~   quant  3.948 -0.087  -0.087   -0.345   -0.345\n48  EmotAdj =~    auto  3.564 -0.487  -0.154   -0.249   -0.249\n\n\n\n\nMeasurement Model\n\n\nmeasurement &lt;- '\n  # Measurement model\n  Reject =~ ave_rats + neg_nom\n  ClassPart =~ coop + auto\n  Achieve =~ quant + lang\n  EmotAdj =~ lone + schavoid\n'\nfit_cfa &lt;- cfa(measurement, data = rejection)\nsummary(fit_cfa, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 49 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        22\n\n  Number of observations                           399\n\nModel Test User Model:\n                                                      \n  Test statistic                                26.841\n  Degrees of freedom                                14\n  P-value (Chi-square)                           0.020\n\nModel Test Baseline Model:\n\n  Test statistic                               974.475\n  Degrees of freedom                                28\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.986\n  Tucker-Lewis Index (TLI)                       0.973\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3705.856\n  Loglikelihood unrestricted model (H1)      -3692.435\n                                                      \n  Akaike (AIC)                                7455.712\n  Bayesian (BIC)                              7543.469\n  Sample-size adjusted Bayesian (SABIC)       7473.662\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.048\n  90 Percent confidence interval - lower         0.019\n  90 Percent confidence interval - upper         0.075\n  P-value H_0: RMSEA &lt;= 0.050                    0.510\n  P-value H_0: RMSEA &gt;= 0.080                    0.024\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.027\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Reject =~                                                             \n    ave_rats          1.000                               0.907    0.956\n    neg_nom           0.792    0.058   13.659    0.000    0.718    0.799\n  ClassPart =~                                                          \n    coop              1.000                               0.443    0.739\n    auto              0.703    0.142    4.938    0.000    0.311    0.503\n  Achieve =~                                                            \n    quant             1.000                               1.845    0.933\n    lang              0.718    0.060   11.880    0.000    1.324    0.745\n  EmotAdj =~                                                            \n    lone              1.000                               0.349    0.625\n    schavoid          0.938    0.173    5.429    0.000    0.328    0.490\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Reject ~~                                                             \n    ClassPart        -0.173    0.029   -5.920    0.000   -0.432   -0.432\n    Achieve          -0.894    0.104   -8.618    0.000   -0.535   -0.535\n    EmotAdj          -0.151    0.026   -5.712    0.000   -0.477   -0.477\n  ClassPart ~~                                                          \n    Achieve           0.333    0.060    5.533    0.000    0.408    0.408\n    EmotAdj           0.066    0.015    4.265    0.000    0.425    0.425\n  Achieve ~~                                                            \n    EmotAdj           0.359    0.057    6.331    0.000    0.558    0.558\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .ave_rats          0.078    0.051    1.517    0.129    0.078    0.086\n   .neg_nom           0.293    0.038    7.691    0.000    0.293    0.362\n   .coop              0.163    0.040    4.104    0.000    0.163    0.454\n   .auto              0.287    0.028   10.360    0.000    0.287    0.747\n   .quant             0.508    0.243    2.089    0.037    0.508    0.130\n   .lang              1.408    0.159    8.851    0.000    1.408    0.445\n   .lone              0.191    0.026    7.230    0.000    0.191    0.610\n   .schavoid          0.340    0.031   10.876    0.000    0.340    0.760\n    Reject            0.823    0.081   10.105    0.000    1.000    1.000\n    ClassPart         0.196    0.044    4.426    0.000    1.000    1.000\n    Achieve           3.402    0.365    9.322    0.000    1.000    1.000\n    EmotAdj           0.122    0.029    4.258    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    ave_rats          0.914\n    neg_nom           0.638\n    coop              0.546\n    auto              0.253\n    quant             0.870\n    lang              0.555\n    lone              0.390\n    schavoid          0.240\n\n\n\n\n\nsemPaths2() function\nsemPaths2 &lt;- function(model, what = 'std', layout = \"tree\", rotation = 1) {\n  semPlot::semPaths(model, what = what, edge.label.cex = 1, edge.color = \"black\", layout = layout, rotation = rotation, weighted = FALSE, asize = 2, label.cex = 1, node.width = 1, style = \"lisrel\")\n}\n\n\n\n# semPaths2: a customized plot function using semPlot::semPaths()\nsemPaths2(fit_cfa)\n\n\n\n\n\n\n\n\n\npsych::lowerCor(rejection)\n\n         av_rt ng_nm coop  auto  quant lang  lone  schvd\nave_rats  1.00                                          \nneg_nom   0.76  1.00                                    \ncoop     -0.31 -0.30  1.00                              \nauto     -0.18 -0.17  0.37  1.00                        \nquant    -0.48 -0.38  0.27  0.25  1.00                  \nlang     -0.38 -0.31  0.20  0.08  0.69  1.00            \nlone     -0.27 -0.19  0.18  0.10  0.34  0.30  1.00      \nschavoid -0.25 -0.25  0.19  0.15  0.22  0.20  0.31  1.00\n\n\n\nmodindices(fit_cfa, sort = TRUE) |&gt; subset(mi &gt; 3) |&gt; print()\n\n       lhs op      rhs     mi    epc sepc.lv sepc.all sepc.nox\n69    auto ~~    quant 12.304  0.141   0.141    0.369    0.369\n70    auto ~~     lang  9.655 -0.114  -0.114   -0.179   -0.179\n44 Achieve =~ schavoid  4.475 -0.111  -0.204   -0.305   -0.305\n43 Achieve =~     lone  4.475  0.118   0.218    0.389    0.389\n75   quant ~~ schavoid  3.183 -0.085  -0.085   -0.204   -0.204\n63 neg_nom ~~ schavoid  3.092 -0.032  -0.032   -0.102   -0.102\n\n\n\nresiduals(fit_cfa, type=\"cor\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n         av_rts neg_nm   coop   auto  quant   lang   lone schavd\nave_rats  0.000                                                 \nneg_nom   0.000  0.000                                          \ncoop     -0.002 -0.043  0.000                                   \nauto      0.024  0.002  0.000  0.000                            \nquant    -0.004  0.015 -0.011  0.055  0.000                     \nlang      0.003  0.004 -0.020 -0.071  0.000  0.000              \nlone      0.012  0.047 -0.017 -0.035  0.016  0.041  0.000       \nschavoid -0.022 -0.059  0.032  0.042 -0.040 -0.003  0.000  0.000\n\n\n\n\nsemTools::reliability(fit_cfa) |&gt; print()\n\n          Reject ClassPart   Achieve   EmotAdj\nalpha  0.8651105 0.5413024 0.8170953 0.4629782\nomega  0.8769819 0.5582674 0.8397666 0.4634457\nomega2 0.8769819 0.5582674 0.8397666 0.4634457\nomega3 0.8769818 0.5582673 0.8397666 0.4634457\navevar 0.7832069 0.3943465 0.7290848 0.3018260\n\n\nRespecification\n\nlatent_mod_modi &lt;- '\n  # Measurement model\n  Reject =~ ave_rats + neg_nom\n  ClassPart =~ coop + auto\n  Achieve =~ quant + lang\n  EmotAdj =~ lone + schavoid\n\n  # Residual covariances\n  auto ~~ lang + quant\n'\nfit_cfa_modi &lt;- cfa(latent_mod_modi, data = rejection)\ncompareFit(fit_cfa, fit_cfa_modi) |&gt; summary()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n             Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)   \nfit_cfa_modi 12 7446.3 7542.0 13.435                                         \nfit_cfa      14 7455.7 7543.5 26.841     13.406 0.11955       2   0.001227 **\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n####################### Model Fit Indices ###########################\n               chisq df pvalue rmsea    cfi    tli  srmr       aic       bic\nfit_cfa_modi 13.435† 12   .338 .017† 0.998† 0.996† .021† 7446.306† 7542.041†\nfit_cfa      26.841  14   .020 .048   .986   .973  .027  7455.712  7543.469 \n\n################## Differences in Fit Indices #######################\n                       df rmsea    cfi    tli  srmr   aic   bic\nfit_cfa - fit_cfa_modi  2 0.031 -0.012 -0.024 0.005 9.406 1.428\n\n\n\n\nsemPaths2(fit_cfa_modi, layout = \"tree\")\n\n\n\n\n\n\n\n\n\n\nStructural Model\n\n구조 부분에 대한 RMSEA (RMSEA-Path) by O’Boyle and Williams (2011); calculator 링크\n\nCondition 10: 측정모형과의 chi-square difference tests\nJames, L. R., Mulaik, S. A., & Brett, J. M. (1982). Causal analysis.\n\n측정 모형: 잠재변수 간의 경로가 모두 free로 설정된 correlated 된 모형\n연구자의 복합 모형: 측정 모형으로부터 잠재변수 간의 경로 중 일부가 제약된 모형\n\n\\(\\Delta RMSEA\\) = \\(\\chi_{composite}^2 - \\chi_{measurement}^2\\)에 대한 \\(RMSEA\\)값 계산\n\ncompareFit(fit_reject, fit_cfa) |&gt; summary()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n           Df    AIC    BIC  Chisq Chisq diff  RMSEA Df diff Pr(&gt;Chisq)    \nfit_cfa    14 7455.7 7543.5 26.841                                         \nfit_reject 15 7468.8 7552.6 41.974     15.133 0.1882       1  0.0001002 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n####################### Model Fit Indices ###########################\n             chisq df pvalue rmsea   cfi   tli  srmr       aic       bic\nfit_cfa    26.841† 14   .020 .048† .986† .973† .027† 7455.712† 7543.469†\nfit_reject 41.974  15   .000 .067  .972  .947  .046  7468.844  7552.613 \n\n################## Differences in Fit Indices #######################\n                     df rmsea    cfi    tli  srmr    aic   bic\nfit_reject - fit_cfa  1 0.019 -0.015 -0.026 0.019 13.133 9.144\n\n\n\n\n\nCompeting Models\n학업 성취도(Achieve) → 정서적 적응(EmotAdj) 추가\n\n\nmod_reject_revised &lt;- '\n  # Measurement model\n  Reject =~ ave_rats + neg_nom\n  ClassPart =~ coop + auto\n  Achieve =~ quant + lang\n  EmotAdj =~ lone + schavoid\n  \n  # Structural model\n  EmotAdj ~ ClassPart + Reject\n  Achieve ~ ClassPart + Reject\n  ClassPart ~ Reject\n  \n  EmotAdj ~ Achieve  # 추가된 부분\n\n  # Residual covariances\n  EmotAdj ~~ 0*Achieve\n'\n\nfit_reject_revised &lt;- sem(mod_reject_revised, data = rejection)\ncompareFit(fit_reject_revised, fit_cfa) |&gt; summary()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n                   Df    AIC    BIC  Chisq  Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nfit_reject_revised 14 7455.7 7543.5 26.841                                     \nfit_cfa            14 7455.7 7543.5 26.841 -1.5522e-10     0       0           \n\n####################### Model Fit Indices ###########################\n                     chisq df pvalue rmsea   cfi   tli  srmr       aic       bic\nfit_reject_revised 26.841  14   .020 .048  .986  .973  .027† 7455.712  7543.469 \nfit_cfa            26.841† 14   .020 .048† .986† .973† .027  7455.712† 7543.469†\n\n################## Differences in Fit Indices #######################\n                             df rmsea cfi tli srmr aic bic\nfit_cfa - fit_reject_revised  0     0   0   0    0   0   0\n\n\n\n\ncompareFit(fit_reject_revised, fit_reject) |&gt; summary()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n                   Df    AIC    BIC  Chisq Chisq diff  RMSEA Df diff Pr(&gt;Chisq)    \nfit_reject_revised 14 7455.7 7543.5 26.841                                         \nfit_reject         15 7468.8 7552.6 41.974     15.133 0.1882       1  0.0001002 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n####################### Model Fit Indices ###########################\n                     chisq df pvalue rmsea   cfi   tli  srmr       aic       bic\nfit_reject_revised 26.841† 14   .020 .048† .986† .973† .027† 7455.712† 7543.469†\nfit_reject         41.974  15   .000 .067  .972  .947  .046  7468.844  7552.613 \n\n################## Differences in Fit Indices #######################\n                                df rmsea    cfi    tli  srmr    aic   bic\nfit_reject - fit_reject_revised  1 0.019 -0.015 -0.026 0.019 13.133 9.144\n\n\n\n학업 성취도(Achieve) ←→ 정서적 적응(EmotAdj) 간의 correlation 추가\n\n\nmod_reject_revised2 &lt;- '\n  # Measurement model\n  Reject =~ ave_rats + neg_nom\n  ClassPart =~ coop + auto\n  Achieve =~ quant + lang\n  EmotAdj =~ lone + schavoid\n  \n  # Structural model\n  EmotAdj ~ ClassPart + Reject\n  Achieve ~ ClassPart + Reject\n  ClassPart ~ Reject\n  \n  # Residual covariances \n  EmotAdj ~~ Achieve  # 추가된 부분\n'\n\nfit_reject_revised2 &lt;- sem(mod_reject_revised2, data = rejection)\ncompareFit(fit_reject_revised2, fit_cfa) |&gt; summary()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n                    Df    AIC    BIC  Chisq  Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nfit_reject_revised2 14 7455.7 7543.5 26.841                                     \nfit_cfa             14 7455.7 7543.5 26.841 -7.5483e-10     0       0           \n\n####################### Model Fit Indices ###########################\n                      chisq df pvalue rmsea   cfi   tli  srmr       aic       bic\nfit_reject_revised2 26.841  14   .020 .048  .986  .973  .027† 7455.712  7543.469 \nfit_cfa             26.841† 14   .020 .048† .986† .973† .027  7455.712† 7543.469†\n\n################## Differences in Fit Indices #######################\n                              df rmsea cfi tli srmr aic bic\nfit_cfa - fit_reject_revised2  0     0   0   0    0   0   0",
    "crumbs": [
      "Keith's",
      "Latent Variable SEM"
    ]
  },
  {
    "objectID": "contents/chap13.html",
    "href": "contents/chap13.html",
    "title": "Chapter 13-14. Path Analysis",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)",
    "crumbs": [
      "Keith's",
      "Path Analysis"
    ]
  },
  {
    "objectID": "contents/chap13.html#데이터-준비",
    "href": "contents/chap13.html#데이터-준비",
    "title": "Chapter 13-14. Path Analysis",
    "section": "데이터 준비",
    "text": "데이터 준비\n\n# Load the data\nnels &lt;- read_csv(\"data/n=1000,stud & par shorter all miss blank.csv\")\nnels |&gt; print()\n\n# A tibble: 1,000 × 93\n  stu_id sch_id sstratid   sex  race ethnic bys42a bys42b bys44a bys44b bys44c\n   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 124966   1249        1     2     4      1      3      4      2      4      4\n2 124972   1249        1     1     4      1      4      5      1      3      3\n3 175551   1755        1     2     3      0     NA      3      2      3      3\n4 180660   1806        1     1     4      1      2     NA      1      4      4\n5 180672   1806        1     2     4      1      2      3      1      4      3\n6 298885   2988        2     1     3      0      5      4      2      3      3\n# ℹ 994 more rows\n# ℹ 82 more variables: bys44d &lt;dbl&gt;, bys44e &lt;dbl&gt;, bys44f &lt;dbl&gt;, bys44g &lt;dbl&gt;,\n#   bys44h &lt;dbl&gt;, bys44i &lt;dbl&gt;, bys44j &lt;dbl&gt;, bys44k &lt;dbl&gt;, bys44l &lt;dbl&gt;,\n#   bys44m &lt;dbl&gt;, bys48a &lt;dbl&gt;, bys48b &lt;dbl&gt;, bys79a &lt;dbl&gt;, byfamsiz &lt;dbl&gt;,\n#   famcomp &lt;dbl&gt;, bygrads &lt;dbl&gt;, byses &lt;dbl&gt;, byfaminc &lt;dbl&gt;, parocc &lt;dbl&gt;,\n#   bytxrstd &lt;dbl&gt;, bytxmstd &lt;dbl&gt;, bytxsstd &lt;dbl&gt;, bytxhstd &lt;dbl&gt;,\n#   bypared &lt;dbl&gt;, bytests &lt;dbl&gt;, par_inv &lt;dbl&gt;, f1s36a1 &lt;dbl&gt;, …\n\n\n\n# SPSS data: labelled data\nlibrary(haven) # install.packages(\"haven\")\nnels_sav &lt;- read_sav(\"data/n=1000,stud & par shorter.sav\")\nnels_sav |&gt; print()\n\n# A tibble: 1,000 × 93\n  stu_id    sch_id    sstratid sex     race    ethnic  bys42a   bys42b   bys44a \n  &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lb&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt; &lt;dbl+lb&gt; &lt;dbl+l&gt;\n1 124966    1249      1        2 [Fem… 4 [Whi… 1 [whi…  3 [2-3…  4 [3-4… 2 [Agr…\n2 124972    1249      1        1 [Mal… 4 [Whi… 1 [whi…  4 [3-4…  5 [4-5… 1 [Str…\n3 175551    1755      1        2 [Fem… 3 [Bla… 0 [blk… NA        3 [2-3… 2 [Agr…\n4 180660    1806      1        1 [Mal… 4 [Whi… 1 [whi…  2 [1-2… NA       1 [Str…\n5 180672    1806      1        2 [Fem… 4 [Whi… 1 [whi…  2 [1-2…  3 [2-3… 1 [Str…\n6 298885    2988      2        1 [Mal… 3 [Bla… 0 [blk…  5 [4-5…  4 [3-4… 2 [Agr…\n# ℹ 994 more rows\n# ℹ 84 more variables: bys44b &lt;dbl+lbl&gt;, bys44c &lt;dbl+lbl&gt;, bys44d &lt;dbl+lbl&gt;,\n#   bys44e &lt;dbl+lbl&gt;, bys44f &lt;dbl+lbl&gt;, bys44g &lt;dbl+lbl&gt;, bys44h &lt;dbl+lbl&gt;,\n#   bys44i &lt;dbl+lbl&gt;, bys44j &lt;dbl+lbl&gt;, bys44k &lt;dbl+lbl&gt;, bys44l &lt;dbl+lbl&gt;,\n#   bys44m &lt;dbl+lbl&gt;, bys48a &lt;dbl+lbl&gt;, bys48b &lt;dbl+lbl&gt;, bys79a &lt;dbl+lbl&gt;,\n#   byfamsiz &lt;dbl+lbl&gt;, famcomp &lt;dbl+lbl&gt;, bygrads &lt;dbl+lbl&gt;, byses &lt;dbl+lbl&gt;,\n#   byfaminc &lt;dbl+lbl&gt;, parocc &lt;dbl&gt;, bytxrstd &lt;dbl+lbl&gt;, bytxmstd &lt;dbl+lbl&gt;, …\n\n\n\nnels_sav$ethnic |&gt; labelled::val_labels() |&gt; print()\n\nblk,namer,hisp    white-asian        missing \n             0              1              8 \n\n\nvariables: byses, bytests, par_inv, ffugrad, ethnic\nUnderrepresented ethnic minority, or URM, is coded so that students from African American, Hispanic, and Native backgrounds are coded 1 and students of Asian and Caucasian descent are coded 0.\n\nnels_gpa &lt;-\n  nels |&gt;\n  select(ethnic, ses = byses, prev = bytests, par = par_inv, gpa = ffugrad) |&gt;\n  na.omit()\n\nnels_gpa |&gt; print()\n\n# A tibble: 811 × 5\n  ethnic    ses  prev     par   gpa\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1      1 -0.563  64.4  1.04    5.25\n2      1  0.123  48.6 -0.0881  3   \n3      0  0.229  49.7 -0.390   2.5 \n4      1  0.687  46.6  0.199   6.5 \n5      1  0.633  54.9  0.975   4.25\n6      0  0.992  38.5 -0.157   6   \n# ℹ 805 more rows\n\n\n\n상관계수와 평균, 표준편차\n\nlibrary(psych)\nnels_gpa |&gt; lowerCor(digits = 3)\n\n       ethnc ses   prev  par   gpa  \nethnic 1.000                        \nses    0.333 1.000                  \nprev   0.330 0.461 1.000            \npar    0.075 0.432 0.445 1.000      \ngpa    0.131 0.299 0.499 0.364 1.000\n\n\n\nnels_gpa |&gt; describe(skew = F) |&gt; print(digits = 3)\n\n       vars   n   mean    sd median    min    max  range    se\nethnic    1 811  0.793 0.406  1.000  0.000  1.000  1.000 0.014\nses       2 811  0.047 0.766  0.011 -2.414  1.874  4.288 0.027\nprev      3 811 52.323 8.584 52.649 30.397 70.240 39.844 0.301\npar       4 811  0.059 0.794  0.191 -3.148  1.493  4.642 0.028\ngpa       5 811  5.760 1.450  6.000  1.000  8.000  7.000 0.051",
    "crumbs": [
      "Keith's",
      "Path Analysis"
    ]
  },
  {
    "objectID": "contents/chap13.html#포화모형",
    "href": "contents/chap13.html#포화모형",
    "title": "Chapter 13-14. Path Analysis",
    "section": "포화모형",
    "text": "포화모형\n모형의 적합도에 대한 신뢰할 만한 정보를 얻기 위해서는 자유도(degree of freedom)가 높도록 모형을 만들어야 함(specification).\n여기서는 자유도가 0인 포화모형을 우선 고려한 후, 이후 더 단순한 모형을 고려할 것임. 자유도가 0이면 모형의 적합도는 의미가 없으며, 이 경우를 포화모형(saturated model)이라고 함.\n\n구조모형\n\n\nlibrary(lavaan)\nlibrary(semTools)\n\nmod &lt;- \"\n  gpa ~ ethnic + ses + prev + par\n  par ~ prev + ses + ethnic\n  prev ~ ses + ethnic\n\"\n\nsem_fit &lt;- sem(model = mod, data = nels_gpa, fixed.x = FALSE)\nsummary(sem_fit, standardized = TRUE, rsquare = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n\n  Number of observations                           811\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  gpa ~                                                                 \n    ethnic           -0.124    0.117   -1.058    0.290   -0.124   -0.035\n    ses               0.093    0.069    1.355    0.175    0.093    0.049\n    prev              0.070    0.006   11.414    0.000    0.070    0.417\n    par               0.292    0.064    4.531    0.000    0.292    0.160\n  par ~                                                                 \n    prev              0.032    0.003   10.040    0.000    0.032    0.345\n    ses               0.333    0.036    9.351    0.000    0.333    0.321\n    ethnic           -0.286    0.063   -4.528    0.000   -0.286   -0.146\n  prev ~                                                                \n    ses               4.431    0.362   12.236    0.000    4.431    0.395\n    ethnic            4.195    0.684    6.135    0.000    4.195    0.198\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  ethnic ~~                                                             \n    ses               0.103    0.011    9.002    0.000    0.103    0.333\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .gpa               1.521    0.076   20.137    0.000    1.521    0.724\n   .par               0.452    0.022   20.137    0.000    0.452    0.719\n   .prev             55.370    2.750   20.137    0.000   55.370    0.752\n    ethnic            0.164    0.008   20.137    0.000    0.164    1.000\n    ses               0.586    0.029   20.137    0.000    0.586    1.000\n\nR-Square:\n                   Estimate\n    gpa               0.276\n    par               0.281\n    prev              0.248\n\n\n\n옵션들\n\nci: confidence interval\nheader: 헤더 표시 여부\nnd: the number of digits\n\n\nsummary(sem_fit, standardized = TRUE, ci = TRUE, header = FALSE, nd = 2) |&gt; print()\n\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n  gpa ~                                                                 \n    ethnic            -0.12     0.12    -1.06     0.29    -0.35     0.11\n    ses                0.09     0.07     1.36     0.18    -0.04     0.23\n    prev               0.07     0.01    11.41     0.00     0.06     0.08\n    par                0.29     0.06     4.53     0.00     0.17     0.42\n  par ~                                                                 \n    prev               0.03     0.00    10.04     0.00     0.03     0.04\n    ses                0.33     0.04     9.35     0.00     0.26     0.40\n    ethnic            -0.29     0.06    -4.53     0.00    -0.41    -0.16\n  prev ~                                                                \n    ses                4.43     0.36    12.24     0.00     3.72     5.14\n    ethnic             4.20     0.68     6.13     0.00     2.85     5.54\n   Std.lv  Std.all\n                  \n    -0.12    -0.03\n     0.09     0.05\n     0.07     0.42\n     0.29     0.16\n                  \n     0.03     0.34\n     0.33     0.32\n    -0.29    -0.15\n                  \n     4.43     0.40\n     4.20     0.20\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n  ethnic ~~                                                             \n    ses                0.10     0.01     9.00     0.00     0.08     0.13\n   Std.lv  Std.all\n                  \n     0.10     0.33\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n   .gpa                1.52     0.08    20.14     0.00     1.37     1.67\n   .par                0.45     0.02    20.14     0.00     0.41     0.50\n   .prev              55.37     2.75    20.14     0.00    49.98    60.76\n    ethnic             0.16     0.01    20.14     0.00     0.15     0.18\n    ses                0.59     0.03    20.14     0.00     0.53     0.64\n   Std.lv  Std.all\n     1.52     0.72\n     0.45     0.72\n    55.37     0.75\n     0.16     1.00\n     0.59     1.00\n\n\n\n표준화된 파라미터 추정치만 표시\n\nstandardizedSolution(sem_fit, type = \"std.all\") |&gt; print()\n\n      lhs op    rhs est.std    se      z pvalue ci.lower ci.upper\n1     gpa  ~ ethnic  -0.035 0.033 -1.058  0.290   -0.099    0.030\n2     gpa  ~    ses   0.049 0.036  1.356  0.175   -0.022    0.120\n3     gpa  ~   prev   0.417 0.034 12.111  0.000    0.349    0.484\n4     gpa  ~    par   0.160 0.035  4.564  0.000    0.091    0.228\n5     par  ~   prev   0.345 0.033 10.462  0.000    0.280    0.409\n6     par  ~    ses   0.321 0.033  9.682  0.000    0.256    0.386\n7     par  ~ ethnic  -0.146 0.032 -4.553  0.000   -0.209   -0.083\n8    prev  ~    ses   0.395 0.030 13.137  0.000    0.336    0.454\n9    prev  ~ ethnic   0.198 0.032  6.224  0.000    0.136    0.261\n10    gpa ~~    gpa   0.724 0.027 27.093  0.000    0.671    0.776\n11    par ~~    par   0.719 0.027 26.858  0.000    0.666    0.771\n12   prev ~~   prev   0.752 0.026 28.610  0.000    0.701    0.804\n13 ethnic ~~ ethnic   1.000 0.000     NA     NA    1.000    1.000\n14 ethnic ~~    ses   0.333 0.031 10.673  0.000    0.272    0.394\n15    ses ~~    ses   1.000 0.000     NA     NA    1.000    1.000\n\n\n파라미터 추정 방식: lavaan website\n기본적으로 ML (Maximum Likelihood) 방법을 사용\n변경하려면, estimator 옵션을 사용\n\nULS: Unweighted Least Squares\nGLS: Generalized Least Squares (weight: \\(S^{-1}\\))\nWLS: Weighted Least Squares (also called ADF: Asymptotically Distribution-Free)\n여러 robust estimators\n\n정규분포 가정에 어긋나는 경우 대안들\n\n정규성을 가정하지 않는 estimator을 선택: WLS(Weighted Least Squares)\nML을 이용하지만, 표준오차만 수정하는 방법:\n\nMLM(ML with robust standard errors and a Satorra-Bentler scaled)\nMLR(ML with robust standard errors and a Yuan-Bentler scaled): missing 수용\n\nBootstrap; the Bollen-Stine bootstrap\n\n예를 들어, MLM을 estimator로 사용하려면,\n\n# MLM estimator\nsem_fit &lt;- sem(\n    model = mod,\n    data = nels_gpa,\n    fixed.x = FALSE,\n    estimator = \"MLM\"\n)\nsummary(sem_fit, standardized = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n\n  Number of observations                           811\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                 0.000       0.000\n  Degrees of freedom                                 0           0\n\nParameter Estimates:\n\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  gpa ~                                                                 \n    ethnic           -0.124    0.119   -1.044    0.297   -0.124   -0.035\n    ses               0.093    0.068    1.367    0.172    0.093    0.049\n    prev              0.070    0.006   11.554    0.000    0.070    0.417\n    par               0.292    0.068    4.282    0.000    0.292    0.160\n  par ~                                                                 \n    prev              0.032    0.003   10.240    0.000    0.032    0.345\n    ses               0.333    0.035    9.491    0.000    0.333    0.321\n    ethnic           -0.286    0.066   -4.333    0.000   -0.286   -0.146\n  prev ~                                                                \n    ses               4.431    0.363   12.211    0.000    4.431    0.395\n    ethnic            4.195    0.692    6.062    0.000    4.195    0.198\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  ethnic ~~                                                             \n    ses               0.103    0.012    8.819    0.000    0.103    0.333\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .gpa               1.521    0.073   20.851    0.000    1.521    0.724\n   .par               0.452    0.025   18.293    0.000    0.452    0.719\n   .prev             55.370    2.475   22.368    0.000   55.370    0.752\n    ethnic            0.164    0.008   19.705    0.000    0.164    1.000\n    ses               0.586    0.026   22.311    0.000    0.586    1.000\n\n\n\n부트스트랩(bootstrap) 방법을 사용한 표준오차 추정치\n\n# Bootstrap\nsem_fit &lt;- sem(\n    model = mod, \n    data = nels_gpa, \n    fixed.x = FALSE,\n    estimator = \"ML\",  # default\n    se = \"bootstrap\",  # standard errors\n    bootstrap = 1000,  # default; the number of bootstrap samples\n)\n\n\nparameterEstimates(sem_fit, standardized = \"std.all\", boot.ci.type = \"bca.simple\") |&gt; print()\n\n      lhs op    rhs    est    se      z pvalue ci.lower ci.upper std.all\n1     gpa  ~ ethnic -0.124 0.116 -1.070  0.285   -0.355    0.101  -0.035\n2     gpa  ~    ses  0.093 0.068  1.369  0.171   -0.053    0.221   0.049\n3     gpa  ~   prev  0.070 0.006 11.320  0.000    0.058    0.082   0.417\n4     gpa  ~    par  0.292 0.064  4.567  0.000    0.165    0.419   0.160\n5     par  ~   prev  0.032 0.003 10.095  0.000    0.026    0.038   0.345\n6     par  ~    ses  0.333 0.035  9.447  0.000    0.269    0.406   0.321\n7     par  ~ ethnic -0.286 0.064 -4.463  0.000   -0.401   -0.147  -0.146\n8    prev  ~    ses  4.431 0.352 12.601  0.000    3.706    5.038   0.395\n9    prev  ~ ethnic  4.195 0.695  6.035  0.000    2.841    5.632   0.198\n10    gpa ~~    gpa  1.521 0.073 20.919  0.000    1.400    1.687   0.724\n11    par ~~    par  0.452 0.024 18.790  0.000    0.406    0.501   0.719\n12   prev ~~   prev 55.370 2.391 23.156  0.000   50.717   60.103   0.752\n13 ethnic ~~ ethnic  0.164 0.008 19.459  0.000    0.148    0.180   1.000\n14 ethnic ~~    ses  0.103 0.012  8.808  0.000    0.082    0.129   0.333\n15    ses ~~    ses  0.586 0.027 21.712  0.000    0.534    0.642   1.000\n\n\n\n\n플롯 그리기\n\ntidySEM 참조\n\nlavaanExtra 참조\n\nsemPlot 참조\n\n\ntidySEM::graph_sem(sem_fit)\n\n\n\n\n\n\n\n\n\nlavaanExtra::nice_tidySEM(sem_fit)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nDefine a customized plot function using semPlot::semPaths()\nsemPaths2 &lt;- function(model, what = 'est', layout = \"tree\", rotation = 1) {\n  semPlot::semPaths(model, what = what, edge.label.cex = 1, edge.color = \"black\", layout = layout, rotation = rotation, weighted = FALSE, asize = 2, label.cex = 1, node.width = 1.2)\n}\n\n\n\n# semPaths2: a customized plot function using semPlot::semPaths()\nsemPaths2(sem_fit, layout = \"spring\", rotation = 1)\n\n\n\n\n\n\n\n\n\n\n간접효과 ses -&gt; par -&gt; gpa\n\nmod2 &lt;- \"\n  \n  gpa ~ b1*ethnic + b2*ses + b3*prev + b4*par\n  par ~ b5*prev + b6*ses + b7*ethnic\n  prev ~ b8*ses + b9*ethnic\n  \n  ses_par_gpa := b6*b4\n\"\n\nsem_fit2 &lt;- sem(model = mod2, data = nels_gpa, fixed.x = FALSE)\nsummary(sem_fit2, standardized = TRUE,  rsquare = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n\n  Number of observations                           811\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  gpa ~                                                                 \n    ethnic    (b1)   -0.124    0.117   -1.058    0.290   -0.124   -0.035\n    ses       (b2)    0.093    0.069    1.355    0.175    0.093    0.049\n    prev      (b3)    0.070    0.006   11.414    0.000    0.070    0.417\n    par       (b4)    0.292    0.064    4.531    0.000    0.292    0.160\n  par ~                                                                 \n    prev      (b5)    0.032    0.003   10.040    0.000    0.032    0.345\n    ses       (b6)    0.333    0.036    9.351    0.000    0.333    0.321\n    ethnic    (b7)   -0.286    0.063   -4.528    0.000   -0.286   -0.146\n  prev ~                                                                \n    ses       (b8)    4.431    0.362   12.236    0.000    4.431    0.395\n    ethnic    (b9)    4.195    0.684    6.135    0.000    4.195    0.198\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  ethnic ~~                                                             \n    ses               0.103    0.011    9.002    0.000    0.103    0.333\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .gpa               1.521    0.076   20.137    0.000    1.521    0.724\n   .par               0.452    0.022   20.137    0.000    0.452    0.719\n   .prev             55.370    2.750   20.137    0.000   55.370    0.752\n    ethnic            0.164    0.008   20.137    0.000    0.164    1.000\n    ses               0.586    0.029   20.137    0.000    0.586    1.000\n\nR-Square:\n                   Estimate\n    gpa               0.276\n    par               0.281\n    prev              0.248\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    ses_par_gpa       0.097    0.024    4.077    0.000    0.097    0.051\n\n\n\n\n\n모든 간접효과: ses -&gt; gpa\nmanymome 참조\n\nlibrary(manymome)\n\n# All indirect paths from x to y\npaths &lt;- all_indirect_paths(sem_fit,\n  x = \"ses\",\n  y = \"gpa\"\n)\npaths |&gt; print()\n\nCall: \nall_indirect_paths(fit = sem_fit, x = \"ses\", y = \"gpa\")\nPath(s): \n  path                     \n1 ses -&gt; par -&gt; gpa        \n2 ses -&gt; prev -&gt; gpa       \n3 ses -&gt; prev -&gt; par -&gt; gpa\n\n\n\n# Indirect effect estimates\nind_est &lt;- many_indirect_effects(paths,\n  fit = sem_fit, R = 1000,\n  boot_ci = TRUE, boot_type = \"bc\"\n)\n\nind_est |&gt; print()\n\n\n==  Indirect Effect(s)   ==\n                            ind CI.lo CI.hi Sig\nses -&gt; par -&gt; gpa         0.097 0.055 0.146 Sig\nses -&gt; prev -&gt; gpa        0.312 0.243 0.385 Sig\nses -&gt; prev -&gt; par -&gt; gpa 0.041 0.022 0.065 Sig\n\n - [CI.lo to CI.hi] are 95.0% bias-corrected confidence intervals by\n   nonparametric bootstrapping with 1000 samples.\n - The 'ind' column shows the indirect effects.\n \n\n\n\n# Standarized estimates\nind_est_std &lt;- many_indirect_effects(paths,\n  fit = sem_fit, R = 1000,\n  boot_ci = TRUE, boot_type = \"bc\",\n  standardized_x = TRUE,\n  standardized_y = TRUE\n)\n\nind_est_std |&gt; print()\n\n\n==  Indirect Effect(s) (Both x-variable(s) and y-variable(s) Standardized)  ==\n                            std CI.lo CI.hi Sig\nses -&gt; par -&gt; gpa         0.051 0.029 0.079 Sig\nses -&gt; prev -&gt; gpa        0.165 0.130 0.204 Sig\nses -&gt; prev -&gt; par -&gt; gpa 0.022 0.012 0.034 Sig\n\n - [CI.lo to CI.hi] are 95.0% bias-corrected confidence intervals by\n   nonparametric bootstrapping with 1000 samples.\n - std: The standardized indirect effects.",
    "crumbs": [
      "Keith's",
      "Path Analysis"
    ]
  },
  {
    "objectID": "contents/chap13.html#단순한-모형",
    "href": "contents/chap13.html#단순한-모형",
    "title": "Chapter 13-14. Path Analysis",
    "section": "단순한 모형",
    "text": "단순한 모형\n포화모형보다 단순한 모형인 경우 자유도 &gt; 0 이며, 이 경우 over-identified(과대 식별) 되었다고 말함.\n자유도가 높을수록 모형이 데이터와 잘 맞지 않은지에 대한 판별을 더 신뢰할 수 있음.\n\n공분산 기반 모형\n\nnels_cov &lt;- read_sav(\"data/chap 14 path via SEM/homework overid 2018.sav\")\nnels_cov |&gt; print()\n\n# A tibble: 8 × 7\n  rowtype_ varname_    Minority   FamBack   PreAch  Homework   Grades\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 n        \"\"         1000      1000      1000     1000      1000    \n2 corr     \"Minority\"    1        -0.304    -0.323   -0.0832   -0.132\n3 corr     \"FamBack\"    -0.304     1         0.479    0.263     0.275\n4 corr     \"PreAch\"     -0.323     0.479     1        0.288     0.489\n5 corr     \"Homework\"   -0.0832    0.263     0.288    1         0.281\n6 corr     \"Grades\"     -0.132     0.275     0.489    0.281     1    \n7 stddev   \"\"            0.419     0.831     8.90     0.806     1.48 \n8 mean     \"\"            0.272     0.0025   52.0      2.56      5.75 \n\n\n\nnels_cov2 &lt;- nels_cov[c(2:6), c(3:7)] |&gt; \n  as.matrix() |&gt; \n  lav_matrix_vechr(diagonal = TRUE) |&gt; \n  getCov(names = nels_cov$varname_[2:6], sds = nels_cov[7, 3:7] |&gt; as.double())\n\nnels_cov2 |&gt; print()\n\n            Minority    FamBack    PreAch    Homework      Grades\nMinority  0.17522596 -0.1057959 -1.202307 -0.02808143 -0.08141289\nFamBack  -0.10579592  0.6907272  3.544405  0.17637451  0.33815207\nPreAch   -1.20230704  3.5444051 79.170845  2.06906701  6.43516479\nHomework -0.02808143  0.1763745  2.069067  0.65011969  0.33545523\nGrades   -0.08141289  0.3381521  6.435165  0.33545523  2.18744100\n\n\n\nnels_cov2 |&gt; cov2cor() |&gt; print()\n\n         Minority FamBack  PreAch Homework  Grades\nMinority   1.0000 -0.3041 -0.3228  -0.0832 -0.1315\nFamBack   -0.3041  1.0000  0.4793   0.2632  0.2751\nPreAch    -0.3228  0.4793  1.0000   0.2884  0.4890\nHomework  -0.0832  0.2632  0.2884   1.0000  0.2813\nGrades    -0.1315  0.2751  0.4890   0.2813  1.0000\n\n\n이제 모형 적합도(model fit)에 대한 정보를 얻을 수 있음!\n\nmod_hw &lt;- \"\n  Grades ~ PreAch + Homework\n  Homework ~ PreAch + FamBack + Minority\n  PreAch ~ FamBack + Minority\n\"\n\nhw_fit &lt;- sem(\n  model = mod_hw, \n  sample.cov = nels_cov2, \n  sample.nobs = 1000, \n  fixed.x = FALSE\n)\n\nsummary(hw_fit, fit.measures = TRUE, estimates = FALSE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 2.169\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.338\n\nModel Test Baseline Model:\n\n  Test statistic                               721.651\n  Degrees of freedom                                 9\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       0.999\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7989.958\n  Loglikelihood unrestricted model (H1)      -7988.874\n                                                      \n  Akaike (AIC)                               16005.917\n  Bayesian (BIC)                             16069.718\n  Sample-size adjusted Bayesian (SABIC)      16028.429\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.009\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.064\n  P-value H_0: RMSEA &lt;= 0.050                    0.854\n  P-value H_0: RMSEA &gt;= 0.080                    0.010\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.008\n\n\nimplied/predicted correlation matrix\n\ninspect(hw_fit, \"cor.all\")[5:1, 5:1] |&gt; print(digits = 3)\n\n         Minority FamBack PreAch Homework Grades\nMinority   1.0000  -0.304 -0.323  -0.0832 -0.156\nFamBack   -0.3041   1.000  0.479   0.2632  0.253\nPreAch    -0.3228   0.479  1.000   0.2884  0.489\nHomework  -0.0832   0.263  0.288   1.0000  0.281\nGrades    -0.1563   0.253  0.489   0.2813  1.000\n\n\n\n공분산 분석\n\n# sample covariance matrix\nnels_cov2 |&gt; print(digits = 2)\n\n         Minority FamBack PreAch Homework Grades\nMinority    0.175   -0.11   -1.2   -0.028 -0.081\nFamBack    -0.106    0.69    3.5    0.176  0.338\nPreAch     -1.202    3.54   79.2    2.069  6.435\nHomework   -0.028    0.18    2.1    0.650  0.335\nGrades     -0.081    0.34    6.4    0.335  2.187\n\n\n\n# implied/predicted covariance matrix\nfitted(hw_fit)$cov[5:1, 5:1] |&gt; print(digits = 2)\n# 또는 inspect(hw_fit, \"cov.all\")\n\n         Minority FamBack PreAch Homework Grades\nMinority    0.175   -0.11   -1.2   -0.028 -0.097\nFamBack    -0.106    0.69    3.5    0.176  0.311\nPreAch     -1.201    3.54   79.1    2.067  6.429\nHomework   -0.028    0.18    2.1    0.649  0.335\nGrades     -0.097    0.31    6.4    0.335  2.185\n\n\nResiduals\n\n# Raw\nresiduals(hw_fit, type = \"raw\")$cov[5:1, 5:1] |&gt; print(digits = 2)\n\n         Minority FamBack PreAch Homework  Grades\nMinority    0.000   0.000      0  0.0e+00 1.5e-02\nFamBack     0.000   0.000      0  0.0e+00 2.7e-02\nPreAch      0.000   0.000      0  0.0e+00 0.0e+00\nHomework    0.000   0.000      0  0.0e+00 5.6e-17\nGrades      0.015   0.027      0  5.6e-17 0.0e+00\n\n\n\n# Standardized\nresiduals(hw_fit, type = \"standardized\")$cov[5:1, 5:1] |&gt; print(digits = 2)\n\n         Minority FamBack PreAch Homework  Grades\nMinority     0.00    0.00      0  0.0e+00 9.6e-01\nFamBack      0.00    0.00      0  0.0e+00 9.1e-01\nPreAch       0.00    0.00      0  0.0e+00 0.0e+00\nHomework     0.00    0.00      0  0.0e+00 5.6e-17\nGrades       0.96    0.91      0  5.6e-17 0.0e+00\n\n\n\n# Standardized like Mplus\nresiduals(hw_fit, type = \"standardized.mplus\")$cov[5:1, 5:1] |&gt; print(digits = 2)\n\n         Minority FamBack PreAch Homework  Grades\nMinority     0.00    0.00      0  0.0e+00 9.7e-01\nFamBack      0.00    0.00      0  0.0e+00 9.1e-01\nPreAch       0.00    0.00      0  0.0e+00 0.0e+00\nHomework     0.00    0.00      0  0.0e+00 5.6e-17\nGrades       0.97    0.91      0  5.6e-17 0.0e+00\n\n\n\n\n상관행렬 분석\n\n# sample correlation\ncov2cor(nels_cov2) |&gt; print(digits = 3)\n\n         Minority FamBack PreAch Homework Grades\nMinority   1.0000  -0.304 -0.323  -0.0832 -0.132\nFamBack   -0.3041   1.000  0.479   0.2632  0.275\nPreAch    -0.3228   0.479  1.000   0.2884  0.489\nHomework  -0.0832   0.263  0.288   1.0000  0.281\nGrades    -0.1315   0.275  0.489   0.2813  1.000\n\n\n\n# implied/predicted correlation\ninspect(hw_fit, \"cor.all\")[5:1, 5:1] |&gt; print(digits = 3)\n\n         Minority FamBack PreAch Homework Grades\nMinority   1.0000  -0.304 -0.323  -0.0832 -0.156\nFamBack   -0.3041   1.000  0.479   0.2632  0.253\nPreAch    -0.3228   0.479  1.000   0.2884  0.489\nHomework  -0.0832   0.263  0.288   1.0000  0.281\nGrades    -0.1563   0.253  0.489   0.2813  1.000\n\n\nResiduals\n\nresiduals(hw_fit, type = \"cor.bollen\")$cov[5:1, 5:1] |&gt; print(digits = 2)\n\n         Minority FamBack PreAch Homework  Grades\nMinority    0.000   0.000      0  0.0e+00 2.5e-02\nFamBack     0.000   0.000      0  0.0e+00 2.2e-02\nPreAch      0.000   0.000      0  0.0e+00 0.0e+00\nHomework    0.000   0.000      0  0.0e+00 5.6e-17\nGrades      0.025   0.022      0  5.6e-17 0.0e+00\n\n\n\n\n모형 적합도 지표들\n\n# select fit statistics\nfit_stats &lt;- c(\"rmr\", \"gfi\", \"nfi\", \"pnfi\", \"fmin\", \"rmsea\", \"aic\", \"ecvi\")\n\nfitMeasures(sem_fit, fit_stats) |&gt; print(nd = 3)\n\n      rmr       gfi       nfi      pnfi      fmin     rmsea       aic      ecvi \n    0.000     1.000     1.000     0.000     0.000     0.000 12495.216     0.037 \n\n\n\n# all fit statistics\nfitMeasures(sem_fit) |&gt; print(nd = 3)\n\n                 npar                  fmin                 chisq \n               15.000                 0.000                 0.000 \n                   df                pvalue        baseline.chisq \n                0.000                    NA               760.622 \n          baseline.df       baseline.pvalue                   cfi \n                9.000                 0.000                 1.000 \n                  tli                  nnfi                   rfi \n                1.000                 1.000                 1.000 \n                  nfi                  pnfi                   ifi \n                1.000                 0.000                 1.000 \n                  rni                  logl     unrestricted.logl \n                1.000             -6232.608             -6232.608 \n                  aic                   bic                ntotal \n            12495.216             12565.690               811.000 \n                 bic2                 rmsea        rmsea.ci.lower \n            12518.057                 0.000                 0.000 \n       rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue \n                0.000                 0.900                    NA \n       rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 \n                0.050                    NA                 0.080 \n                  rmr            rmr_nomean                  srmr \n                0.000                 0.000                 0.000 \n         srmr_bentler   srmr_bentler_nomean                  crmr \n                0.000                 0.000                 0.000 \n          crmr_nomean            srmr_mplus     srmr_mplus_nomean \n                0.000                 0.000                 0.000 \n                cn_05                 cn_01                   gfi \n                1.000                 1.000                 1.000 \n                 agfi                  pgfi                   mfi \n                1.000                 0.000                 1.000 \n                 ecvi \n                0.037 \n\n\n\n\nComparing Competing Models\n\n\n\n\nmod_hw_reduced &lt;- \"\n  Grades ~ PreAch + 0*Homework\n  Homework ~ 0*PreAch + FamBack + Minority\n  PreAch ~ FamBack + Minority\n\"\n\nhw_fit_reduced &lt;- sem(\n  model = mod_hw_reduced, \n  sample.cov = nels_cov2, \n  sample.nobs = 1000, \n  fixed.x = FALSE\n)\n\nsummary(hw_fit_reduced, fit.measures = TRUE, standardized = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 11 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                69.679\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               721.651\n  Degrees of freedom                                 9\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.908\n  Tucker-Lewis Index (TLI)                       0.793\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -8023.714\n  Loglikelihood unrestricted model (H1)      -7988.874\n                                                      \n  Akaike (AIC)                               16069.427\n  Bayesian (BIC)                             16123.412\n  Sample-size adjusted Bayesian (SABIC)      16088.476\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.128\n  90 Percent confidence interval - lower         0.103\n  90 Percent confidence interval - upper         0.155\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.999\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.071\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Grades ~                                                              \n    PreAch            0.081    0.005   17.728    0.000    0.081    0.489\n    Homework          0.000                               0.000    0.000\n  Homework ~                                                            \n    PreAch            0.000                               0.000    0.000\n    FamBack           0.254    0.031    8.186    0.000    0.254    0.262\n    Minority         -0.007    0.062   -0.109    0.913   -0.007   -0.003\n  PreAch ~                                                              \n    FamBack           4.496    0.305   14.750    0.000    4.496    0.420\n    Minority         -4.147    0.605   -6.852    0.000   -4.147   -0.195\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  FamBack ~~                                                            \n    Minority         -0.106    0.011   -9.200    0.000   -0.106   -0.304\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Grades            1.663    0.074   22.361    0.000    1.663    0.761\n   .Homework          0.604    0.027   22.361    0.000    0.604    0.931\n   .PreAch           58.190    2.602   22.361    0.000   58.190    0.736\n    FamBack           0.690    0.031   22.361    0.000    0.690    1.000\n    Minority          0.175    0.008   22.361    0.000    0.175    1.000\n\n\n\n모형 비교 1: initial vs. reduced model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlavTestLRT(hw_fit, hw_fit_reduced) |&gt; print()\n# anova(hw_fit, hw_fit_reduced)\n\n\nChi-Squared Difference Test\n\n               Df   AIC   BIC   Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nhw_fit          2 16006 16070  2.1687                                          \nhw_fit_reduced  4 16069 16123 69.6789      67.51 0.18098       2   2.19e-15 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n\n\nfit_stats &lt;- c(\"rmsea\", \"srmr\", \"cfi\", \"aic\")\nfitMeasures(hw_fit, fit_stats) |&gt; print(nd = 3)\nfitMeasures(hw_fit_reduced, fit_stats) |&gt; print(nd = 3)\n\n    rmsea      srmr       cfi       aic \n    0.009     0.008     1.000 16005.917 \n    rmsea      srmr       cfi       aic \n    0.128     0.071     0.908 16069.427 \n\n\n\nsemTools::compareFit(hw_fit, hw_fit_reduced) |&gt; summary()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n               Df   AIC   BIC   Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nhw_fit          2 16006 16070  2.1687                                          \nhw_fit_reduced  4 16069 16123 69.6789      67.51 0.18098       2   2.19e-15 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n####################### Model Fit Indices ###########################\n                 chisq df pvalue rmsea    cfi    tli  srmr        aic\nhw_fit          2.169†  2   .338 .009† 1.000† 0.999† .008† 16005.917†\nhw_fit_reduced 69.679   4   .000 .128   .908   .793  .071  16069.427 \n                      bic\nhw_fit         16069.718†\nhw_fit_reduced 16123.412 \n\n################## Differences in Fit Indices #######################\n                        df rmsea    cfi    tli  srmr   aic    bic\nhw_fit_reduced - hw_fit  2 0.119 -0.092 -0.206 0.063 63.51 53.695\n\n\n\n모형 비교 2: initial vs. larger model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmod_hw_larger &lt;- \"\n  Grades ~ PreAch + Homework + FamBack\n  Homework ~ PreAch + FamBack + Minority\n  PreAch ~ FamBack + Minority\n\"\n\nhw_fit_larger &lt;- sem(\n  model = mod_hw_larger, \n  sample.cov = nels_cov2, \n  sample.nobs = 1000, \n  fixed.x = FALSE\n)\n\n\nlavTestLRT(hw_fit_larger, hw_fit) |&gt; print()\n\n\nChi-Squared Difference Test\n\n              Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nhw_fit_larger  1 16007 16076 1.3304                                    \nhw_fit         2 16006 16070 2.1687    0.83821     0       1     0.3599\n\n\n\nfitMeasures(hw_fit, fit_stats) |&gt; print(nd = 3)\nfitMeasures(hw_fit_larger, fit_stats) |&gt; print(nd = 3)\n\n    rmsea      srmr       cfi       aic \n    0.009     0.008     1.000 16005.917 \n    rmsea      srmr       cfi       aic \n    0.018     0.008     1.000 16007.079 \n\n\n\nsemTools::compareFit(hw_fit, hw_fit_larger) |&gt; summary(fit.measures = fit_stats)\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n              Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nhw_fit_larger  1 16007 16076 1.3304                                    \nhw_fit         2 16006 16070 2.1687    0.83821     0       1     0.3599\n\n####################### Model Fit Indices ###########################\n              rmsea  srmr    cfi        aic\nhw_fit_larger .018  .008† 1.000  16007.079 \nhw_fit        .009† .008  1.000† 16005.917†\n\n################## Differences in Fit Indices #######################\n                        rmsea  srmr cfi    aic\nhw_fit - hw_fit_larger -0.009 0.001   0 -1.162",
    "crumbs": [
      "Keith's",
      "Path Analysis"
    ]
  },
  {
    "objectID": "contents/chap13.html#more-complex-models",
    "href": "contents/chap13.html#more-complex-models",
    "title": "Chapter 13-14. Path Analysis",
    "section": "More Complex Models",
    "text": "More Complex Models\n\nEquivalent and Nonequivalent Models\n\n모형 적합도가 동일한 모형들(동등모형)들이 다수 혹은 무수한 많이 존재할 수 있음.\n\n충분히 가능성이 있는 동등모형들에 대해 고민할 필요가 있음.\n\nNearly-equivalent models 또는 존재하기에 구조모형에 대해서 신중할 필요가 있음.\n\n\n\n\n\n\n\n동등한 모형을 제거하기 위한 제안들\n\n\n\n\n\n\nSource: p. 196, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n\n번역 by Google Translate\n\n실험적 또는 종단적 설계에서 시간적 선행성은 결과 이전에 조작되거나 측정된 원인 간의 직접적 효과를 역전시키는 것을 배제합니다(역인과성 없음 또는 시간적 역방향 인과성).\n횡단면 설계에서 분석된 모델의 일부가 이전 실험적 또는 종단적 설계에서 평가된 경우 해당 연구의 결과는 일부 인과적 순서를 배제하는 데 도움이 될 수 있습니다.\n인구 통계적 특성이나 안정적인 성격 특성과 같은 특정 변수는 내생적일 가능성이 낮거나 불가능할 수 있습니다. 예를 들어, 태도 변수에서 연대기적 연령으로의 직접적 효과를 지정하는 것은 비논리적입니다.\n변수의 특성을 감안할 때 일부 인과적 순서는 이론적으로 의심스러울 수 있습니다. 예를 들어, 부모의 IQ는 그 반대보다 자녀의 IQ에 영향을 미칠 가능성이 더 높을 수 있습니다.\n매개 변수로 지정된 변수는 잠재적으로 변경 가능해야 합니다. 그렇지 않으면 매개 변수가 될 가능성이 낮습니다. 예를 들어, 안정적이고 비교적 변하지 않는 특성으로 개념화된 변수는 원인으로 지정할 수 있지만 매개 변수로 지정할 수는 없습니다(주제 상자 7.1).\n일부 매개변수를 이론이나 이전 연구 결과와 호환되는 0이 아닌 값으로 고정하면 해당 매개변수를 포함하는 동등한 모델이 배제됩니다. 이는 이러한 고정된 값이 지정된 경로나 변수의 임의적 재구성에 적합하지 않기 때문입니다(Mulaik, 2009b).\n모델의 다른 몇 가지 변수와만 선택적으로 연관된 변수를 추가하면 동등한 버전의 수를 줄이는 데 도움이 될 수 있습니다. 모델에 X → Y 경로가 있다고 가정합니다. X를 직접 유발하지만 Y를 유발하지 않는 것으로 추정되는 변수를 추가하면 X가 Y와 비교하여 고유한 부모를 가지므로 X와 Y가 모두 내생적이라면 규칙 11.2가 적용되지 않습니다. 이 전략은 일반적으로 데이터를 수집하기 전에 구현해야 합니다.\n동등한 모델은 변수 수준에서 동일한 잔차를 갖지만 사례 수준의 잔차는 이러한 모델에 따라 달라질 수 있습니다. Raykov와 Penev(2001)는 더 낮은 표준화된 평균 개별 사례 잔차가 있는 모델이 더 높은 평균을 가진 동등한 버전보다 선호될 것이라고 제안했습니다. 복잡한 점은 잠재 변수가 있는 구조적 모델이 요인 점수 불확정성으로 인해 개별 사례에 대한 고유한 예측을 생성하지 않는다는 것입니다. 이 개념은 14장에서 설명합니다. Raykov-Penev 방법을 적용하는 것은 사례 잔차가 회귀 잔차와 더 직접적으로 유사한 명백한 변수 경로 모델에 더 간단합니다.\n\n(원문)\n\nTemporal precedence in experimental or longitudinal designs precludes reversing direct effects between causes manipulated or measured before outcomes (no retrocausality, or backwards causation in time).\nIf any part of a model analyzed in a cross-sectional design has been evaluated in prior experimental or longitudinal designs, results from those studies may help to rule out some causal orderings.\nCertain variables, such as demographic characteristics or stable personality characteristics, may be unlikely or impossible to be endogenous. For example, specifying a direct effect from an attitudinal variable to chronological age in years is illogical.\nSome causal orderings may be theoretically doubtful, given the nature of the variables. For example, parental IQ may be more likely to affect child IQ than the reverse.\nVariables specified as mediators must be potentially changeable; otherwise, they are unlikely mediators. For example, variables conceptualized as stable, relatively unchanging traits could be specified as causes, but not mediators (Topic Box 7.1).\nFixing some parameters to nonzero values compatible with theory or results from prior studies would rule out equivalent models involving those parameters. This is because such fixed values are not suitable for arbitrary reconfigurations of the paths or variables for which they were specified (Mulaik, 2009b).\nAdding variables that are selectively associated with just a few of the other variables in the model can help to reduce the number of equivalent versions. Suppose that a model has the path X → Y. Adding a variable presumed to directly cause X but not Y means that X has a unique parent compared to Y, so Rule 11.2 would not apply, if both X and Y were endogenous. This strategy must usually be implemented before the data are collected.\nAlthough equivalent models have identical residuals at the variable level, residuals at the case level can vary over such models. Raykov and Penev (2001) suggested that models with the lower standardized average individual case residuals would be preferred over equivalent versions with higher averages. A complication is that structural models with latent variables do not generate unique predictions for individual cases due to factor score indeterminacy, a concept explained Chapter 14. Applying the Raykov–Penev method is more straightforward for manifest-variable path models, where case residuals are more directly analogous to regression residuals.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFor a just-identified model,\n\n완전한 모델이므로 어떻게 대체해도 모형은 동일함\n\n경로 a → b 는 경로 b → a 로 대체\n또는 상관 관계로 대체; a ↔︎ b\n내생변수인 경우 disturbances가 상관 관계로 대체\n\n\nFor overidentified models,\n\njust-identified 부분에 대해서는 동일한 룰이 적용됨\n\nOriginal 모형에서 Homework에서 Previous Achievement로 경로를 뒤집어도 동일한 모형이 됨\n\n그 외의 경우, a, b가 동일한 원인들을 갖는다면, a, b의 경로 방향이 반대로 대체될 수 있음. 또는 상관관계로 대체될 수 있음\n\nHomework에서 Grades로의 경로 뒤집거나 d2와 d3의 상관 관계로 대체할 수 없음\nGrades와 Homework이 동일한 원인들을 갖지 않기 때문임\n\na, b가 동일한 원일들을 갖지 않는다면,\n\na → b의 경로는 b의 원인들이 a의 모든 원인들을 포함하면 대체(경로 또는 상관)될 수 있음\n\n\n\n\n\n\n\n예를 들어, original 모형과 모형 B(homework와 previous achievement가 뒤집힌)를 비교하면, \n\nmod_hw_reversed &lt;- \"\n  Grades ~ PreAch + Homework\n  Homework ~ FamBack + Minority\n  PreAch ~ Homework + FamBack + Minority\n\"\nhw_fit_reversed &lt;- sem(model = mod_hw_reversed, sample.cov = nels_cov2, sample.nobs = 1000, fixed.x = FALSE)\n\n\n\n\nsemTools::compareFit(hw_fit, hw_fit_reversed) |&gt; summary(fit.measures = fit_stats)\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n                Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nhw_fit           2 16006 16070 2.1687                                    \nhw_fit_reversed  2 16006 16070 2.1687          0     0       0           \n\n####################### Model Fit Indices ###########################\n                rmsea  srmr    cfi        aic\nhw_fit          .009† .008† 1.000† 16005.917†\nhw_fit_reversed .009† .008  1.000† 16005.917†\n\n################## Differences in Fit Indices #######################\n                         rmsea srmr cfi aic\nhw_fit_reversed - hw_fit     0    0   0   0\n\n\n\nDirectionality Revisited\n중요한 경로 지정의 오류에 대해 모형 적합도 지표는 도움이 되는가?\nGrades → Homework의 경로가 바뀌면, 비동등 모형이 되는데, 적합도 지표의 차이를 보면,\n\n\nmod_hw_reversed2 &lt;- \"\n  Grades ~ PreAch\n  Homework ~ PreAch + FamBack + Minority + Grades\n  PreAch ~ FamBack + Minority\n\"\nhw_fit_reversed2 &lt;- sem(model = mod_hw_reversed2, sample.cov = nels_cov2, sample.nobs = 1000, fixed.x = FALSE)\n\n\nfit_stats &lt;- c(\"rmsea\", \"srmr\", \"cfi\", \"tli\", \"aic\", \"bic\")\nfitMeasures(hw_fit, fit_stats) |&gt; print(nd = 3)\nfitMeasures(hw_fit_reversed2, fit_stats) |&gt; print(nd = 3)\n\n    rmsea      srmr       cfi       tli       aic       bic \n    0.009     0.008     1.000     0.999 16005.917 16069.718 \n    rmsea      srmr       cfi       tli       aic       bic \n    0.036     0.013     0.996     0.983 16008.367 16072.167 \n\n\n\n\nNonrecursive Models\n\n\n\nGet the covariance matrix\nlower &lt;- \"\n  1\n  -0.181 1\n  0.09 0.05 1\n  0.05 0.09 -0.181 1\n  -0.2 0.32 -0.115 0.09 1\n  -0.076 0.087 -0.34 0.2 0.598 1\n\"\nsd &lt;- \"8.7 8.1 10.4 7.3 9.7 8\"\ncov &lt;- getCov(lower, sd = sd, names = c('mper_con', 'man_self', 'wper_con', 'wom_self', 'm_trust', 'w_trust'))\n\n\n\nmod_trust &lt;- \"\n  # regression\n  m_trust ~ w_trust + mper_con + man_self\n  w_trust ~ m_trust + wper_con + wom_self\n\n  # covariance\n  wper_con ~~ 0*man_self + wom_self + mper_con\n  mper_con ~~ 0*wom_self + man_self\n  wom_self ~~ man_self\n  m_trust ~~ w_trust\n\"\ntrust_fit &lt;- sem(model = mod_trust, sample.cov = cov, sample.nobs = 300)\nsummary(trust_fit, fit.measures = T, standardized = \"std.all\") |&gt; print()\n\nlavaan 0.6-19 ended normally after 90 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        17\n\n  Number of observations                           300\n\nModel Test User Model:\n                                                      \n  Test statistic                                 1.438\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.838\n\nModel Test Baseline Model:\n\n  Test statistic                               255.291\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.040\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6305.088\n  Loglikelihood unrestricted model (H1)      -6304.369\n                                                      \n  Akaike (AIC)                               12644.176\n  Bayesian (BIC)                             12707.140\n  Sample-size adjusted Bayesian (SABIC)      12653.226\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.050\n  P-value H_0: RMSEA &lt;= 0.050                    0.950\n  P-value H_0: RMSEA &gt;= 0.080                    0.008\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.019\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n  m_trust ~                                                    \n    w_trust           0.422    0.149    2.834    0.005    0.348\n    mper_con         -0.140    0.052   -2.690    0.007   -0.125\n    man_self          0.320    0.056    5.671    0.000    0.267\n  w_trust ~                                                    \n    m_trust           0.233    0.109    2.142    0.032    0.282\n    wper_con         -0.220    0.038   -5.788    0.000   -0.285\n    wom_self          0.135    0.052    2.569    0.010    0.123\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n  man_self ~~                                                  \n    wper_con          0.000                               0.000\n  wper_con ~~                                                  \n    wom_self        -14.475    4.408   -3.284    0.001   -0.191\n  mper_con ~~                                                  \n    wper_con          9.759    5.063    1.927    0.054    0.108\n    wom_self          0.000                               0.000\n    man_self        -13.436    4.092   -3.284    0.001   -0.191\n  man_self ~~                                                  \n    wom_self          6.378    3.309    1.927    0.054    0.108\n .m_trust ~~                                                   \n   .w_trust           1.957   10.044    0.195    0.846    0.041\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n   .m_trust          56.819    6.825    8.326    0.000    0.602\n   .w_trust          40.379    5.635    7.166    0.000    0.630\n    mper_con         75.586    6.169   12.253    0.000    1.000\n    man_self         65.520    5.347   12.253    0.000    1.000\n    wper_con        108.012    8.815   12.253    0.000    1.000\n    wom_self         53.217    4.343   12.253    0.000    1.000\n\n\n\n\n\nLongitudinal Models\nDo job stress and emotional exhaustion (or burnout) have reciprocal effects?\nEstimated via longitudinal data.\n\n\n\n\n\n\nThe study assessed stress and the three components of burnout (emotional exhaustion, depersonalisation, and low personal accomplishment) in a 3-year longitudinal study of a representative sample of 331 UK doctors.\n\nMcManus, I. C., Winder, B. C., & Gordon, D. (2002). The causal links between stress and burnout in a longitudinal study of UK doctors. The Lancet, 359(9323), 2089-2090.\n\n\n\n\n\n\n\nGet the covariance matrix\nlower &lt;- \"\n  21.623                            \n  2.052 9.797                       \n  10.98 4.548 19.625                    \n  0.186 -0.027 0.76 8.18                \n  10.614 2.773 8.911 -0.789 20.43           \n  0.808 6.377 2.756 -0.131 3.46 9.302       \n  7.301 3.795 11.361 -0.024 9.939 4.889 16.892  \n  -0.374 -0.772 0.037 4.737 -2.729 -0.777 -1.059 7.673\n\"\ncov &lt;- getCov(lower, names = c(\"Stress_2\", \"Depersonal_2\", \"EExhaust_2\", \"PAcomplish_2\",  \"Stress_1\", \"Depersonal_1\", \"EExhaust_1\", \"PAcomplish_1\"))\n\n\n\nmod_stress &lt;- \"\n  # regression\n  Stress_2 ~ Depersonal_1 + Stress_1 + EExhaust_2\n  EExhaust_2 ~ Depersonal_1 + Stress_1 + PAcomplish_1\n  \n  # covariance\n\"\nstress_fit &lt;- sem(model = mod_stress, sample.cov = cov, sample.nobs = 331, fixed.x = FALSE)\nsummary(stress_fit, fit.measures = T, standardized = \"std.all\") |&gt; print()\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        14\n\n  Number of observations                           331\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.790\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.374\n\nModel Test Baseline Model:\n\n  Test statistic                               243.540\n  Degrees of freedom                                 7\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.006\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4412.408\n  Loglikelihood unrestricted model (H1)      -4412.013\n                                                      \n  Akaike (AIC)                                8852.816\n  Bayesian (BIC)                              8906.046\n  Sample-size adjusted Bayesian (SABIC)       8861.637\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.139\n  P-value H_0: RMSEA &lt;= 0.050                    0.544\n  P-value H_0: RMSEA &gt;= 0.080                    0.276\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.010\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n  Stress_2 ~                                                   \n    Depersonal_1     -0.173    0.068   -2.538    0.011   -0.114\n    Stress_1          0.367    0.050    7.287    0.000    0.357\n    EExhaust_2        0.417    0.051    8.214    0.000    0.397\n  EExhaust_2 ~                                                 \n    Depersonal_1      0.149    0.073    2.047    0.041    0.103\n    Stress_1          0.434    0.050    8.642    0.000    0.443\n    PAcomplish_1      0.174    0.080    2.188    0.029    0.109\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n  Depersonal_1 ~~                                              \n    Stress_1          3.450    0.779    4.429    0.000    0.251\n    PAcomplish_1     -0.775    0.465   -1.666    0.096   -0.092\n  Stress_1 ~~                                                  \n    PAcomplish_1     -2.721    0.702   -3.875    0.000   -0.218\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n   .Stress_2         13.248    1.030   12.865    0.000    0.615\n   .EExhaust_2       15.292    1.189   12.865    0.000    0.782\n    Depersonal_1      9.274    0.721   12.865    0.000    1.000\n    Stress_1         20.368    1.583   12.865    0.000    1.000\n    PAcomplish_1      7.650    0.595   12.865    0.000    1.000",
    "crumbs": [
      "Keith's",
      "Path Analysis"
    ]
  },
  {
    "objectID": "contents/chap13.html#exercises",
    "href": "contents/chap13.html#exercises",
    "title": "Chapter 13-14. Path Analysis",
    "section": "Exercises",
    "text": "Exercises\n연습문제 14장 5번; Henry, Tolan, and Gorman-Smith (2001)\n\nFully mediated model\nPartially mediated model: peer delinquency → individual violence is set to zero\n\n학생들의 delinquency와 violence 각각에 대해 더 중요하게 미치는 변수는 무엇인가?\n각각의 변수에 미치는 효과의 크기는? (R-squared)\nFamily가 미치는 간접효과는 어떠한가?\nFully vs. partially mediated model 비교\n\n\n\nhenri &lt;- haven::read_sav(\"data/chap 14 path via SEM/henry et al.sav\")\nhenri |&gt; print()\n\n# A tibble: 246 × 5\n  i_delin i_violen p_delin p_violen family\n    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1  1.47     0.773    1.25    1.29   -0.652\n2  3.19     1.13     0.745   1.00   -2.66 \n3 -0.0792   0.0759   0.276  -0.0967  1.71 \n4  3.83     2.45     0.279  -0.276   1.69 \n5  0.969   -0.0923  -0.884  -0.502   4.61 \n6 -0.0740   0.321   -0.755  -0.978  -0.844\n# ℹ 240 more rows\n\n\n\npsych::lowerCor(henri)\n\n         i_dln i_vln p_dln p_vln famly\ni_delin   1.00                        \ni_violen  0.43  1.00                  \np_delin   0.28  0.27  1.00            \np_violen  0.32  0.30  0.68  1.00      \nfamily   -0.02 -0.17 -0.15 -0.14  1.00\n\n\n\n# Fully mediated model\nmod_henri &lt;- \"\n  # regression\n  i_violen ~ dv*p_delin + vv*p_violen\n  i_delin ~ dd*p_delin + vd*p_violen\n  p_violen ~ fv*family\n  p_delin ~ fd*family\n\n  # covariance\n  p_delin ~~ p_violen\n  i_delin ~~ i_violen\n\n  # indirect effect\n  i_violen_ind := dv*fd + vv*fv\n  i_delin_ind := vd*fv + dd*fd\n\"\nhenri_fit &lt;- sem(model = mod_henri, data = henri, fixed.x = FALSE)\nsummary(henri_fit, fit.measures = TRUE, standardized = \"std.all\") |&gt; print()\n\nlavaan 0.6-19 ended normally after 24 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                           246\n\nModel Test User Model:\n                                                      \n  Test statistic                                 5.713\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.057\n\nModel Test Baseline Model:\n\n  Test statistic                               253.063\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.985\n  Tucker-Lewis Index (TLI)                       0.924\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1576.587\n  Loglikelihood unrestricted model (H1)      -1573.731\n                                                      \n  Akaike (AIC)                                3179.175\n  Bayesian (BIC)                              3224.744\n  Sample-size adjusted Bayesian (SABIC)       3183.535\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.087\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.174\n  P-value H_0: RMSEA &lt;= 0.050                    0.169\n  P-value H_0: RMSEA &gt;= 0.080                    0.642\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.032\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n  i_violen ~                                                   \n    p_delin   (dv)    0.228    0.139    1.640    0.101    0.135\n    p_violen  (vv)    0.359    0.145    2.472    0.013    0.204\n  i_delin ~                                                    \n    p_delin   (dd)    0.378    0.267    1.418    0.156    0.116\n    p_violen  (vd)    0.812    0.279    2.914    0.004    0.239\n  p_violen ~                                                   \n    family    (fv)   -0.026    0.012   -2.137    0.033   -0.135\n  p_delin ~                                                    \n    family    (fd)   -0.029    0.012   -2.341    0.019   -0.148\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n .p_violen ~~                                                  \n   .p_delin           0.158    0.018    8.759    0.000    0.673\n .i_violen ~~                                                  \n   .i_delin           0.450    0.083    5.389    0.000    0.366\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n   .i_violen          0.641    0.058   11.091    0.000    0.902\n   .i_delin           2.357    0.213   11.091    0.000    0.891\n   .p_violen          0.225    0.020   11.091    0.000    0.982\n   .p_delin           0.245    0.022   11.091    0.000    0.978\n    family            6.426    0.579   11.091    0.000    1.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n    i_violen_ind     -0.016    0.007   -2.180    0.029   -0.048\n    i_delin_ind      -0.032    0.015   -2.172    0.030   -0.049\n\n\n\n\n# Partially mediated model with a direct effect\nmod_henri_partial &lt;- \"\n  # regression\n  i_violen ~ 0*p_delin + vv*p_violen + fvd*family\n  i_delin ~ dd*p_delin + vd*p_violen + fdd*family\n  p_violen ~ fv*family\n  p_delin ~ fd*family\n\n  # covariance\n  p_delin ~~ p_violen\n  i_delin ~~ i_violen\n\n  # indirect effect\n  i_violen_ind := 0*fd + vv*fv\n  i_delin_ind := vd*fv + dd*fd\n\n  # total effect\n  i_violen_total := 0*fd + vv*fv + fvd\n  i_delin_total := vd*fv + dd*fd + fdd\n\"\nhenri_fit_partial &lt;- sem(model = mod_henri_partial, data = henri, fixed.x = FALSE)\nsummary(henri_fit_partial, fit.measures = TRUE, standardized = \"std.all\") |&gt; print()\n\nlavaan 0.6-19 ended normally after 33 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        14\n\n  Number of observations                           246\n\nModel Test User Model:\n                                                      \n  Test statistic                                 2.225\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.136\n\nModel Test Baseline Model:\n\n  Test statistic                               253.063\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.995\n  Tucker-Lewis Index (TLI)                       0.950\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1574.843\n  Loglikelihood unrestricted model (H1)      -1573.731\n                                                      \n  Akaike (AIC)                                3177.686\n  Bayesian (BIC)                              3226.761\n  Sample-size adjusted Bayesian (SABIC)       3182.381\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.071\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.200\n  P-value H_0: RMSEA &lt;= 0.050                    0.251\n  P-value H_0: RMSEA &gt;= 0.080                    0.591\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.018\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n  i_violen ~                                                   \n    p_delin           0.000                               0.000\n    p_violen  (vv)    0.491    0.107    4.578    0.000    0.279\n    family   (fvd)   -0.043    0.020   -2.104    0.035   -0.128\n  i_delin ~                                                    \n    p_delin   (dd)    0.240    0.248    0.966    0.334    0.074\n    p_violen  (vd)    0.923    0.270    3.421    0.001    0.272\n    family   (fdd)    0.018    0.039    0.463    0.644    0.028\n  p_violen ~                                                   \n    family    (fv)   -0.026    0.012   -2.137    0.033   -0.135\n  p_delin ~                                                    \n    family    (fd)   -0.029    0.012   -2.341    0.019   -0.148\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n .p_violen ~~                                                  \n   .p_delin           0.158    0.018    8.759    0.000    0.673\n .i_violen ~~                                                  \n   .i_delin           0.459    0.083    5.501    0.000    0.375\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n   .i_violen          0.636    0.057   11.091    0.000    0.896\n   .i_delin           2.358    0.213   11.091    0.000    0.895\n   .p_violen          0.225    0.020   11.091    0.000    0.982\n   .p_delin           0.245    0.022   11.091    0.000    0.978\n    family            6.426    0.579   11.091    0.000    1.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n    i_violen_ind     -0.013    0.006   -1.936    0.053   -0.038\n    i_delin_ind      -0.031    0.014   -2.107    0.035   -0.048\n    i_violen_total   -0.055    0.021   -2.637    0.008   -0.166\n    i_delin_total    -0.012    0.041   -0.305    0.760   -0.019\n\n\n\n\nsemTools::compareFit(henri_fit, henri_fit_partial) |&gt; summary()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n                  Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)\nhenri_fit_partial  1 3177.7 3226.8 2.2247                                      \nhenri_fit          2 3179.2 3224.7 5.7134     3.4887 0.10058       1    0.06179\n                   \nhenri_fit_partial  \nhenri_fit         .\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n####################### Model Fit Indices ###########################\n                   chisq df pvalue rmsea   cfi   tli  srmr       aic       bic\nhenri_fit_partial 2.225†  1   .136 .071† .995† .950† .018† 3177.686† 3226.761 \nhenri_fit         5.713   2   .057 .087  .985  .924  .032  3179.175  3224.744†\n\n################## Differences in Fit Indices #######################\n                              df rmsea   cfi    tli  srmr   aic    bic\nhenri_fit - henri_fit_partial  1 0.016 -0.01 -0.026 0.013 1.489 -2.017\n\n\n\nBootstrap으로 표준오차를 추정하려면, se =\"bootstrap\"\n\nhenri_fit_partial_boot &lt;- sem(\n  model = mod_henri_partial,\n  data = henri,\n  fixed.x = FALSE,\n  se = \"bootstrap\",\n  iseed = 123 # seed for reproducibility\n)\n\n\nparameterEstimates(\n  henri_fit_partial_boot, \n  boot.ci.type = \"bca.simple\", # bias-corrected\n  standardized = \"std.all\" # standardized estimates\n) |&gt; filter(label != \"\") |&gt; print()\n\n              lhs op             rhs          label    est    se      z pvalue\n1        i_violen  ~        p_violen             vv  0.491 0.103  4.747  0.000\n2        i_violen  ~          family            fvd -0.043 0.021 -1.988  0.047\n3         i_delin  ~         p_delin             dd  0.240 0.231  1.039  0.299\n4         i_delin  ~        p_violen             vd  0.923 0.247  3.741  0.000\n5         i_delin  ~          family            fdd  0.018 0.038  0.477  0.633\n6        p_violen  ~          family             fv -0.026 0.013 -1.906  0.057\n7         p_delin  ~          family             fd -0.029 0.014 -2.152  0.031\n8    i_violen_ind :=      0*fd+vv*fv   i_violen_ind -0.013 0.007 -1.744  0.081\n9     i_delin_ind :=     vd*fv+dd*fd    i_delin_ind -0.031 0.016 -1.919  0.055\n10 i_violen_total :=  0*fd+vv*fv+fvd i_violen_total -0.055 0.022 -2.452  0.014\n11  i_delin_total := vd*fv+dd*fd+fdd  i_delin_total -0.012 0.039 -0.322  0.747\n   ci.lower ci.upper std.all\n1     0.281    0.693   0.279\n2    -0.085    0.004  -0.128\n3    -0.213    0.686   0.074\n4     0.407    1.388   0.272\n5    -0.056    0.093   0.028\n6    -0.051    0.002  -0.135\n7    -0.055   -0.002  -0.148\n8    -0.029   -0.001  -0.038\n9    -0.065    0.000  -0.048\n10   -0.098   -0.006  -0.166\n11   -0.082    0.066  -0.019\n\n\nmanymome 패키지를 사용하면,\n간접효과 family → i_delin의 경우\n\nlibrary(manymome)\n# All indirect paths from x to y\npaths &lt;- all_indirect_paths(henri_fit_partial,\n  x = \"family\",\n  y = \"i_delin\",\n)\npaths |&gt; print()\n\nCall: \nall_indirect_paths(fit = henri_fit_partial, x = \"family\", y = \"i_delin\")\nPath(s): \n  path                         \n1 family -&gt; p_violen -&gt; i_delin\n2 family -&gt; p_delin -&gt; i_delin \n\n\n\n# Indirect effect estimates\nind_est &lt;- many_indirect_effects(paths,\n  fit = henri_fit_partial, R = 1000,\n  boot_ci = TRUE, boot_type = \"bc\",\n  standardized_x = TRUE,\n  standardized_y = TRUE\n)\nind_est |&gt; print()\n\n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100% elapsed=06s  \n\n==  Indirect Effect(s) (Both x-variable(s) and y-variable(s) Standardized)  ==\n                                 std  CI.lo  CI.hi Sig\nfamily -&gt; p_violen -&gt; i_delin -0.037 -0.097 -0.002 Sig\nfamily -&gt; p_delin -&gt; i_delin  -0.011 -0.045  0.007    \n\n - [CI.lo to CI.hi] are 95.0% bias-corrected confidence intervals by\n   nonparametric bootstrapping with 1000 samples.\n - std: The standardized indirect effects.\n \n\n\n\n# total indirect effect\nind_est[[1]] + ind_est[[2]]\n\n\n== Indirect Effect (Both ‘family’ and ‘i_delin’ Standardized) ==\n                                                   \n Path:                family -&gt; p_violen -&gt; i_delin\n Path:                family -&gt; p_delin -&gt; i_delin \n Function of Effects: -0.048                       \n 95.0% Bootstrap CI:  [-0.101 to 0.001]            \n\nComputation of the Function of Effects:\n (family-&gt;p_violen-&gt;i_delin)\n+(family-&gt;p_delin-&gt;i_delin) \n\n\nBias-corrected confidence interval formed by nonparametric\nbootstrapping with 1000 bootstrap samples.\n\n\n플롯\n\nsemPaths2 &lt;- function(model, what = 'std', layout = \"tree\", rotation = 1) {\n  semPlot::semPaths(model, what = what, edge.label.cex = 1, edge.color = \"black\", layout = layout, rotation = rotation, weighted = FALSE, asize = 2, label.cex = 1, node.width = 1.5)\n}\n\n\nsemPaths2(henri_fit, layout = \"tree\", rotation = 1)\n\n\n\n\n\n\n\n\n\nsemPaths2(henri_fit_parital, layout = \"tree\", rotation = 1)",
    "crumbs": [
      "Keith's",
      "Path Analysis"
    ]
  },
  {
    "objectID": "contents/Kline/kchap21.html",
    "href": "contents/Kline/kchap21.html",
    "title": "Chapter 21. Latent Growth Curve Models",
    "section": "",
    "text": "Load libraries\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\n\n\n\n참고 문헌\n\n\nGrimm, K. J., Ram, N., & Estabrook, R. (2017). Growth modeling: Structural equation and multilevel modeling approaches. Guilford Press.\n\nLittle, T. D. (2024). Longitudinal structural equation modeling (2e). Guilford Press.\n\nNewsom, J. (2024). Longitudinal structural equation modeling: A comprehensive introduction (2e). Routledge.\n\n\n\n# read in summary statistics\nkimspoonLower.cor &lt;- '\n1.00\n .35 1.00\n .28  .35 1.00\n .29  .40  .45 1.00\n .00  .07  .14  .04 1.00\n .00 -.09 -.12 -.10  .26 1.00 '\n \n# name the variables and convert to full correlation matrix\nkimspoon.cor &lt;- lavaan::getCov(kimspoonLower.cor, names = c(\"R1\", \"R2\",\n \"R3\", \"R4\", \"abuse\", \"neglect\"))\n \n# display the correlations\nkimspoon.cor |&gt; print()\n\n          R1    R2    R3    R4 abuse neglect\nR1      1.00  0.35  0.28  0.29  0.00    0.00\nR2      0.35  1.00  0.35  0.40  0.07   -0.09\nR3      0.28  0.35  1.00  0.45  0.14   -0.12\nR4      0.29  0.40  0.45  1.00  0.04   -0.10\nabuse   0.00  0.07  0.14  0.04  1.00    0.26\nneglect 0.00 -0.09 -0.12 -0.10  0.26    1.00\n\n\n\n# add the standard deviations and convert to covariances\nkimspoon.cov &lt;- lavaan::cor2cov(kimspoon.cor, sds = c(.05,.77,.76,1.15,\n 7.75,4.09))\n\n# create mean vector\nkimspoon.mean = c(.04,.61,.57,.83,7.17,3.03)\n \n# display the covariances and means\nkimspoon.cov |&gt; print()\nkimspoon.mean |&gt; print()\n\n              R1        R2        R3        R4     abuse   neglect\nR1      0.002500  0.013475  0.010640  0.016675  0.000000  0.000000\nR2      0.013475  0.592900  0.204820  0.354200  0.417725 -0.283437\nR3      0.010640  0.204820  0.577600  0.393300  0.824600 -0.373008\nR4      0.016675  0.354200  0.393300  1.322500  0.356500 -0.470350\nabuse   0.000000  0.417725  0.824600  0.356500 60.062500  8.241350\nneglect 0.000000 -0.283437 -0.373008 -0.470350  8.241350 16.728100\n[1] 0.04 0.61 0.57 0.83 7.17 3.03\n\n\n\n# for all models, error variance for R1\n# is fixed to equal zero\n\n# lavaan function growth() automatically fixes\n# intercepts of indicators to zero but specifies \n# latent growth factor means as free parameters \n\n# the direct tracing of the constant (delta-1) on\n# the latent growth factors are means but are labeled\n# as \"intercepts\" in lavaan output\n\n# specify all models\n\n# model 1\n# no growth (intercept only)\n\nnoGrowth.model &lt;- '\n  # specify intercept\n  # fix all loadings to 1.0\n  Intercept =~ 1*R1 + 1*R2 + 1*R3 + 1*R4\n  # fix error variance for r1 to zero\n  R1 ~~ 0*R1\n'\n\n\n# model 2\n# latent basis growth model\n# (curve fitting, level and shape)\n# this model is retained\n\n# maccallum-rmsea for model 2\n# exact fit test\n# power at N = 150\nsemTools::findRMSEApower(0, .05, 4, 150, .05, 1) |&gt; print()\n\n# minimum N for power at least .90\nsemTools::findRMSEAsamplesize(0, .05, 4, .90, .05, 1) |&gt; print()\n\nbasis.model &lt;- '\n  Intercept =~ 1*R1 + 1*R2 + 1*R3 + 1*R4\n  # specify shape, first and last loadings fixed\n  Shape =~ 0*R1 + R2 + R3 + 1*R4\n  R1 ~~ 0*R1\n'\n\n[1] 0.136742\n[1] 1542\n\n\n\n# model 3\n# linear growth model\n\nlinear.model &lt;- '\n  Intercept =~ 1*R1 + 1*R2 + 1*R3 + 1*R4 \n  # all loadings fixed to constants\n  Linear =~ 0*R1 + 1*R2 + 2*R3 + 3*R4\n  R1 ~~ 0*R1\n'\n\n\n# fit model 1 to data\nnoGrowth &lt;- lavaan::growth(noGrowth.model, sample.cov = kimspoon.cov,\n sample.mean = kimspoon.mean, sample.nobs = 150)\n\n# fit model 2 to data\nbasis &lt;- lavaan::growth(basis.model, sample.cov = kimspoon.cov,\n sample.mean = kimspoon.mean, sample.nobs = 150)\n\n# fit model 3 to data\nlinear &lt;- lavaan::growth(linear.model, sample.cov = kimspoon.cov,\n sample.mean = kimspoon.mean, sample.nobs = 150)\n\n\n# model chi-squares and chi-square difference tests\nanova(noGrowth, basis) |&gt; print()\n\n\nChi-Squared Difference Test\n\n         Df    AIC    BIC    Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nbasis     4 610.60 640.71   2.8109                                          \nnoGrowth  9 865.42 880.47 267.6294     264.82 0.58858       5  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nanova(basis, linear) |&gt; print()\n\n\nChi-Squared Difference Test\n\n       Df   AIC    BIC   Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nbasis   4 610.6 640.71  2.8109                                          \nlinear  6 638.6 662.69 34.8116     32.001 0.31623       2  1.125e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n# model 1 parameter estimates, global fit statistics,\n# residuals\n# very poor fit\nlavaan::summary(noGrowth, fit.measures = TRUE, estimates = FALSE)\nlavaan::fitted(noGrowth)\nlavaan::residuals(noGrowth, type = \"standardized\")\nlavaan::residuals(noGrowth, type = \"cor.bollen\")\n\n\n\n# model 2 parameter estimates, global fit statistics,\n# residuals\n# retained model\nlavaan::summary(basis, fit.measures = TRUE, rsquare = TRUE)\nlavaan::standardizedSolution(basis)\n\n\n\n# variance and standard error for Intercept are close to zero,\n# so estimates are printed at 5-decimal accuracy, not 3 (default)\nprint(lavaan::parameterEstimates(basis), nd = 5)\n\n\n# implied covariances and means for observed variables\nlavaan::fitted(basis)\n# implied means for latent growth factors & observed variables\nlavaan::lavInspect(basis, \"mean.lv\")\nlavaan::lavInspect(basis, \"mean.ov\")\n\n\n# residuals\nlavaan::residuals(basis, type = \"raw\")\nlavaan::residuals(basis, type = \"standardized\")\nlavaan::residuals(basis, type = \"cor.bollen\")\n\n\n# model 3 parameter estimates, global fit statistics,\n# residuals\nlavaan::summary(linear, fit.measures = TRUE)\nlavaan::fitted(linear) \nlavaan::residuals(linear, type = \"raw\")\nlavaan::residuals(linear, type = \"standardized\")\nlavaan::residuals(linear, type = \"cor.bollen\")",
    "crumbs": [
      "Kline's",
      "Latent Growth Curve"
    ]
  },
  {
    "objectID": "contents/Kline/kchap16-3.html",
    "href": "contents/Kline/kchap16-3.html",
    "title": "Chapter 16. Composite Models",
    "section": "",
    "text": "Load libraries\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(cSEM)\nlibrary(psych)",
    "crumbs": [
      "Kline's",
      "Composite",
      "Composite: PLS-PM"
    ]
  },
  {
    "objectID": "contents/Kline/kchap16-3.html#partial-least-squares-path-modeling-pls-pm-algorithm",
    "href": "contents/Kline/kchap16-3.html#partial-least-squares-path-modeling-pls-pm-algorithm",
    "title": "Chapter 16. Composite Models",
    "section": "Partial least squares path modeling (PLS-PM) algorithm",
    "text": "Partial least squares path modeling (PLS-PM) algorithm\nSoftware: cSEM\n\n\nSource: Fig 16.3 (p. 295)\n\n\n# set global seed for random number generation\nset.seed(123)\n\n# variable order is acculscl, status, percent, educ, income,\n# interpers, job, scl90d\n# read correlation matrix\nshen.cor &lt;- matrix(c(1.00, .44, .69, .21, .23, .12, .09, .03,\n                      .44,1.00, .54, .08, .15, .08, .06, .02,\n                      .69, .54,1.00, .16, .19, .08, .04,-.02,\n                      .21, .08, .16,1.00, .19, .08, .01,-.07,\n                      .23, .15, .19, .19,1.00,-.03,-.02,-.11,\n                      .12, .08, .08, .08,-.03,1.00, .38, .37,\n                      .09, .06, .04, .01,-.02, .38,1.00, .46,\n                      .03, .02,-.02,-.07,-.11, .37, .46,1.00),\n                      ncol = 8, nrow = 8)\n\n# generate raw scores and save to dataframe\nshen.data &lt;- semTools::kd(shen.cor, 983, type=\"exact\")\n\n# rename columns in data frame and display correlation matrix\nnames(shen.data) &lt;- c(\"acculscl\", \"status\", \"percent\", \"educ\", \"income\",\n \"interpers\", \"job\", \"scl90d\")\n\n# display correlation matrix\ncor(shen.data) |&gt; print()\n\n          acculscl status percent  educ income interpers   job scl90d\nacculscl      1.00   0.44    0.69  0.21   0.23      0.12  0.09   0.03\nstatus        0.44   1.00    0.54  0.08   0.15      0.08  0.06   0.02\npercent       0.69   0.54    1.00  0.16   0.19      0.08  0.04  -0.02\neduc          0.21   0.08    0.16  1.00   0.19      0.08  0.01  -0.07\nincome        0.23   0.15    0.19  0.19   1.00     -0.03 -0.02  -0.11\ninterpers     0.12   0.08    0.08  0.08  -0.03      1.00  0.38   0.37\njob           0.09   0.06    0.04  0.01  -0.02      0.38  1.00   0.46\nscl90d        0.03   0.02   -0.02 -0.07  -0.11      0.37  0.46   1.00\n\n\n\n# descriptive statistics\npsych::describe(shen.data) |&gt; print()\n\n          vars   n mean sd median trimmed  mad   min  max range  skew kurtosis\nacculscl     1 983    0  1  -0.03    0.00 1.04 -3.77 3.15  6.92  0.00     0.11\nstatus       2 983    0  1  -0.01   -0.01 1.00 -3.19 3.05  6.25  0.06    -0.05\npercent      3 983    0  1  -0.04   -0.01 1.04 -3.46 3.01  6.47  0.03    -0.10\neduc         4 983    0  1   0.00    0.00 1.03 -3.67 3.37  7.04 -0.01     0.01\nincome       5 983    0  1   0.00    0.00 1.00 -3.14 3.45  6.60  0.03     0.22\ninterpers    6 983    0  1   0.01    0.01 0.99 -3.57 3.11  6.68 -0.05     0.00\njob          7 983    0  1   0.02    0.00 1.06 -3.23 3.15  6.38 -0.03    -0.23\nscl90d       8 983    0  1   0.02    0.00 1.01 -2.73 2.98  5.71  0.05    -0.10\n            se\nacculscl  0.03\nstatus    0.03\npercent   0.03\neduc      0.03\nincome    0.03\ninterpers 0.03\njob       0.03\nscl90d    0.03\n\n\n\n# specify composite model\n\nshen.model &lt;- '\n  # outer model (measurement)\n  # exogenous composites\n  Acculturation &lt;~ acculscl + status + percent\n  SES &lt;~ educ + income\n  \n  # endogenous composites\n  Stress &lt;~ interpers + job\n  Depression &lt;~ scl90d\n\n  # inner model (structural)\n  Stress ~ Acculturation\n  Depression ~ SES + Stress\n'\n\n\n# fit model to data with package cSEM for cca\n\n# the algorithm is basic PLS-PM \n# dominant indicators specified for all composites\n# with multiple indicators\n# by default, the single indicator for depression\n# is the dominant indicator\n# outer weights are PLS mode A (correlation weights)\n# inner weights are factor (factorial)\n# bootstrapped standard errors, 1000 generated samples\n# seed for bootstrapping (123) initializes random\n# number generation in cSEM functions for boostrapping\n# the global seed in R is also set to the same value (123)\n# thus, bootstrapped estimates of standard errors and\n# percentiles for distributions of global fit test\n# statistics are reproducible\n\nshen &lt;- cSEM::csem(.data = shen.data, .model = shen.model, \n .dominant_indicators = c(Acculturation = \"acculscl\", SES = \"educ\",\n  Stress = \"interpers\"), .approach_weights = \"PLS-PM\", \n .PLS_modes = \"modeA\", .PLS_weight_scheme_inner = \"factorial\", \n .resample_method = \"bootstrap\", .R = 1000, .seed = 123,\n .disattenuate = FALSE)\n\n\n# check solution for problems\ncSEM::verify(shen)\n\n\n________________________________________________________________________________\nVerify admissibility:\n\n  admissible\nDetails:\n  Code   Status    Description\n  1      ok        Convergence achieved                                   \n  2      ok        All absolute standardized loading estimates &lt;= 1       \n  3      ok        Construct VCV is positive semi-definite                \n  4      ok        All reliability estimates &lt;= 1                         \n  5      ok        Model-implied indicator VCV is positive semi-definite  \n________________________________________________________________________________\n\n\n\n\n# parameter estimates with bootstrapped standard errors\ncSEM::summarize(shen)\n\n\n________________________________________________________________________________\n----------------------------------- Overview -----------------------------------\n    General information:\n    ------------------------\n    Estimation status                = Ok\n    Number of observations           = 983\n    Weight estimator                 = PLS-PM\n    Inner weighting scheme           = \"factorial\"\n    Type of indicator correlation    = Pearson\n    Path model estimator             = OLS\n    Second-order approach            = NA\n    Type of path model               = Linear\n    Disattenuated                    = No\n    Resample information:\n    ---------------------\n    Resample method                  = \"bootstrap\"\n    Number of resamples              = 1000\n    Number of admissible results     = 1000\n    Approach to handle inadmissibles = \"drop\"\n    Sign change option               = \"none\"\n    Random seed                      = 123\n    Construct details:\n    ------------------\n    Name           Modeled as     Order         Mode      \n    Acculturation  Composite      First order   \"modeA\"   \n    SES            Composite      First order   \"modeA\"   \n    Stress         Composite      First order   \"modeA\"   \n    Depression     Composite      First order   \"modeA\"   \n----------------------------------- Estimates ----------------------------------\nEstimated path coefficients:\n============================\n                                                                        CI_percentile   \n  Path                      Estimate  Std. error   t-stat.   p-value         95%        \n  Stress ~ Acculturation      0.1166      0.0276    4.2251    0.0000 [ 0.0687; 0.1765 ] \n  Depression ~ SES           -0.1214      0.0298   -4.0779    0.0000 [-0.1798;-0.0713 ] \n  Depression ~ Stress         0.5039      0.0230   21.9210    0.0000 [ 0.4586; 0.5479 ] \nEstimated loadings:\n===================\n                                                                           CI_percentile   \n  Loading                      Estimate  Std. error   t-stat.   p-value         95%        \n  Acculturation =~ acculscl      0.8952      0.0393   22.8018    0.0000 [ 0.8105; 0.9668 ] \n  Acculturation =~ status        0.7509      0.0706   10.6426    0.0000 [ 0.5965; 0.8567 ] \n  Acculturation =~ percent       0.8582      0.0408   21.0136    0.0000 [ 0.7484; 0.8988 ] \n  SES =~ educ                    0.6440      0.1646    3.9115    0.0001 [ 0.2346; 0.8785 ] \n  SES =~ income                  0.8735      0.1293    6.7530    0.0000 [ 0.6337; 0.9974 ] \n  Stress =~ interpers            0.7942      0.0193   41.1856    0.0000 [ 0.7531; 0.8284 ] \n  Stress =~ job                  0.8639      0.0127   67.9643    0.0000 [ 0.8371; 0.8874 ] \n  Depression =~ scl90d           1.0000          NA        NA        NA [     NA;     NA ] \nEstimated weights:\n==================\n                                                                           CI_percentile   \n  Weight                       Estimate  Std. error   t-stat.   p-value         95%        \n  Acculturation &lt;~ acculscl      0.5327      0.0946    5.6283    0.0000 [ 0.3908; 0.7721 ] \n  Acculturation &lt;~ status        0.3551      0.1097    3.2366    0.0012 [ 0.1299; 0.5630 ] \n  Acculturation &lt;~ percent       0.2989      0.0886    3.3740    0.0007 [ 0.0526; 0.4114 ] \n  SES &lt;~ educ                    0.4959      0.1853    2.6766    0.0074 [ 0.0579; 0.7864 ] \n  SES &lt;~ income                  0.7793      0.1556    5.0067    0.0000 [ 0.4826; 0.9885 ] \n  Stress &lt;~ interpers            0.5446      0.0220   24.7726    0.0000 [ 0.5019; 0.5883 ] \n  Stress &lt;~ job                  0.6569      0.0224   29.2846    0.0000 [ 0.6140; 0.7004 ] \n  Depression &lt;~ scl90d           1.0000          NA        NA        NA [     NA;     NA ] \nEstimated construct correlations:\n=================================\n                                                                      CI_percentile   \n  Correlation             Estimate  Std. error   t-stat.   p-value         95%        \n  Acculturation ~~ SES      0.2745      0.0382    7.1933    0.0000 [ 0.1969; 0.3267 ] \nEstimated indicator correlations:\n=================================\n                                                                     CI_percentile   \n  Correlation            Estimate  Std. error   t-stat.   p-value         95%        \n  acculscl ~~ status       0.4400      0.0258   17.0772    0.0000 [ 0.3887; 0.4887 ] \n  acculscl ~~ percent      0.6900      0.0176   39.2730    0.0000 [ 0.6550; 0.7228 ] \n  status ~~ percent        0.5400      0.0219   24.6519    0.0000 [ 0.4945; 0.5799 ] \n  educ ~~ income           0.1900      0.0300    6.3323    0.0000 [ 0.1294; 0.2526 ] \n  interpers ~~ job         0.3800      0.0283   13.4416    0.0000 [ 0.3241; 0.4361 ] \n------------------------------------ Effects -----------------------------------\nEstimated total effects:\n========================\n                                                                            CI_percentile   \n  Total effect                  Estimate  Std. error   t-stat.   p-value         95%        \n  Stress ~ Acculturation          0.1166      0.0276    4.2251    0.0000 [ 0.0687; 0.1765 ] \n  Depression ~ Acculturation      0.0588      0.0143    4.1068    0.0000 [ 0.0348; 0.0902 ] \n  Depression ~ SES               -0.1214      0.0298   -4.0779    0.0000 [-0.1798;-0.0713 ] \n  Depression ~ Stress             0.5039      0.0230   21.9210    0.0000 [ 0.4586; 0.5479 ] \nEstimated indirect effects:\n===========================\n                                                                            CI_percentile   \n  Indirect effect               Estimate  Std. error   t-stat.   p-value         95%        \n  Depression ~ Acculturation      0.0588      0.0143    4.1068    0.0000 [ 0.0348; 0.0902 ] \n________________________________________________________________________________\n\n\n\n\n# test overall (global) model fit\n# 1000 bootstrap replications\n# seed value set\ncSEM::testOMF(shen, .R = 1000, .seed = 123)\n\n\n________________________________________________________________________________\n--------- Test for overall model fit based on Beran & Srivastava (1985) --------\nNull hypothesis:\n       +------------------------------------------------------------------+\n       |                                                                  |\n       |   H0: The model-implied indicator covariance matrix equals the   |\n       |   population indicator covariance matrix.                        |\n       |                                                                  |\n       +------------------------------------------------------------------+\nTest statistic and critical value: \n                                        Critical value\n    Distance measure    Test statistic    95%   \n    dG                      0.0062      0.0088  \n    SRMR                    0.0254      0.0408  \n    dL                      0.0232      0.0600  \n    dML                     0.0329      0.0462  \n    \nDecision: \n                            Significance level\n    Distance measure             95%        \n    dG                      Do not reject    \n    SRMR                    Do not reject    \n    dL                      Do not reject    \n    dML                     Do not reject    \n    \nAdditional information:\n    Out of 1000 bootstrap replications 1000 are admissible.\n    See ?verify() for what constitutes an inadmissible result.\n    The seed used was: 123\n________________________________________________________________________________\n\n\n\n\n# model quality criteria\ncSEM::assess(shen, .quality_criterion = c(\"df\", \"r2\", \"r2_adj\", \"f2\", \n \"chi_square\"))\n\n________________________________________________________________________________\n\n    Construct         R2          R2_adj    \n    Stress          0.0136        0.0126    \n    Depression      0.2684        0.2669    \n\n--------------------------- Distance and fit measures --------------------------\n\n\n    Chi_square     = 32.35609\n\n    Degrees of freedom    = 15\n\n-------------------------- Effect sizes (Cohen's f^2) --------------------------\n\n  Dependent construct: 'Stress'\n\n    Independent construct       f^2    \n    Acculturation             0.0138   \n\n  Dependent construct: 'Depression'\n\n    Independent construct       f^2    \n    SES                       0.0201   \n    Stress                    0.3471   \n________________________________________________________________________________\n\n\n\n# model-implied correlations among composites\ncSEM::fit(shen, .type_vcv = \"construct\") |&gt; print()\n\n              Acculturation         SES     Stress  Depression\nAcculturation    1.00000000  0.27450941 0.11664735  0.02545829\nSES              0.27450941  1.00000000 0.03202079 -0.10524925\nStress           0.11664735  0.03202079 1.00000000  0.50002142\nDepression       0.02545829 -0.10524925 0.50002142  1.00000000\n\n\n\n# model-implied correlations among indicators\npredicted &lt;- cSEM::fit(shen, .type_vcv = \"indicator\")\npredicted |&gt; round(3) |&gt; print()\n\n          acculscl status percent   educ income interpers   job scl90d\nacculscl     1.000  0.440   0.690  0.158  0.215     0.083 0.090  0.023\nstatus       0.440  1.000   0.540  0.133  0.180     0.070 0.076  0.019\npercent      0.690  0.540   1.000  0.152  0.206     0.080 0.086  0.022\neduc         0.158  0.133   0.152  1.000  0.190     0.016 0.018 -0.068\nincome       0.215  0.180   0.206  0.190  1.000     0.022 0.024 -0.092\ninterpers    0.083  0.070   0.080  0.016  0.022     1.000 0.380  0.397\njob          0.090  0.076   0.086  0.018  0.024     0.380 1.000  0.432\nscl90d       0.023  0.019   0.022 -0.068 -0.092     0.397 0.432  1.000\n\n\n\n# calculate correlation residuals\n# rounded to 3-decimal places\n\ncor_residuals = shen.cor - predicted\nround(cor_residuals, digits = 3) |&gt; print()\n\n          acculscl status percent   educ income interpers    job scl90d\nacculscl     0.000  0.000   0.000  0.052  0.015     0.037  0.000  0.007\nstatus       0.000  0.000   0.000 -0.053 -0.030     0.010 -0.016  0.001\npercent      0.000  0.000   0.000  0.008 -0.016     0.000 -0.046 -0.042\neduc         0.052 -0.053   0.008  0.000  0.000     0.064 -0.008 -0.002\nincome       0.015 -0.030  -0.016  0.000  0.000    -0.052 -0.044 -0.018\ninterpers    0.037  0.010   0.000  0.064 -0.052     0.000  0.000 -0.027\njob          0.000 -0.016  -0.046 -0.008 -0.044     0.000  0.000  0.028\nscl90d       0.007  0.001  -0.042 -0.002 -0.018    -0.027  0.028  0.000",
    "crumbs": [
      "Kline's",
      "Composite",
      "Composite: PLS-PM"
    ]
  },
  {
    "objectID": "contents/Kline/kchap16-1.html",
    "href": "contents/Kline/kchap16-1.html",
    "title": "Chapter 16. Composite Models",
    "section": "",
    "text": "Load libraries\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)",
    "crumbs": [
      "Kline's",
      "Composite",
      "Reflective"
    ]
  },
  {
    "objectID": "contents/Kline/kchap16-1.html#reflective-measurement",
    "href": "contents/Kline/kchap16-1.html#reflective-measurement",
    "title": "Chapter 16. Composite Models",
    "section": "Reflective measurement",
    "text": "Reflective measurement\nPartial SR model with reflective measurement component\n\n\nSource: Fig 16.1 (p. 289)\n\n\n# input the correlations in lower diagnonal form\nshenLower.cor &lt;- '\n1.00\n .44 1.00\n .69  .54 1.00\n .21  .08  .16 1.00\n .23  .15  .19  .19 1.00\n .12  .08  .08  .08 -.03 1.00\n .09  .06  .04  .01 -.02  .38 1.00\n .03  .02 -.02 -.07 -.11  .37  .46 1.00 '\n\n# name the variables and convert to full correlation matrix\nshen.cor &lt;- lavaan::getCov(shenLower.cor, names = c(\"acculscl\", \"status\",\n \"percent\", \"educ\", \"income\", \"interpers\", \"job\", \"scl90d\"))\n\n# add the standard deviations and convert to covariances\nshen.cov &lt;- lavaan::cor2cov(shen.cor, sds = c(3.60,3.30,2.45,3.27,3.44,2.99,\n 3.58,3.70))\n\n\n\n# display correlations and covariances\nshen.cor |&gt; print()\n\n          acculscl status percent  educ income interpers   job scl90d\nacculscl      1.00   0.44    0.69  0.21   0.23      0.12  0.09   0.03\nstatus        0.44   1.00    0.54  0.08   0.15      0.08  0.06   0.02\npercent       0.69   0.54    1.00  0.16   0.19      0.08  0.04  -0.02\neduc          0.21   0.08    0.16  1.00   0.19      0.08  0.01  -0.07\nincome        0.23   0.15    0.19  0.19   1.00     -0.03 -0.02  -0.11\ninterpers     0.12   0.08    0.08  0.08  -0.03      1.00  0.38   0.37\njob           0.09   0.06    0.04  0.01  -0.02      0.38  1.00   0.46\nscl90d        0.03   0.02   -0.02 -0.07  -0.11      0.37  0.46   1.00\n\n\n\nshen.cov |&gt; round(2) |&gt; print()\n\n          acculscl status percent  educ income interpers   job scl90d\nacculscl     12.96   5.23    6.09  2.47   2.85      1.29  1.16   0.40\nstatus        5.23  10.89    4.37  0.86   1.70      0.79  0.71   0.24\npercent       6.09   4.37    6.00  1.28   1.60      0.59  0.35  -0.18\neduc          2.47   0.86    1.28 10.69   2.14      0.78  0.12  -0.85\nincome        2.85   1.70    1.60  2.14  11.83     -0.31 -0.25  -1.40\ninterpers     1.29   0.79    0.59  0.78  -0.31      8.94  4.07   4.09\njob           1.16   0.71    0.35  0.12  -0.25      4.07 12.82   6.09\nscl90d        0.40   0.24   -0.18 -0.85  -1.40      4.09  6.09  13.69\n\n\n\n# maccallum-rmsea for whole model\n# exact fit test\n# power at N = 983\nsemTools::findRMSEApower(0, .05, 16, 983, .05, 1) |&gt; print()\n\n[1] 0.9931794\n\n\n\n# minimum N for power at least .90\nsemTools::findRMSEAsamplesize(0, .05, 16, .90, .05, 1) |&gt; print()\n\n[1] 605\n\n\n\n# scl90d score reliability (alpha) is .90 from \n# derogatis et al. (1976)\n# sample variance is 3.70**2 = 13.690\n# error variance fixed to (1 - .90) * 13.690 = 1.369\n\n# specify reflective model\n\nshenSR.model &lt;- '\n  # measurement model with error covariance\n  Acculturation =~ acculscl + status + percent\n  status ~~ percent\n  SES =~ educ + income\n  Stress =~ interpers + job\n  Depression =~ 1*scl90d\n  scl90d ~~ 1.369*scl90d\n  \n  # structural model\n  Stress ~ a*Acculturation\n  Depression ~ SES + b*Stress \n\n  # define indirect effect of acculturation\n  ab := a * b\n'\n\n\n# fit model to data\nshenSR &lt;- lavaan::sem(shenSR.model, sample.cov = shen.cov,\n sample.nobs = 983, fixed.x = FALSE)\nlavaan::summary(shenSR, fit.measures = TRUE, rsquare = TRUE) |&gt; print()\n\nlavaan 0.6.17 ended normally after 98 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        20\n\n  Number of observations                           983\n\nModel Test User Model:\n                                                      \n  Test statistic                                21.341\n  Degrees of freedom                                16\n  P-value (Chi-square)                           0.166\n\nModel Test Baseline Model:\n\n  Test statistic                              1606.002\n  Degrees of freedom                                28\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.997\n  Tucker-Lewis Index (TLI)                       0.994\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -19671.382\n  Loglikelihood unrestricted model (H1)     -19660.711\n                                                      \n  Akaike (AIC)                               39382.764\n  Bayesian (BIC)                             39480.576\n  Sample-size adjusted Bayesian (SABIC)      39417.056\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.018\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.037\n  P-value H_0: RMSEA &lt;= 0.050                    0.999\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.023\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  Acculturation =~                                    \n    acculscl          1.000                           \n    status            0.450    0.051    8.880    0.000\n    percent           0.523    0.051   10.214    0.000\n  SES =~                                              \n    educ              1.000                           \n    income            1.158    0.201    5.775    0.000\n  Stress =~                                           \n    interpers         1.000                           \n    job               1.432    0.121   11.862    0.000\n  Depression =~                                       \n    scl90d            1.000                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  Stress ~                                            \n    Acculturtn (a)    0.088    0.023    3.876    0.000\n  Depression ~                                        \n    SES              -0.578    0.138   -4.179    0.000\n    Stress     (b)    1.524    0.132   11.588    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .status ~~                                           \n   .percent           1.623    0.307    5.294    0.000\n  Acculturation ~~                                    \n    SES               2.420    0.366    6.613    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .scl90d            1.369                           \n   .acculscl          1.333    1.083    1.231    0.218\n   .status            8.523    0.451   18.918    0.000\n   .percent           2.814    0.323    8.723    0.000\n   .educ              8.739    0.541   16.142    0.000\n   .income            9.215    0.641   14.384    0.000\n   .interpers         6.114    0.355   17.203    0.000\n   .job               7.025    0.556   12.626    0.000\n    Acculturation    11.614    1.227    9.464    0.000\n    SES               1.944    0.463    4.197    0.000\n   .Stress            2.727    0.360    7.583    0.000\n   .Depression        5.438    0.647    8.402    0.000\n\nR-Square:\n                   Estimate\n    scl90d            0.899\n    acculscl          0.897\n    status            0.217\n    percent           0.531\n    educ              0.182\n    income            0.220\n    interpers         0.315\n    job               0.451\n    Stress            0.032\n    Depression        0.556\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    ab                0.134    0.035    3.825    0.000\n\n\n\n\nlavaan::standardizedSolution(shenSR) |&gt; print(nd = 2)\n\n             lhs op           rhs label est.std   se      z pvalue ci.lower\n1  Acculturation =~      acculscl          0.95 0.04  21.43   0.00     0.86\n2  Acculturation =~        status          0.47 0.03  13.65   0.00     0.40\n3  Acculturation =~       percent          0.73 0.04  19.64   0.00     0.66\n4         status ~~       percent          0.33 0.04   7.83   0.00     0.25\n5            SES =~          educ          0.43 0.05   8.82   0.00     0.33\n6            SES =~        income          0.47 0.05   9.31   0.00     0.37\n7         Stress =~     interpers          0.56 0.03  17.97   0.00     0.50\n8         Stress =~           job          0.67 0.03  21.10   0.00     0.61\n9     Depression =~        scl90d          0.95 0.00 397.85   0.00     0.94\n10        scl90d ~~        scl90d          0.10 0.00  22.23   0.00     0.09\n11        Stress  ~ Acculturation     a    0.18 0.04   4.20   0.00     0.10\n12    Depression  ~           SES         -0.23 0.05  -4.97   0.00    -0.32\n13    Depression  ~        Stress     b    0.73 0.03  21.09   0.00     0.66\n14      acculscl ~~      acculscl          0.10 0.08   1.23   0.22    -0.06\n15        status ~~        status          0.78 0.03  24.69   0.00     0.72\n16       percent ~~       percent          0.47 0.05   8.68   0.00     0.36\n17          educ ~~          educ          0.82 0.04  19.83   0.00     0.74\n18        income ~~        income          0.78 0.05  16.45   0.00     0.69\n19     interpers ~~     interpers          0.68 0.04  19.51   0.00     0.62\n20           job ~~           job          0.55 0.04  12.83   0.00     0.46\n21 Acculturation ~~ Acculturation          1.00 0.00     NA     NA     1.00\n22           SES ~~           SES          1.00 0.00     NA     NA     1.00\n23        Stress ~~        Stress          0.97 0.02  63.77   0.00     0.94\n24    Depression ~~    Depression          0.44 0.05   8.47   0.00     0.34\n25 Acculturation ~~           SES          0.51 0.06   9.00   0.00     0.40\n26            ab :=           a*b    ab    0.13 0.03   4.01   0.00     0.07\n   ci.upper\n1      1.03\n2      0.53\n3      0.80\n4      0.41\n5      0.52\n6      0.57\n7      0.62\n8      0.73\n9      0.95\n10     0.11\n11     0.26\n12    -0.14\n13     0.80\n14     0.27\n15     0.85\n16     0.58\n17     0.90\n18     0.87\n19     0.75\n20     0.63\n21     1.00\n22     1.00\n23     1.00\n24     0.55\n25     0.62\n26     0.19\n\n\n\n# predicted covariances\nlavaan::fitted(shenSR) |&gt; print()\n\n# predicted correlation matrix for indicators\nlavaan::lavInspect(shenSR, \"cor.ov\")\n\n# predicted covariance matrix for factors\nlavaan::lavInspect(shenSR, \"cov.lv\")\n\n# predicted correlation matrix for factors\nlavaan::lavInspect(shenSR, \"cor.lv\")\n\n\n# residuals\nlavaan::residuals(shenSR, type = \"raw\")\nlavaan::residuals(shenSR, type = \"standardized.mplus\")\nlavaan::residuals(shenSR, type = \"cor.bollen\")\n\n\n# calculate factor reliability coefficients (semTools)\n# note that semTools calculates AVE based on the unstandardized\n# solution, not the standardized solution, so the result for\n# AVE below will not match those in the composite models chapter,\n# which are based on the standardized solution\n\nsemTools::reliability(shenSR) |&gt; print()\n\n       Acculturation       SES    Stress\nalpha      0.7684442 0.3189834 0.5443112\nomega      0.7397905 0.3351805 0.5591078\nomega2     0.7397905 0.3351805 0.5591078\nomega3     0.7400098 0.3380762 0.5580098\navevar     0.5751605 0.2021933 0.3954391",
    "crumbs": [
      "Kline's",
      "Composite",
      "Reflective"
    ]
  },
  {
    "objectID": "contents/Kline/composite.html",
    "href": "contents/Kline/composite.html",
    "title": "Chapter 13. Multiple-Indicator Measurement",
    "section": "",
    "text": "Load libraries\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)",
    "crumbs": [
      "Kline's",
      "Composite"
    ]
  },
  {
    "objectID": "contents/Kline/composite.html#reflective-measurement-and-effect-indicators",
    "href": "contents/Kline/composite.html#reflective-measurement-and-effect-indicators",
    "title": "Chapter 13. Multiple-Indicator Measurement",
    "section": "Reflective Measurement and Effect Indicators",
    "text": "Reflective Measurement and Effect Indicators\n\n\nFIGURE 13.2. Measurement models for a set of three indicators: reflective measurement with effect indicators (a), causal–for‑ mative measurement with causal indicators (b), and composite (composite–formative) measurement with composite indicators (c). L, latent; M, manifest; C, composite.\nSource: Fig 13.2 (p. 219)",
    "crumbs": [
      "Kline's",
      "Composite"
    ]
  },
  {
    "objectID": "contents/Kline/kchap16-2.html",
    "href": "contents/Kline/kchap16-2.html",
    "title": "Chapter 16. Composite Models",
    "section": "",
    "text": "Load libraries\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)",
    "crumbs": [
      "Kline's",
      "Composite",
      "Composite: Formative"
    ]
  },
  {
    "objectID": "contents/Kline/kchap16-2.html#formative-measurement",
    "href": "contents/Kline/kchap16-2.html#formative-measurement",
    "title": "Chapter 16. Composite Models",
    "section": "Formative measurement",
    "text": "Formative measurement\n\nZero-error variance model\npartially reduced form model for SES only\n\n\n\nFIGURE 16.2. Structural regression models with causal indicators for a latent socioeconomic status composite with a disturbance (a). Zero‐error variance model with composite indicators for an SES composite with no disturbance (b). A partially reduced form model with no SES composite (c). Model (a) is not identified, models (b) and (c) are equivalent. (p. 292)\n\n\n# input the correlations in lower diagnonal form\nshenLower.cor &lt;- '\n1.00\n .44 1.00\n .69  .54 1.00\n .21  .08  .16 1.00\n .23  .15  .19  .19 1.00\n .12  .08  .08  .08 -.03 1.00\n .09  .06  .04  .01 -.02  .38 1.00\n .03  .02 -.02 -.07 -.11  .37  .46 1.00 '\n\n# name the variables and convert to full correlation matrix\nshen.cor &lt;- lavaan::getCov(shenLower.cor, names = c(\"acculscl\", \"status\",\n \"percent\", \"educ\", \"income\", \"interpers\", \"job\", \"scl90d\"))\n\n# add the standard deviations and convert to covariances\nshen.cov &lt;- cor2cov(shen.cor, sds = c(3.60,3.30,2.45,3.27,3.44,2.99,\n 3.58,3.70))\n\n\n# display correlations and covariances\nshen.cor |&gt; print()\n\n          acculscl status percent  educ income interpers   job scl90d\nacculscl      1.00   0.44    0.69  0.21   0.23      0.12  0.09   0.03\nstatus        0.44   1.00    0.54  0.08   0.15      0.08  0.06   0.02\npercent       0.69   0.54    1.00  0.16   0.19      0.08  0.04  -0.02\neduc          0.21   0.08    0.16  1.00   0.19      0.08  0.01  -0.07\nincome        0.23   0.15    0.19  0.19   1.00     -0.03 -0.02  -0.11\ninterpers     0.12   0.08    0.08  0.08  -0.03      1.00  0.38   0.37\njob           0.09   0.06    0.04  0.01  -0.02      0.38  1.00   0.46\nscl90d        0.03   0.02   -0.02 -0.07  -0.11      0.37  0.46   1.00\n\n\n\nshen.cov |&gt; round(2) |&gt; print()\n\n          acculscl status percent  educ income interpers   job scl90d\nacculscl     12.96   5.23    6.09  2.47   2.85      1.29  1.16   0.40\nstatus        5.23  10.89    4.37  0.86   1.70      0.79  0.71   0.24\npercent       6.09   4.37    6.00  1.28   1.60      0.59  0.35  -0.18\neduc          2.47   0.86    1.28 10.69   2.14      0.78  0.12  -0.85\nincome        2.85   1.70    1.60  2.14  11.83     -0.31 -0.25  -1.40\ninterpers     1.29   0.79    0.59  0.78  -0.31      8.94  4.07   4.09\njob           1.16   0.71    0.35  0.12  -0.25      4.07 12.82   6.09\nscl90d        0.40   0.24   -0.18 -0.85  -1.40      4.09  6.09  13.69\n\n\n\nFor the single indicator latent\n\nscl90d score reliability (alpha) is .90 from derogatis et al. (1976)\nsample variance is 3.70**2 = 13.690\nerror variance fixed to (1 - .90) * 13.690 = 1.369\n\n\n\nSpecify figure 16.2(b)\nzero error variance model\n\n# ses with composite indicators (educ, income)\n# educ, income each covary with acculturation factor\n\n# special lavaan syntax for composite indicators\n# \"&lt;~\" defines a composite where the disturbance\n# variance is automatically fixed to zero\n# composite is explicitly scaled with an ULI constraint\n\nshenSEScomposite.model &lt;- '\n# ses composite\n SES &lt;~ 1*educ + income\n\n# reflective factors\n Acculturation =~ acculscl + status + percent\n status ~~ percent\n Stress =~ interpers + job\n Depression =~ scl90d\n scl90d ~~ 1.369*scl90d\n\n# covariances among exogenous acculturation factor\n# and composite indicators of ses explicitly defined\n# as free parameters\n Acculturation ~~ income + educ\n income ~~ educ\n\n# covariance between ses composite and acculturation\n# factor fixed to zero\n Acculturation ~~ 0*SES\n\n# structural model\n Stress ~ Acculturation\n\n# depression regressed on ses composite and\n# reflective stress factor\n# coefficient for SES labeled for calculation of\n# indirect effects through the SES composite\n Depression ~ b*SES + Stress '\n\n\n\nSpecify figure 16.2(c)\npartially reduced form model with no SES composite\n\n# specify figure 16.2(c)\n# partially reduced form model with\n# no SES composite\n\nshenSESnoComposite.model &lt;- \"\n  # reflective factors\n  Acculturation =~ acculscl + status + percent\n  status ~~ percent\n  Stress =~ interpers + job\n  Depression =~ scl90d\n  scl90d ~~ 1.369*scl90d\n\n  # covariances among exogenous acculturation factor\n  # and the measured exogenous variables income and education\n  # explicitly declared as free parameters\n  Acculturation ~~ income + educ\n  income ~~ educ\n\n  # structural model\n  Stress ~ Acculturation\n\n  # depression regressed on income, educ, and\n  # reflective stress factor\n  Depression ~ educ + income + Stress \"\n\n\n# fit figure 16.2(b) to data\nshenSEScomposite &lt;- lavaan::sem(shenSEScomposite.model, sample.cov = shen.cov,\n sample.nobs = 983, fixed.x=FALSE)\n\n # fit figure 16.2(c) to data\nshenSESnoComposite &lt;- lavaan::sem(shenSESnoComposite.model, sample.cov = shen.cov, sample.nobs=983, fixed.x = FALSE)\n\n\n\nGlobal fit statistics for both models\nwhich are equivalent\n\n# define fit statistics for later comparison display\nfit.stats &lt;- c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"srmr\", \"rmsea\", \"rmsea.ci.lower\", \"rmsea.ci.upper\")\n\n\n# figure 16.2(b)\nlavaan::fitMeasures(shenSEScomposite, fit.stats) |&gt; print()\n\n         chisq             df         pvalue            cfi           srmr \n        22.294         15.000          0.100          0.995          0.024 \n         rmsea rmsea.ci.lower rmsea.ci.upper \n         0.022          0.000          0.040 \n\n\n\n# figure 16.2(c)\nlavaan::fitMeasures(shenSESnoComposite, fit.stats) |&gt; print()\n\n         chisq             df         pvalue            cfi           srmr \n        22.294         15.000          0.100          0.995          0.024 \n         rmsea rmsea.ci.lower rmsea.ci.upper \n         0.022          0.000          0.040 \n\n\n\n\nResiduals for both models\nwhich are also equal\n\n# figure 16.2(b)\nlavaan::residuals(shenSEScomposite, type = \"raw\") |&gt; print()\nlavaan::residuals(shenSEScomposite, type = \"standardized.mplus\") |&gt; print()\nlavaan::residuals(shenSEScomposite, type = \"cor.bollen\") |&gt; print()\n\n$type\n[1] \"raw\"\n\n$cov\n          acclsc status percnt intrpr    job scl90d   educ income\nacculscl   0.000                                                 \nstatus    -0.008  0.000                                          \npercent    0.000  0.000  0.000                                   \ninterpers  0.491  0.434  0.173  0.000                            \njob       -0.005  0.192 -0.250 -0.042  0.000                     \nscl90d    -0.264 -0.050 -0.523 -0.018  0.105  0.042              \neduc       0.001 -0.233  0.008  0.614 -0.127  0.129  0.000       \nincome    -0.026  0.427  0.119 -0.503 -0.530 -0.343  0.000  0.000\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n          acclsc status percnt intrpr    job scl90d   educ income\nacculscl   0.000                                                 \nstatus    -3.815  0.000                                          \npercent    0.144  0.000  0.000                                   \ninterpers  1.975  1.473  0.887  0.000                            \njob       -0.024  0.564 -1.198 -1.896  0.000                     \nscl90d    -1.123 -0.142 -2.354 -0.303  1.424  0.567              \neduc       0.042 -0.838  0.062  1.999 -0.349  0.687  0.000       \nincome    -1.255  1.477  1.008 -1.565 -1.391 -1.724  0.000  0.000\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n          acclsc status percnt intrpr    job scl90d   educ income\nacculscl   0.000                                                 \nstatus    -0.001  0.000                                          \npercent    0.000  0.000  0.000                                   \ninterpers  0.046  0.044  0.024  0.000                            \njob        0.000  0.016 -0.029 -0.004  0.000                     \nscl90d    -0.020 -0.004 -0.058 -0.002  0.007  0.000              \neduc       0.000 -0.022  0.001  0.063 -0.011  0.011  0.000       \nincome    -0.002  0.038  0.014 -0.049 -0.043 -0.027  0.000  0.000\n\n\n\n\n# figure 16.2(c)\nlavaan::residuals(shenSESnoComposite, type = \"raw\") |&gt; print()\nlavaan::residuals(shenSESnoComposite, type = \"standardized.mplus\") |&gt; print()\nlavaan::residuals(shenSESnoComposite, type = \"cor.bollen\") |&gt; print()\n\n$type\n[1] \"raw\"\n\n$cov\n          acclsc status percnt intrpr    job scl90d   educ income\nacculscl   0.000                                                 \nstatus    -0.008  0.000                                          \npercent    0.000  0.000  0.000                                   \ninterpers  0.491  0.434  0.173  0.000                            \njob       -0.005  0.192 -0.250 -0.042  0.000                     \nscl90d    -0.264 -0.050 -0.523 -0.018  0.105  0.042              \neduc       0.001 -0.233  0.008  0.614 -0.127  0.129  0.000       \nincome    -0.026  0.427  0.119 -0.503 -0.530 -0.343  0.000  0.000\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n          acclsc status percnt intrpr    job scl90d   educ income\nacculscl   0.000                                                 \nstatus    -3.860  0.000                                          \npercent    0.144  0.000  0.000                                   \ninterpers  1.975  1.473  0.887  0.000                            \njob       -0.024  0.564 -1.198 -1.897  0.000                     \nscl90d    -1.123 -0.142 -2.354 -0.303  1.424  0.567              \neduc       0.042 -0.838  0.062  1.999 -0.349  0.687  0.000       \nincome    -1.255  1.477  1.008 -1.565 -1.391 -1.724  0.000  0.000\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n          acclsc status percnt intrpr    job scl90d   educ income\nacculscl   0.000                                                 \nstatus    -0.001  0.000                                          \npercent    0.000  0.000  0.000                                   \ninterpers  0.046  0.044  0.024  0.000                            \njob        0.000  0.016 -0.029 -0.004  0.000                     \nscl90d    -0.020 -0.004 -0.058 -0.002  0.007  0.000              \neduc       0.000 -0.022  0.001  0.063 -0.011  0.011  0.000       \nincome    -0.002  0.038  0.014 -0.049 -0.043 -0.027  0.000  0.000\n\n\n\n\n\nParameter estimates and residuals\n\n# figure 16.2(b)\nlavaan::summary(shenSEScomposite, header = FALSE, fit.measures = FALSE,\n standardized = TRUE, rsquare = TRUE) |&gt; print()\n\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Acculturation =~                                                      \n    acculscl          1.000                               3.434    0.954\n    status            0.444    0.050    8.784    0.000    1.523    0.462\n    percent           0.516    0.051   10.070    0.000    1.771    0.723\n  Stress =~                                                             \n    interpers         1.000                               1.679    0.562\n    job               1.456    0.125   11.655    0.000    2.445    0.683\n  Depression =~                                                         \n    scl90d            1.000                               3.502    0.948\n\nComposites:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SES &lt;~                                                                \n    educ              1.000                               0.192    0.627\n    income            1.013    0.501    2.023    0.043    0.194    0.669\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Stress ~                                                              \n    Acculturtn        0.068    0.021    3.241    0.001    0.139    0.139\n  Depression ~                                                          \n    SES        (b)   -0.095    0.032   -3.014    0.003   -0.141   -0.141\n    Stress            1.469    0.126   11.704    0.000    0.704    0.704\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .status ~~                                                             \n   .percent           1.665    0.307    5.424    0.000    1.665    0.336\n  Acculturation ~~                                                      \n    income            2.871    0.404    7.101    0.000    0.836    0.243\n    educ              2.469    0.382    6.455    0.000    0.719    0.220\n  educ ~~                                                               \n    income            2.135    0.365    5.852    0.000    2.135    0.190\n  SES ~~                                                                \n    Acculturation     0.000                                 NaN      NaN\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .scl90d            1.369                               1.369    0.100\n   .acculscl          1.157    1.115    1.038    0.299    1.157    0.089\n   .status            8.559    0.451   18.997    0.000    8.559    0.787\n   .percent           2.862    0.323    8.856    0.000    2.862    0.477\n   .interpers         6.112    0.357   17.099    0.000    6.112    0.684\n   .job               6.823    0.569   11.988    0.000    6.823    0.533\n    educ             10.682    0.482   22.170    0.000   10.682    1.000\n    income           11.822    0.533   22.170    0.000   11.822    1.000\n    SES               0.000                               0.000    0.000\n    Acculturation    11.790    1.256    9.386    0.000    1.000    1.000\n   .Stress            2.765    0.365    7.574    0.000    0.981    0.981\n   .Depression        6.036    0.591   10.210    0.000    0.492    0.492\n\nR-Square:\n                   Estimate\n    scl90d            0.900\n    acculscl          0.911\n    status            0.213\n    percent           0.523\n    interpers         0.316\n    job               0.467\n    Stress            0.019\n    Depression        0.508\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nDefine a customized plot function using semPlot::semPaths()\nsemPaths2 &lt;- function(model, what = 'est', layout = \"tree2\", rotation = 2) {\n  semPlot::semPaths(model, what = what, edge.label.cex = 1, edge.color = \"black\", layout = layout, rotation = rotation, weighted = FALSE, asize = 2, label.cex = 1, node.width = 1.2)\n}\n\n\n\n# semPaths2: a customized plot function using semPlot::semPaths()\nsemPaths2(shenSEScomposite, layout = \"tree2\", rotation = 2)\n\n\n\n\n\n\n\n\n\n# figure 16.2(c)\nlavaan::summary(shenSESnoComposite, header = FALSE, fit.measures = FALSE, \n standardized = TRUE, rsquare = TRUE) |&gt; print()\n\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Acculturation =~                                                      \n    acculscl          1.000                               3.434    0.954\n    status            0.444    0.050    8.784    0.000    1.523    0.462\n    percent           0.516    0.051   10.070    0.000    1.771    0.723\n  Stress =~                                                             \n    interpers         1.000                               1.679    0.562\n    job               1.456    0.125   11.655    0.000    2.445    0.683\n  Depression =~                                                         \n    scl90d            1.000                               3.502    0.948\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Stress ~                                                              \n    Acculturation     0.068    0.021    3.241    0.001    0.139    0.139\n  Depression ~                                                          \n    educ             -0.095    0.032   -3.014    0.003   -0.027   -0.089\n    income           -0.096    0.030   -3.209    0.001   -0.027   -0.095\n    Stress            1.469    0.126   11.704    0.000    0.704    0.704\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .status ~~                                                             \n   .percent           1.665    0.307    5.424    0.000    1.665    0.336\n  Acculturation ~~                                                      \n    income            2.871    0.404    7.101    0.000    0.836    0.243\n    educ              2.469    0.382    6.455    0.000    0.719    0.220\n  educ ~~                                                               \n    income            2.135    0.365    5.852    0.000    2.135    0.190\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .scl90d            1.369                               1.369    0.100\n   .acculscl          1.157    1.115    1.038    0.299    1.157    0.089\n   .status            8.559    0.451   18.997    0.000    8.559    0.787\n   .percent           2.862    0.323    8.856    0.000    2.862    0.477\n   .interpers         6.112    0.357   17.099    0.000    6.112    0.684\n   .job               6.823    0.569   11.988    0.000    6.823    0.533\n    educ             10.682    0.482   22.170    0.000   10.682    1.000\n    income           11.822    0.533   22.170    0.000   11.822    1.000\n    Acculturation    11.790    1.256    9.386    0.000    1.000    1.000\n   .Stress            2.765    0.365    7.574    0.000    0.981    0.981\n   .Depression        6.036    0.591   10.210    0.000    0.492    0.492\n\nR-Square:\n                   Estimate\n    scl90d            0.900\n    acculscl          0.911\n    status            0.213\n    percent           0.523\n    interpers         0.316\n    job               0.467\n    Stress            0.019\n    Depression        0.508\n\n\n\n\nsemPaths2(shenSESnoComposite, layout = \"tree\", rotation = 2)\n\n\n\n\n\n\n\n\n\n\nCompare estimates\n\ndf1 &lt;- parameterEstimates(shenSEScomposite) |&gt; \n  mutate(est = est |&gt; round(3)) |&gt; \n  select(lhs, op, rhs, est)\ndf2 &lt;- parameterEstimates(shenSESnoComposite) |&gt; \n  mutate(est = est |&gt; round(3)) |&gt; \n  select(lhs, op, rhs, est)\n\n# join df1 and df2\nfull_join(df1, df2, by = c(\"lhs\", \"op\", \"rhs\"), suffix = c(\".composite\", \".noComposite\")) |&gt; print()\n\n             lhs op           rhs est.composite est.noComposite\n1            SES &lt;~          educ         1.000              NA\n2            SES &lt;~        income         1.013              NA\n3  Acculturation =~      acculscl         1.000           1.000\n4  Acculturation =~        status         0.444           0.444\n5  Acculturation =~       percent         0.516           0.516\n6         status ~~       percent         1.665           1.665\n7         Stress =~     interpers         1.000           1.000\n8         Stress =~           job         1.456           1.456\n9     Depression =~        scl90d         1.000           1.000\n10        scl90d ~~        scl90d         1.369           1.369\n11 Acculturation ~~        income         2.871           2.871\n12 Acculturation ~~          educ         2.469           2.469\n13          educ ~~        income         2.135           2.135\n14           SES ~~ Acculturation         0.000              NA\n15        Stress  ~ Acculturation         0.068           0.068\n16    Depression  ~           SES        -0.095              NA\n17    Depression  ~        Stress         1.469           1.469\n18      acculscl ~~      acculscl         1.157           1.157\n19        status ~~        status         8.559           8.559\n20       percent ~~       percent         2.862           2.862\n21     interpers ~~     interpers         6.112           6.112\n22           job ~~           job         6.823           6.823\n23          educ ~~          educ        10.682          10.682\n24        income ~~        income        11.822          11.822\n25           SES ~~           SES         0.000              NA\n26 Acculturation ~~ Acculturation        11.790          11.790\n27        Stress ~~        Stress         2.765           2.765\n28    Depression ~~    Depression         6.036           6.036\n29    Depression  ~          educ            NA          -0.095\n30    Depression  ~        income            NA          -0.096",
    "crumbs": [
      "Kline's",
      "Composite",
      "Composite: Formative"
    ]
  },
  {
    "objectID": "contents/Kline/kchap16-4.html",
    "href": "contents/Kline/kchap16-4.html",
    "title": "Chapter 16. Composite Models",
    "section": "",
    "text": "Load libraries\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(cSEM)\nlibrary(psych)",
    "crumbs": [
      "Kline's",
      "Composite",
      "Composite: HO"
    ]
  },
  {
    "objectID": "contents/Kline/kchap16-4.html#henselerogasawara-ho-specification-and-ml-analysis",
    "href": "contents/Kline/kchap16-4.html#henselerogasawara-ho-specification-and-ml-analysis",
    "title": "Chapter 16. Composite Models",
    "section": "Henseler–Ogasawara (HO) specification and ML analysis",
    "text": "Henseler–Ogasawara (HO) specification and ML analysis\n\nYu, X., Schuberth, F., & Henseler, J. (2023). Specifying composites in structural equation modeling: A refinement of the Henseler–Ogasawara specification. Statistical Analysis and Data Mining: The ASA Data Science Journal, 16(4), 348-357.\nSchuberth, F. (2021). The Henseler-Ogasawara specification of composites in structural equation modeling: A tutorial. Psychological Methods.\n\n\n\nSource: Fig 16.4 (p. 302)\n\n\n# input the correlations in lower diagnonal form\nshenLower.cor &lt;- '\n 1.00\n  .44 1.00\n  .69  .54 1.00\n  .21  .08  .16 1.00\n  .23  .15  .19  .19 1.00\n  .12  .08  .08  .08 -.03 1.00\n  .09  .06  .04  .01 -.02  .38 1.00\n  .03  .02 -.02 -.07 -.11  .37  .46 1.00 '\n\n# name the variables and convert to full correlation matrix\nshen.cor &lt;- lavaan::getCov(shenLower.cor, names = c(\"acculscl\", \"status\",\n \"percent\", \"educ\", \"income\", \"interpers\", \"job\", \"scl90d\"))\n \n# add the standard deviations and convert to covariances\nshen.cov &lt;- lavaan::cor2cov(shen.cor, sds = c(3.60,3.30,2.45,3.27,3.44,2.99,\n 3.58,3.70))\n\n\n# display correlations and covariances\nshen.cor |&gt; print()\n\n          acculscl status percent  educ income interpers   job scl90d\nacculscl      1.00   0.44    0.69  0.21   0.23      0.12  0.09   0.03\nstatus        0.44   1.00    0.54  0.08   0.15      0.08  0.06   0.02\npercent       0.69   0.54    1.00  0.16   0.19      0.08  0.04  -0.02\neduc          0.21   0.08    0.16  1.00   0.19      0.08  0.01  -0.07\nincome        0.23   0.15    0.19  0.19   1.00     -0.03 -0.02  -0.11\ninterpers     0.12   0.08    0.08  0.08  -0.03      1.00  0.38   0.37\njob           0.09   0.06    0.04  0.01  -0.02      0.38  1.00   0.46\nscl90d        0.03   0.02   -0.02 -0.07  -0.11      0.37  0.46   1.00\n\n\n\nshen.cov |&gt; print(digits = 2)\n\n          acculscl status percent  educ income interpers   job scl90d\nacculscl      13.0   5.23    6.09  2.47   2.85      1.29  1.16   0.40\nstatus         5.2  10.89    4.37  0.86   1.70      0.79  0.71   0.24\npercent        6.1   4.37    6.00  1.28   1.60      0.59  0.35  -0.18\neduc           2.5   0.86    1.28 10.69   2.14      0.78  0.12  -0.85\nincome         2.8   1.70    1.60  2.14  11.83     -0.31 -0.25  -1.40\ninterpers      1.3   0.79    0.59  0.78  -0.31      8.94  4.07   4.09\njob            1.2   0.71    0.35  0.12  -0.25      4.07 12.82   6.09\nscl90d         0.4   0.24   -0.18 -0.85  -1.40      4.09  6.09  13.69\n\n\n\nHO specification\n\nexcrescent variables, ex1 to ex4 error variances of all indicators fixed to zero\nstart values of zero are needed for two indicators of the acculturation composite for estimation to normally converge\n\n\nshenHO.model &lt;- '\n  # composites, excrescent variables, and indicators\n  # acculturation\n  Acculturation =~ acculscl+ start(0)*percent+ start(0)*status \n  ex1 =~ percent + acculscl + status\n  ex2 =~ status + acculscl + 0*percent\n  acculscl ~~ 0*acculscl\n    percent ~~ 0*percent\n  status ~~ 0*status\n\n  # SES\n  SES =~ educ + income\n  ex4 =~ income + educ\n  educ ~~ 0*educ\n  income ~~ 0*income\n  \n  # stress\n  Stress =~ interpers + job\n  ex3 =~ job + interpers\n  interpers ~~ 0*interpers\n  job ~~ 0*job\n  \n  # depression, fix indicator error variance\n  # to 1.369\n  Depression =~ scl90d\n  scl90d ~~ 1.369*scl90d \n  \n  # covariance between SES and acculturation\n  # is a free parameter\n  Acculturation ~~ SES\n  \n  # constrain covariances to zero\n  ex1 ~~ 0*Acculturation + 0*Stress + 0*SES + 0*Depression\n    + 0*ex2 + 0*ex3 + 0*ex4\n  ex2 ~~ 0*Acculturation + 0*Stress + 0*SES + 0*Depression\n    + 0*ex3 + 0*ex4\n  ex3 ~~ 0*Acculturation + 0*Stress + 0*SES + 0*Depression\n    + 0*ex4\n  ex4 ~~ 0*Acculturation + 0*Stress + 0*SES + 0*Depression\n  \n  # structural model\n  Stress ~ a*Acculturation\n  Depression ~ b*Stress + SES\n  \n  # define indirect effect\n  ab := a * b\n'\n\n\n# fit ho-specified model to data\n# estimates for excrescent variables have no interpretive value\n \nshenHO &lt;- lavaan::sem(model = shenHO.model, sample.cov = shen.cov,\n sample.nobs = 983)  \nlavaan::summary(shenHO, standardized = TRUE, fit.measures = TRUE,\n rsquare = TRUE) |&gt; print()\n\nlavaan 0.6.17 ended normally after 245 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n  Number of observations                           983\n\nModel Test User Model:\n                                                      \n  Test statistic                                19.334\n  Degrees of freedom                                15\n  P-value (Chi-square)                           0.199\n\nModel Test Baseline Model:\n\n  Test statistic                              1606.002\n  Degrees of freedom                                28\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.997\n  Tucker-Lewis Index (TLI)                       0.995\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -19670.378\n  Loglikelihood unrestricted model (H1)     -19660.711\n                                                      \n  Akaike (AIC)                               39382.757\n  Bayesian (BIC)                             39485.459\n  Sample-size adjusted Bayesian (SABIC)      39418.763\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.017\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.037\n  P-value H_0: RMSEA &lt;= 0.050                    0.999\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.022\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Acculturation =~                                                      \n    acculscl          1.000                               3.564    0.991\n    percent           0.519    0.051   10.074    0.000    1.849    0.755\n    status            0.507    0.086    5.870    0.000    1.805    0.547\n  ex1 =~                                                                \n    percent           1.000                               1.606    0.656\n    acculscl         -0.198    0.241   -0.820    0.412   -0.318   -0.088\n    status            0.397    0.207    1.920    0.055    0.637    0.193\n  ex2 =~                                                                \n    status            1.000                               2.686    0.814\n    acculscl         -0.140    0.149   -0.941    0.346   -0.376   -0.105\n    percent           0.000                               0.000    0.000\n  SES =~                                                                \n    educ              1.000                               2.379    0.728\n    income            1.173    0.196    5.969    0.000    2.790    0.811\n  ex4 =~                                                                \n    income            1.000                               2.009    0.584\n    educ             -1.115    0.261   -4.268    0.000   -2.241   -0.686\n  Stress =~                                                             \n    interpers         1.000                               2.233    0.747\n    job               1.440    0.122   11.798    0.000    3.216    0.899\n  ex3 =~                                                                \n    job               1.000                               1.570    0.439\n    interpers        -1.265    0.211   -5.989    0.000   -1.986   -0.665\n  Depression =~                                                         \n    scl90d            1.000                               3.501    0.948\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Stress ~                                                              \n    Acculturtn (a)    0.076    0.020    3.768    0.000    0.122    0.122\n  Depression ~                                                          \n    Stress     (b)    0.839    0.064   13.034    0.000    0.535    0.535\n    SES              -0.190    0.046   -4.119    0.000   -0.129   -0.129\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Acculturation ~~                                                      \n    SES               2.447    0.367    6.674    0.000    0.289    0.289\n    ex1               0.000                               0.000    0.000\n  ex1 ~~                                                                \n   .Stress            0.000                               0.000    0.000\n    SES               0.000                               0.000    0.000\n   .Depression        0.000                               0.000    0.000\n    ex2               0.000                               0.000    0.000\n    ex3               0.000                               0.000    0.000\n    ex4               0.000                               0.000    0.000\n  Acculturation ~~                                                      \n    ex2               0.000                               0.000    0.000\n  ex2 ~~                                                                \n   .Stress            0.000                               0.000    0.000\n    SES               0.000                               0.000    0.000\n   .Depression        0.000                               0.000    0.000\n    ex3               0.000                               0.000    0.000\n    ex4               0.000                               0.000    0.000\n  Acculturation ~~                                                      \n    ex3               0.000                               0.000    0.000\n .Stress ~~                                                             \n    ex3               0.000                               0.000    0.000\n  SES ~~                                                                \n    ex3               0.000                               0.000    0.000\n  ex3 ~~                                                                \n   .Depression        0.000                               0.000    0.000\n  ex4 ~~                                                                \n    ex3               0.000                               0.000    0.000\n  Acculturation ~~                                                      \n    ex4               0.000                               0.000    0.000\n  ex4 ~~                                                                \n   .Stress            0.000                               0.000    0.000\n  SES ~~                                                                \n    ex4               0.000                               0.000    0.000\n  ex4 ~~                                                                \n   .Depression        0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .acculscl          0.000                               0.000    0.000\n   .percent           0.000                               0.000    0.000\n   .status            0.000                               0.000    0.000\n   .educ              0.000                               0.000    0.000\n   .income            0.000                               0.000    0.000\n   .interpers         0.000                               0.000    0.000\n   .job               0.000                               0.000    0.000\n   .scl90d            1.369                               1.369    0.100\n    Acculturation    12.704    0.678   18.746    0.000    1.000    1.000\n    ex1               2.578    0.593    4.346    0.000    1.000    1.000\n    ex2               7.213    0.651   11.079    0.000    1.000    1.000\n    SES               5.660    1.105    5.123    0.000    1.000    1.000\n    ex4               4.037    1.087    3.712    0.000    1.000    1.000\n   .Stress            4.912    0.582    8.447    0.000    0.985    0.985\n    ex3               2.463    0.543    4.540    0.000    1.000    1.000\n   .Depression        8.603    0.450   19.126    0.000    0.702    0.702\n\nR-Square:\n                   Estimate\n    acculscl          1.000\n    percent           1.000\n    status            1.000\n    educ              1.000\n    income            1.000\n    interpers         1.000\n    job               1.000\n    scl90d            0.900\n    Stress            0.015\n    Depression        0.298\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    ab                0.064    0.017    3.770    0.000    0.065    0.065\n\n\n\n\n# predicted covariances\nlavaan::fitted(shenHO)\n\n# predicted correlations\nlavaan::lavInspect(shenHO, \"cor.lv\")\nlavaan::lavInspect(shenHO, \"cor.ov\")\n\n# residuals\nlavaan::residuals(shenHO, type = \"raw\")\nlavaan::residuals(shenHO, type = \"standardized.mplus\")\nlavaan::residuals(shenHO, type = \"cor.bollen\")",
    "crumbs": [
      "Kline's",
      "Composite",
      "Composite: HO"
    ]
  },
  {
    "objectID": "contents/chap12.html",
    "href": "contents/chap12.html",
    "title": "Chapter 12. Path Modeling",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)",
    "crumbs": [
      "Keith's",
      "Path Modeling"
    ]
  },
  {
    "objectID": "contents/chap12.html#연습문제",
    "href": "contents/chap12.html#연습문제",
    "title": "Chapter 12. Path Modeling",
    "section": "연습문제",
    "text": "연습문제\n\nTable 12.1에서 familiy background에 대해서도 동일한 분석을 수행해보세요.\n예전 salary 데이터에 대해서도 동일한 분석을 수행해보세요; 링크",
    "crumbs": [
      "Keith's",
      "Path Modeling"
    ]
  },
  {
    "objectID": "contents/chap16.html",
    "href": "contents/chap16.html",
    "title": "Chapter 15-16. Confirmatory Factor Analysis",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)",
    "crumbs": [
      "Keith's",
      "Factor Analysis"
    ]
  },
  {
    "objectID": "contents/chap16.html#errors",
    "href": "contents/chap16.html#errors",
    "title": "Chapter 15-16. Confirmatory Factor Analysis",
    "section": "Errors",
    "text": "Errors\nLinear model: \\(y = a \\cdot x + b + \\epsilon\\),  \\(\\epsilon\\): errors\n\n\\(E(Y|X=x_i) = a \\cdot x_i + b\\)  (\\(E\\): expectation, 기대값)\n\n\\(x_i\\)에 대해서 conditional mean이 선형함수로 결정됨을 가정\n\n\\(Var(Y|X=x_i) = \\sigma^2\\)\n\\(Y|X=x_i \\sim N(a \\cdot x_i + b, \\sigma^2)\\)\n\nErrors의 소스들:\n\nReducible error: 모형이 잡아내지 못한 신호; 영향을 미치지만 측정하지 않은 변수가 존재\nIrreducible error:\n\n측정 오차 (measurement error): ex. 성별, 젠더, 키, 온도, 강수량, 지능, 불쾌지수, …\nrandom processes: 물리적 세계의 불확실성 (stochastic vs. deterministic world)\n예를 들어, 동전을 4번 던질 때,\n\nH, H, T, H\nH, T, T, H\nT, T, H, H\nT, H, T, H\nT, T, T, H\n…\n\n이들은 보통 Gaussian 분포를 이루거나 가정: ex. 측정 오차들, 동전 앞면의 개수들, 키, 몸무게, IQ, …\n\n그 외에 자연스럽게 나타나는 분포들이 있음; Binomial, Poisson, Exponential, Gamma, Beta, …\n\n\nGaussian/Normal distribution\n\n랜덤한 값들의 합/평균들이 나타내는 분포\n\n측정 오차의 분포(error distribution)\n다양한 힘들의 상호작용으로 인한 분포\n\n분산이 유한한 분포 중에 정보 엔트로피가 최대(maximum entropy)인 분포\n중심극한정리(Central Limit Theorem)\n\n\\(X \\sim N(\\mu, \\sigma^2)\\), density function: \\(\\displaystyle f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\\)\n\\(X \\sim N(0, 1)\\): \\(\\displaystyle f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}x^2}\\), (standard normal distribution)\n예를 들어, 1000명의 사람들이 각자 16번의 동전 던지기를 시행하면서 좌우로 움직인다면,\n\n\n# A tibble: 1,000 × 16\n      V1    V2    V3    V4    V5    V6    V7    V8    V9   V10   V11   V12   V13   V14\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1     1    -1    -1    -1    -1    -1     1    -1    -1    -1    -1    -1     1\n 2     1     1    -1     1     1     1    -1    -1     1    -1     1    -1     1     1\n 3    -1    -1     1    -1    -1     1    -1    -1     1    -1    -1    -1    -1    -1\n 4    -1    -1     1    -1    -1     1     1     1    -1    -1     1    -1    -1    -1\n 5     1     1     1     1    -1     1    -1     1     1    -1    -1    -1     1     1\n 6    -1     1    -1     1    -1    -1     1     1     1     1     1     1     1     1\n 7    -1     1    -1     1     1     1    -1     1     1     1     1    -1     1     1\n 8     1    -1    -1     1    -1    -1    -1     1    -1    -1    -1     1     1     1\n 9     1    -1     1    -1     1     1    -1    -1    -1     1    -1     1     1     1\n10    -1    -1     1     1    -1     1     1    -1    -1     1    -1     1     1     1\n# ℹ 990 more rows\n# ℹ 2 more variables: V15 &lt;dbl&gt;, V16 &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nMesaurement errors\nReliability (신뢰도): 측정이 반복될 때, 얼마나 일관성 있는지\nReliability (신뢰도): \\(\\displaystyle \\rho = \\frac{V_{\\text{True}}}{V_{\\text{Observed}}} = \\frac{V_{\\text{True}}}{V_{\\text{True}} + V_{\\text{Error}}}\\)\n\n\nValidity (타당도): 측정이 실제로 측정하고자 하는 것을 얼마나 잘 측정하는지\n\n다면적인 측정을 통해 공통 요인을 추출해 타당도를 확보\n여러 측정도구를 사용해 타당도를 확보\nConvergent validity (수렴타당도): 동일한 구성개념을 측정하는 서로 다른 테스트들이 서로 높은 상관관계를 나타내는가?\nDiscriminant validity (차별타당도): 서로 다른 구성개념을 측정하는 테스트들이 서로 낮은 상관관계를 나타내는가?\n\n\n\n\n\n\n\n다음과 같이 독해력을 3가지로 측정하는 예를 생각하면,\n\n테스트 1: 참가자가 글을 읽고, 해당 글을 가장 잘 설명하는 그림을 4개의 선택지 중에서 선택.\n테스트 2: 참가자가 지시문을 읽고, 그 지시에 따라 행동을 실행 (예: “일어서서 테이블 주위를 걷고, 다시 앉으세요”).\n테스트 3: 참가자가 단어나 문장이 빠진 글을 읽고, 글의 의미를 바탕으로 빠진 단어나 문장을 채움.\n\n추가로 측정하는 능력 각각: unique variance\n\n읽은 내용을 시각적 이미지로 변환하는 능력.\n읽은 내용을 행동으로 옮기는 능력.\n자신의 지식에서 가장 적합한 단어나 문장을 선택해 글에 삽입하는 능력.\n\n\n\n\n\n\nFactor analysis (요인분석): indicators (지표들, manifest variable)의 분산을 공통 분산(common variance)과 고유 분산(unique variance)으로 분해\n\n(Common) Factors (요인): 구성개념을 나타내는 잠재변수에 대한 근사치로서(proxy) indicator들의 공통 분산으로 구성됨.\n고유 분산(unique variance) = 지표들이 갖는 고유/특정 분산(specific variance) + 측정오차(random measurement error)\n\n특정 분산: 공통 요인들로 설명되지 않는 분산\n측정오차: 측정 과정에서 발생하는 오차\n\n각 indicator들은 공통 요인과 고유 분산에 의해 결정/설명된다고 봄.\n\n공통 요인의 값을 구체적으로 구하기 어려움: “factor score indeterminacy”\n보통 요인의 개수를 결정하기 위해 탐색적 요인 분석(exploratory factor analysis, EFA)을 사용하며, 요인의 구조를 파악한 후 이를 확인하는 작업으로 확인적 요인 분석(confirmatory factor analysis, CFA)을 사용하지만, 현실적으로는 이 둘을 확실히 구분하기는 어려움.\n요인에 개수와 구조는 일반적으로 통계적 접근으로만 결정하기 어려움: 이론적 근거와 함께 고려\n\n\n예를 들어, 3-indicator, 1-factor 모형을 보면,\n\\(x_1 = \\lambda_{11} \\xi_1 + \\delta_1\\)\n\\(x_2 = \\lambda_{21} \\xi_1 + \\delta_2\\)\n\\(x_3 = \\lambda_{31} \\xi_1 + \\delta_3\\)\n(회귀계수 \\(\\lambda\\): factor loading, 요인부하량)\n가정: \\(COV(\\xi_1, \\delta_i) = 0\\),  \\(COV(\\delta_i, \\delta_j) = 0\\)\n따라서, \\(\\mathbf{\\Theta_{\\delta}} =\n\\begin{bmatrix}\nV(\\delta_1) \\\\\n0 & V(\\delta_2) \\\\\n0 & 0 & V(\\delta_3)\n\\end{bmatrix}\\)\n\n\n\n\nImplied covariance matrix:\n\\[\n\\Sigma(\\theta) = \\begin{bmatrix}\n\\lambda_{11}^2 V(\\xi_1) + V(\\delta_1)  \\\\\n\\lambda_{21} \\lambda_{11} V(\\xi_1) & \\lambda_{21}^2 V(\\xi_1) + V(\\delta_2) \\\\\n\\lambda_{31} \\lambda_{11} V(\\xi_1) & \\lambda_{31} \\lambda_{21} V(\\xi_1) & \\lambda_{31}^2 V(\\xi_1) + V(\\delta_3)\n\\end{bmatrix}\n\\]\nIdentified되려면, 적어도 한 개의 파라미터를 줄여야 함: 고정하거나 동등하게 설정\n\n\\(\\lambda_{11} = 1\\): unit loading identification (ULI)\n\n잠재변수가 지표와 동일한 단위를 갖게됨: \\(x_1 = 1 \\cdot \\xi_1 + \\delta_1\\)\nHave the same scale in the more limited sense that on average a one-unit shift of \\(\\xi_1\\) leads to a one unit shift in \\(x_1\\)\n잠재변수의 단위(metric)가 의미를 갖고 중요한 관심사인 경우\n\n\\(V(\\xi_1) = 1\\): unit variance identification (UVI)\n\n잠재변수가 표준화됨: 평균 0, 분산 1\n모든 요인들의 부하(factor loading)를 추정할 수 있고,\n공분산이 상관계수의 의미를 갖게됨.\n\n또는 예를 들어, 두 파라미터가 동일하도록 제약: \\(\\lambda_{11} = \\lambda_{21}\\)\n또는 effect coding: \\(\\displaystyle \\frac{\\lambda_{11} + \\lambda_{21} + \\lambda_{31}}{3} = 1\\); 요인의 분산이 모든 지표들에 의해 좀 더 안정적으로 추정됨.\n\n앞서 살펴본 homework와 grade의 관계에 대한 모형을 잠재변수를 통해 살펴보면,",
    "crumbs": [
      "Keith's",
      "Factor Analysis"
    ]
  },
  {
    "objectID": "contents/chap16.html#confirmatory-factor-analysis",
    "href": "contents/chap16.html#confirmatory-factor-analysis",
    "title": "Chapter 15-16. Confirmatory Factor Analysis",
    "section": "Confirmatory Factor Analysis",
    "text": "Confirmatory Factor Analysis\n예제: The Differential Ability Scales, Second Edition (DAS-II; Elliott, 2007)\n\nVerbal reason­ing (Verbal Ability)\nNonverbal, inductive reasoning (Nonverbal Reasoning)\nVisual–spatial reasoning (Spatial)\nShort-term memory (Memory)\n\n\n\n\n\n\n\n\n제약 방식\n\n\n\nUnit-loading identification (ULI)\n\n첫번째 indicator의 loading을 1로 설정: default\n다른 indicator의 loading을 1로 하려면: NA 사용하여 free the first parameter\n\n'f1 =~ NA*x1 + 1*x2'\nUnit-variance identification (UVI)\n\nfactor의 분산을 1로 직접 제약\n\n'f1 =~ NA*x1 + x2 + x3\n f1 ~~ 1*f1'\n\nstandardized solutions: std.lv, std.all, std.nox\n\nstd.lv: latent variable만 표준화, 즉 위와 같이 factor의 분산을 1로 설정\nstd.all: latent와 indicator 모두 표준화\n\nsummary(model, standardized=TRUE)  # 또는 type을 지정\nstandardizedSolution(fit, type=\"...\")\n\nEffect codings\n'f1 =~ a*x1 + b*x2 + c*x3\n  a + b + c == 3'  # constraint\n\n\n\n\nLoad the data\ndas2 &lt;- haven::read_sav(\"data/chap 16 CFA 1/das 2 cov.sav\")\ndas2cov &lt;- das2[1:12, 3:14] |&gt;\n  as.matrix() |&gt;\n  lav_matrix_vechr(diagonal = TRUE) |&gt;\n  getCov(names = c(\"wdss\", \"vsss\", \"sqss\", \"soss\", \"rpss\", \"rdss\", \"psss\", \"pcss\", \"nvss\", \"mass\", \"dfss\", \"dbss\"))\n\n\nDAS-II 모형의 CFA:\n\ndas2_model &lt;- '\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss\n  Spatial =~ pcss + rdss + rpss\n  Memory =~ dfss + dbss + soss\n'\nfit &lt;- sem(das2_model, sample.cov = das2cov, sample.nobs = 800)\nsummary(fit, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 164 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        30\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               127.986\n  Degrees of freedom                                48\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              4335.087\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.981\n  Tucker-Lewis Index (TLI)                       0.974\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33708.567\n  Loglikelihood unrestricted model (H1)     -33644.574\n                                                      \n  Akaike (AIC)                               67477.134\n  Bayesian (BIC)                             67617.673\n  Sample-size adjusted Bayesian (SABIC)      67522.406\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.046\n  90 Percent confidence interval - lower         0.036\n  90 Percent confidence interval - upper         0.055\n  P-value H_0: RMSEA &lt;= 0.050                    0.762\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.027\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              1.000                               7.463    0.739\n    wdss              0.942    0.049   19.266    0.000    7.029    0.733\n    vsss              1.100    0.053   20.900    0.000    8.207    0.805\n  Nonverbal =~                                                          \n    psss              1.000                               5.570    0.541\n    mass              1.306    0.093   14.104    0.000    7.273    0.710\n    sqss              1.406    0.094   15.030    0.000    7.831    0.808\n  Spatial =~                                                            \n    pcss              1.000                               7.499    0.814\n    rdss              0.982    0.047   20.996    0.000    7.365    0.737\n    rpss              0.795    0.049   16.388    0.000    5.961    0.591\n  Memory =~                                                             \n    dfss              1.000                               7.387    0.669\n    dbss              1.035    0.058   17.961    0.000    7.643    0.754\n    soss              1.119    0.061   18.392    0.000    8.265    0.778\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal ~~                                                             \n    Nonverbal        33.464    3.077   10.875    0.000    0.805    0.805\n    Spatial          42.028    3.283   12.801    0.000    0.751    0.751\n    Memory           45.428    3.691   12.309    0.000    0.824    0.824\n  Nonverbal ~~                                                          \n    Spatial          37.267    3.202   11.638    0.000    0.892    0.892\n    Memory           35.230    3.285   10.724    0.000    0.856    0.856\n  Spatial ~~                                                            \n    Memory           44.873    3.539   12.680    0.000    0.810    0.810\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .nvss             46.169    2.923   15.793    0.000   46.169    0.453\n   .wdss             42.471    2.663   15.949    0.000   42.471    0.462\n   .vsss             36.520    2.709   13.482    0.000   36.520    0.352\n   .psss             74.838    4.000   18.711    0.000   74.838    0.707\n   .mass             51.973    3.120   16.657    0.000   51.973    0.496\n   .sqss             32.554    2.475   13.154    0.000   32.554    0.347\n   .pcss             28.663    2.240   12.794    0.000   28.663    0.338\n   .rdss             45.628    2.886   15.808    0.000   45.628    0.457\n   .rpss             66.343    3.650   18.177    0.000   66.343    0.651\n   .dfss             67.275    3.897   17.263    0.000   67.275    0.552\n   .dbss             44.461    2.880   15.440    0.000   44.461    0.432\n   .soss             44.555    3.050   14.609    0.000   44.555    0.395\n    Verbal           55.704    4.882   11.410    0.000    1.000    1.000\n    Nonverbal        31.029    4.002    7.754    0.000    1.000    1.000\n    Spatial          56.231    4.351   12.925    0.000    1.000    1.000\n    Memory           54.573    5.447   10.018    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    nvss              0.547\n    wdss              0.538\n    vsss              0.648\n    psss              0.293\n    mass              0.504\n    sqss              0.653\n    pcss              0.662\n    rdss              0.543\n    rpss              0.349\n    dfss              0.448\n    dbss              0.568\n    soss              0.605\n\n\n\n\n\n\n\n\n\n\nComposite Reliability\n\n\n\n이상적으로 요인이 지표들을 50% 이상 설명해 주기를 바람: \\(R^2 &gt; 0.5\\)\n좀 더 느슨하게는 average variance extracted (AVE)가 0.5 이상이면 좋음.\nsemTools::AVE(fit)\n# Verbal Nonverbal   Spatial    Memory \n#  0.579     0.477     0.509     0.537 \nComposite reliability에 대한 참고: pp. 239-241, Kline(2023)\n# Cronbach's alpha: uni-dimensional, tau-equivalent reliabiliy; factor loading을 동일하게 설정/가정\nsemTools::compRelSEM(fit, tau.eq = TRUE, obs.var = TRUE)\n# Verbal Nonverbal   Spatial    Memory \n#  0.804     0.713     0.754     0.776\n\n# Cofficient omega: a model-based alternative; permit covariances among the indicators\nsemTools::compRelSEM(fit)  # tau.eq = FALSE\n# Verbal Nonverbal   Spatial    Memory \n#  0.804     0.737     0.753     0.776 \n        \\(\\displaystyle\\omega = \\frac{(\\sum \\lambda_i)^2 \\cdot \\phi}{(\\sum \\lambda_i)^2 \\cdot \\phi + \\sum \\theta_{ii}}\\),   \\(\\phi\\): factor variance,   \\(\\theta_{ii}\\): error variance\n# Composite reliability\nsemTools::reliability(fit) # depricated\n#        Verbal Nonverbal Spatial Memory\n# alpha   0.804     0.713   0.754  0.776\n# omega   0.805     0.728   0.755  0.776\n# omega2  0.805     0.728   0.755  0.776\n# omega3  0.804     0.737   0.753  0.776\n# avevar  0.579     0.477   0.509  0.537\nCronbach’s alpha: psych::alpha()\n\n\n\nImplied Covariance/Correlation Matrix\n\n# implied covariance matrix\nfitted(fit)$cov |&gt; round(2) |&gt; print()\n\n       nvss   wdss   vsss   psss   mass   sqss   pcss   rdss   rpss   dfss   dbss   soss\nnvss 101.87                                                                             \nwdss  52.46  91.88                                                                      \nvsss  61.25  57.69 103.87                                                               \npsss  33.46  31.52  36.80 105.87                                                        \nmass  43.69  41.15  48.04  40.51 104.87                                                 \nsqss  47.05  44.31  51.73  43.62  56.96  93.88                                          \npcss  42.03  39.58  46.21  37.27  48.66  52.39  84.89                                   \nrdss  41.28  38.88  45.39  36.60  47.79  51.46  55.23  99.87                            \nrpss  33.41  31.46  36.73  29.62  38.68  41.65  44.70  43.90 101.87                     \ndfss  45.43  42.79  49.95  35.23  46.00  49.53  44.87  44.07  35.67 121.85              \ndbss  47.00  44.27  51.68  36.45  47.59  51.24  46.42  45.60  36.90  56.46 102.87       \nsoss  50.82  47.87  55.88  39.41  51.46  55.41  50.20  49.31  39.90  61.05  63.16 112.86\n\n\n\n# implied correlation matrix\ninspect(fit, \"cor.ov\") |&gt; print()  \n# cor.ov: observed variables only\n# cor.lv: latent variables only\n# cor.all: all variables\n\n      nvss  wdss  vsss  psss  mass  sqss  pcss  rdss  rpss  dfss  dbss  soss\nnvss 1.000                                                                  \nwdss 0.542 1.000                                                            \nvsss 0.595 0.591 1.000                                                      \npsss 0.322 0.320 0.351 1.000                                                \nmass 0.423 0.419 0.460 0.384 1.000                                          \nsqss 0.481 0.477 0.524 0.438 0.574 1.000                                    \npcss 0.452 0.448 0.492 0.393 0.516 0.587 1.000                              \nrdss 0.409 0.406 0.446 0.356 0.467 0.531 0.600 1.000                        \nrpss 0.328 0.325 0.357 0.285 0.374 0.426 0.481 0.435 1.000                  \ndfss 0.408 0.404 0.444 0.310 0.407 0.463 0.441 0.400 0.320 1.000            \ndbss 0.459 0.455 0.500 0.349 0.458 0.521 0.497 0.450 0.360 0.504 1.000      \nsoss 0.474 0.470 0.516 0.361 0.473 0.538 0.513 0.464 0.372 0.521 0.586 1.000\n\n\n\n\nResiduals\nresiduals(fit, type = \"cor.bentler\")\n\n\n\n\n\n\nResidual types\n\n\n\n\n“raw”: returns the raw (= unscaled) difference between the observed and the expected (model-implied) summary statistics.\n“cor”, or “cor.bollen”: the observed and model implied covariance matrices are first transformed to a correlation matrix (using cov2cor()), before the residuals are computed.\n“cor.bentler”: both the observed and model implied covariance matrices are rescaled by dividing the elements by the square roots of the corresponding variances of the observed covariance matrix.\n“normalized”: the residuals are divided by the square root of the asymptotic variance of the corresponding summary statistic (the variance estimate depends on the choice for the se argument). Unfortunately, the corresponding normalized residuals are not entirely correct, and this option is only available for historical interest.\n“standardized”: the residuals are divided by the square root of the asymptotic variance of these residuals. The resulting standardized residuals elements can be interpreted as z-scores.\n“standardized.mplus”: the residuals are divided by the square root of the asymptotic variance of these residuals. However, a simplified formula is used (see the Mplus reference below) which often results in negative estimates for the variances, resulting in many NA values for the standardized residuals.\n\n\n\n\n\n\n\n\n\nSummary options\n\n\n\n참고로 residuals()은 lavResiduals() 함수의 shortcut; 조금 다름…\nlavResiduals(fit, type = \"cor\", summary = TRUE)\nsummary: Logical. If TRUE, show various summaries of the (possibly scaled) residuals.\n\nWhen type = “raw”, we compute the RMR.\nWhen type = “cor.bentler”, we compute the SRMR.\nWhen type = “cor.bollen”, we compute the CRMR.\n\nAn unbiased version of these summaries is also computed, as well as a standard error, a z-statistic and a p-value for the test of exact fit based on these summaries.\n\n\n\nresiduals(fit) |&gt; print()\n\n$type\n[1] \"raw\"\n\n$cov\n       nvss   wdss   vsss   psss   mass   sqss   pcss   rdss   rpss   dfss   dbss   soss\nnvss  0.000                                                                             \nwdss  1.468  0.000                                                                      \nvsss -1.326  0.239  0.000                                                               \npsss  5.487  5.436  5.151  0.000                                                        \nmass -3.742  0.796 -0.103 -1.562  0.000                                                 \nsqss -3.101 -2.363  1.203 -4.672  2.968  0.000                                          \npcss  4.914 -2.630  2.726  1.684 -1.716  1.540  0.000                                   \nrdss  2.665 -7.918 -1.445  4.345 -6.843 -1.523  0.700  0.000                            \nrpss  0.550 -3.500 -0.779  4.334  1.272  2.298 -3.749  4.038  0.000                     \ndfss  4.509  1.158  1.983  1.724 -6.048 -2.588 -0.928  1.869 -2.710  0.000              \ndbss -4.052 -2.318 -0.742  0.506  3.348  1.693  1.517  1.344  1.051 -0.529  0.000       \nsoss  1.112  2.070 -0.953 -2.460  3.471 -1.478 -1.262 -1.368  0.045  0.869 -0.242  0.000\n\n\n\n\nresiduals(fit, type=\"standardized.mplus\") |&gt; print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n       nvss   wdss   vsss   psss   mass   sqss   pcss   rdss   rpss   dfss   dbss   soss\nnvss  0.000                                                                             \nwdss  1.254  0.000                                                                      \nvsss -1.943  0.291  0.000                                                               \npsss  2.257  2.334  2.280  0.000                                                        \nmass -2.104  0.434 -0.062 -0.845  0.005                                                 \nsqss -2.319 -1.795  0.929 -4.384  3.076  0.005                                          \npcss  2.944 -1.947  1.958  0.993 -1.501  1.590  0.000                                   \nrdss  1.364 -4.943 -0.892  2.004 -5.056 -1.389  1.043  0.005                            \nrpss  0.238 -1.638 -0.369  1.681  0.610  1.368 -4.457  2.365  0.007                     \ndfss  2.014  0.562  1.012  0.650 -3.041 -1.630 -0.552  0.859 -1.075  0.007              \ndbss -2.629 -1.506 -0.520  0.233  1.838  1.240  1.101  0.775  0.490 -0.377  0.006       \nsoss  0.648  1.226 -0.686 -1.161  1.901 -1.264 -1.034 -0.840  0.021  0.621 -0.280  0.005\n\n\n\n\nresiduals(fit, type = \"cor.bollen\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       nvss   wdss   vsss   psss   mass   sqss   pcss   rdss   rpss   dfss   dbss   soss\nnvss  0.000                                                                             \nwdss  0.015  0.000                                                                      \nvsss -0.013  0.002  0.000                                                               \npsss  0.053  0.055  0.049  0.000                                                        \nmass -0.036  0.008 -0.001 -0.015  0.000                                                 \nsqss -0.032 -0.025  0.012 -0.047  0.030  0.000                                          \npcss  0.053 -0.030  0.029  0.018 -0.018  0.017  0.000                                   \nrdss  0.026 -0.083 -0.014  0.042 -0.067 -0.016  0.008  0.000                            \nrpss  0.005 -0.036 -0.008  0.042  0.012  0.023 -0.040  0.040  0.000                     \ndfss  0.040  0.011  0.018  0.015 -0.054 -0.024 -0.009  0.017 -0.024  0.000              \ndbss -0.040 -0.024 -0.007  0.005  0.032  0.017  0.016  0.013  0.010 -0.005  0.000       \nsoss  0.010  0.020 -0.009 -0.023  0.032 -0.014 -0.013 -0.013  0.000  0.007 -0.002  0.000\n\n\n\n\n# 필터링: 절대값이 0.05보다 작은 값은 NA로 대체\nresid_cor &lt;- residuals(fit, type = \"cor.bollen\")$cov\nresid_cor[abs(resid_cor) &lt; 0.05] &lt;- NA\nresid_cor |&gt; print()\n\n       nvss   wdss   vsss   psss   mass   sqss   pcss   rdss   rpss   dfss   dbss   soss\nnvss      .                                                                             \nwdss     NA      .                                                                      \nvsss     NA     NA      .                                                               \npsss  0.053  0.055     NA      .                                                        \nmass     NA     NA     NA     NA      .                                                 \nsqss     NA     NA     NA     NA     NA      .                                          \npcss  0.053     NA     NA     NA     NA     NA      .                                   \nrdss     NA -0.083     NA     NA -0.067     NA     NA      .                            \nrpss     NA     NA     NA     NA     NA     NA     NA     NA      .                     \ndfss     NA     NA     NA     NA -0.054     NA     NA     NA     NA      .              \ndbss     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA      .       \nsoss     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA      .\n\n\n\n\nModification Indices\n파라미터를 추가로 추정하여 \\(\\chi^2\\) 값의 변화를 확인\n\nmodindices(fit, sort = TRUE, maximum.number = 8) |&gt; print()\n\n          lhs op  rhs     mi    epc sepc.lv sepc.all sepc.nox\n109      mass ~~ sqss 17.701 12.611  12.611    0.307    0.307\n48  Nonverbal =~ rdss 14.845 -1.191  -6.635   -0.664   -0.664\n54    Spatial =~ wdss 14.410 -0.320  -2.400   -0.250   -0.250\n57    Spatial =~ mass 13.497 -0.725  -5.436   -0.531   -0.531\n102      psss ~~ sqss 13.490 -8.793  -8.793   -0.178   -0.178\n35     Verbal =~ psss 12.725  0.396   2.956    0.287    0.287\n123      pcss ~~ rpss 12.675 -8.036  -8.036   -0.184   -0.184\n111      mass ~~ rdss 11.561 -7.250  -7.250   -0.149   -0.149\n\n\n\n# 필터링: MI &gt; 10 이상인 것만 출력\nmodindices(fit, sort = TRUE) |&gt; subset(mi &gt; 10) |&gt; print()\n\n          lhs op  rhs     mi    epc sepc.lv sepc.all sepc.nox\n109      mass ~~ sqss 17.701 12.611  12.611    0.307    0.307\n48  Nonverbal =~ rdss 14.845 -1.191  -6.635   -0.664   -0.664\n54    Spatial =~ wdss 14.410 -0.320  -2.400   -0.250   -0.250\n57    Spatial =~ mass 13.497 -0.725  -5.436   -0.531   -0.531\n102      psss ~~ sqss 13.490 -8.793  -8.793   -0.178   -0.178\n35     Verbal =~ psss 12.725  0.396   2.956    0.287    0.287\n123      pcss ~~ rpss 12.675 -8.036  -8.036   -0.184   -0.184\n111      mass ~~ rdss 11.561 -7.250  -7.250   -0.149   -0.149\n87       wdss ~~ rdss 11.198 -6.360  -6.360   -0.144   -0.144\n\n\n\n# 필터링: operator가 =~인 것, 즉 요인부하량만 출력\nmodindices(fit, sort = TRUE, maximum.number = 8) |&gt; subset(op == \"=~\") |&gt; print()\n\n         lhs op  rhs     mi    epc sepc.lv sepc.all sepc.nox\n48 Nonverbal =~ rdss 14.845 -1.191  -6.635   -0.664   -0.664\n54   Spatial =~ wdss 14.410 -0.320  -2.400   -0.250   -0.250\n57   Spatial =~ mass 13.497 -0.725  -5.436   -0.531   -0.531\n35    Verbal =~ psss 12.725  0.396   2.956    0.287    0.287",
    "crumbs": [
      "Keith's",
      "Factor Analysis"
    ]
  },
  {
    "objectID": "contents/chap16.html#testing-competing-models",
    "href": "contents/chap16.html#testing-competing-models",
    "title": "Chapter 15-16. Confirmatory Factor Analysis",
    "section": "Testing Competing Models",
    "text": "Testing Competing Models\n\n\nCross-loadings\n\n\ndas2_model_crossload &lt;- \"\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss\n  Spatial =~ pcss + rdss + rpss\n  Memory =~ dfss + dbss + soss + rdss\n\"\nfit_crossload &lt;- sem(das2_model_crossload, sample.cov = das2cov, sample.nobs = 800)\nsummary(fit_crossload, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 168 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        31\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               127.495\n  Degrees of freedom                                47\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              4335.087\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.981\n  Tucker-Lewis Index (TLI)                       0.974\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33708.321\n  Loglikelihood unrestricted model (H1)     -33644.574\n                                                      \n  Akaike (AIC)                               67478.643\n  Bayesian (BIC)                             67623.866\n  Sample-size adjusted Bayesian (SABIC)      67525.424\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.046\n  90 Percent confidence interval - lower         0.037\n  90 Percent confidence interval - upper         0.056\n  P-value H_0: RMSEA &lt;= 0.050                    0.725\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.027\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              1.000                               7.462    0.739\n    wdss              0.942    0.049   19.268    0.000    7.031    0.734\n    vsss              1.100    0.053   20.895    0.000    8.206    0.805\n  Nonverbal =~                                                          \n    psss              1.000                               5.572    0.542\n    mass              1.305    0.093   14.105    0.000    7.273    0.710\n    sqss              1.405    0.094   15.030    0.000    7.830    0.808\n  Spatial =~                                                            \n    pcss              1.000                               7.459    0.809\n    rdss              1.066    0.143    7.457    0.000    7.947    0.795\n    rpss              0.799    0.049   16.365    0.000    5.959    0.590\n  Memory =~                                                             \n    dfss              1.000                               7.378    0.668\n    dbss              1.035    0.058   17.941    0.000    7.638    0.753\n    soss              1.119    0.061   18.372    0.000    8.259    0.777\n    rdss             -0.085    0.134   -0.631    0.528   -0.625   -0.063\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal ~~                                                             \n    Nonverbal        33.468    3.077   10.876    0.000    0.805    0.805\n    Spatial          42.014    3.278   12.818    0.000    0.755    0.755\n    Memory           45.449    3.691   12.312    0.000    0.825    0.825\n  Nonverbal ~~                                                          \n    Spatial          37.148    3.196   11.624    0.000    0.894    0.894\n    Memory           35.272    3.288   10.729    0.000    0.858    0.858\n  Spatial ~~                                                            \n    Memory           45.113    3.565   12.654    0.000    0.820    0.820\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .nvss             46.191    2.924   15.797    0.000   46.191    0.453\n   .wdss             42.446    2.662   15.945    0.000   42.446    0.462\n   .vsss             36.524    2.709   13.484    0.000   36.524    0.352\n   .psss             74.823    3.999   18.708    0.000   74.823    0.707\n   .mass             51.970    3.121   16.652    0.000   51.970    0.496\n   .sqss             32.571    2.476   13.153    0.000   32.571    0.347\n   .pcss             29.264    2.475   11.822    0.000   29.264    0.345\n   .rdss             44.468    3.433   12.953    0.000   44.468    0.445\n   .rpss             66.362    3.654   18.160    0.000   66.362    0.651\n   .dfss             67.407    3.900   17.285    0.000   67.407    0.553\n   .dbss             44.528    2.879   15.466    0.000   44.528    0.433\n   .soss             44.648    3.049   14.643    0.000   44.648    0.396\n    Verbal           55.681    4.881   11.407    0.000    1.000    1.000\n    Nonverbal        31.044    4.003    7.755    0.000    1.000    1.000\n    Spatial          55.629    4.457   12.482    0.000    1.000    1.000\n    Memory           54.441    5.441   10.005    0.000    1.000    1.000\n\n\n\n이전 모형과 비교하면,\n\n# nested models\nlavTestLRT(fit, fit_crossload) |&gt; print()\n\n\nChi-Squared Difference Test\n\n              Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nfit_crossload 47 67479 67624 127.49                                    \nfit           48 67477 67618 127.99    0.49144     0       1     0.4833\n\n\n\n# nested models\nsemTools::compareFit(fit, fit_crossload) |&gt; summary()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n              Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nfit_crossload 47 67479 67624 127.49                                    \nfit           48 67477 67618 127.99    0.49144     0       1     0.4833\n\n####################### Model Fit Indices ###########################\n                 chisq df pvalue rmsea   cfi   tli  srmr        aic        bic\nfit_crossload 127.495† 47   .000 .046  .981  .974  .027† 67478.643  67623.866 \nfit           127.986  48   .000 .046† .981† .974† .027  67477.134† 67617.673†\n\n################## Differences in Fit Indices #######################\n                    df  rmsea cfi   tli srmr    aic    bic\nfit - fit_crossload  1 -0.001   0 0.001    0 -1.509 -6.193\n\n\n\n\n\n3-factor model\n\n\ndas2_model_3f &lt;- \"\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss + pcss + rdss + rpss\n  Memory =~ dfss + dbss + soss\n\"\nfit_3f &lt;- sem(das2_model_3f, sample.cov = das2cov, sample.nobs = 800)\nsummary(fit_3f, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 118 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        27\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               163.856\n  Degrees of freedom                                51\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              4335.087\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.974\n  Tucker-Lewis Index (TLI)                       0.966\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33726.502\n  Loglikelihood unrestricted model (H1)     -33644.574\n                                                      \n  Akaike (AIC)                               67507.004\n  Bayesian (BIC)                             67633.489\n  Sample-size adjusted Bayesian (SABIC)      67547.749\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.053\n  90 Percent confidence interval - lower         0.044\n  90 Percent confidence interval - upper         0.062\n  P-value H_0: RMSEA &lt;= 0.050                    0.304\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              1.000                               7.489    0.742\n    wdss              0.936    0.049   19.275    0.000    7.012    0.732\n    vsss              1.095    0.052   20.951    0.000    8.199    0.805\n  Nonverbal =~                                                          \n    psss              1.000                               5.579    0.542\n    mass              1.258    0.089   14.055    0.000    7.018    0.685\n    sqss              1.367    0.090   15.158    0.000    7.625    0.787\n    pcss              1.285    0.085   15.071    0.000    7.169    0.778\n    rdss              1.249    0.088   14.196    0.000    6.968    0.697\n    rpss              1.044    0.083   12.615    0.000    5.826    0.577\n  Memory =~                                                             \n    dfss              1.000                               7.398    0.670\n    dbss              1.033    0.057   17.972    0.000    7.638    0.753\n    soss              1.117    0.061   18.407    0.000    8.261    0.778\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal ~~                                                             \n    Nonverbal        33.675    3.044   11.063    0.000    0.806    0.806\n    Memory           45.656    3.701   12.335    0.000    0.824    0.824\n  Nonverbal ~~                                                          \n    Memory           35.599    3.266   10.899    0.000    0.862    0.862\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .nvss             45.794    2.913   15.720    0.000   45.794    0.450\n   .wdss             42.712    2.672   15.987    0.000   42.712    0.465\n   .vsss             36.642    2.713   13.506    0.000   36.642    0.353\n   .psss             74.738    3.945   18.945    0.000   74.738    0.706\n   .mass             55.615    3.135   17.740    0.000   55.615    0.530\n   .sqss             35.749    2.262   15.808    0.000   35.749    0.381\n   .pcss             33.501    2.087   16.049    0.000   33.501    0.395\n   .rdss             51.329    2.919   17.585    0.000   51.329    0.514\n   .rpss             67.931    3.626   18.733    0.000   67.931    0.667\n   .dfss             67.121    3.893   17.241    0.000   67.121    0.551\n   .dbss             44.529    2.884   15.443    0.000   44.529    0.433\n   .soss             44.616    3.054   14.611    0.000   44.616    0.395\n    Verbal           56.079    4.893   11.460    0.000    1.000    1.000\n    Nonverbal        31.130    3.956    7.868    0.000    1.000    1.000\n    Memory           54.727    5.454   10.034    0.000    1.000    1.000\n\n\n\n\n# Compare models\nsemTools::compareFit(fit, fit_3f) |&gt; summary()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n       Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nfit    48 67477 67618 127.99                                          \nfit_3f 51 67507 67633 163.86      35.87 0.11703       3  7.978e-08 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n####################### Model Fit Indices ###########################\n          chisq df pvalue rmsea   cfi   tli  srmr        aic        bic\nfit    127.986† 48   .000 .046† .981† .974† .027† 67477.134† 67617.673†\nfit_3f 163.856  51   .000 .053  .974  .966  .029  67507.004  67633.489 \n\n################## Differences in Fit Indices #######################\n             df rmsea    cfi    tli  srmr   aic    bic\nfit_3f - fit  3 0.007 -0.008 -0.008 0.002 29.87 15.816\n\n\n\n\n\nTransform 4-factor to 3-factor model with constraints (2-step)\n3-factor 모델이 초기 모델에 내포되어 있음을 확인할 수 있음.\n\ndas2_model_constrain &lt;- \"\n  # free the first paraemters\n  Verbal =~ NA*nvss + wdss + vsss\n  Nonverbal =~ NA*psss + mass + sqss\n  Spatial =~ NA*pcss + rdss + rpss\n  Memory =~ NA*dfss + dbss + soss\n\n  # unit variance indentification\n  Verbal ~~ 1*Verbal\n  Nonverbal ~~ 1*Nonverbal\n  Spatial ~~ 1*Spatial\n  Memory ~~ 1*Memory\n\n  # equality constraints\n  Spatial ~~ 1*Nonverbal\n\n  Nonverbal ~~ a*Verbal\n  Spatial ~~ a*Verbal\n  Nonverbal ~~ b*Memory\n  Spatial ~~ b*Memory\n\"\n\nfit_3f_2 &lt;- sem(das2_model_constrain, sample.cov = das2cov, sample.nobs = 800)\nsummary(fit_3f_2, standardized = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 30 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        29\n  Number of equality constraints                     2\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               163.856\n  Degrees of freedom                                51\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              7.489    0.327   22.920    0.000    7.489    0.742\n    wdss              7.012    0.312   22.489    0.000    7.012    0.732\n    vsss              8.199    0.320   25.584    0.000    8.199    0.805\n  Nonverbal =~                                                          \n    psss              5.579    0.355   15.736    0.000    5.579    0.542\n    mass              7.018    0.333   21.071    0.000    7.018    0.685\n    sqss              7.625    0.299   25.518    0.000    7.625    0.787\n  Spatial =~                                                            \n    pcss              7.169    0.286   25.104    0.000    7.169    0.778\n    rdss              6.968    0.323   21.558    0.000    6.968    0.697\n    rpss              5.826    0.344   16.960    0.000    5.826    0.577\n  Memory =~                                                             \n    dfss              7.398    0.369   20.068    0.000    7.398    0.670\n    dbss              7.638    0.326   23.404    0.000    7.638    0.753\n    soss              8.261    0.338   24.429    0.000    8.261    0.778\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Nonverbal ~~                                                          \n    Spatial           1.000                               1.000    1.000\n  Verbal ~~                                                             \n    Nonverbal  (a)    0.806    0.021   38.739    0.000    0.806    0.806\n    Spatial    (a)    0.806    0.021   38.739    0.000    0.806    0.806\n  Nonverbal ~~                                                          \n    Memory     (b)    0.862    0.019   45.886    0.000    0.862    0.862\n  Spatial ~~                                                            \n    Memory     (b)    0.862    0.019   45.886    0.000    0.862    0.862\n  Verbal ~~                                                             \n    Memory            0.824    0.022   36.764    0.000    0.824    0.824\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Verbal            1.000                               1.000    1.000\n    Nonverbal         1.000                               1.000    1.000\n    Spatial           1.000                               1.000    1.000\n    Memory            1.000                               1.000    1.000\n   .nvss             45.794    2.913   15.720    0.000   45.794    0.450\n   .wdss             42.712    2.672   15.987    0.000   42.712    0.465\n   .vsss             36.642    2.713   13.506    0.000   36.642    0.353\n   .psss             74.738    3.945   18.945    0.000   74.738    0.706\n   .mass             55.615    3.135   17.740    0.000   55.615    0.530\n   .sqss             35.749    2.262   15.808    0.000   35.749    0.381\n   .pcss             33.501    2.087   16.049    0.000   33.501    0.395\n   .rdss             51.329    2.919   17.585    0.000   51.329    0.514\n   .rpss             67.931    3.626   18.733    0.000   67.931    0.667\n   .dfss             67.121    3.893   17.241    0.000   67.121    0.551\n   .dbss             44.529    2.884   15.443    0.000   44.529    0.433\n   .soss             44.616    3.054   14.611    0.000   44.616    0.395\n\n\n\n\n# Compare models\ncompareFit(fit_3f, fit_3f_2) |&gt; summary() |&gt; print()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n         Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nfit_3f   51 67507 67633 163.86                                    \nfit_3f_2 51 67507 67633 163.86  1.387e-09     0       0           \n\n####################### Model Fit Indices ###########################\n            chisq df pvalue rmsea   cfi   tli  srmr        aic        bic\nfit_3f   163.856† 51   .000 .053† .974† .966† .029† 67507.004† 67633.489†\nfit_3f_2 163.856  51   .000 .053  .974  .966  .029  67507.004  67633.489 \n\n################## Differences in Fit Indices #######################\n                  df rmsea cfi tli srmr aic bic\nfit_3f_2 - fit_3f  0     0   0   0    0   0   0\n\nThe following lavaan models were compared:\n    fit_3f\n    fit_3f_2\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.\n\n\n\n\nModification indices for 3-factor model\n\nmodindices(fit_3f, sort = TRUE, maximum.number = 8) |&gt; print()\n\n       lhs op  rhs     mi     epc sepc.lv sepc.all sepc.nox\n93    mass ~~ sqss 26.473  10.256  10.256    0.230    0.230\n106   pcss ~~ rdss 26.463   9.511   9.511    0.229    0.229\n95    mass ~~ rdss 24.101 -10.817 -10.817   -0.202   -0.202\n71    wdss ~~ rdss 15.597  -7.636  -7.636   -0.163   -0.163\n111   rdss ~~ rpss 15.509   9.200   9.200    0.156    0.156\n31  Verbal =~ psss  9.941   0.323   2.420    0.235    0.235\n94    mass ~~ pcss  7.622  -5.266  -5.266   -0.122   -0.122\n99    mass ~~ soss  7.343   5.784   5.784    0.116    0.116\n\n\n\n가령, mass와 rdss의 잔차/오차분산 간의 상관관계를 추정하면,\n\ndas2_model_3f_modi &lt;- \"\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss + pcss + rdss + rpss\n  Memory =~ dfss + dbss + soss\n\n  mass ~~ rdss\n\"\nfit_3f_modi &lt;- sem(das2_model_3f_modi, sample.cov = das2cov, sample.nobs = 800)\nparameterEstimates(fit_3f_modi, standardized = \"std.all\") |&gt; subset(lhs == \"mass\" | lhs == \"rdss\") |&gt; print()\n\n    lhs op  rhs     est    se      z pvalue ci.lower ci.upper std.all\n13 mass ~~ rdss -10.954 2.110 -5.192      0  -15.089   -6.818  -0.217\n18 mass ~~ mass  52.562 3.080 17.066      0   46.526   58.599   0.501\n21 rdss ~~ rdss  48.374 2.864 16.889      0   42.760   53.987   0.484\n\n\n\nanova(fit_3f, fit_3f_modi) |&gt; print()\n\n\nChi-Squared Difference Test\n\n            Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nfit_3f_modi 50 67483 67614 138.09                                          \nfit_3f      51 67507 67633 163.86     25.764 0.17594       1  3.858e-07 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n\n\n\nResiduals for 3-factor model\n\nresiduals(fit_3f, \"cor\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       nvss   wdss   vsss   psss   mass   sqss   pcss   rdss   rpss   dfss   dbss   soss\nnvss  0.000                                                                             \nwdss  0.015  0.000                                                                      \nvsss -0.014  0.004  0.000                                                               \npsss  0.051  0.055  0.048  0.000                                                        \nmass -0.023  0.023  0.015 -0.002  0.000                                                 \nsqss -0.021 -0.012  0.026 -0.036  0.065  0.000                                          \npcss  0.039 -0.040  0.017 -0.011 -0.036 -0.008  0.000                                   \nrdss  0.019 -0.088 -0.021  0.020 -0.078 -0.033  0.065  0.000                            \nrpss -0.012 -0.051 -0.025  0.014 -0.009 -0.005 -0.009  0.073  0.000                     \ndfss  0.038  0.011  0.017  0.012 -0.043 -0.016 -0.018  0.013 -0.038  0.000              \ndbss -0.041 -0.023 -0.007  0.002  0.045  0.028  0.008  0.010 -0.004 -0.005  0.000       \nsoss  0.009  0.022 -0.008 -0.026  0.045 -0.004 -0.022 -0.016 -0.015  0.007 -0.002  0.000\n\n\n\n\nresiduals(fit_3f_modi, \"cor\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       nvss   wdss   vsss   psss   mass   sqss   pcss   rdss   rpss   dfss   dbss   soss\nnvss  0.000                                                                             \nwdss  0.014  0.000                                                                      \nvsss -0.014  0.004  0.000                                                               \npsss  0.056  0.060  0.054  0.000                                                        \nmass -0.031  0.016  0.007 -0.012  0.000                                                 \nsqss -0.013 -0.005  0.035 -0.032  0.051  0.000                                          \npcss  0.047 -0.033  0.025 -0.007 -0.050 -0.003  0.000                                   \nrdss  0.012 -0.095 -0.028  0.010  0.000 -0.047  0.051  0.000                            \nrpss -0.008 -0.048 -0.021  0.014 -0.022 -0.004 -0.008  0.060  0.000                     \ndfss  0.039  0.012  0.018  0.016 -0.052 -0.010 -0.012  0.005 -0.036  0.000              \ndbss -0.041 -0.023 -0.006  0.006  0.035  0.033  0.013  0.000 -0.003 -0.005  0.000       \nsoss  0.008  0.021 -0.008 -0.022  0.034  0.002 -0.017 -0.027 -0.013  0.007 -0.002  0.000\n\n\n\nWord Definitions(wdss)와 Recall of Designs(rdss) 간의 공분산에 대한 논의: 양수 vs. 음수\n\n# Keith's Table 16.3: normalized? standardized?\nvars &lt;- c(\"pcss\", \"soss\", \"dbss\", \"dfss\", \"rpss\", \"rdss\", \"sqss\", \"mass\", \"psss\", \"vsss\", \"wdss\", \"nvss\")\n\nresid_cor &lt;- residuals(fit_3f, \"normalized\")$cov[vars, vars]\n\nlibrary(corrplot)\ncorrplot(resid_cor, method = 'number', type = 'lower', is.corr = FALSE, bg = 'grey50')\n\n\n\n\n\n\n\n\n\nresid_cor &lt;- residuals(fit_3f, \"standardized.mplus\")$cov[vars, vars]\ncorrplot(resid_cor, method = 'number', type = 'lower', is.corr = FALSE, bg = 'grey50')\n\n\n\n\n\n\n\n\nWord Definitions(wdss)와 Recall of Designs(rdss) 간의 상관에 대한 논의;\n\n모형이 추정하는 상관계수: \\(r = 0.73*0.81*0.70 = 0.41\\)\n실제 상관계수: \\(r = 0.32\\)\n그 차이는 대략 0.09\n\n\n\n\n\n\n\nRules for Covariances\n\n\n\n\\(COV(\\xi_1, \\xi_2) = COV(aX_1, bX_2) = ab~COV(X_1, X_2)\\)\n\n\n\n# Keith's Table 16.4\nresid_cor &lt;- residuals(fit_3f, \"cor\")$cov[vars, vars]\ncorrplot(resid_cor, method = 'number', type = 'lower', is.corr = FALSE, bg = 'grey50')",
    "crumbs": [
      "Keith's",
      "Factor Analysis"
    ]
  },
  {
    "objectID": "contents/chap16.html#higher-order-model",
    "href": "contents/chap16.html#higher-order-model",
    "title": "Chapter 15-16. Confirmatory Factor Analysis",
    "section": "Higher-order model",
    "text": "Higher-order model\nHierarchical CFA model\nFirst-order factors들에 대한 인과모형: 요인들 간의 관계를 설명\nSecond-order factors에 대한 indicators들: first order factors\n\n아래와 같은 모형은 first-order factors 간의 상관관계를 g로 다 설명할 수 있다는 가설을 반영\ng로 설명되지 않는 부분인 disturbance가 존재: 내생변수가 됨\nIdentified되려며 적어도 3개의 1st-order 요인이 존재해야 함\n\n\n\ndas2_model_higher &lt;- \"\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss\n  Spatial =~ pcss + rdss + rpss\n  Memory =~ dfss + dbss + soss\n\n  G =~ Verbal + Nonverbal + Spatial + Memory\n\"\nfit_higher &lt;- sem(das2_model_higher, sample.cov = das2cov, sample.nobs = 800)\nsummary(fit_higher, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 107 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        28\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               141.972\n  Degrees of freedom                                50\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              4335.087\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.978\n  Tucker-Lewis Index (TLI)                       0.972\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33715.560\n  Loglikelihood unrestricted model (H1)     -33644.574\n                                                      \n  Akaike (AIC)                               67487.120\n  Bayesian (BIC)                             67618.289\n  Sample-size adjusted Bayesian (SABIC)      67529.374\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.048\n  90 Percent confidence interval - lower         0.039\n  90 Percent confidence interval - upper         0.057\n  P-value H_0: RMSEA &lt;= 0.050                    0.626\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              1.000                               7.474    0.740\n    wdss              0.937    0.049   19.183    0.000    7.000    0.730\n    vsss              1.100    0.053   20.914    0.000    8.223    0.807\n  Nonverbal =~                                                          \n    psss              1.000                               5.558    0.540\n    mass              1.315    0.093   14.090    0.000    7.308    0.714\n    sqss              1.405    0.094   14.951    0.000    7.810    0.806\n  Spatial =~                                                            \n    pcss              1.000                               7.506    0.815\n    rdss              0.983    0.047   20.993    0.000    7.378    0.738\n    rpss              0.790    0.049   16.285    0.000    5.933    0.588\n  Memory =~                                                             \n    dfss              1.000                               7.344    0.665\n    dbss              1.047    0.058   17.904    0.000    7.690    0.758\n    soss              1.123    0.062   18.215    0.000    8.248    0.776\n  G =~                                                                  \n    Verbal            1.000                               0.858    0.858\n    Nonverbal         0.828    0.063   13.044    0.000    0.955    0.955\n    Spatial           1.057    0.060   17.475    0.000    0.903    0.903\n    Memory            1.046    0.069   15.141    0.000    0.913    0.913\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .nvss             46.017    2.926   15.728    0.000   46.017    0.452\n   .wdss             42.890    2.682   15.990    0.000   42.890    0.467\n   .vsss             36.246    2.715   13.352    0.000   36.246    0.349\n   .psss             74.975    4.009   18.702    0.000   74.975    0.708\n   .mass             51.465    3.111   16.543    0.000   51.465    0.491\n   .sqss             32.880    2.488   13.213    0.000   32.880    0.350\n   .pcss             28.554    2.247   12.706    0.000   28.554    0.336\n   .rdss             45.447    2.888   15.738    0.000   45.447    0.455\n   .rpss             66.671    3.666   18.187    0.000   66.671    0.654\n   .dfss             67.912    3.927   17.293    0.000   67.912    0.557\n   .dbss             43.728    2.868   15.248    0.000   43.728    0.425\n   .soss             44.833    3.067   14.620    0.000   44.833    0.397\n   .Verbal           14.741    2.026    7.274    0.000    0.264    0.264\n   .Nonverbal         2.698    1.030    2.619    0.009    0.087    0.087\n   .Spatial          10.412    1.923    5.415    0.000    0.185    0.185\n   .Memory            8.941    1.826    4.897    0.000    0.166    0.166\n    G                41.114    4.197    9.797    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    nvss              0.548\n    wdss              0.533\n    vsss              0.651\n    psss              0.292\n    mass              0.509\n    sqss              0.650\n    pcss              0.664\n    rdss              0.545\n    rpss              0.346\n    dfss              0.443\n    dbss              0.575\n    soss              0.603\n    Verbal            0.736\n    Nonverbal         0.913\n    Spatial           0.815\n    Memory            0.834\n\n\n\n1차 모형과 고차 모형이 내포된 관계라고 볼 수 있으나, 일반적으로 통계적 검증보다는 이론적/실증적 근거가 필요\n고차모형이 더 간결할 모형이나 통계적으로는 더 좋지 못한 결과\n\ncompareFit(fit, fit_higher) |&gt; summary()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n           Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)    \nfit        48 67477 67618 127.99                                           \nfit_higher 50 67487 67618 141.97     13.986 0.086552       2  0.0009183 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n####################### Model Fit Indices ###########################\n              chisq df pvalue rmsea   cfi   tli  srmr        aic        bic\nfit        127.986† 48   .000 .046† .981† .974† .027† 67477.134† 67617.673†\nfit_higher 141.972  50   .000 .048  .978  .972  .029  67487.120  67618.289 \n\n################## Differences in Fit Indices #######################\n                 df rmsea    cfi    tli  srmr   aic   bic\nfit_higher - fit  2 0.002 -0.003 -0.003 0.003 9.986 0.617\n\n\n\nTotal effect\n\n2차 요인(일반적인 지능)이 indicator(하위 검사)와 어떻게 관련되는지 확인\n직접 효과가 없으니 간접 효과가 총효과\n\n\nstandardizedSolution(fit_higher) |&gt; subset(op == \"=~\") |&gt; print()\n\n         lhs op       rhs est.std    se      z pvalue ci.lower ci.upper\n1     Verbal =~      nvss   0.740 0.020 37.089      0    0.701    0.780\n2     Verbal =~      wdss   0.730 0.020 35.780      0    0.690    0.770\n3     Verbal =~      vsss   0.807 0.017 46.773      0    0.773    0.841\n4  Nonverbal =~      psss   0.540 0.028 19.326      0    0.485    0.595\n5  Nonverbal =~      mass   0.714 0.021 34.116      0    0.673    0.755\n6  Nonverbal =~      sqss   0.806 0.017 46.108      0    0.772    0.840\n7    Spatial =~      pcss   0.815 0.017 47.124      0    0.781    0.849\n8    Spatial =~      rdss   0.738 0.020 36.690      0    0.699    0.778\n9    Spatial =~      rpss   0.588 0.026 22.257      0    0.536    0.640\n10    Memory =~      dfss   0.665 0.023 28.727      0    0.620    0.711\n11    Memory =~      dbss   0.758 0.019 39.560      0    0.721    0.796\n12    Memory =~      soss   0.776 0.018 42.114      0    0.740    0.813\n13         G =~    Verbal   0.858 0.018 47.282      0    0.822    0.894\n14         G =~ Nonverbal   0.955 0.016 58.393      0    0.923    0.987\n15         G =~   Spatial   0.903 0.017 52.367      0    0.869    0.937\n16         G =~    Memory   0.913 0.017 54.992      0    0.881    0.946\n\n\n가령, G &gt; Verbal &gt; nvss, wdss, vsss에 대해서 총효과를 계산하면,\n\nest &lt;- standardizedSolution(fit_higher) |&gt; subset(op == \"=~\")  # loadings only\nsubset(est, lhs == \"Verbal\" | rhs == \"Verbal\") |&gt; print()\n\n      lhs op    rhs est.std    se      z pvalue ci.lower ci.upper\n1  Verbal =~   nvss   0.740 0.020 37.089      0    0.701    0.780\n2  Verbal =~   wdss   0.730 0.020 35.780      0    0.690    0.770\n3  Verbal =~   vsss   0.807 0.017 46.773      0    0.773    0.841\n13      G =~ Verbal   0.858 0.018 47.282      0    0.822    0.894\n\n\n\n# G &gt; Verbal\n(subset(est, lhs == \"Verbal\")[, \"est.std\"] * subset(est, rhs == \"Verbal\")[, \"est.std\"]) |&gt; print(digits = 2)\n\n[1] 0.64 0.63 0.69\n\n\nCalcuate the standard errors\n\ndas2_model_higher_ind &lt;- \"\n  Verbal =~ NA*nvss + 1*wdss + a*vsss\n  Nonverbal =~ psss + mass + sqss\n  Spatial =~ pcss + rdss + rpss\n  Memory =~ dfss + dbss + soss\n\n  G =~ b*NA*Verbal + 1*Nonverbal + Spatial + Memory\n\n  indirect := a*b\n\"\nfit_higher_ind &lt;- sem(das2_model_higher_ind, sample.cov = das2cov, sample.nobs = 800)\nstandardizedSolution(fit_higher_ind) |&gt; subset(lhs == \"indirect\") |&gt; print()\n\n        lhs op rhs    label est.std    se      z pvalue ci.lower ci.upper\n34 indirect := a*b indirect   0.692 0.021 33.754      0    0.652    0.732\n\n\n\nsemPaths2 &lt;- function(model, what = 'std', layout = \"tree\", rotation = 1) {\n  semPlot::semPaths(model, what = what, edge.label.cex = 0.9, edge.color = \"black\", layout = layout, rotation = rotation, weighted = FALSE, asize = 2, label.cex = 1, node.width = 1.2)\n}\nsemPaths2(fit_higher)",
    "crumbs": [
      "Keith's",
      "Factor Analysis"
    ]
  },
  {
    "objectID": "contents/chap16.html#bifactor-model",
    "href": "contents/chap16.html#bifactor-model",
    "title": "Chapter 15-16. Confirmatory Factor Analysis",
    "section": "Bifactor model",
    "text": "Bifactor model\n또는 Nested-factor models, general-specific models\n\n보통, indicators들이 general factor로 이루어졌다고 보지만, 하위의 domain-specific factors에 대해 살펴보고자 하는 상황에서 발생\nGeneral factor G: 모든 indicator들에 대한 직접적인 공통요인이나 G는 domain-specific factors에 대해서는 인과적 효과가 없다고 가정\nIndicators의 분산은 3개의 요인들로 분할됨: 1) general factor, 2) domain-specific factors, 3) errors\n보통, 요인들 간의 상관은 없는 것으로 가정: canonical\n\n상관을 가정하면 과소추정되거나 추정이 불가능해지기 쉬움\n\nHierchical 모형에 비해 더 해석 가능하며, 구조모형에 들어갈 때에도 보다 유리함; Chen et al., 2006; Gignac, 2008; pp. 256-257, Kline(2023)\n\nBifactor model\n\nG와 domain-specific factors 모두 외생변수\nFactor loadings: domain-specific factor과 general factor의 loading은 서로의 요인이 통제된 상태\n고차 모형에 비해 제약이 더 적은 (복잡한) 모형으로 이해될 수 있음 (즉, 내포된 모형); 즉, 보통 더 나은 적합도를 보임\n\nHierarchical model\n\nDomain-specific는 외생변수; g에 의해 설명되지 않는 부분 disturbance가 존재\nIndicators의 error가 고려되면서 g와 disturbance가 추정됨\n\n\n두 가지 모형에 대한 선택은 이론적/실증적 근거에 결정!\n저자의 견해 참고; 위계적 모형 비교 (pp. 424-427)\n\n\n이 두 모델은 또한 지능의 본질에 대해 상당히 다른 이론을 암시합니다. 고차 모형에 따르면 12가지 테스트가 서로 상관관계가 있는 주된 이유는 네 가지 기본 인지 능력을 측정하기 때문이며, g는 이러한 광범위한 인지 능력에 영향을 미치고 g는 특정 테스트에 간접적으로만 영향을 미친다고 합니다. 반면 이중 요인 모델에서는 이 12가지 검사 간의 상관 관계에는 두 가지 이유가 있다고 말합니다. 첫째, 이들 검사는 모두 G를 측정하고 둘째, 이들 검사는 모두 서로 독립적인 다른 광범위한 인지능력을 측정하기 때문입니다. 이중 요인 모델에서 G는 구체적인 테스트에 직접적인 영향을 미칩니다. 따라서 고차 모델에서 g는 그 기반이 되는 광범위한 인지 능력의 특성으로 이해할 수 있으며, 이러한 인지 능력은 g와 어느 정도 관련이 있는 것으로 이해할 수 있습니다. 이원 요인 모델에서는 G의 특성은 구체적인 테스트로 직접 레퍼런스 할 수 있습니다. (p. 424)\n\nTranslated with DeepL.com (free version)\n\n\ndas2_model_bifactor &lt;- \"\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss\n  Spatial =~ pcss + rdss + rpss\n  Memory =~ dfss + dbss + soss\n\n  G =~ nvss + wdss + vsss + psss + mass + sqss + pcss + rdss + rpss + dfss + dbss + soss\n\n  mass ~~ 0*mass\n\n  # orthogonal factors (uncorrelated factors)\n  Verbal ~~ 0*Nonverbal\n  Verbal ~~ 0*Spatial\n  Verbal ~~ 0*Memory\n  Nonverbal ~~ 0*Spatial\n  Nonverbal ~~ 0*Memory\n  Spatial ~~ 0*Memory\n  G ~~ 0*Verbal\n  G ~~ 0*Nonverbal\n  G ~~ 0*Spatial\n  G ~~ 0*Memory\n\"\nfit_bif &lt;- sem(das2_model_bifactor, sample.cov = das2cov, sample.nobs = 800)\n\n# 위에서처럼 orthogonal factor로 직접 설정하거나 다름와 같이 옵션으로 설정\nfit_bif &lt;- sem(das2_model_bifactor, sample.cov = das2cov, sample.nobs = 800,\n  auto.cov.lv.x = FALSE) # uncorrelated factors\n\nsummary(fit_bif, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 769 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        35\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               108.487\n  Degrees of freedom                                43\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              4335.087\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.985\n  Tucker-Lewis Index (TLI)                       0.976\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33698.817\n  Loglikelihood unrestricted model (H1)     -33644.574\n                                                      \n  Akaike (AIC)                               67467.635\n  Bayesian (BIC)                             67631.596\n  Sample-size adjusted Bayesian (SABIC)      67520.452\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.044\n  90 Percent confidence interval - lower         0.033\n  90 Percent confidence interval - upper         0.054\n  P-value H_0: RMSEA &lt;= 0.050                    0.839\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.025\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              1.000                               3.673    0.364\n    wdss              1.189    0.210    5.661    0.000    4.368    0.456\n    vsss              0.999    0.162    6.159    0.000    3.668    0.360\n  Nonverbal =~                                                          \n    psss              1.000                               0.109    0.011\n    mass             70.327  219.185    0.321    0.748    7.641    0.746\n    sqss             11.347   35.167    0.323    0.747    1.233    0.127\n  Spatial =~                                                            \n    pcss              1.000                               1.686    0.183\n    rdss              3.889    2.449    1.588    0.112    6.558    0.656\n    rpss              1.110    0.273    4.070    0.000    1.872    0.185\n  Memory =~                                                             \n    dfss              1.000                               3.289    0.298\n    dbss              0.837    0.227    3.686    0.000    2.752    0.271\n    soss              1.136    0.350    3.247    0.001    3.735    0.352\n  G =~                                                                  \n    nvss              1.000                               6.481    0.642\n    wdss              0.902    0.052   17.389    0.000    5.846    0.610\n    vsss              1.106    0.058   19.161    0.000    7.168    0.703\n    psss              0.863    0.065   13.335    0.000    5.591    0.543\n    mass              1.052    0.067   15.680    0.000    6.818    0.666\n    sqss              1.143    0.065   17.587    0.000    7.408    0.765\n    pcss              1.064    0.062   17.301    0.000    6.896    0.748\n    rdss              1.004    0.065   15.458    0.000    6.507    0.651\n    rpss              0.846    0.064   13.239    0.000    5.481    0.543\n    dfss              1.026    0.071   14.479    0.000    6.648    0.602\n    dbss              1.088    0.067   16.313    0.000    7.051    0.695\n    soss              1.152    0.070   16.457    0.000    7.466    0.703\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal ~~                                                             \n    Nonverbal         0.000                               0.000    0.000\n    Spatial           0.000                               0.000    0.000\n    Memory            0.000                               0.000    0.000\n  Nonverbal ~~                                                          \n    Spatial           0.000                               0.000    0.000\n    Memory            0.000                               0.000    0.000\n  Spatial ~~                                                            \n    Memory            0.000                               0.000    0.000\n  Verbal ~~                                                             \n    G                 0.000                               0.000    0.000\n  Nonverbal ~~                                                          \n    G                 0.000                               0.000    0.000\n  Spatial ~~                                                            \n    G                 0.000                               0.000    0.000\n  Memory ~~                                                             \n    G                 0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .mass              0.000                               0.000    0.000\n   .nvss             46.382    3.255   14.248    0.000   46.382    0.455\n   .wdss             38.625    3.797   10.172    0.000   38.625    0.420\n   .vsss             39.039    3.016   12.945    0.000   39.039    0.376\n   .psss             74.591    3.972   18.778    0.000   74.591    0.705\n   .sqss             37.487    2.240   16.737    0.000   37.487    0.399\n   .pcss             34.500    2.494   13.834    0.000   34.500    0.406\n   .rdss             14.524   26.056    0.557    0.577   14.524    0.145\n   .rpss             68.332    3.954   17.282    0.000   68.332    0.671\n   .dfss             66.827    4.711   14.186    0.000   66.827    0.548\n   .dbss             45.581    3.270   13.938    0.000   45.581    0.443\n   .soss             43.167    4.798    8.998    0.000   43.167    0.382\n    Verbal           13.491    3.298    4.091    0.000    1.000    1.000\n    Nonverbal         0.012    0.074    0.160    0.873    1.000    1.000\n    Spatial           2.844    1.991    1.428    0.153    1.000    1.000\n    Memory           10.820    4.345    2.490    0.013    1.000    1.000\n    G                41.999    4.389    9.569    0.000    1.000    1.000\n\n\n\nmass에 대한 variance가 음수가 되어 mass ~~ 0*mass으로 설정했음.\n\ninspect(fit_bif, what = \"est\") |&gt; print()\n\n$lambda\n     Verbal Nnvrbl Spatil Memory     G\nnvss  1.000  0.000  0.000  0.000 1.000\nwdss  1.189  0.000  0.000  0.000 0.902\nvsss  0.999  0.000  0.000  0.000 1.106\npsss  0.000  1.000  0.000  0.000 0.863\nmass  0.000 70.327  0.000  0.000 1.052\nsqss  0.000 11.347  0.000  0.000 1.143\npcss  0.000  0.000  1.000  0.000 1.064\nrdss  0.000  0.000  3.889  0.000 1.004\nrpss  0.000  0.000  1.110  0.000 0.846\ndfss  0.000  0.000  0.000  1.000 1.026\ndbss  0.000  0.000  0.000  0.837 1.088\nsoss  0.000  0.000  0.000  1.136 1.152\n\n$theta\n       nvss   wdss   vsss   psss   mass   sqss   pcss   rdss   rpss   dfss   dbss   soss\nnvss 46.382                                                                             \nwdss  0.000 38.625                                                                      \nvsss  0.000  0.000 39.039                                                               \npsss  0.000  0.000  0.000 74.591                                                        \nmass  0.000  0.000  0.000  0.000  0.000                                                 \nsqss  0.000  0.000  0.000  0.000  0.000 37.487                                          \npcss  0.000  0.000  0.000  0.000  0.000  0.000 34.500                                   \nrdss  0.000  0.000  0.000  0.000  0.000  0.000  0.000 14.524                            \nrpss  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 68.332                     \ndfss  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 66.827              \ndbss  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 45.581       \nsoss  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 43.167\n\n$psi\n          Verbal Nnvrbl Spatil Memory      G\nVerbal    13.491                            \nNonverbal  0.000  0.012                     \nSpatial    0.000  0.000  2.844              \nMemory     0.000  0.000  0.000 10.820       \nG          0.000  0.000  0.000  0.000 41.999\n\n\n\n\nparameterTable(fit_bif) |&gt; print()\n\n   id       lhs op       rhs user block group free ustart exo label plabel  start    est      se\n1   1    Verbal =~      nvss    1     1     1    0      1   0         .p1.  1.000  1.000   0.000\n2   2    Verbal =~      wdss    1     1     1    1     NA   0         .p2.  0.967  1.189   0.210\n3   3    Verbal =~      vsss    1     1     1    2     NA   0         .p3.  1.074  0.999   0.162\n4   4 Nonverbal =~      psss    1     1     1    0      1   0         .p4.  1.000  1.000   0.000\n5   5 Nonverbal =~      mass    1     1     1    3     NA   0         .p5.  1.538 70.327 219.185\n6   6 Nonverbal =~      sqss    1     1     1    4     NA   0         .p6.  1.538 11.347  35.167\n7   7   Spatial =~      pcss    1     1     1    0      1   0         .p7.  1.000  1.000   0.000\n8   8   Spatial =~      rdss    1     1     1    5     NA   0         .p8.  1.171  3.889   2.449\n9   9   Spatial =~      rpss    1     1     1    6     NA   0         .p9.  0.857  1.110   0.273\n10 10    Memory =~      dfss    1     1     1    0      1   0        .p10.  1.000  1.000   0.000\n11 11    Memory =~      dbss    1     1     1    7     NA   0        .p11.  1.016  0.837   0.227\n12 12    Memory =~      soss    1     1     1    8     NA   0        .p12.  1.125  1.136   0.350\n13 13         G =~      nvss    1     1     1    0      1   0        .p13.  1.000  1.000   0.000\n14 14         G =~      wdss    1     1     1    9     NA   0        .p14.  0.905  0.902   0.052\n15 15         G =~      vsss    1     1     1   10     NA   0        .p15.  1.056  1.106   0.058\n16 16         G =~      psss    1     1     1   11     NA   0        .p16.  0.737  0.863   0.065\n17 17         G =~      mass    1     1     1   12     NA   0        .p17.  0.852  1.052   0.067\n18 18         G =~      sqss    1     1     1   13     NA   0        .p18.  0.923  1.143   0.065\n19 19         G =~      pcss    1     1     1   14     NA   0        .p19.  0.868  1.064   0.062\n20 20         G =~      rdss    1     1     1   15     NA   0        .p20.  0.828  1.004   0.065\n21 21         G =~      rpss    1     1     1   16     NA   0        .p21.  0.676  0.846   0.064\n22 22         G =~      dfss    1     1     1   17     NA   0        .p22.  0.914  1.026   0.071\n23 23         G =~      dbss    1     1     1   18     NA   0        .p23.  0.922  1.088   0.067\n24 24         G =~      soss    1     1     1   19     NA   0        .p24.  0.989  1.152   0.070\n25 25      mass ~~      mass    1     1     1    0      0   0        .p25.  0.000  0.000   0.000\n26 26    Verbal ~~ Nonverbal    1     1     1    0      0   0        .p26.  0.000  0.000   0.000\n27 27    Verbal ~~   Spatial    1     1     1    0      0   0        .p27.  0.000  0.000   0.000\n28 28    Verbal ~~    Memory    1     1     1    0      0   0        .p28.  0.000  0.000   0.000\n29 29 Nonverbal ~~   Spatial    1     1     1    0      0   0        .p29.  0.000  0.000   0.000\n30 30 Nonverbal ~~    Memory    1     1     1    0      0   0        .p30.  0.000  0.000   0.000\n31 31   Spatial ~~    Memory    1     1     1    0      0   0        .p31.  0.000  0.000   0.000\n32 32    Verbal ~~         G    1     1     1    0      0   0        .p32.  0.000  0.000   0.000\n33 33 Nonverbal ~~         G    1     1     1    0      0   0        .p33.  0.000  0.000   0.000\n34 34   Spatial ~~         G    1     1     1    0      0   0        .p34.  0.000  0.000   0.000\n35 35    Memory ~~         G    1     1     1    0      0   0        .p35.  0.000  0.000   0.000\n36 36      nvss ~~      nvss    0     1     1   20     NA   0        .p36. 50.936 46.382   3.255\n37 37      wdss ~~      wdss    0     1     1   21     NA   0        .p37. 45.943 38.625   3.797\n38 38      vsss ~~      vsss    0     1     1   22     NA   0        .p38. 51.935 39.039   3.016\n39 39      psss ~~      psss    0     1     1   23     NA   0        .p39. 52.934 74.591   3.972\n40 40      sqss ~~      sqss    0     1     1   24     NA   0        .p40. 46.941 37.487   2.240\n41 41      pcss ~~      pcss    0     1     1   25     NA   0        .p41. 42.447 34.500   2.494\n42 42      rdss ~~      rdss    0     1     1   26     NA   0        .p42. 49.938 14.524  26.056\n43 43      rpss ~~      rpss    0     1     1   27     NA   0        .p43. 50.936 68.332   3.954\n44 44      dfss ~~      dfss    0     1     1   28     NA   0        .p44. 60.924 66.827   4.711\n45 45      dbss ~~      dbss    0     1     1   29     NA   0        .p45. 51.436 45.581   3.270\n46 46      soss ~~      soss    0     1     1   30     NA   0        .p46. 56.429 43.167   4.798\n47 47    Verbal ~~    Verbal    0     1     1   31     NA   0        .p47.  0.050 13.491   3.298\n48 48 Nonverbal ~~ Nonverbal    0     1     1   32     NA   0        .p48.  0.050  0.012   0.074\n49 49   Spatial ~~   Spatial    0     1     1   33     NA   0        .p49.  0.050  2.844   1.991\n50 50    Memory ~~    Memory    0     1     1   34     NA   0        .p50.  0.050 10.820   4.345\n51 51         G ~~         G    0     1     1   35     NA   0        .p51.  0.050 41.999   4.389",
    "crumbs": [
      "Keith's",
      "Factor Analysis"
    ]
  },
  {
    "objectID": "contents/chap16.html#single-indicator-model",
    "href": "contents/chap16.html#single-indicator-model",
    "title": "Chapter 15-16. Confirmatory Factor Analysis",
    "section": "Single-indicator model",
    "text": "Single-indicator model\n\n하나로만 측정된 잠재변수에 대한 모형\n신뢰도(reliabiliy)를 추정한 후 모형에 반영\n\n이 경우, the estimated (internal consistency) reliability for the Digits Forward test, across ages 5–8, is .91 (Elliott, 2007),\n\\(\\displaystyle V_{\\text{Error}} = (1 - \\rho) * V_{\\text{Observed}} = (1-0.91)*121.523 = 10.937\\)\n앞서 고차원 모형에서는 67.91로 추정되었음\n\n신뢰도를 1로 설정하여 완벽히 측정된다고 가정할 수도 있음; \\(\\displaystyle V_{\\text{Error}} = 0\\)\n잠재변수 없이 지표변수만으로 모형을 구성할 수도 있음\n\n\n\n\n\n\n\nReliability\n\n\n\nReliability (신뢰도): \\(\\displaystyle \\rho = \\frac{V_{\\text{True}}}{V_{\\text{Observed}}} = \\frac{V_{\\text{True}}}{V_{\\text{True}} + V_{\\text{Error}}}\\)\n\n\n\n위에서 계산한 신뢰도로 모형을 구성하면; dfss  ~~ 10.94*dfss\n\ndas2_model_single &lt;- \"\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss\n  Spatial =~ pcss + rdss + rpss\n  Memory =~ 1*dfss\n\n  G =~ Verbal + Nonverbal + Spatial + Memory\n\n  dfss ~~ 10.94*dfss\n\"\nfit_single &lt;- sem(das2_model_single, sample.cov = das2cov, sample.nobs = 800)\nsummary(fit_single, fit.measures = TRUE, standardized = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 114 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        23\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               115.280\n  Degrees of freedom                                32\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              3295.596\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.974\n  Tucker-Lewis Index (TLI)                       0.964\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -28207.812\n  Loglikelihood unrestricted model (H1)     -28150.172\n                                                      \n  Akaike (AIC)                               56461.624\n  Bayesian (BIC)                             56569.370\n  Sample-size adjusted Bayesian (SABIC)      56496.332\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.057\n  90 Percent confidence interval - lower         0.046\n  90 Percent confidence interval - upper         0.068\n  P-value H_0: RMSEA &lt;= 0.050                    0.142\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.032\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              1.000                               7.501    0.743\n    wdss              0.928    0.049   19.010    0.000    6.961    0.726\n    vsss              1.097    0.053   20.784    0.000    8.231    0.808\n  Nonverbal =~                                                          \n    psss              1.000                               5.631    0.547\n    mass              1.277    0.091   14.001    0.000    7.193    0.702\n    sqss              1.394    0.093   15.001    0.000    7.852    0.810\n  Spatial =~                                                            \n    pcss              1.000                               7.523    0.817\n    rdss              0.979    0.047   20.818    0.000    7.365    0.737\n    rpss              0.787    0.049   16.184    0.000    5.920    0.586\n  Memory =~                                                             \n    dfss              1.000                              10.531    0.954\n  G =~                                                                  \n    Verbal            1.000                               0.850    0.850\n    Nonverbal         0.843    0.066   12.848    0.000    0.954    0.954\n    Spatial           1.077    0.063   17.075    0.000    0.913    0.913\n    Memory            1.041    0.072   14.514    0.000    0.630    0.630\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .dfss             10.940                              10.940    0.090\n   .nvss             45.604    2.946   15.477    0.000   45.604    0.448\n   .wdss             43.432    2.727   15.929    0.000   43.432    0.473\n   .vsss             36.113    2.763   13.071    0.000   36.113    0.348\n   .psss             74.154    4.003   18.523    0.000   74.154    0.700\n   .mass             53.124    3.218   16.507    0.000   53.124    0.507\n   .sqss             32.224    2.567   12.553    0.000   32.224    0.343\n   .pcss             28.291    2.272   12.450    0.000   28.291    0.333\n   .rdss             45.637    2.914   15.660    0.000   45.637    0.457\n   .rpss             66.831    3.682   18.150    0.000   66.831    0.656\n   .Verbal           15.610    2.179    7.163    0.000    0.277    0.277\n   .Nonverbal         2.832    1.169    2.423    0.015    0.089    0.089\n   .Spatial           9.412    2.049    4.593    0.000    0.166    0.166\n   .Memory           66.832    4.289   15.582    0.000    0.603    0.603\n    G                40.659    4.231    9.609    0.000    1.000    1.000\n\n\n\n잠재변수 없이 지표변수만으로 모형을 구성하면,\n\n\ndas2_model_single2 &lt;- \"\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss\n  Spatial =~ pcss + rdss + rpss\n\n  G =~ Verbal + Nonverbal + Spatial + dfss\n\"\nfit_single2 &lt;- sem(das2_model_single2, sample.cov = das2cov, sample.nobs = 800)\nsummary(fit_single2, fit.measures = TRUE, standardized = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 99 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        23\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               115.280\n  Degrees of freedom                                32\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              3295.596\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.974\n  Tucker-Lewis Index (TLI)                       0.964\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -28207.812\n  Loglikelihood unrestricted model (H1)     -28150.172\n                                                      \n  Akaike (AIC)                               56461.624\n  Bayesian (BIC)                             56569.370\n  Sample-size adjusted Bayesian (SABIC)      56496.332\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.057\n  90 Percent confidence interval - lower         0.046\n  90 Percent confidence interval - upper         0.068\n  P-value H_0: RMSEA &lt;= 0.050                    0.142\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.032\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              1.000                               7.501    0.743\n    wdss              0.928    0.049   19.010    0.000    6.961    0.726\n    vsss              1.097    0.053   20.784    0.000    8.231    0.808\n  Nonverbal =~                                                          \n    psss              1.000                               5.631    0.547\n    mass              1.277    0.091   14.001    0.000    7.193    0.702\n    sqss              1.394    0.093   15.001    0.000    7.852    0.810\n  Spatial =~                                                            \n    pcss              1.000                               7.523    0.817\n    rdss              0.979    0.047   20.818    0.000    7.365    0.737\n    rpss              0.787    0.049   16.184    0.000    5.920    0.586\n  G =~                                                                  \n    Verbal            1.000                               0.850    0.850\n    Nonverbal         0.843    0.066   12.848    0.000    0.954    0.954\n    Spatial           1.077    0.063   17.075    0.000    0.913    0.913\n    dfss              1.041    0.072   14.514    0.000    6.639    0.601\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .nvss             45.604    2.946   15.477    0.000   45.604    0.448\n   .wdss             43.432    2.727   15.929    0.000   43.432    0.473\n   .vsss             36.113    2.763   13.071    0.000   36.113    0.348\n   .psss             74.154    4.003   18.523    0.000   74.154    0.700\n   .mass             53.124    3.218   16.507    0.000   53.124    0.507\n   .sqss             32.224    2.567   12.553    0.000   32.224    0.343\n   .pcss             28.291    2.272   12.450    0.000   28.291    0.333\n   .rdss             45.637    2.914   15.660    0.000   45.637    0.457\n   .rpss             66.831    3.682   18.150    0.000   66.831    0.656\n   .dfss             77.772    4.289   18.133    0.000   77.772    0.638\n   .Verbal           15.610    2.179    7.163    0.000    0.277    0.277\n   .Nonverbal         2.832    1.169    2.423    0.015    0.089    0.089\n   .Spatial           9.412    2.049    4.593    0.000    0.166    0.166\n    G                40.659    4.231    9.609    0.000    1.000    1.000",
    "crumbs": [
      "Keith's",
      "Factor Analysis"
    ]
  },
  {
    "objectID": "contents/chap18.html",
    "href": "contents/chap18.html",
    "title": "Chapter 18. Multigroup Models, Panel Models, Dangers and Assumptions",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)\nA Latent Variable Homework Model\nSingle indicators와 correlated errors를 가지는 모형을 구성",
    "crumbs": [
      "Keith's",
      "Multigroup Models"
    ]
  },
  {
    "objectID": "contents/chap21.html",
    "href": "contents/chap21.html",
    "title": "Chapter 21. Latent Growth Models",
    "section": "",
    "text": "Load libraries\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)",
    "crumbs": [
      "Keith's",
      "Latent Growth Models"
    ]
  },
  {
    "objectID": "contents/cleaning.html",
    "href": "contents/cleaning.html",
    "title": "Cleaning",
    "section": "",
    "text": "select(), mutate(), filter(), rename() : 기본 tidyverse verbs\n\nrowSums(), rowMeans() : composite 변수들의 합 또는 평균을 구함\n\nfactor() : 카테고리 변수의 변환\n\n앞서 다운받은 데이터: altruism.csv 파일 링크\n\nlibrary(tidyverse)\n\n# import data\nhelping &lt;- read_csv(\"data/altruism.csv\")\nhelping |&gt; print()\n\n# A tibble: 120 x 12\n     id pho_1 pho_2 pho_3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n# i 114 more rows\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n\n\nrename()\n\nhelping |&gt;\n    rename(ph1 = pho_1, ph2 = pho_2, ph3 = pho_3) |&gt;\n    print()\n\n# A tibble: 120 x 12\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n# i 114 more rows\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n변형 후에는 꼭 변수에 assign!\n\nhelping &lt;-     # 원래 데이터에 overwrite\n    helping |&gt;\n    rename(ph1 = pho_1, ph2 = pho_2, ph3 = pho_3)\n\nhelping |&gt; print()\n\n# A tibble: 120 x 12\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n# i 114 more rows\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n\n\n\nrowSums(row, na.rm = TRUE) 함수를 이용하는 것이 직접 덧셈보다 더 적절함\nph1, ph2, ph3 세 문항을 더하려면,\n\n# 먼저 문항을 선택/확인\nhelping |&gt;\n  select(ph1:ph3) |&gt; # position!\n  print()\n\n# A tibble: 120 x 3\n    ph1   ph2   ph3\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    95    95    95\n2    58    62    NA\n3   100    50    50\n4    77    77    64\n5    NA    NA    NA\n6   100    75   100\n# i 114 more rows\n\n\n\nhelping |&gt;\n  select(ph1:ph3) |&gt;\n  rowSums(na.rm = TRUE) |&gt;\n  print()\n\n  [1] 285 120 200 218   0 275 257 178 256 189 215 226 209 246 159 197 205 225\n [19] 150 195  44   0   0 125 225 211 270 176 241 205   0 220  98  79 143 165\n [37]  49 294 300 292 101 285 208 230 255 150 299 188 208 205 138 267 187 300\n [55] 195 300 236  59 226 193 213 250  32 228 250 300 300 190 230 281 196 268\n [73] 240 250  39 233 211 198 199 234 300 215 240   9 261 209 281 201 270 255\n [91] 177 235 161   0 242 151 182 170   3 222 172 194 300 300 293 238 243 260\n[109] 197 294 280 195 255   1 162 278 176 262 300 164\n\n\n\nhelping[\"phone\"] &lt;-    # \"phone\"이라는 새로운 변수에 assign!\n  helping |&gt;\n  select(ph1:ph3) |&gt;\n  rowSums(na.rm = TRUE)\n\nhelping |&gt; print(width = Inf)\n\n# A tibble: 120 x 13\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n  emp_q26 phone\n    &lt;dbl&gt; &lt;dbl&gt;\n1      70   285\n2      59   120\n3     100   200\n4      69   218\n5      NA     0\n6      90   275\n# i 114 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\n다음과 같이 직접 더하는 것은 부적절\nhelping |&gt;\n  mutate(phone = ph1 + ph2 + ph3) \n#      id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n# 1     1    95    95    95     1  2004      80      NA      80      80      70\n# 2     2    58    62    NA     0  2003      62      58      59      57      56\n# 3     3   100    50    50    NA  2003      90      51      51      51      52\n# 4     4    77    77    64     1  2004      66      72      88      82      67\n# 5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n# 6     6   100    75   100     0  2004     100      60      70      55      70\n#   emp_q26 phone\n#     &lt;dbl&gt; &lt;dbl&gt;\n# 1      70   285\n# 2      59    NA\n# 3     100   200\n# 4      69   218\n# 5      NA    NA\n# 6      90   275\n# # … with 114 more rows\n\n\n\n\n\nrowMeans(row, na.rm = TRUE) 함수를 이용하는 것이 적절함\n\n# 먼저, 평균을 낼 문항을 선택/확인\nhelping |&gt;\n  select(emp_q20, emp_q22:emp_q26) |&gt;  # \":\" operator와 \",\" 섞어써도 무방\n  print()\n\n# A tibble: 120 x 6\n  emp_q20 emp_q22 emp_q23 emp_q24 emp_q25 emp_q26\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1      80      NA      80      80      70      70\n2      62      58      59      57      56      59\n3      90      51      51      51      52     100\n4      66      72      88      82      67      69\n5      NA      NA      NA      NA      NA      NA\n6     100      60      70      55      70      90\n# i 114 more rows\n\n\n\nhelping[\"persp\"] &lt;- helping |&gt;    # \"persp\"라는 새로운 변수에 assign!\n  select(emp_q20, emp_q22:emp_q26) |&gt;\n  rowMeans(na.rm = TRUE)\n\nhelping |&gt; print(width = Inf)\n\n# A tibble: 120 x 14\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n  emp_q26 phone persp\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      70   285  76  \n2      59   120  58.5\n3     100   200  65.8\n4      69   218  74  \n5      NA     0 NaN  \n6      90   275  74.2\n# i 114 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nTidyverse에서 row-wise operation을 하려면,\nhelping |&gt;\n    rowwise() |&gt;\n    mutate(persp = mean(c(emp_q20, c_across(emp_q22:emp_q26)), na.rm = TRUE)) |&gt;\n    ungroup()\n참고: column-wise operation\n\n\n\n\n\n\n표준화(standardize): scale(x) 함수를 이용\n중심화(center): scale(x, scale = FALSE) 함수를 이용\n\n\nhelping |&gt; \n    mutate(\n        phone_z = scale(phone) %&gt;% as.vector,  # scale()은 matrix로 반환; vector 변환 필요\n        persp_z = scale(persp) %&gt;% as.vector\n    ) |&gt;\n    print(n = 3, width = Inf)\n\n# A tibble: 120 x 16\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n  emp_q26 phone persp  phone_z persp_z\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1      70   285  76    1.05      0.104\n2      59   120  58.5 -1.00     -0.981\n3     100   200  65.8 -0.00643  -0.527\n# i 117 more rows\n\n\nacross() 함수를 이용하면 여러 변수를 한 번에 변환할 수 있음\n\nhelping |&gt; \n    mutate(across(.cols = c(phone, persp),\n                  .fns = ~(scale(.) %&gt;% as.vector),\n                  .names = \"{.col}_z\")) |&gt;  # 변수명을 일괄 변경: 변수명 + \"_z\"\n    print(n = 3, width = Inf)\n\n# A tibble: 120 x 16\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n  emp_q26 phone persp  phone_z persp_z\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1      70   285  76    1.05      0.104\n2      59   120  58.5 -1.00     -0.981\n3     100   200  65.8 -0.00643  -0.527\n# i 117 more rows\n\n\n\n\n\n카테고리 변수는 R의 factor 타입으로 바꾸어 분석하는 것이 유리함.\n간단한 연산은 직접 계산.\n\nhelping |&gt;\n  mutate(\n    sex = factor(sex, levels = c(0, 1), labels = c(\"male\", \"female\")),  # factor 타입의 변수로 변환\n    age = 2023 - age  # 출생년도로부터 나이 계산\n  ) |&gt;\n  print(width = Inf)\n\n# A tibble: 120 x 14\n     id   ph1   ph2   ph3 sex      age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95 female    19      80      NA      80      80      70\n2     2    58    62    NA male      20      62      58      59      57      56\n3     3   100    50    50 NA        20      90      51      51      51      52\n4     4    77    77    64 female    19      66      72      88      82      67\n5     5    NA    NA    NA NA        NA      NA      NA      NA      NA      NA\n6     6   100    75   100 male      19     100      60      70      55      70\n  emp_q26 phone persp\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      70   285  76  \n2      59   120  58.5\n3     100   200  65.8\n4      69   218  74  \n5      NA     0 NaN  \n6      90   275  74.2\n# i 114 more rows\n\n\n\n\n\nfilter()를 활용\n예를 들어, 5번째 행을 지우려면\n\nhelping |&gt;\n    filter(!id == 5) |&gt; # !는 not의 의미\n    print()\n\n# 다시 helping에 assign 해야 수정됨!\n\n# A tibble: 119 x 14\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     6   100    75   100     0  2004     100      60      70      55      70\n6     7    77    94    86     1  2004      91      93      85      91      73\n# i 113 more rows\n# i 3 more variables: emp_q26 &lt;dbl&gt;, phone &lt;dbl&gt;, persp &lt;dbl&gt;\n\n\n여러 행을 지우려면?\n%in% 응용\n\nhelping |&gt;\n    filter(!id %in% c(1, 3, 5)) |&gt; # !는 not의 의미\n    print()\n\n# A tibble: 117 x 14\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     2    58    62    NA     0  2003      62      58      59      57      56\n2     4    77    77    64     1  2004      66      72      88      82      67\n3     6   100    75   100     0  2004     100      60      70      55      70\n4     7    77    94    86     1  2004      91      93      85      91      73\n5     8    90    68    20     0  2004      67      66      31      67      63\n6     9   100    79    77     0  2003      61      51      30      51      51\n# i 111 more rows\n# i 3 more variables: emp_q26 &lt;dbl&gt;, phone &lt;dbl&gt;, persp &lt;dbl&gt;\n\n\n\n\n\nselect() 활용\nemp_q23, emp_q25 두 열을 삭제\n\nhelping |&gt;\n    select(-emp_q23, -emp_q25) |&gt;\n    print()\n\n# A tibble: 120 x 12\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q24 emp_q26 phone\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      70   285\n2     2    58    62    NA     0  2003      62      58      57      59   120\n3     3   100    50    50    NA  2003      90      51      51     100   200\n4     4    77    77    64     1  2004      66      72      82      69   218\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA     0\n6     6   100    75   100     0  2004     100      60      55      90   275\n# i 114 more rows\n# i 1 more variable: persp &lt;dbl&gt;\n\n\nemp_q23부터 emp_q26 열을 삭제 (위치의 의미로)\n\nhelping |&gt;\n    select(-(emp_q23:emp_q26)) |&gt;  # () 꼭 필요\n    print()\n\n# A tibble: 120 x 10\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 phone persp\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA   285  76  \n2     2    58    62    NA     0  2003      62      58   120  58.5\n3     3   100    50    50    NA  2003      90      51   200  65.8\n4     4    77    77    64     1  2004      66      72   218  74  \n5     5    NA    NA    NA    NA    NA      NA      NA     0 NaN  \n6     6   100    75   100     0  2004     100      60   275  74.2\n# i 114 more rows",
    "crumbs": [
      "R tutorial",
      "Cleaning"
    ]
  },
  {
    "objectID": "contents/cleaning.html#유용한-함수들",
    "href": "contents/cleaning.html#유용한-함수들",
    "title": "Cleaning",
    "section": "",
    "text": "select(), mutate(), filter(), rename() : 기본 tidyverse verbs\n\nrowSums(), rowMeans() : composite 변수들의 합 또는 평균을 구함\n\nfactor() : 카테고리 변수의 변환\n\n앞서 다운받은 데이터: altruism.csv 파일 링크\n\nlibrary(tidyverse)\n\n# import data\nhelping &lt;- read_csv(\"data/altruism.csv\")\nhelping |&gt; print()\n\n# A tibble: 120 x 12\n     id pho_1 pho_2 pho_3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n# i 114 more rows\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n\n\nrename()\n\nhelping |&gt;\n    rename(ph1 = pho_1, ph2 = pho_2, ph3 = pho_3) |&gt;\n    print()\n\n# A tibble: 120 x 12\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n# i 114 more rows\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n변형 후에는 꼭 변수에 assign!\n\nhelping &lt;-     # 원래 데이터에 overwrite\n    helping |&gt;\n    rename(ph1 = pho_1, ph2 = pho_2, ph3 = pho_3)\n\nhelping |&gt; print()\n\n# A tibble: 120 x 12\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n# i 114 more rows\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n\n\n\nrowSums(row, na.rm = TRUE) 함수를 이용하는 것이 직접 덧셈보다 더 적절함\nph1, ph2, ph3 세 문항을 더하려면,\n\n# 먼저 문항을 선택/확인\nhelping |&gt;\n  select(ph1:ph3) |&gt; # position!\n  print()\n\n# A tibble: 120 x 3\n    ph1   ph2   ph3\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    95    95    95\n2    58    62    NA\n3   100    50    50\n4    77    77    64\n5    NA    NA    NA\n6   100    75   100\n# i 114 more rows\n\n\n\nhelping |&gt;\n  select(ph1:ph3) |&gt;\n  rowSums(na.rm = TRUE) |&gt;\n  print()\n\n  [1] 285 120 200 218   0 275 257 178 256 189 215 226 209 246 159 197 205 225\n [19] 150 195  44   0   0 125 225 211 270 176 241 205   0 220  98  79 143 165\n [37]  49 294 300 292 101 285 208 230 255 150 299 188 208 205 138 267 187 300\n [55] 195 300 236  59 226 193 213 250  32 228 250 300 300 190 230 281 196 268\n [73] 240 250  39 233 211 198 199 234 300 215 240   9 261 209 281 201 270 255\n [91] 177 235 161   0 242 151 182 170   3 222 172 194 300 300 293 238 243 260\n[109] 197 294 280 195 255   1 162 278 176 262 300 164\n\n\n\nhelping[\"phone\"] &lt;-    # \"phone\"이라는 새로운 변수에 assign!\n  helping |&gt;\n  select(ph1:ph3) |&gt;\n  rowSums(na.rm = TRUE)\n\nhelping |&gt; print(width = Inf)\n\n# A tibble: 120 x 13\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n  emp_q26 phone\n    &lt;dbl&gt; &lt;dbl&gt;\n1      70   285\n2      59   120\n3     100   200\n4      69   218\n5      NA     0\n6      90   275\n# i 114 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\n다음과 같이 직접 더하는 것은 부적절\nhelping |&gt;\n  mutate(phone = ph1 + ph2 + ph3) \n#      id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n# 1     1    95    95    95     1  2004      80      NA      80      80      70\n# 2     2    58    62    NA     0  2003      62      58      59      57      56\n# 3     3   100    50    50    NA  2003      90      51      51      51      52\n# 4     4    77    77    64     1  2004      66      72      88      82      67\n# 5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n# 6     6   100    75   100     0  2004     100      60      70      55      70\n#   emp_q26 phone\n#     &lt;dbl&gt; &lt;dbl&gt;\n# 1      70   285\n# 2      59    NA\n# 3     100   200\n# 4      69   218\n# 5      NA    NA\n# 6      90   275\n# # … with 114 more rows\n\n\n\n\n\nrowMeans(row, na.rm = TRUE) 함수를 이용하는 것이 적절함\n\n# 먼저, 평균을 낼 문항을 선택/확인\nhelping |&gt;\n  select(emp_q20, emp_q22:emp_q26) |&gt;  # \":\" operator와 \",\" 섞어써도 무방\n  print()\n\n# A tibble: 120 x 6\n  emp_q20 emp_q22 emp_q23 emp_q24 emp_q25 emp_q26\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1      80      NA      80      80      70      70\n2      62      58      59      57      56      59\n3      90      51      51      51      52     100\n4      66      72      88      82      67      69\n5      NA      NA      NA      NA      NA      NA\n6     100      60      70      55      70      90\n# i 114 more rows\n\n\n\nhelping[\"persp\"] &lt;- helping |&gt;    # \"persp\"라는 새로운 변수에 assign!\n  select(emp_q20, emp_q22:emp_q26) |&gt;\n  rowMeans(na.rm = TRUE)\n\nhelping |&gt; print(width = Inf)\n\n# A tibble: 120 x 14\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n  emp_q26 phone persp\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      70   285  76  \n2      59   120  58.5\n3     100   200  65.8\n4      69   218  74  \n5      NA     0 NaN  \n6      90   275  74.2\n# i 114 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nTidyverse에서 row-wise operation을 하려면,\nhelping |&gt;\n    rowwise() |&gt;\n    mutate(persp = mean(c(emp_q20, c_across(emp_q22:emp_q26)), na.rm = TRUE)) |&gt;\n    ungroup()\n참고: column-wise operation\n\n\n\n\n\n\n표준화(standardize): scale(x) 함수를 이용\n중심화(center): scale(x, scale = FALSE) 함수를 이용\n\n\nhelping |&gt; \n    mutate(\n        phone_z = scale(phone) %&gt;% as.vector,  # scale()은 matrix로 반환; vector 변환 필요\n        persp_z = scale(persp) %&gt;% as.vector\n    ) |&gt;\n    print(n = 3, width = Inf)\n\n# A tibble: 120 x 16\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n  emp_q26 phone persp  phone_z persp_z\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1      70   285  76    1.05      0.104\n2      59   120  58.5 -1.00     -0.981\n3     100   200  65.8 -0.00643  -0.527\n# i 117 more rows\n\n\nacross() 함수를 이용하면 여러 변수를 한 번에 변환할 수 있음\n\nhelping |&gt; \n    mutate(across(.cols = c(phone, persp),\n                  .fns = ~(scale(.) %&gt;% as.vector),\n                  .names = \"{.col}_z\")) |&gt;  # 변수명을 일괄 변경: 변수명 + \"_z\"\n    print(n = 3, width = Inf)\n\n# A tibble: 120 x 16\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n  emp_q26 phone persp  phone_z persp_z\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1      70   285  76    1.05      0.104\n2      59   120  58.5 -1.00     -0.981\n3     100   200  65.8 -0.00643  -0.527\n# i 117 more rows\n\n\n\n\n\n카테고리 변수는 R의 factor 타입으로 바꾸어 분석하는 것이 유리함.\n간단한 연산은 직접 계산.\n\nhelping |&gt;\n  mutate(\n    sex = factor(sex, levels = c(0, 1), labels = c(\"male\", \"female\")),  # factor 타입의 변수로 변환\n    age = 2023 - age  # 출생년도로부터 나이 계산\n  ) |&gt;\n  print(width = Inf)\n\n# A tibble: 120 x 14\n     id   ph1   ph2   ph3 sex      age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95 female    19      80      NA      80      80      70\n2     2    58    62    NA male      20      62      58      59      57      56\n3     3   100    50    50 NA        20      90      51      51      51      52\n4     4    77    77    64 female    19      66      72      88      82      67\n5     5    NA    NA    NA NA        NA      NA      NA      NA      NA      NA\n6     6   100    75   100 male      19     100      60      70      55      70\n  emp_q26 phone persp\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      70   285  76  \n2      59   120  58.5\n3     100   200  65.8\n4      69   218  74  \n5      NA     0 NaN  \n6      90   275  74.2\n# i 114 more rows\n\n\n\n\n\nfilter()를 활용\n예를 들어, 5번째 행을 지우려면\n\nhelping |&gt;\n    filter(!id == 5) |&gt; # !는 not의 의미\n    print()\n\n# 다시 helping에 assign 해야 수정됨!\n\n# A tibble: 119 x 14\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     6   100    75   100     0  2004     100      60      70      55      70\n6     7    77    94    86     1  2004      91      93      85      91      73\n# i 113 more rows\n# i 3 more variables: emp_q26 &lt;dbl&gt;, phone &lt;dbl&gt;, persp &lt;dbl&gt;\n\n\n여러 행을 지우려면?\n%in% 응용\n\nhelping |&gt;\n    filter(!id %in% c(1, 3, 5)) |&gt; # !는 not의 의미\n    print()\n\n# A tibble: 117 x 14\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     2    58    62    NA     0  2003      62      58      59      57      56\n2     4    77    77    64     1  2004      66      72      88      82      67\n3     6   100    75   100     0  2004     100      60      70      55      70\n4     7    77    94    86     1  2004      91      93      85      91      73\n5     8    90    68    20     0  2004      67      66      31      67      63\n6     9   100    79    77     0  2003      61      51      30      51      51\n# i 111 more rows\n# i 3 more variables: emp_q26 &lt;dbl&gt;, phone &lt;dbl&gt;, persp &lt;dbl&gt;\n\n\n\n\n\nselect() 활용\nemp_q23, emp_q25 두 열을 삭제\n\nhelping |&gt;\n    select(-emp_q23, -emp_q25) |&gt;\n    print()\n\n# A tibble: 120 x 12\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q24 emp_q26 phone\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      70   285\n2     2    58    62    NA     0  2003      62      58      57      59   120\n3     3   100    50    50    NA  2003      90      51      51     100   200\n4     4    77    77    64     1  2004      66      72      82      69   218\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA     0\n6     6   100    75   100     0  2004     100      60      55      90   275\n# i 114 more rows\n# i 1 more variable: persp &lt;dbl&gt;\n\n\nemp_q23부터 emp_q26 열을 삭제 (위치의 의미로)\n\nhelping |&gt;\n    select(-(emp_q23:emp_q26)) |&gt;  # () 꼭 필요\n    print()\n\n# A tibble: 120 x 10\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 phone persp\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA   285  76  \n2     2    58    62    NA     0  2003      62      58   120  58.5\n3     3   100    50    50    NA  2003      90      51   200  65.8\n4     4    77    77    64     1  2004      66      72   218  74  \n5     5    NA    NA    NA    NA    NA      NA      NA     0 NaN  \n6     6   100    75   100     0  2004     100      60   275  74.2\n# i 114 more rows",
    "crumbs": [
      "R tutorial",
      "Cleaning"
    ]
  },
  {
    "objectID": "contents/cleaning.html#이상치-발견",
    "href": "contents/cleaning.html#이상치-발견",
    "title": "Cleaning",
    "section": "이상치 발견",
    "text": "이상치 발견\nOutliers을 찾는 방법은 다양하고 복잡한 테크닉을 요하기도 하는데, 앞으로 점차 익히게 될 것임\n예를 들어, age에 잘못 기입한 경우가 있는데\n\nhelping &lt;- read_csv(\"data/altruism.csv\")\n\nhelping |&gt;\n    ggplot(aes(x = age)) +\n    geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nage는 출생년도를 물어봤으나 다른 답을 한 경우들이 있음\n값은 2002 ~ 2004 사이가 정상이므로 filter()를 써서 확인해 볼 수 있음\n\nhelping |&gt;\n    filter(age &lt; 2002 | age &gt; 2004) |&gt;\n    print()\n\n# A tibble: 7 x 12\n     id pho_1 pho_2 pho_3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1    11    65    94    56     1   203      62      86      58      47      62\n2    21    17    10    17     1 20004       0       6       1       0       4\n3    43    90    88    30     1   507     100      78      62     100      78\n4    52   100    82    85     1   723      87      83      89     100      88\n5    59    76    86    64     0   709     100      93      67      94      79\n6   108    75   100    85     1  2005     100     100     100     100      97\n7   118    92    76    94     0  1108      55      51      51      60      53\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n값을 수정\n\n# 이상치에 대한 id를 우선 추출\nids_anomaly = helping |&gt;\n    filter(age &lt; 2002 | age &gt; 2004) |&gt;\n    pull(id)  # vector로 반환\nids_anomaly |&gt; print()\n\n[1]  11  21  43  52  59 108 118\n\n\n\n# 모두 NA로 변경하는 경우\nhelping |&gt;\n    # ifelse(조건, 참일 때 값, 거짓일 때 값)\n    mutate(\n        age = ifelse(age &gt; 2004, 2004, age),\n        age = ifelse(age &lt; 2002, NA, age)\n        # 대신, age = ifelse(age &gt; 2004, 2004, ifelse(age &lt; 2002, NA, age))\n        ) |&gt; \n    filter(id %in% ids_anomaly) |&gt; \n    print()\n\n# A tibble: 7 x 12\n     id pho_1 pho_2 pho_3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1    11    65    94    56     1    NA      62      86      58      47      62\n2    21    17    10    17     1  2004       0       6       1       0       4\n3    43    90    88    30     1    NA     100      78      62     100      78\n4    52   100    82    85     1    NA      87      83      89     100      88\n5    59    76    86    64     0    NA     100      93      67      94      79\n6   108    75   100    85     1  2004     100     100     100     100      97\n7   118    92    76    94     0    NA      55      51      51      60      53\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n\n# 직접 입력하는 방식\nhelping[helping$id %in% ids_anomaly, \"age\"] |&gt; print()\n\n# A tibble: 7 x 1\n    age\n  &lt;dbl&gt;\n1   203\n2 20004\n3   507\n4   723\n5   709\n6  2005\n7  1108\n\n\n\n# 직접 입력하는 방식\nhelping[helping$id %in% ids_anomaly, \"age\"] &lt;- c(NA, 2004, NA, NA, NA, 2005, NA)\nhelping |&gt;\n    filter(id %in% ids_anomaly) |&gt;\n    print()\n\n# A tibble: 7 x 12\n     id pho_1 pho_2 pho_3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1    11    65    94    56     1    NA      62      86      58      47      62\n2    21    17    10    17     1  2004       0       6       1       0       4\n3    43    90    88    30     1    NA     100      78      62     100      78\n4    52   100    82    85     1    NA      87      83      89     100      88\n5    59    76    86    64     0    NA     100      93      67      94      79\n6   108    75   100    85     1  2005     100     100     100     100      97\n7   118    92    76    94     0    NA      55      51      51      60      53\n# i 1 more variable: emp_q26 &lt;dbl&gt;",
    "crumbs": [
      "R tutorial",
      "Cleaning"
    ]
  },
  {
    "objectID": "contents/cleaning.html#샘플-r-script",
    "href": "contents/cleaning.html#샘플-r-script",
    "title": "Cleaning",
    "section": "샘플 R script",
    "text": "샘플 R script\n\nlibrary(tidyverse)\n\n# import data\nhelping &lt;- read_csv(\"data/altruism.csv\")\n\n# rename\nhelping &lt;- helping |&gt;\n  rename(ph1 = pho_1, ph2 = pho_2, ph3 = pho_3) \n\n# delete reponses\nhelping &lt;- helping |&gt;\n  filter(!id == 5)\n\n# scoring\nhelping[\"phone\"] &lt;- helping |&gt;\n  select(ph1:ph3) |&gt;\n  rowMeans(na.rm = TRUE)\n\nhelping[\"persp\"] &lt;- helping |&gt; \n  select(emp_q20, emp_q22, emp_q24:emp_q26) |&gt;\n  rowMeans(na.rm = TRUE)\n\n# substitute anolamies\nhelping &lt;- helping |&gt;\n  mutate(age = ifelse(age &gt; 2004, 2004, ifelse(age &lt; 2002, NA, age)))\n\n# factors and etc.\nhelping &lt;- helping |&gt;\n  mutate(\n    sex = factor(sex, levels = c(0, 1), labels = c(\"male\", \"female\")),\n    age = 2023 - age\n  )\n\n# select variables\nhelping &lt;- helping |&gt;\n  select(id, sex, age, phone, persp)\n\n정리된 파일로 분석 시작!\n\nhelping |&gt; print()\n\n# A tibble: 119 x 5\n     id sex      age phone persp\n  &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1 female    19  95    75  \n2     2 male      20  60    58.4\n3     3 NA        20  66.7  68.8\n4     4 female    19  72.7  71.2\n5     6 male      19  91.7  75  \n6     7 female    19  85.7  82.6\n# i 113 more rows",
    "crumbs": [
      "R tutorial",
      "Cleaning"
    ]
  },
  {
    "objectID": "contents/equations.html",
    "href": "contents/equations.html",
    "title": "Structural Equations",
    "section": "",
    "text": "Load libraries\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)",
    "crumbs": [
      "SEM Equations"
    ]
  },
  {
    "objectID": "contents/equations.html#structural-equation-model-sem-matrix-representation",
    "href": "contents/equations.html#structural-equation-model-sem-matrix-representation",
    "title": "Structural Equations",
    "section": "Structural Equation Model (SEM) Matrix Representation",
    "text": "Structural Equation Model (SEM) Matrix Representation\n\nMeasures of political democracy and industrialization for 75 developing countries in 1960 and 1965\n\ny1. freedom of the press, 1960\ny2. freedom of political opposition, 1960\ny3. fairness of elections, 1960\nx1. natural log of GNP per capita, 1960\nx2. natural log of energy consumption per capita, 1960\n\n\n\n\n\n\n \n\n\n외생변수(Exogenous variables): \\(X1, X2\\)\n내생변수(Endogenous variables): \\(Y1, Y2, Y3\\)\n경로계수(Path coefficients)\n\n\\(\\gamma\\): \\(X\\)와 \\(Y\\)의 관계\n\\(\\beta\\): \\(Y\\)와 \\(Y\\)의 관계\n\n오차항(Error terms): \\(\\zeta\\)\n\n\n\n\n\nFigure 4.2 Union Sentiment Model for Southern Textile Workers\nSource: p. 83, Bollen, K. A. (1989). Structural equations with latent variables\n\nMatrix representation of the model:\n\\[\n\\mathbf{Y} = \\mathbf{B}\\mathbf{Y} + \\mathbf{\\Gamma}\\mathbf{X} + \\boldsymbol{\\zeta}\n\\]\n\\[\n\\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ y_3\n\\end{bmatrix} =\n\\begin{bmatrix}\n0 & 0 & 0 \\\\\n\\beta_{21} & 0 & 0 \\\\\n\\beta_{31} & \\beta_{32} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ y_3\n\\end{bmatrix} +\n\\begin{bmatrix}\n0 & \\gamma_{12} \\\\\n0 & \\gamma_{22} \\\\\n\\gamma_{31} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\ x_2\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\zeta_1 \\\\ \\zeta_2 \\\\ \\zeta_3\n\\end{bmatrix}\n\\]\n가정 1: 내생변수의 오차항과 외생변수는 서로 독립; \\(\\zeta \\perp X\\)\n가정 2: 각 변수는 평균이 0이고, 변수들은 multivariate normal을 따름\n\n\n\n\n\n\nThe covariance matrix of exogenous variables:\n\\[\n\\mathbf{\\Phi} =\n\\begin{bmatrix}\n\\phi_{11} & \\phi_{12} \\\\\n\\phi_{21} & \\phi_{22}\n\\end{bmatrix}\n\\]\n\n\nThe covariance matrix of error terms:\n\\[\n\\mathbf{\\Psi} =\n\\begin{bmatrix}\n\\psi_{11} & 0 & 0 \\\\\n0 & \\psi_{22} & 0 \\\\\n0 & 0 & \\psi_{33}\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathbf{Y}\\) is the vector of endogenous variables\n\\(\\mathbf{X}\\) is the vector of exogenous variables\n\\(\\mathbf{B}\\) is the matrix of path coefficients between endogenous variables\n\\(\\mathbf{\\Gamma}\\) is the matrix of path coefficients from exogenous to endogenous variables\n\n\n\n\n\n\n\\(\\boldsymbol{\\zeta}\\) is the vector of error terms\n\\(\\mathbf{\\Phi}\\) is the covariance matrix of exogenous variables\n\\(\\mathbf{\\Psi}\\) is the covariance matrix of error terms\n\n\n\n\n\n\n\ndata and model\n# A dataset from Bollen (1989) containing measures of political democracy and industrialization for 75 developing countries in 1960 and 1965. \nbollen1989a &lt;- MIIVsem::bollen1989a |&gt; as_tibble()\n\n# fig 4.2\nmodel &lt;- '\n  y3 ~ x1 + y1 + y2\n  y2 ~ x2 + y1\n  y1 ~ x2\n'\nfit &lt;- sem(model, data = bollen1989a, fixed.x = FALSE)\n\n\n\n# coefficients: betas & gamma\ninspect(fit, what = \"est\")$beta |&gt; round(2) |&gt; print()\n\n   y3   y2   y1   x1   x2\ny3  0 0.06 0.76 0.36 0.00\ny2  0 0.00 0.88 0.00 0.16\ny1  0 0.00 0.00 0.00 0.56\nx1  0 0.00 0.00 0.00 0.00\nx2  0 0.00 0.00 0.00 0.00\n\n\n\n# covariances: psi & phi\ninspect(fit, what = \"est\")$psi |&gt; round(2) |&gt; print()\n\n     y3   y2   y1   x1   x2\ny3 5.64                    \ny2 0.00 9.71               \ny1 0.00 0.00 6.09          \nx1 0.00 0.00 0.00 0.53     \nx2 0.00 0.00 0.00 0.98 2.25",
    "crumbs": [
      "SEM Equations"
    ]
  },
  {
    "objectID": "contents/equations.html#parameter-estimation",
    "href": "contents/equations.html#parameter-estimation",
    "title": "Structural Equations",
    "section": "Parameter Estimation",
    "text": "Parameter Estimation\n\n\nSource: p. 86, Bollen, K. A. (1989). Structural equations with latent variables\n\n\\(y_1 = \\gamma_{11}x_1 + \\zeta_1\\)\n\\(y_2 = \\beta_{21}y_1 + \\zeta_2\\)\nMatrix representation:\n\n\n\n\n\n\n\\(\\begin{bmatrix}\ny_1 \\\\ y_2\n\\end{bmatrix} =\n\\begin{bmatrix}\n0 & 0 \\\\\n\\beta_{21} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\ny_1 \\\\ y_2\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\gamma_{11} \\\\\n0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\zeta_1 \\\\ \\zeta_2\n\\end{bmatrix}\\)\n\n\n\\(\\mathbf{\\Phi} =\n\\begin{bmatrix}\n\\phi_{11}\n\\end{bmatrix}\\)\n\n\n\\(\\mathbf{\\Psi} =\n\\begin{bmatrix}\n\\psi_{11} & \\psi_{12}\\\\\n\\psi_{21} & \\psi_{22}\\\\\n\\end{bmatrix}\\)\n\n\n\n가정들:\n\n내생변수의 잔차는 외생변수와 독립: \\(cov(\\zeta_1, x_1) = 0\\), \\(cov(\\zeta_2, x_1) = 0\\)\n내생변수의 잔차들은 서로 독립: \\(cov(\\zeta_1, \\zeta_2) = 0\\) 즉, \\(\\psi_{12} = 0\\)\n\n추정은 가능하나 non-recursive한 피드백 루프가 있는 모형이 됨.\n\n\n\n\n\n\n\n\nCoveriance에 대한 성질\n\n\n\n\\(Cov(aX+bY,~ cU+dV) = ac~Cov(X, U) + ad~Cov(X, V) + bc~Cov(Y, U) + bd~Cov(Y, V)\\)\n\n\n\\(\\begin{align}\n  cov(y_1, y_2) & = cov(\\gamma_{11}x_1 + \\zeta_1, \\beta_{21}y_1 + \\zeta_2) \\\\\n                & = \\gamma_{11} \\beta_{21} cov(x_1, y_1) + \\gamma_{11} cov(x_1, \\zeta_2) + \\beta_{21} cov(y_1, \\zeta_1) + cov(\\zeta_1, \\zeta_2) \\\\\n                & = \\gamma_{11} \\beta_{21} * \\gamma_{11} \\phi_{11} + \\beta_{21} \\psi_{11} \\\\\n                & = \\beta_{21}( \\gamma_{11}^2 \\phi_{11} + \\psi_{11})\n\\end{align}\\)\n\n\\(cov(x_1, y_1) = cov(x_1, \\gamma_{11}x_1 + \\zeta_1) = \\gamma_{11} cov(x_1, x_1) + cov(x_1, \\zeta_1) = \\gamma_{11} var(x_1) = \\gamma_{11} \\phi_{11}\\)\n\\(cov(y_1, \\zeta_1) = cov(\\gamma_{11}x_1 + \\zeta_1, \\zeta_1) = \\gamma_{11} cov(x_1, \\zeta_1) + cov(\\zeta_1, \\zeta_1) = var(\\zeta_1) = \\psi_{11}\\)\n\\(cov(\\zeta_1, \\zeta_2) = \\psi_{12}\\)\n\n\\[\\begin{equation}\n\\begin{bmatrix}\n\\text{VAR}(y_1) \\\\\n\\text{COV}(y_2, y_1) & \\text{VAR}(y_2) \\\\\n\\text{COV}(x_1, y_1) & \\text{COV}(x_1, y_2) & \\text{VAR}(x_1)\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\gamma_{11}^2\\phi_{11} + \\psi_{11} \\\\\n\\beta_{21}(\\gamma_{11}^2\\phi_{11} + \\psi_{11}) & \\beta_{21}^2(\\gamma_{11}^2\\phi_{11} + \\psi_{11}) + \\psi_{22} \\\\\n\\gamma_{11}\\phi_{11} & \\beta_{21}\\gamma_{11}\\phi_{11} & \\phi_{11}\n\\end{bmatrix}\n\\end{equation}\\]\n\n\n데이터로부터 최선의 파라미터를 추정\n\n\n\n\n\n\n기본적인 구조 모형의 가설(hypothesis):\n\\(\\Sigma = \\Sigma(\\theta)\\)\n즉, 모집단의 공분산 = 모수(parameter)로 표현되는(implied) 공분산\n\n\n\n\n모집단의 공분산에 대한 추정치: 관찰된 공분산 행렬 \\(\\mathbf{S}\\)\n파라미터로 표현된 공분산 행렬의 추정치: \\(\\hat{\\Sigma}(\\theta)\\)\n\n이제 이 둘의 차 \\(\\mathbf{S} - \\hat{\\Sigma}(\\theta)\\)를 최소화하는 파라미터들(통칭 \\(\\theta\\))를 찾고자 함.\n예를 들어,\n\n# covariance matrix\nlower &lt;- '\n  225.2\n  32.1 100.3\n  111.4 26.5 105.2'\ncov &lt;- getCov(lower, names = c(\"x1\", \"y1\", \"y2\"))\n\n\nmodel &lt;- '\n  # regressopm\n  y2 ~ 0*x1 + y1\n  y1 ~ x1\n  \n  # variacne\n  # y2 ~~ y1  # assumed to be uncorrelated\n'\nfit &lt;- sem(model, sample.cov = cov, sample.nobs = 200, fixed.x = F)\n\n\nparameterEstimates(fit) |&gt; print()\n\n  lhs op rhs     est     se      z pvalue ci.lower ci.upper\n1  y2  ~  x1   0.000  0.000     NA     NA    0.000    0.000\n2  y2  ~  y1   0.264  0.070  3.776  0.000    0.127    0.401\n3  y1  ~  x1   0.143  0.046  3.092  0.002    0.052    0.233\n4  y2 ~~  y2  97.708  9.771 10.000  0.000   78.557  116.858\n5  y1 ~~  y1  95.246  9.525 10.000  0.000   76.578  113.914\n6  x1 ~~  x1 224.074 22.407 10.000  0.000  180.156  267.992\n\n\n\n# implied covariance matrix\nsigma_hat &lt;- lavInspect(fit, \"cov.all\")[3:1, 3:1] |&gt; round(1)\nsigma_hat |&gt; print()\n\n      x1   y1    y2\nx1 224.1 31.9   8.4\ny1  31.9 99.8  26.4\ny2   8.4 26.4 104.7\n\n\n\n# 공분산 행렬 차이: residual covariance matrix\n(cov - sigma_hat) |&gt; print()\n\n      x1  y1    y2\nx1   1.1 0.2 103.0\ny1   0.2 0.5   0.1\ny2 103.0 0.1   0.5\n\n\n\n즉, \\(\\mathbf{S} - \\hat{\\Sigma}(\\theta) =\n\\begin{bmatrix}\n1.1 \\\\\n0.2 & 0.5 \\\\\n103.0 & 0.1 & 0.5\n\\end{bmatrix}\\)\n\n\n이는 어떤 다른 파라미터 값에 대해서도 이 공분산 차이보다 더 작을 수 없다는 의미임.\n한편, 어떤 의미에서 차이가 작다는지를 정의해야 함.\n\n\n\nEstimation\n\n\n\n\n\n\n\\(\\mathbf{S} - \\hat{\\Sigma}(\\theta)\\)가 작다는 것을 어떻게 정의할 것인가?\n혹은, 두 행렬 \\(\\mathbf{S}\\), \\(\\hat{\\Sigma}(\\theta)\\) 사이의 거리를 어떻게 정의할 것인가?\n\nFitting function(목적 함수)를 통해 그 함수가 최소가 되는 파라미터를 찾는 방식\n\n\n\n\n\nLikelihood의 관점에서 \\(\\mathbf{S}\\)와 \\(\\hat{\\Sigma}(\\theta)\\)의 거리를 측정: maximum likelihood estimation(MLE); scale invariant, scale free\n\n\\(F_{ML} = log|\\Sigma(\\theta)| + tr(S\\Sigma^{-1}(\\theta)) - log|S| - (p + q)\\)\n\n각 성분들의 제곱의 합이 최소가 되도록 추정: unweighted least squares(ULS); scale에 따라 변함.\n\n\\(F_{ULS} = \\frac{1}{2} tr[(\\mathbf{S} - \\Sigma(\\theta))^2]\\)\n\nFully weighted least squares(WLS): 적절한 \\(\\mathbf{W}\\)를 선택해 공분산의 크기를 조정, 분포에 대한 가정 없으나 큰 표본을 요구함.\n\n\\(F_{WLS} = \\frac{1}{2} tr\\left[\\left[\\left(\\mathbf{S} - \\Sigma(\\theta)\\right)\\mathbf{W}^{-1}\\right]^2\\right]\\)\nGeneralized least squares(GLS): \\(\\mathbf{W}\\)를 \\(\\mathbf{S}\\)로 선택하는 경우; scale invariant, scale free\n\n\nMaximum Likelihood Estimation\n관측 변수들이 multivariate normal distribution을 따른다고 가정하면,\n\nLog-likelihood \\(\\displaystyle L(\\theta) = \\frac{-N(p + q)}{2}log(2\\pi) - \\frac{N}{2}\\left(log|\\Sigma(\\theta)| + tr(S\\Sigma^{-1}(\\theta))\\right)\\)\n\\(F_{ML} = log|\\Sigma(\\theta)| + tr(S\\Sigma^{-1}(\\theta)) - log|S| - (p + q)\\)\n\n즉, likelihood를 최대화하는 것은 \\(F_{ML}\\)를 최소화하는 것과 동일함.\nFitting function으로써 \\(F_{ML}\\)의 장점은 asymptotic properties를 가지고 있음.\n\nMultivariate normal을 가정하고, N이 충분히 클 때,\n주어진 모형이 참이라면, random sampling 변화에 따라 \\((N-1)*F_{ML}\\)은 \\(\\chi^2\\) 분포 따름\n\n\n\n\n\n\n\nNon-normality에 대한 대응\n\n\n\n\nRobust ML\nBootstrapping: Bollen-Stine bootstrap\n분포에 대한 가정이 없는 방법: WLS(fully weighted least squares)\n\nML 파라미터 추정치는 일반적으로 robust하나, 표준오차는 문제가 될 수 있음.\n다음과 같은 ML에 대한 robust estimation들은 표준오차에 대한 보정을 제공함.\n\nOption “MLM” is for complete data sets only and generates the mean-adjusted Satorra-Bentler scaled chi-square.\nOption “MLR” can be applied in complete or incomplete data sets, and it generates a mean adjusted chi-square based on the Yuan-Bentler T2 statistic. Because this method accommodates missing data, it is the most flexible option listed here.\nOption “MLMV” is for complete data sets only and computes a mean- and variance-adjusted scaled chi-square.\nOption “MLMVS” generates a mean- and variance adjusted chi-square with a correction for heteroscedasticity by Satterthwaite (1941). Model degrees of freedom are estimated, and the method is for complete data sets.\n\n\nSource: p. 137, 163, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n\n\n\n\n\nModel Indentification\n위의 예에서,\n\n파라미터를 추정하기 위해 주어진 정보의 갯수: 3+2+1=6개\n\n관찰 변수가 \\(p\\)개 일 때, \\(p(p+1)/2\\)\n\n추정하고자 하는 파라미터의 개수: 5개\n\n즉, 식은 6개이며 미지수는 5개이므로 추정가능함!\n이 때, 자유도(degree of freedom) df = 6 - 5 = 1\n\nover-identified (과대 식별): df &gt; 0, 그 차이(자유도)가 클수록 유리함!\njust-identified (적정 식별): df = 0, 포화 모형 (saturated model)\nunder-identified model (과소 식별): df &lt; 0, 파라미터 추정 불가능\n\n\nsummary(fit, estimate = F) |&gt; print()\n\nlavaan 0.6-19 ended normally after 10 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                           200\n\nModel Test User Model:\n                                                      \n  Test statistic                               139.381\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.000\n\n\n\n# info for parameters\nlavaan::parTable(fit) |&gt; print()\n\n  id lhs op rhs user block group free ustart exo label plabel   start     est\n1  1  y2  ~  x1    1     1     1    0      0   0         .p1.   0.000   0.000\n2  2  y2  ~  y1    1     1     1    1     NA   0         .p2.   0.111   0.264\n3  3  y1  ~  x1    1     1     1    2     NA   0         .p3.   0.143   0.143\n4  4  y2 ~~  y2    0     1     1    3     NA   0         .p4.  48.671  97.708\n5  5  y1 ~~  y1    0     1     1    4     NA   0         .p5.  95.246  95.246\n6  6  x1 ~~  x1    0     1     1    5     NA   0         .p6. 224.074 224.074\n      se\n1  0.000\n2  0.070\n3  0.046\n4  9.771\n5  9.525\n6 22.407",
    "crumbs": [
      "SEM Equations"
    ]
  },
  {
    "objectID": "contents/equations.html#overall-fit-measures",
    "href": "contents/equations.html#overall-fit-measures",
    "title": "Structural Equations",
    "section": "Overall Fit Measures",
    "text": "Overall Fit Measures\n두 행렬 \\(\\mathbf{S}\\)와 \\(\\hat{\\Sigma}(\\theta)\\)간의 거리를 측정하여 모형이 전체적으로 얼마나 적합한지를 평가함.\n부분적인(local) 적합도를 살펴보는 것으로는 드러나지 못하는 “전체적 부적절함”을 판별할 수는 지표를 얻을 수 있음.\n\n반대로, 전체적으로 적합하다고 해서 부분적으로 적합하다는 보장은 없음.\n따라서, 여러 측면에서 동시에 평가하는 것이 중요함!\n\n\nResiduals\nIdeally, all residuals should be near zero for a “good” model. But the sample residuals are affected by several factors: (1) the departure of \\(\\Sigma\\) from \\(\\Sigma(\\theta)\\), (2) the scales of the observed variables, and (3) sampling fluctuations\n\nCoveriance residuals: 두 공분산의 차이 \\(\\mathbf{S} - \\hat{\\Sigma}(\\theta)\\)를 직접 확인\nCorrelation residuals: 각 공분산을 표준화하여 (변수들의 단위와 독립적인) 상관계수로 변환 후 비교\n\n\\(\\displaystyle r_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}\\), \\(\\sigma_{ij}\\): 해당 공분산\n그 값이 0.1 이상이면 주의\n\nStandardized residuals: 표준오차로 나눈 잔차들을 비교\n\ndivided by the square root of the asymptotic variance of these residuals (by lavaan)\n표본의 크기를 고려해 분산을 조정한 값\nN이 충분히 크다면, z-score로 이해할 수 있음\n2.0 이상이면 유의(significant)하다고 보고 주의; 단, 표본크기에 대한 가설검증의 취약성을 그대로 지님.\n\n\n표본의 크기가 커질수록, 공분산/상관 잔차의 값이 줄어드는 경향이 있음.\n예제: Reisenzein(1986)의 실험\n\n\n\n\n\n\n연구 내용\n\n\n\n\n\n\n지하철에서 한 사람이 쓰러지는 가상의 시나리오에서 동정심과 분노가 도움 행동을 매개하는지를 조사\n절반의 참가자에게는 그 사람이 술에 취했다는 정보 제공, 나머지 절반에게는 병에 걸렸다는 정보 제공\n동정심과 분노는 각각 세 가지 지표로 측정됨\n\n동정심\nx1: 그 사람에 대해 얼마나 동정심(sympathy)을 느끼십니까?\nx2: 나는 이 사람에게 연민(pity)을 느낍니다.\nx3: 이 사람에 대해 얼마나 걱정(concern)이 되십니까?\n분노\nx4: 그 사람에게 얼마나 화가 나시나요?\nx5: 그 사람에게 얼마나 짜증을 느끼시겠습니까?\nx6: 그 사람 때문에 기분이 나빠질 것입니다.\n\n\n\n\nSource: p. 260, Bollen, K. A. (1989). Structural equations with latent variables\n\n\n\nFigure 7.4 Confirmatory Factor Analysis Model for Sympathy (\\(\\xi_1\\)) and Anger (\\(\\xi_2\\)).\nEach Measured with Three Indicators (\\(X_1\\) to \\(X_6\\))\n\nMatrix representation for the CFA model:\n\\(x_1 = \\lambda_{11}\\xi_1 + \\delta_1\\),   \\(x_2 = \\lambda_{21}\\xi_1 + \\delta_2\\),  \\(x_3 = \\lambda_{31}\\xi_1 + \\delta_3\\),\n\\(x_4 = \\lambda_{42}\\xi_2 + \\delta_4\\),  \\(x_5 = \\lambda_{52}\\xi_2 + \\delta_5\\),  \\(x_6 = \\lambda_{62}\\xi_2 + \\delta_6\\)\n\\(\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\lambda_{11} & 0 \\\\\n\\lambda_{21} & 0 \\\\\n\\lambda_{31} & 0 \\\\\n0 & \\lambda_{42} \\\\\n0 & \\lambda_{52} \\\\\n0 & \\lambda_{62}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\xi_1 \\\\ \\xi_2\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\delta_1 \\\\ \\delta_2 \\\\ \\delta_3 \\\\ \\delta_4 \\\\ \\delta_5 \\\\ \\delta_6\n\\end{bmatrix}\\)\n\\(\\mathbf{x} = \\mathbf{\\Lambda}\\mathbf{\\xi} + \\boldsymbol{\\delta}\\)\n가정: \\(COV(\\xi_i, \\delta_j) = 0\\) for all \\(i\\) and \\(j\\)\n\n\n\n\n\n\nThe covariance matrix of exogenous variables:\n\\[\\mathbf{\\Phi} =\n\\begin{bmatrix}\n\\phi_{11} &  \\\\\n\\phi_{21} & \\phi_{22}\n\\end{bmatrix}\n\\]\n\n\nThe covariance matrices of the errors of measurement\n\\[\\mathbf{\\Theta_{\\delta}} =\n\\begin{bmatrix}\nV(\\delta_1) \\\\\n0 & V(\\delta_2) \\\\\n0 & 0 & V(\\delta_3) \\\\\n0 & 0 & 0 & V(\\delta_4) \\\\\n0 & 0 & 0 & 0 & V(\\delta_5) \\\\\n0 & 0 & 0 & 0 & 0 & V(\\delta_6)\n\\end{bmatrix}\n\\]\n\n\n\n\n# fig 7.4\nlower &lt;- \"\n  6.982\n  4.686 6.047\n  4.335 3.307 5.037 \n  -2.294 -1.453 -1.979 5.569\n  -2.209 -1.262 -1.738 3.931 5.328\n  -1.671 -1.401 -1.564 3.915 3.601 4.977\n\"\ncov &lt;- getCov(lower, names = c(\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\"))\n\nmodel_sa &lt;- '\n  sympathy =~ x1 + x2 + x3\n  anger =~ x4 + x5 + x6\n'\nfit_sa &lt;- sem(model_sa, sample.cov = cov, sample.nobs = 138)\n\n\n# coveriance residuals\nresiduals(fit_sa) |&gt; print()\n\n$type\n[1] \"raw\"\n\n$cov\n       x1     x2     x3     x4     x5     x6\nx1  0.000                                   \nx2  0.028  0.000                            \nx3 -0.016 -0.027  0.000                     \nx4 -0.073  0.247 -0.387  0.000              \nx5 -0.170  0.297 -0.277 -0.030  0.000       \nx6  0.334  0.136 -0.126  0.013  0.020  0.000\n\n\n\n\n# correlation residuals\nresiduals(fit_sa, type = \"cor\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       x1     x2     x3     x4     x5     x6\nx1  0.000                                   \nx2  0.004  0.000                            \nx3 -0.003 -0.005  0.000                     \nx4 -0.012  0.043 -0.074  0.000              \nx5 -0.028  0.053 -0.054 -0.006  0.000       \nx6  0.057  0.025 -0.025  0.002  0.004  0.000\n\n\n\n\n# standardized residuals\nresiduals(fit_sa, type = \"standardized\") |&gt; print()\n\n$type\n[1] \"standardized\"\n\n$cov\n       x1     x2     x3     x4     x5     x6\nx1  0.000                                   \nx2  1.353  0.000                            \nx3 -1.035 -0.379  0.000                     \nx4 -0.392  0.871 -1.503  0.000              \nx5 -0.723  0.967 -1.006 -1.203  0.000       \nx6  1.585  0.483 -0.493  0.568  0.566  0.000\n\n\n\nNormalized vs standardized residuals: ratios of covariance residuals over the standard error of the sample covariance, not the standard error of the difference between sample and predicted values.\n\n# normalized residuals\nresiduals(fit_sa, type = \"normalized\") |&gt; print()\n\n$type\n[1] \"normalized\"\n\n$cov\n       x1     x2     x3     x4     x5     x6\nx1  0.000                                   \nx2  0.042  0.000                            \nx3 -0.026 -0.049  0.000                     \nx4 -0.130  0.488 -0.809  0.000              \nx5 -0.311  0.604 -0.601 -0.053  0.000       \nx6  0.644  0.284 -0.285  0.023  0.038  0.000\n\n\n\n\n\nChi-Square( \\(\\chi^2\\)) Test\n공분산의 잔차들이 모집단에서 0과 다른지에 대한 통계적 검증하고자 함.\n\n위에서 standardize한 residuals를 이용하는 것은 대략적인 테스트임.\n소위, simultaneous significance test, 즉 모든 공분산이 0인지를 동시에 검증하는 것이 필요함.\n\n\nChi-Square( \\(\\chi^2\\)) Test\n\n\\(H_0\\): \\(\\Sigma\\) = \\(\\Sigma(\\theta)\\) ; 공분산 행렬과 모수로 표현된 공분산 행렬이 같다. 즉, 연구자의 모형은 데이터와 완전히 일치함!\n\\(H_0\\) 가정하에, sampling error로부터 \\((N-1)*F_{ML}\\) 값들은 점근적으로 \\(\\chi^2(df_M)\\) 분포 따름\n이는 포화모형(완전한 모형)에 비해 연구자의 “제한이 가해진 모형” 간의 (likelihood) 차이가 특정 분포를 따른다는 것을 의미함.\n\nLog-likelihood ratio: \\(\\displaystyle -2log\\left(\\frac{L_0}{L_1}\\right) = (N-1)F_{ML} \\sim \\chi^2(df_M)\\)\n\n\\(L_0\\): the likelihood maximized for the researcher’s model under \\(H_0\\)\n\\(L_1\\): the same quantity for an unrestricted model\n\n\\(df_M\\): the difference in the number of parameters between the two models\n\n\n위의 CFA의 예에서,\n\\(\\begin{align}\n  \\chi^2_{ML} = (N)*F_{ML}(S, \\hat{\\Sigma}(\\theta)) & = N*\\left(log|\\Sigma(\\theta)| + tr(\\Sigma^{-1}(\\theta)S) - log|S| - (p + q)\\right) \\\\\n  & = 138 * 0.0698 = 9.632 \\\\\n\\end{align}\\)\n이 값은 다음과 같이 해석할 수 있음.\n\\(\\begin{align}\n  -2log\\left(\\frac{L_0}{L_1}\\right) & = -2(logL_0 - logL_1) \\\\\n  & = -2*[-1654.635 - (-1649.817)] = 9.635 \\\\\n\\end{align}\\)\nDegree of freedom = \\(\\displaystyle\\frac{6*7}{2} - 13 = 21 - 13 = 8\\)\n\n\n\n\n\n\nChi-Square 분포 특성\n\n\n\n\n자유도(df)가 커질수록 분포의 모양이 정규분포에 가까워짐\n평균 = df, 분산 = 2df\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\chi^2\\) 분포로부터 \\(p = 0.292\\), Bootstrap 샘플 중 29.8%\n\n\\(H_0\\): \\(\\Sigma\\) = \\(\\Sigma(\\theta)\\)이 참이라면, 상정한 모형의 \\(\\chi^2\\) 값 9.6보다 더 큰 \\(\\chi^2\\)값이 나올 확률이 대략 30%임.\n이는 완전한 모형(포화모형)과 (likelihood 관점에서) 크게 다르지 않다는 것을 의미함.\n기존의 관행처럼 \\(p\\) &lt; 0.05 일 때, 모형이 완전한 적합도을 보이지 못한다고 판단할 수 있음.\n즉, 공분산의 잔차 중 일부가 모집단에서 0이 아닐 수 있음을 의미함.\n\n\n# Fitting function의 최소값\nfitmeasures(fit_sa, \"fmin\") |&gt; print(nd = 4)\n\n  fmin \n0.0349 \n\n\n\n# 설정한 모형의 log-likelihood & unrestricted 모형의 log-likelihood, chi-square 값\nfitMeasures(fit_sa, c(\"chisq\", \"pvalue\", \"logl\", \"unrestricted.logl\")) |&gt; print()\n\n            chisq            pvalue              logl unrestricted.logl \n            9.635             0.292         -1657.646         -1652.828 \n\n\n\nsummary(fit_sa, fit.measures = T, estimates = F) |&gt; print()\n\nlavaan 0.6-19 ended normally after 37 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                           138\n\nModel Test User Model:\n                                                      \n  Test statistic                                 9.635\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.292\n\nModel Test Baseline Model:\n\n  Test statistic                               473.058\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.996\n  Tucker-Lewis Index (TLI)                       0.993\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1657.646\n  Loglikelihood unrestricted model (H1)      -1652.828\n                                                      \n  Akaike (AIC)                                3341.291\n  Bayesian (BIC)                              3379.345\n  Sample-size adjusted Bayesian (SABIC)       3338.218\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.038\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.111\n  P-value H_0: RMSEA &lt;= 0.050                    0.526\n  P-value H_0: RMSEA &gt;= 0.080                    0.217\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.030\n\n\n\n\n\n\n\n\n\\(\\chi^2\\) Test에서 주의할 점\n\n\n\n\n변수들이 multivariate normal에서 벗어날수록 (특히 kurtosis) bias가 발생할 수 있음\n표본의 크기가 충분히 커야 함; 최소 100개 이상\n\n파라미터의 수에 비례해서 더 많은 표본이 필요함; 가령 N &lt; 200, p &gt; 30 이면, \\(\\chi^2\\) 값이 부풀려짐.\n\n변수 간의 상관관계가 높을수록 틀린 모형에 대한 \\(\\chi^2\\) 값이 커짐\n측정에 대한 신뢰도(reliability)가 낮을수록 통계적 검증력(power)를 낮춰 가설을 기각하기 어려움.\n영가설이 “완벽한 모형”(\\(\\Sigma = \\Sigma(\\theta)\\))이라는 점에서 현실적인 가설이 아님\n기존의 가설 검증의 문제점을 그대로 지니고 있음; statistical significance vs. substantive significance\n\nN이 큰 경우, \\(\\chi^2\\)값이 커져 가설을 쉽게 기각할 수 있음; 실제로는 \\(\\Sigma\\)와 \\(\\Sigma(\\theta)\\)의 차이가 사소함에도 불구하고, 완벽한 모형은 아니라고 판단할 수 있음.\n반대로, N이 작은 경우, \\(\\chi^2\\)값이 작아져 가설을 기각하기 어려움; 실제로는 \\(\\Sigma\\)와 \\(\\Sigma(\\theta)\\)의 차이가 크더라도 완벽한 모형으로 판단할 수 있음.\n\n\n\n\n모든 적합도 지수는 \\(\\mathbf{S}\\)와 \\(\\hat{\\Sigma}(\\theta)\\)의 함수임.\n특히, fitting function \\(F_{ML}\\)은 \\(\\mathbf{S}\\)와 \\(\\hat{\\Sigma}(\\theta)\\)의 차이를 측정해주는 스칼러 함수임.\n\n\\(F_{ML} \\to 0\\) as \\(N \\to \\infty\\)\n추정할 파라미터를 늘리면 \\(F_{ML}\\)는 감소함; 즉, 항상 더 나쁜 모형이 됨.\n\n\\(F_{ML}\\) 값 자체는 해석하기 어렵기 때문에, \\(F_{ML}\\)을 이용한 \\(\\mathbf{S}\\)와 \\(\\hat{\\Sigma}(\\theta)\\)의 차이를 측정해주는 다른 지표들이 유용함.\n\n\n\n\n\n\nModel fit indices\n\n\n\n\\(\\chi^2\\) 테스트가 가설 검증의 논리로 yes/no를 판단하는 것인 반면,\n적합도 지표들은 모형이 데이터에 얼마나 잘 부합하는지를 연속적인 값으로 표현함.\n과거 많은 지표들이 제안되어 왔으나, 시뮬레이션 연구등을 통해 현재까지 살아남아 널리 사용되고 보고되는 지표들은\nRMSEA, CFI/TLI, SRMR\n\n\n\n\n\n\n\n\n일화\n\n\n\n\nSörbom(2001)이 Karl Jöreskog와 함께 한 1985년 워크숍에 대한 일화는 방금 설명한 근사적 적합도 지수에 대한 맥락을 제공합니다.\n우리는 방금 GFI와 AGFI를 프로그램에 추가했습니다. Karl은 강의에서 카이 제곱만 있으면 충분하다고 말했습니다. 그러자 한 참가자가 “그럼 왜 GFI를 추가했나요?”라고 물었습니다. 그러자 Karl은 “음, 사용자들이 LISREL이 항상 그렇게 큰 카이 제곱을 생성하면 사용을 중단하겠다고 위협합니다. 그래서 사람들을 행복하게 할 무언가를 발명해야 했습니다. GFI가 그 목적에 부합합니다. (p. 10)\nby Google Translate\n\n\nSource: p. 164, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n\n\n\n\n\nIncremental Fit Index\nBase 모형과 비교하여 (추가된 파라미터로 인해) 연구자의 모형이 얼마나 개선되었는지를 측정함.\nBase 모형은 기본적으로 가장 적합도가 낮은 모형, 즉 모든 변수들이 서로 독립이라고 가정한 모형으로 선택: baseline model, indepdendent(null) model\n이는 \\(R^2\\)와 유사한 접근임.\n\\(\\displaystyle\\Delta_1 = \\frac{\\chi^2_b - \\chi^2_m}{\\chi^2_b - df_m}\\)\n\nN의 크기에 덜 영향을 받도록 조정했음.\n연구자의 모형이 맞다면, 분자의 값들의 평균을 분모의 값으로 이해할 수 있음. (\\(E(\\chi^2_m) = df_m\\))\n\nTucker–Lewis Index (TLI): Tucker & Lewis (1973), Bentler & Bonnett (1980)\nTLI = \\(\\displaystyle\\frac{\\chi^2_b/df_b - \\chi^2_m/df_m}{\\chi^2_b/df_b - 1}\\)\n\n동일한 \\(F_{ML}\\)을 가질 때, 더 복잡한 모형(파라미터의 개수가 더 많은)이 더 낮은 \\(df_m\\)를 가지게 되어 더 높은 값을 갖도록 함.\n소위 parsimony-adjusted, 혹은 complexity에 대한 penalty를 부여함.\n동시에 N에 따른 표본분포의 평균이 변하지 않도록 조정함.\n\n\n\nRMSEA\nSteiger–Lind Root Mean Square Error of Approximation (Steiger, 1990)\n\\(\\displaystyle RMSEA = \\sqrt{\\frac{\\hat{\\delta}}{df_m(N-1)}} = \\sqrt{\\frac{\\chi^2_{ML} - df_m}{df_m(N-1)}} = \\sqrt{\\frac{F_{ML}}{df_m} - \\frac{1}{N-1}}\\)\n\n연구자의 모형이 맞다면, \\(\\chi^2_{ML}\\) 값의 표본 분포가 \\(\\chi^2(df_m)\\)를 따르는데 반해,\n연구자의 모형이 맞지 않다면, non-central \\(\\chi^2(df_m, ~\\delta)\\) 분포를 따름\n\n\\(\\delta\\): non-centrality parameter\n평균은 \\(df_m + \\delta\\)임.\n\n\\(\\delta\\): 완전한 모형에서 벗어난 정도라도 볼 수 있고, \\(\\chi^2_{ML} - df_m\\)로 추정할 수 있음.\n\n\\(\\delta = 0\\)이면, 연구자의 모형이 완벽하고 central \\(\\chi^2(df_m)\\) 분포를 따름.\n음수가 되지 않도록 \\(max(\\chi^2_{ML} - df_m, 0)\\)으로 계산함.\n\n\nRMSEA에 대한 confidence interval:\n\\(\\displaystyle RMSEA_{lower} = \\sqrt{max\\left(0, \\frac{\\hat{\\delta}_{.05}}{(N-1)df_m}\\right)}\\),    \\(\\displaystyle RMSEA_{upper} = \\sqrt{\\frac{\\hat{\\delta}_{.95}}{(N-1)df_m}}\\)\nRMSEA &lt;= 0.05에 대한 가설검증: 0.05보다 작을 때, 모형이 데이터에 잘 부합한다고 보고 가설검증의 기준으로 사용함.\n\n\n\n\n\n\nRMSEA의 해석\n\n\n\nBrowne과 Cudeck(1993)은 \\(\\hat\\epsilon\\) ≤ .05가 유리한 결과라고 제안했지만, Chen et al.(2008)의 이후 컴퓨터 시뮬레이션 연구 결과는 \\(\\hat\\epsilon\\)이 단독으로 사용되든 90% 신뢰 구간과 함께 사용되든 .05 또는 다른 값의 보편적 임계값을 거의 뒷받침하지 않는다는 것을 나타냈습니다.\nBrowne과 Cudeck(1993)은 또한 \\(\\hat\\epsilon\\) ≥ .10이 모델 적합도가 낮음을 나타낼 수 있지만 보장은 없다고 제안했습니다.\nBrowne and Cudeck (1993) suggested that \\(\\hat\\epsilon\\) ≤ .05 is a favorable result, but results of later computer simulation studies by Chen et al. (2008) indicated little support for a universal threshold of .05—or any other value—regardless of whether \\(\\hat\\epsilon\\) is used alone or jointly with its 90% confidence interval.\nBrowne and Cudeck (1993) also suggested that \\(\\hat\\epsilon\\) ≥ .10 may indicate poor model fit, but there is no guarantee.\n\nSource: p. 167, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n\n\n\n예를 들어, 위의 CFA 예에서, 표본의 크기 N=200으로 생성된 데이터로부터 얻은 결과로 보면,\n\\(\\chi^2_{ML}(8) = 13.963\\)을 얻고, \\(\\hat\\delta = 13.963 - 8 = 5.963\\)임.\n따라서, \\(\\chi^2_{ML}\\)값의 표본 분포는 \\(\\chi^2(8, ~5.963)\\)를 따름.\n\nset.seed(123)\ndf &lt;- semTools::kd(cov, n = 200, type = \"exact\") |&gt; as_tibble()\nmod_sa &lt;- \"\n  sympathy =~ x1 + x2 + x3\n  anger =~ x4 + x5 + x6\n\"\nfit_sa &lt;- sem(mod_sa, data = df)\nfitMeasures(fit_sa, c(\"chisq\", \"df\")) |&gt; print()\n\n chisq     df \n13.963  8.000 \n\n\n\n\n\n\n\n\n\n\n\n\nfitMeasures(fit_sa, c(\"rmsea\", \"rmsea.ci.lower\", \"rmsea.ci.upper\", \"rmsea.pvalue\")) |&gt; print()\n\n         rmsea rmsea.ci.lower rmsea.ci.upper   rmsea.pvalue \n         0.061          0.000          0.113          0.317 \n\n\n\n\nCFI\nComparative Fit Index\n\\(\\displaystyle CFI = \\frac{\\hat\\delta_b - \\hat\\delta_m}{\\hat\\delta_b}\\)\n\n\\(\\hat\\delta_b = max(0, \\chi^2_{b} - df_m)\\),   \\(\\hat\\delta_m = max(0, \\chi^2_{ML} - df_m)\\)\n\nBase 모형에 비해 연구자 모형의 non-centrality parameter가 얼마나 줄었는지 그 비율을 측정함.\nTLI와 상관이 매우 높아, 주로 두 지표 중 하나만 보고함.\n\n\n\n\n\n\n\nCFI의 해석\n\n\n\nHu와 Bentler(1995)는 CFI ≥ .95가 유리한 결과라고 제안했는데, 이 기준은 Hu와 Bentler(1999)와 같은 일부 후기 몬테카를로 연구의 결과와 일반적으로 일치합니다. 하지만 훨씬 더 최근의 시뮬레이션 연구의 결과는 모델의 변화와 데이터의 비정규성 정도에 대한 CFI에 대한 특정 임계값의 일반성을 뒷받침하지 못했습니다(Fan & Sivo, 2005; Yuan, 2005). Brosseau-Liard와 Savalei(2014)가 설명한 비정규성에 대한 CFI의 robust 형태는 lavaan이 robust ML 방법에 따라 인쇄합니다.\nHu and Bentler (1995) suggested that CFI ≥ .95 is a favorable result, a benchmark generally consistent with results from some later Monte Carlo studies, such as Hu and Bentler (1999). But results from even more recent simulation studies failed to support the generality of any specific cutoff for the CFI over variations in models and degrees of nonnormality in the data (Fan & Sivo, 2005; Yuan, 2005). Robust forms of the CFI for nonnormal described by Brosseau-Liard and Savalei (2014) are printed by lavaan for robust ML methods.\n\nSource: p. 169, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n\n\n\n\n\nSRMR\nStandardized Root Mean Square Residual\n\\(\\displaystyle SRMR = \\sqrt{Ave\\left[\\left(\\frac{\\sigma_{ij} - \\hat\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}\\right)^2\\right]}\\)\n\n상관계수의 잔차의 제곱의 평균에 대한 제곱근에 근사로 볼 수 있음.\n대략적으로 상관계수의 차이의 평균으로 해석할 수 있음.\n프로그램마다 조금 다르게 계산될 수 있음.\nSRMR &lt; .08 일반적으로 유리한 것으로 간주됨(Hu & Bentler, 1999)\n하지만, 잔차 상관행렬의 각 성분을 직접 살펴보는 것이 더 좋음; 절대값이 0.1 이상이면 주의\n\n\n\n\n\n\n\nThresholds for Fit Indices\n\n\n\nKeith’s 교재 14장 ADVICE: MEASURES OF FIT 참고.\n다음은 Kline’s(2023)에서 발췌\n연속 근사 적합 지수에 대한 자연스러운 질문은 “허용 가능한” 또는 “좋은” 모델 적합도를 나타내는 값 범위와 관련이 있습니다. 안타깝게도 이 질문에 대한 간단한 답은 없습니다. 근사 적합 지수 값과 사양 오류의 심각성 또는 유형 사이에 카이 제곱 검정과 마찬가지로 직접적인 대응 관계가 없기 때문입니다. 또한 SEM에 대한 문헌이나 웹 페이지에서 볼 수 있는 RMSEA, CFI, SRMR 등의 지수에 대한 많은 해석 지침 또는 경험 규칙은 신뢰할 수 없습니다. 즉, 다양한 분야에서 분석된 광범위한 모델과 데이터에서 모델이 데이터에 “좋은” 적합도와 “나쁜” 적합도를 실제로 구별한다는 증거가 거의 없습니다.\nA natural question about continuous approximate fit indexes involves the range of values indicating “acceptable” or even “good” model fit. Well, unfortunately, there is no simple answer to this question because there is no direct correspondence between values of approximate fit indexes and the seriousness or types of specification error, just as for the chi-square test. Also, many interpretive guidelines or rules of thumb for the RMSEA, CFI, and SRMR, among other indexes, seen in the literature or web pages about SEM are untrustworthy; that is, there is little evidence that they actually differentiate between model “good” versus “poor” fit to the data across the wide range of models and data analyzed in different disciplines.\n\nSource: p. 170, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n\n\n\n\n\n\n\n\n\nDynamic (flexible) thresholds\n\n\n\nFlexible Cutoffs: Package FCO\nTailored cutoff values are generated for the RMSEA, CFI, TLI, and SRMR depending on the CFA model, the degree of nonnormality, and the amount of accepted uncertainty.\nDynamic Fit Index Cutoffs For Latent Variable Models\nSimulates fit index cutoffs for latent variable models that are tailored to the user’s model statement, model type, and sample size.\n\nSource: p. 171, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n\n\n\nKeith’s 교재 14장 ADVICE: MEASURES OF FIT 참고\n흔히 적용하는 적합도 지수의 임계값들:",
    "crumbs": [
      "SEM Equations"
    ]
  },
  {
    "objectID": "contents/equations.html#적합도에-평가에-대한-가이드라인-by-klein2023",
    "href": "contents/equations.html#적합도에-평가에-대한-가이드라인-by-klein2023",
    "title": "Structural Equations",
    "section": "적합도에 평가에 대한 가이드라인 by Klein(2023)",
    "text": "적합도에 평가에 대한 가이드라인 by Klein(2023)\n\nSource: p. 172, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n\n번역 by Google Translate\n다음에 설명된 방법은 SEM(Appelbaum et al., 2018)에 대한 보고 표준과 일치하며 연구자에게 과거 경험에서 사실이었던 것보다 더 많은 정보를 보고할 것을 요구합니다.\n\n동시 추정 방법을 사용하는 경우 자유도와 p 값이 있는 모델 카이 제곱을 보고합니다. 결과 변수가 이분형이고 경로 계수가 이진 로지스틱 또는 프로빗 회귀 방법을 사용하여 추정되는 경우와 같이 모델 카이 제곱을 사용할 수 없는 일부 분석이 있습니다. 예를 들어 Muthén 및 Muthén(1998–2017, 3장)을 참조하세요. 그러나 이는 예외적인 경우입니다.\n모델이 정확한 적합성 검정(exact-fit test)에 실패하면 (a) 직접 그렇게 말하고 표본 크기에 관계없이 (b) 모델을 잠정적으로 거부합니다. 다음으로 (c) 부적합의 크기와 가능한 원인을 모두 진단합니다(로컬 적합성 검사). 그 이유는 통계적으로 유의하지만 미미한 모델-데이터 불일치를 감지하여 실패를 설명하는 것입니다. 이는 대규모 샘플에서 발생할 가능성이 가장 높습니다. 모델을 거부하기로 한 초기 결정은 철회될 수 있지만, 관찰된 모델-데이터 불일치가 실제로 중요하지 않은 이유에 대한 설명과 함께 로컬 적합성 증거에 근거해야 합니다.\n모델이 정확한 적합성 검정(exact-fit test)을 통과하더라도 여전히 지역적 적합성을 검사해야 합니다. 그 이유는 통계적으로 유의하지 않지만 모델에 의심을 품기에 충분히 큰 모델-데이터 불일치를 감지하는 것입니다. 이는 소규모 샘플에서 발생할 가능성이 가장 높습니다. 지역적 적합성에 대한 증거가 상당한 불일치를 나타내는 경우 카이제곱 검정을 통과했더라도 모델을 거부해야 합니다.\n상관 관계, 표준화 또는 정규화된 잔차와 같은 잔차 행렬을 원고 본문에 보고합니다. 모델이 너무 커서 다루기 힘들다면 (a) 보충 자료에 이러한 표를 제공하고 (b) 논문에 더 큰 잔차의 위치와 부호와 같은 잔차 패턴을 설명합니다. 모델이 어떻게 잘못 지정되었는지 이해하는 데 진단적 가치가 있을 수 있는 패턴을 찾습니다. 잔차에 대한 정보가 없는 결과 보고서는 불충분합니다. 불행히도 이 분야에서 불완전한 보고는 예외가 아니라 표준입니다. 예를 들어, 조직 관리 분야에서 발표된 144개의 SEM 연구를 검토한 결과, 이러한 작업의 약 17%에서만 잔차가 언급되었습니다(Zhang et al., 2021).\n근사적 적합 지수 값을 보고하는 경우 이 장에서 앞서 설명한 최소 집합(RMSEA, CFI, SRMR)에 대한 값을 포함합니다. 그러나 이러한 전역적 적합 통계에 대해 고정 또는 동적 임계값에만 의존하여 모델을 유지하는 것을 정당화하려고 하지 마십시오. 이는 특히 모델이 정확한 적합 테스트(exact-fit test)에 실패하고 잔차 패턴이 사소하지 않은 모형의 설정 오류(specification error)를 시사하는 경우에 해당합니다.\n초기 모델을 다시 지정하는 경우 그렇게 하는 근거를 설명하세요. 또한 잔차와 같은 진단 통계가 재지정에서 어떤 역할을 했는지 설명해야 합니다. 즉, 모델에 대한 양적 결과, 관련 이론, 원래 모델의 수정 사항 간의 연결을 지적하세요(3장). 정확한 적합도 검정(exact-fit test)에 여전히 실패한 다시 지정된 모델을 유지하는 경우 모델-데이터 불일치가 진정으로 미미하다는 것을 보여주세요. 그렇지 않으면 모델에 대한 상당한 공분산 증거가 없다는 것을 보여주지 못한 것입니다.\n모델 적합도에 대한 통계적 증거는 중요하지만 모델을 유지할지 여부를 결정하는 유일한 요소는 아닙니다. 예를 들어, 연구 문제를 감안할 때 파라미터 추정치는 타당해야 합니다. 동일한 인과 과정으로 생성된 향후 재현연구들에서 데이터 세트에 적합할 가능성이 합리적으로 높은 모델은 특정 데이터 세트에 잘 맞는 모델보다 선호되어야 합니다. 이는 잠재적으로 임의의 모든 데이터에 적합할 수 있는 복잡하고 과도하게 많은 파라미터가 포함된 모델의 경우에 특히 그렇습니다. 이러한 모델은 (a) 보다 간결한 모델보다 반증 가능성이 낮고 (b) 샘플과 설정의 변화에 ​​대해 일반화할 가능성이 낮습니다(Preacheret al., 2013).\n모델이 유지되는 경우 연구자는 각각 동일한 데이터를 정확히 또는 거의 잘 설명하는 동등하거나 거의 동등한 버전보다 해당 모델이 선호되어야 하는 이유를 설명해야 합니다. 이 단계는 통계적 단계보다 훨씬 논리적이며, 심각한 경쟁 모델을 구별하기 위해 향후 연구에서 수행할 수 있는 작업을 설명하는 것도 포함됩니다. 동등하거나 거의 동등한 모델에 대한 완전한 보고는 드뭅니다. 따라서 신중한 독자는 이런 방식으로 자신의 SEM 분석을 두드러지게 만들 수 있습니다.\n어떤 모델도 유지되지 않는 경우 분석에서 테스트한 이론에 대한 의미를 설명하는 데 학자로서의 기술이 필요합니다.\n\n결국 모델을 유지했는지 여부에 관계없이 진정한 영예는 최선을 다해 철저한 평가 프로세스를 논리적인 끝까지 따르는 데서 나옵니다. 시인 랄프 왈도 에머슨은 이렇게 말했습니다. 잘한 일에 대한 보상은 그것을 해냈다는 것입니다(Mikis, 2012, p. 294).\n\n\n\n\n\n\n원문\n\n\n\n\n\nThe method outlined next is consistent with reporting standards for SEM (Appelbaum et al., 2018) and also calls on researchers to report more information about model fit than has been true in past experience:\n\nIf you use a simultaneous estimation method, report the model chi-square with its degrees of freedom and p value. There are some analyses, such as when outcome variables are dichotomous and path coefficients are estimated using methods for binary logistic or probit regression, where a model chi-square may be unavailable—see Muthén and Muthén (1998–2017, chap. 3) for examples—but these are exceptional cases.\nIf the model fails the exact-fit test, then (a) directly say so and, regardless of sample size, (b) tentatively reject the model. Next, (c) diagnose both the magnitude and possible sources of misfit (inspect local fit). The rationale is to detect statistically significant but slight model–data discrepancies that explain the failure. This is most likely to happen in a large sample. The initial decision to reject the model could be rescinded, but only based on local fit evidence along with explanation about why observed model–data discrepancies are actually inconsequential.\nIf the model passes the exact-fit test, you still have to inspect local fit. The rationale is to detect model–data discrepancies that are not statistically significant but still great enough to cast doubt on the model. This most likely occurs in a small sample. If evidence about local fit indicates appreciable discrepancies, then the model should be rejected even though it passed the chisquare test.\nReport a matrix of residuals, such as correlation, standardized, or normalized residuals, in the body of the manuscript. If the model is so large that doing so would be unwieldy, then (a) provide such tables in the supplemental materials and (b) describe in the manuscript the pattern of residuals, such as the locations of larger residuals and their signs. Look for patterns that may be of diagnostic value in understanding how the model may be misspecified. Any report of the results without information about the residuals is deficient. Unfortunately, incomplete reporting in this area is the norm rather than the exception. For example, in our review of 144 published SEM studies in the area of organizational management, residuals were mentioned in only about 17% of these works (Zhang et al., 2021).\nIf you report values of approximate fit indexes, then include those for the minimal set described earlier in this chapter. But do not try to justify retaining the model by depending solely on thresholds, either fixed or dynamic, for such global fit statistics. This is especially true if the model failed the exact-fit test and the pattern of residuals suggests a specification error that is not trivial.\nIf you respecify the initial model, explain the rationale for doing so. You should also explain the role that diagnostic statistics, such as residuals, played in the respecification. That is, point out the connections between the numerical results for the model, relevant theory, and modifications to the original model (Chapter 3). If you retain a respecified model that still fails the exact-fit test, then demonstrate that model–data discrepancies are truly slight; otherwise, you have neglected to show that there is no appreciable covariance evidence against the model.\nStatistical evidence about model fit is important, but it is not the sole factor in deciding whether to retain a model. For example, the parameter estimates should make sense, given the research problem. Models with reasonable prospects for fitting data sets in future replications generated by the same causal processes should be preferred over models that fit a particular data set well. This is especially true for complex, overparameterized models that could potentially fit just about any arbitrary data. Such models are (a) less falsifiable than more parsimonious models and (b) less likely to generalize over variations in samples and settings (Preacheret al., 2013).\nIf a model is retained, then the researcher should explain why that model should be preferred over equivalent or near-equivalent versions that, respectively explain the same data exactly as well or nearly as well. This step is much more logical than statistical, and it also involves describing what might be done in future research to differentiate between any serious competing models. Complete reporting about equivalent or near-equivalent models is rare; thus, conscientious readers can really distinguish their own SEM analyses by addressing this issue. The generation and assessment of equivalent versions of structural models are covered in the next chapter.\nIf no model is retained, then your skills as a scholar are needed to explain the implications for the theory tested in your analysis.\n\nAt the end of the day, regardless of whether or not you have a retained a model, the real honor comes from following, to the best of your ability, a thorough evaluation process to its logical end. The poet Ralph Waldo Emerson put it this way: The reward of a thing well done is to have done it (Mikis, 2012, p. 294).",
    "crumbs": [
      "SEM Equations"
    ]
  },
  {
    "objectID": "contents/equations.html#local-fit-measures",
    "href": "contents/equations.html#local-fit-measures",
    "title": "Structural Equations",
    "section": "Local Fit Measures",
    "text": "Local Fit Measures\nParameter estimation\n\n개별 파라미터의 추정값이 기대 혹은 이론과 얼마나 부합하는지 평가\n부적절한 추정값은 모형의 부적합을 나타낼 수 있음; 모집단에서 불가능한 추정값\n\n추정된 분산이 음수일 때; Heywood case\n추정된 상관계수가 1보다 클 때\n\n실제로 경계값(boundary)일 가능성이 있다면 수용할 수 있음\n그렇지 않다면,\n\n모형이 심하게 잘못 지정되었을 가능성이 있음\n이상치(outlier)가 존재할 수 있음\n매우 이상한 표본을 사용했을 수 있음\n\n\n\\(R^2\\): 잔차의 분산(\\(1-R^2\\))을 통해 설명력을 평가할 필요가 있음",
    "crumbs": [
      "SEM Equations"
    ]
  },
  {
    "objectID": "contents/equations.html#respecification",
    "href": "contents/equations.html#respecification",
    "title": "Structural Equations",
    "section": "Respecification",
    "text": "Respecification\n모형이 데이터에 부합하지 않는다고 판단될 때, 모형의 specification을 조정하여 모형을 개선할 수 있음.\n\n탐색적 분석의 성격이 강하므로, 가이드/조언 정도로 접근할 필요가 있음.\n\n이론적 정당화가 반드시 필요함.\n이상적으로, 새로운 표본을 이용하여 재검증하는 것이 바람직함.\n\n다음과 같이 기계적인 절차를 통한 적합도 개선은 피해야 함.\n\n파라미터를 추가로 추정(free parameter)하는 절차를 통해 점차 적합도를 계속 높일 수 있음\n적합도가 높아져 데이터에 더 잘 적합(fit)되지만, 그것이 더 진실된 모형이 되는 것은 아님.\n\n모형의 복잡도가 증가하는 것에 대한 패널티가 적용되는 적합도 지수도 있음.\n\n반대로, 파라미터의 제약을 늘려 적합도가 심각히 낮아지지 않는 선까지 모형을 단순화할 수 있음.\n\n\n\n\n\n\n\nEmprical vs. Theoretical\n\n\n\n\n가령, 아래 그림에서 b를 free parameter로 추정한 후 유의하지 않다고 통계적으로 파악되었기 때문에 경로 b에 제약을 가해 오른쪽 모형을 만드는 것은 올바르지 못함; empirical/statistical-based\n\n모형에 대한 설정은 이론적인 근거를 바탕으로 해야 함; theory/substantive-based\n\n \n\n\n\n\n예제: CFA of perceived air quality: an environmental study of the relation between objective and subjective air quality\n\nx1: overall air quality\nx2: the clarity\nx3: color\nx4: odor\n\n\nSource: p. 297, Bollen, K. A. (1989). Structural Equations with Latent Variables.\n\n\n\n\n\n\nlower &lt;- \"\n  0.331\n  0.431 1.160\n  0.406 0.847 0.898\n  0.216 0.272 0.312 0.268\n\"\ncov &lt;- getCov(lower, names = c(\"overall\", \"clarity\", \"color\", \"odor\"))\n\nmodel_air &lt;- '\n  air_quality =~ overall + clarity + color + odor\n'\nfit_air &lt;- sem(model_air, sample.cov = cov, sample.nobs = 74)\nsummary(fit_air, fit.measures = T, standardized = \"std.all\") |&gt; print()\n\nlavaan 0.6-19 ended normally after 23 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         8\n\n  Number of observations                            74\n\nModel Test User Model:\n                                                      \n  Test statistic                                21.194\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               211.967\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.907\n  Tucker-Lewis Index (TLI)                       0.720\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -234.488\n  Loglikelihood unrestricted model (H1)       -223.891\n                                                      \n  Akaike (AIC)                                 484.975\n  Bayesian (BIC)                               503.408\n  Sample-size adjusted Bayesian (SABIC)        478.197\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.360\n  90 Percent confidence interval - lower         0.232\n  90 Percent confidence interval - upper         0.506\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.063\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n  air_quality =~                                               \n    overall           1.000                               0.817\n    clarity           1.977    0.223    8.865    0.000    0.863\n    color             1.894    0.195    9.717    0.000    0.939\n    odor              0.754    0.117    6.445    0.000    0.685\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n   .overall           0.109    0.022    5.023    0.000    0.332\n   .clarity           0.293    0.066    4.427    0.000    0.256\n   .color             0.104    0.043    2.396    0.017    0.118\n   .odor              0.140    0.025    5.654    0.000    0.531\n    air_quality       0.218    0.052    4.186    0.000    1.000\n\n\n\nColor의 측정오차와 clarity의 측정오차가 상관관계가 있을 수 있음.\n\n“설문지를 실시한 사람은 일부 응답자가 색상과 선명도 질문의 차이를 구별하는 데 어려움을 겪었다고 밝혔습니다. 이는 맑은 공기는 색상이 없을 것으로 예상하고, 반대로 공기의 색상이 나쁘면 선명도 평가도 나쁠 것이기 때문에 타당해 보였습니다. 한 지표에 대한 응답 오류가 부분적으로 구분할 수 없는 다른 지표의 오류와 상관관계가 있을 수 있기 때문에 선명도 측정 오류가 색상 측정 오류와 상관관계가 있었을 수 있습니다.”\n\n\nmodel_air_modi &lt;- '\n  air_quality =~ overall + clarity + color + odor\n  color ~~ clarity\n'\nfit_air_modi &lt;- sem(model_air_modi, sample.cov = cov, sample.nobs = 74)\nsummary(fit_air_modi, fit.measures = T, standardized = \"std.all\") |&gt; print()\n\nlavaan 0.6-19 ended normally after 26 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                            74\n\nModel Test User Model:\n                                                      \n  Test statistic                                 5.581\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.018\n\nModel Test Baseline Model:\n\n  Test statistic                               211.967\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.978\n  Tucker-Lewis Index (TLI)                       0.867\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -226.681\n  Loglikelihood unrestricted model (H1)       -223.891\n                                                      \n  Akaike (AIC)                                 471.363\n  Bayesian (BIC)                               492.099\n  Sample-size adjusted Bayesian (SABIC)        463.737\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.249\n  90 Percent confidence interval - lower         0.082\n  90 Percent confidence interval - upper         0.466\n  P-value H_0: RMSEA &lt;= 0.050                    0.029\n  P-value H_0: RMSEA &gt;= 0.080                    0.952\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.024\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n  air_quality =~                                               \n    overall           1.000                               0.940\n    clarity           1.438    0.208    6.914    0.000    0.722\n    color             1.396    0.175    7.959    0.000    0.797\n    odor              0.738    0.096    7.679    0.000    0.771\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n .clarity ~~                                                   \n   .color             0.256    0.076    3.364    0.001    0.608\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n   .overall           0.038    0.025    1.493    0.135    0.115\n   .clarity           0.547    0.107    5.116    0.000    0.478\n   .color             0.323    0.072    4.501    0.000    0.365\n   .odor              0.107    0.022    4.852    0.000    0.405\n    air_quality       0.289    0.059    4.923    0.000    1.000\n\n\n\n\nModification indices\n\n파라미터가 추가로 추정할 때 \\(\\chi^2\\) 값이 얼마나 감소하는지를 추정; Lagrangian multiplier test\n한번에 하나씩 변화시켜야 함; 하나를 수정하면, 다른 것들이 영향을 받을 수 있음\n\n\nmodificationIndices(fit_air, sort = T) |&gt; print()\n\n       lhs op     rhs     mi    epc sepc.lv sepc.all sepc.nox\n13 clarity ~~   color 16.282  0.342   0.342    1.959    1.959\n12 overall ~~    odor 16.282  0.069   0.069    0.558    0.558\n14 clarity ~~    odor  9.906 -0.097  -0.097   -0.479   -0.479\n11 overall ~~   color  9.906 -0.123  -0.123   -1.159   -1.159\n10 overall ~~ clarity  0.254 -0.019  -0.019   -0.108   -0.108\n15   color ~~    odor  0.254 -0.014  -0.014   -0.115   -0.115\n\n\n\nfitMeasures(fit_air, c(\"chisq\", \"df\")) |&gt; print()\nfitMeasures(fit_air_modi, c(\"chisq\", \"df\")) |&gt; print()\n\n chisq     df \n21.194  2.000 \nchisq    df \n5.581 1.000 \n\n\n\n# Chi-Squared Difference Test\nanova(fit_air, fit_air_modi) |&gt; print()\n\n# 또는 \nlavTestLRT(fit_air, fit_air_modi) |&gt; print()\n\n\nChi-Squared Difference Test\n\n             Df    AIC    BIC   Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nfit_air_modi  1 471.36 492.10  5.5814                                          \nfit_air       2 484.98 503.41 21.1940     15.613 0.44437       1  7.774e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nChi-Squared Difference Test\n\n             Df    AIC    BIC   Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nfit_air_modi  1 471.36 492.10  5.5814                                          \nfit_air       2 484.98 503.41 21.1940     15.613 0.44437       1  7.774e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n공분산 잔차을 통해 수정한다면?\n\n잔차가 높은 변수 쌍에 문제가 있다는 것을 일부 암시해주지만, 반드시 문제가 있다고 볼 수 없음.\nimplied covariance는 여러 파라미터들의 함수이므로 복잡하게 작용됨.\n\n\nresiduals(fit_air, type = \"cor\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n        overll clarty  color   odor\noverall  0.000                     \nclarity -0.009  0.000              \ncolor   -0.023  0.019  0.000       \nodor     0.166 -0.103 -0.007  0.000\n\n\n\n\nresiduals(fit_air, type = \"normalized\") |&gt; print()\n\n$type\n[1] \"normalized\"\n\n$cov\n        overll clarty  color   odor\noverall  0.000                     \nclarity -0.065  0.000              \ncolor   -0.157  0.129  0.000       \nodor     1.153 -0.797 -0.054  0.000\n\n\n\n\nresiduals(fit_air, type = \"standardized\") |&gt; print()\n\n$type\n[1] \"standardized\"\n\n$cov\n        overll clarty  color   odor\noverall  0.000                     \nclarity -0.532  0.000              \ncolor   -3.700  3.200  0.000       \nodor     3.200 -3.700 -0.532  0.000\n\n\n\n\n\n\n\n\n\nTips for Inspecting the Residuals\n\n\n\nSource: p. 173, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n잔차에 대해 보고하는 것이 중요하지만, 카이제곱 검정의 결과와 근사 적합 지수 값과 마찬가지로 잔차 크기와 모델의 잘못된 지정(specification) 유형 또는 정도 사이에는 신뢰할 수 있는 연관성이 없다는 점을 알아야 합니다.\n예를 들어 상대적으로 작은 상관 잔차로 나타나는 지정(specification) 오류의 정도는 경미할 수도 있지만 심각할 수도 있습니다. 한 가지 이유는 다음 장에서 정의되는 수정 지수(modification index)를 포함한 잔차(residuals) 및 기타 진단 통계 값 자체가 잘못된 지정의 영향을 받기 때문입니다. 두 번째 이유는 모델의 한 부분의 잘못된 지정으로 인해 모델의 다른 부분의 추정이 왜곡되어 전역 추정(global estimation)으로 오류가 전파되기 때문입니다. 세 번째는 잔차가 동일하지만 인과 효과의 모순된 패턴을 갖는 등가(equivalent) 모델입니다. 그러나 우리는 일반적으로 모델의 어느 부분이 잘못된지 미리 알 수 없으므로 잔차가 우리에게 말하는 것을 정확히 이해하기 어려울 수 있습니다.\n잔차 패턴을 검사하는 것이 때로는 도움이 될 수 있습니다. \\(r_{XY}\\) &gt; 0인 한 쌍의 변수 X와 Y가 간접적인 인과 경로로만 연결되어 있다고 가정합니다. 해당 쌍의 잔차는 양수입니다. 즉, 모델이 해당 쌍의 연관성을 과소 예측한다는 의미입니다. 이 경우 X와 Y 사이에 직접적인 인과관계가 없다는 가설은 의심스러울 수 있으며, 따라서 이들 사이에 직접적인 인과관계를 추가하는 재지정(re-specification)이 고려해 볼 수 있습니다. 또 다른 가능성은 두 변수가 모두 내생변수인 경우 교란(disturbance) 상관 관계를 지정하는 것입니다. 그러나 모델에 추가할 효과 유형(인과 대 비인과 관계) 또는 방향성(예: X가 Y를 유발하고 Y가 X를 유발함)은 잔차가 우리에게 말해 줄 수 있는 것이 아닙니다. 전역 적합(global fit) 통계에 마법이 없는 것처럼 진단 통계에도 마법이 없습니다. 적어도 연구자들이 재지정(respecificaiton)에 대해 매우 신중하게 생각해야 하는 부담을 덜어줄 수 있는 것은 없습니다.\n번역 by Google Translate",
    "crumbs": [
      "SEM Equations"
    ]
  },
  {
    "objectID": "contents/equations.html#non-nested-models",
    "href": "contents/equations.html#non-nested-models",
    "title": "Structural Equations",
    "section": "Non-nested Models",
    "text": "Non-nested Models\nSource: pp. 190-194, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n동일한 데이터, 동일한 변수로 구성된 두 개의 서로 다른 모형이 내포되어 있지 않은 경우\n\n서로 다른 이론에 기반한 두 개 이상의 모형을 비교하는 경우가 종종 발생\n모델 적합도와 모델 복잡성을 모두 반영하여 이 두 속성이 서로 균형 이루도록 함; 복잡성에 대한 페널티가 부과\n비슷한 적합도를 가진 모형 중에서는 더 간단한 모형을 선호함; 모집단에서의 일반화 가능성이 높음!\n현재까지 매우 신뢰할 만한 지표는 없음\n정보이론에 기반하거나 bootstrapping을 통한 방법이 제안됨\n\n\nAIC\nAkaike Information Criterion\n\n3가지 버전이 있음; 프로그램마다 다름\n\n두 모형에 대한 차이를 보는 것이 목적이므로, 어떤 버전도 상관 없음\n표본의 수가 커질 수록, \\(L_0\\)가 감소하는데, 따라서 복잡성에 대한 페널티의 상대적 중요성이 감소함.\n\n연구자 모형의 log-likelihood를 \\(L_0\\), free parameter의 수를 \\(q\\)라 하면,\n\\(AIC = -2\\log L_0 + 2q\\) \\(\\begin{align}\nAIC_2 & = \\chi^2_{ML} + 2q \\\\~\n  & = -2\\log L_0 + 2\\log L_1 + 2q, ~~ L_1: likelihood ~for ~the ~perfect ~model\\\\\n\\end{align}\\)\n\\(AIC_3 = \\chi^2_{ML} - 2df_M\\)\n\n\nBIC\nBayesian Information Criterion\n\n모형의 복잡성에 대한 페널티가 AIC에 비해 (표본이 커질 수록) 더 큼\n표본의 수가 커질 수록 \\(L_0\\)가 감소하는 것을 상쇄시키면서 복잡성에 대한 페널티를 더 강하게 부과함.\nAIC와 BIC는 전혀 다른 이론적 기반을 가지고 있음\n\n\\(BIC = -2\\log L_0 + q\\log N\\)\n\\(BIC_2 = \\chi^2_{ML} + q\\log N\\)\n\nAIC는 BIC보다 복잡한 모델을 선호하는 경향이 있음.\n모델이 너무 복잡해지는 과적합(overfitting)이 우려되는 경우, BIC가 더 적합할 수 있음.\n모델이 너무 단순해지는 과소적합(underfitting)이 더 큰 문제라면, AIC가 유리할 수 있음.\n탐색적 요인 분석에서 잠재 변수를 정확하게 추정하는 것이 목표일 때:\n\n요인을 너무 많이 추정하는 것이 더 큰 오류로 간주된다면, BIC가 AIC보다 나은 선택일 수 있음.\n요인을 너무 적게 추정하는 것이 더 심각한 오류라면, AIC가 더 유리할 수 있음.\n\nLin et al. (2017)의 매개경로 모델 시뮬레이션 결과에 따르면:\n\n잘못 지정된 파라미터가 적을 때, AIC가 더 정확했으며, BIC는 지나치게 간결한 모델을 선택하는 경향이 있었음.\n잘못 지정된 파라미터가 많을 때, BIC가 더 정확해졌고, AIC는 지나치게 복잡한 모델을 선택함.",
    "crumbs": [
      "SEM Equations"
    ]
  },
  {
    "objectID": "contents/interaction.html",
    "href": "contents/interaction.html",
    "title": "Interactions",
    "section": "",
    "text": "Load libraries\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\n이전 수업 링크",
    "crumbs": [
      "Keith's",
      "Path Modeling",
      "Interactions"
    ]
  },
  {
    "objectID": "contents/interaction.html#continuous-vs.-continuous",
    "href": "contents/interaction.html#continuous-vs.-continuous",
    "title": "Interactions",
    "section": "Continuous vs. Continuous",
    "text": "Continuous vs. Continuous\n나이가 듦(age)에 따른 지구력(endurance)의 감소가 운동을 한 기간(exercise)에 따라 변화하는가? (p.275)\nendurance: the number of minutes of sustained jogging on a treadmill\nexercise: the number of years of vigorous physical exercise\n\n\n연구자의 관심변수에 따라 다르게 표현될 수 있음.\n나이가 지구력에 미치는 부정적 영향을 운동이 완화시키는지 관심; 보효요인 (protective factor) &lt;-&gt; 위험요인 (risk factor)\n이 때, 운동 기간을 moderator (조절변수)라고 말하고, 그 moderating effect (조절효과)를 가지는지 검증.\n통계적으로는 나이과 운동기간이 서로 상호작용(interact)하여 지구력에 영향을 미치는 것으로 나타남.\n\nData: c07e01dt\n\nacad2 &lt;- read_csv('data/c07e01dt.csv')\nacad2 |&gt; print()\n\n# A tibble: 245 x 3\n     age exercise endurance\n   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1    60       10        18\n 2    40        9        36\n 3    29        2        51\n 4    47       10        18\n 5    48        9        23\n 6    42        6        30\n 7    55        8         8\n 8    43       19        40\n 9    39        9        28\n10    51       14        15\n# i 235 more rows\n\n\n\nmod_interact &lt;- lm(endurance ~ age * exercise, data = acad2)  \n# 동일: endurance ~ age + exercise + age:exercise\n\ncar::S(mod_interact)\n\nCall: lm(formula = endurance ~ age * exercise, data = acad2)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  53.17896    7.52661   7.065 1.71e-11 ***\nage          -0.76596    0.15980  -4.793 2.87e-06 ***\nexercise     -1.35095    0.66626  -2.028 0.043694 *  \nage:exercise  0.04724    0.01359   3.476 0.000604 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard deviation: 9.7 on 241 degrees of freedom\nMultiple R-squared: 0.2061\nF-statistic: 20.86 on 3 and 241 DF,  p-value: 4.764e-12 \n    AIC     BIC \n1814.57 1832.07 \n\n\nlavvan에서는 상호작용 항(interaction term)이나 범주형 변수에 대해 처리해주지 않음\n\n직접 상호작용 항을 만들어 주어야 함.\n범주형 변수의 경우는 dummy variable까지 만든 후 상호작용 항까지 만들어 주어야 함.\n\nfastDummies::dummy_cols() 함수를 사용하면 쉽게 만들 수 있음.\n\n\n\nacad2 &lt;- acad2 |&gt; \n  mutate(age_c = jtools::center(age),\n         exercise_c = jtools::center(exercise),\n         age_x_exercise_c = age_c * exercise_c)\n\nacad2 |&gt; print()\n\n# A tibble: 245 x 6\n     age exercise endurance  age_c exercise_c age_x_exercise_c\n   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;            &lt;dbl&gt;\n 1    60       10        18  10.8      -0.673            -7.28\n 2    40        9        36  -9.18     -1.67             15.4 \n 3    29        2        51 -20.2      -8.67            175.  \n 4    47       10        18  -2.18     -0.673             1.47\n 5    48        9        23  -1.18     -1.67              1.98\n 6    42        6        30  -7.18     -4.67             33.6 \n 7    55        8         8   5.82     -2.67            -15.5 \n 8    43       19        40  -6.18      8.33            -51.5 \n 9    39        9        28 -10.2      -1.67             17.0 \n10    51       14        15   1.82      3.33              6.04\n# i 235 more rows\n\n\n\n# lavaan model\nmodel &lt;- '\n  endurance ~ age_c + exercise_c + age_x_exercise_c\n'\n\nfit &lt;- sem(model, data = acad2)\nsummary(fit, standardized = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         4\n\n  Number of observations                           245\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  endurance ~                                                           \n    age_c            -0.262    0.064   -4.119    0.000   -0.262   -0.244\n    exercise_c        0.973    0.135    7.183    0.000    0.973    0.429\n    age_x_exercs_c    0.047    0.013    3.504    0.000    0.047    0.201\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .endurance        92.546    8.362   11.068    0.000   92.546    0.794",
    "crumbs": [
      "Keith's",
      "Path Modeling",
      "Interactions"
    ]
  },
  {
    "objectID": "contents/interaction.html#moderated-mediation",
    "href": "contents/interaction.html#moderated-mediation",
    "title": "Interactions",
    "section": "Moderated Mediation",
    "text": "Moderated Mediation\np. 424, Introduction to Mediation, Moderation, and Conditional Process Analysis (3e) by Andrew F. Hayes\n\n\n\n\n\n\n연구 설명\n\n\n\n\n\n11.3 예시: 업무 팀에게 자신의 감정 숨기기 수년에 걸쳐 대중음악은 우리의 직관을 강화해왔고, 친한 친구들이 제공한 조언이나 친한 친구들이 제공하고 토크쇼 심리학자들은 감정을 억누르고 다른 사람의 눈에 띄지 않도록 숨겨서는 좋은 결과를 얻을 수 없다고 강조합니다. 당대의 아티스트들은 “자신을 표현하라”(마돈나)고, “침묵의 강령”(빌리 조엘)에 따라 살면 결코 과거를 잊지 못할 것이며 “절대 말하지 않을 것들”(에이브릴 라빈)의 목록이 길수록 인생에서 원하는 것을 얻을 가능성이 줄어든다는 것을 조심하라고 말합니다. 따라서 다른 사람이 “나랑 얘기 좀 하자”(아니타 베이커)는 요청을 할 때는 경계심을 풀고 마음속에 있는 이야기를 “소통”(B-52)하는 것이 중요합니다. 하지만 적어도 일부 업무 관련 상황에서는 반드시 그렇지는 않다고 M. S. Cole 외(2008)의 팀워크에 관한 연구에 따르면 말합니다. 이 연구자들에 따르면, 때로는 함께 일하는 다른 사람들이 자신을 괴롭히는 행동이나 말에 대해 자신의 감정을 숨기는 것이 더 나을 수 있으며, 그러한 감정이 팀의 관심의 초점이 되어 팀이 적시에 효율적인 방식으로 작업을 수행하는 데 방해가 되지 않도록 하는 것이 더 나을 수 있습니다. 이 연구는 조건부 프로세스 모델의 추정과 해석의 메커니즘을 설명하는 첫 번째 사례의 데이터를 TEAMS라는 데이터 파일로 제공하며, www.afhayes.com 에서 확인할 수 있습니다. 이 연구는 자동차 부품 제조 회사에 고용된 60개의 작업 팀을 대상으로 진행되었으며, 회사 직원 200여 명을 대상으로 작업 팀에 대한 일련의 질문과 팀 감독자에 대한 다양한 인식에 대한 설문조사에 대한 응답을 기반으로 합니다. 이 연구의 일부 변수는 그룹 수준에서 측정된 것으로 같은 팀원들이 말한 내용을 종합하여 도출한 것입니다. 다행히 팀원들이 팀에 대한 질문에 응답하는 방식이 매우 유사하여 이러한 종류의 집계를 정당화할 수 있었습니다. 다른 변수는 순전히 팀 상사의 보고를 기반으로 합니다.\n이 분석과 관련된 네 가지 변수를 측정했습니다. 팀원들이 다른 팀원들의 업무를 약화시키거나 변화와 혁신을 방해하는 행동을 얼마나 자주 했는지 등 팀원들의 역기능적 행동에 대한 일련의 질문(데이터 파일에서 DYSFUNC, 점수가 높을수록 팀 내 역기능적 행동이 많음을 나타냄)을 던져 팀원들의 역기능적 행동을 측정했습니다. 또한 팀원들에게 직장에서 ‘화가 났다’, ‘역겨웠다’ 등을 얼마나 자주 느끼는지 물어봄으로써 그룹의 부정적인 정서적 분위기를 측정했습니다(NEGTONE, 점수가 높을수록 업무 환경의 부정적인 정서적 분위기를 더 많이 반영함). 팀 상사에게는 팀이 얼마나 효율적이고 적시에 일을 처리하는지, 팀이 생산 목표를 달성하는지 등 전반적인 팀 성과에 대한 평가를 제공하도록 요청했습니다(데이터의 성과, 점수가 높을수록 성과가 좋음을 반영하는 척도). 또한 슈퍼바이저는 팀원들이 자신의 감정에 대해 보내는 비언어적 신호를 얼마나 쉽게 읽을 수 있는지를 측정하는 일련의 질문, 즉 비언어적 부정적 표현력(데이터 파일의 NEGEXP, 점수가 높을수록 팀원들이 부정적인 감정 상태를 비언어적으로 더 잘 표현한다는 의미)에 응답했습니다. 이 연구의 목표는 업무 팀원의 역기능적 행동이 업무 팀의 성과에 부정적인 영향을 미칠 수 있는 메커니즘을 조사하는 것이었습니다. 연구진은 역기능적 행동(X)으로 인해 상사와 다른 직원들이 직면하고 관리하려고 하는 부정적인 감정(M)으로 가득 찬 업무 환경이 조성되면 업무에 집중하지 못하고 업무 수행에 방해가 된다는 중재 모델을 제안했습니다(Y). 그러나 이 모델에 따르면 팀원들이 부정적인 감정(W)을 조절할 수 있게 되면, 즉 자신의 감정을 다른 사람에게 숨길 수 있게 되면 업무 환경의 부정적인 분위기와 다른 사람의 감정을 관리하는 데 집중할 필요 없이 당면한 업무에 집중할 수 있게 됩니다. 즉, 이 모델에서는 업무 환경의 부정적인 정서적 어조가 팀 성과에 미치는 영향은 팀원이 자신의 감정을 숨기는 능력에 따라 달라지며, 부정적 감정을 숨기지 않고 표현하는 팀에서 부정적인 정서적 어조가 성과에 미치는 부정적 영향이 더 강하다고 가정합니다.\n\n\n\n\nData: Introduction to Mediation, Moderation, and Conditional Process Analysis; “data files and code”\n\nteams &lt;- read_csv(\"data/teams.csv\")\nteams &lt;- teams |&gt; \n  mutate(\n    negtone_c = jtools::center(negtone),\n    negexp_c = jtools::center(negexp),\n    negtone_negexp_c = negtone_c*negexp_c,\n  )\nteams |&gt; print()\n\n# A tibble: 60 x 7\n   dysfunc negtone negexp perform negtone_c negexp_c negtone_negexp_c\n     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;\n 1   -0.23   -0.51  -0.49    0.12    -0.557   -0.482           0.268 \n 2   -0.13    0.22  -0.49    0.52     0.173   -0.482          -0.0832\n 3    0      -0.08   0.84   -0.08    -0.127    0.848          -0.108 \n 4   -0.33   -0.11   0.84   -0.08    -0.157    0.848          -0.133 \n 5    0.39   -0.48   0.17    0.12    -0.527    0.178          -0.0940\n 6    1.02    0.72  -0.82    1.12     0.673   -0.812          -0.546 \n 7   -0.35   -0.18  -0.66   -0.28    -0.227   -0.652           0.148 \n 8   -0.23   -0.13  -0.16    0.32    -0.177   -0.152           0.0269\n 9    0.39    0.52  -0.16   -1.08     0.473   -0.152          -0.0717\n10   -0.08   -0.26  -0.16   -0.28    -0.307   -0.152           0.0466\n# i 50 more rows\n\n\n\n# standard deviation of negative expression \nsd(teams$negexp_c) |&gt; print()\n\n[1] 0.543701\n\n\n\nmod &lt;- \"\n  # models\n    perform ~ c*dysfunc + b1*negtone_c + b2*negexp_c + b3*negtone_negexp_c\n    negtone_c ~ a*dysfunc\n  \n  # conditional effects: m-sd, m, m+sd\n    b_low := b1 + b3*(-0.5437)  # mean - sd; sd = 0.5437\n    b_mean := b1 + b3*0  # mean\n    b_high := b1 + b3*0.5437  # mean + sd\n  \n  # conditional indirect effects\n    ab_low := a * b_low\n    ab_mean := a * b_mean\n    ab_high := a * b_high\n  \n  # index of moderated mediation\n    index_Mod_Med := a*b3\n\"\n\nfit &lt;- lavaan::sem(model = mod, data = teams, se = \"bootstrap\", meanstructure = T)\nsummary(fit, standardized = T) |&gt; print()\n\nlavaan 0.6-18 ended normally after 2 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                            60\n\nModel Test User Model:\n                                                      \n  Test statistic                                 6.999\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.030\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws             1000\n  Number of successful bootstrap draws            1000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  perform ~                                                             \n    dysfunc    (c)    0.366    0.191    1.917    0.055    0.366    0.270\n    negtone_c (b1)   -0.431    0.128   -3.376    0.001   -0.431   -0.451\n    negexp_c  (b2)   -0.044    0.099   -0.442    0.659   -0.044   -0.047\n    ngtn_ngx_ (b3)   -0.517    0.244   -2.121    0.034   -0.517   -0.278\n  negtone_c ~                                                           \n    dysfunc    (a)    0.620    0.221    2.805    0.005    0.620    0.438\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .perform          -0.032    0.060   -0.531    0.596   -0.032   -0.064\n   .negtone_c        -0.021    0.059   -0.367    0.714   -0.021   -0.041\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .perform           0.185    0.033    5.663    0.000    0.185    0.742\n   .negtone_c         0.219    0.050    4.428    0.000    0.219    0.808\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    b_low            -0.150    0.223   -0.675    0.499   -0.150   -0.300\n    b_mean           -0.431    0.128   -3.375    0.001   -0.431   -0.451\n    b_high           -0.713    0.135   -5.271    0.000   -0.713   -0.601\n    ab_low           -0.093    0.153   -0.611    0.541   -0.093   -0.131\n    ab_mean          -0.267    0.118   -2.267    0.023   -0.267   -0.198\n    ab_high          -0.442    0.161   -2.739    0.006   -0.442   -0.264\n    index_Mod_Med    -0.320    0.190   -1.683    0.092   -0.320   -0.122\n\n\n\n\n# 모든 parameter에 대한 결과\nparameterEstimates(fit, level = 0.95, boot.ci.type = \"bca.simple\") |&gt; print()\n\n                lhs op              rhs         label    est    se      z\n1           perform  ~          dysfunc             c  0.366 0.191  1.917\n2           perform  ~        negtone_c            b1 -0.431 0.128 -3.376\n3           perform  ~         negexp_c            b2 -0.044 0.099 -0.442\n4           perform  ~ negtone_negexp_c            b3 -0.517 0.244 -2.121\n5         negtone_c  ~          dysfunc             a  0.620 0.221  2.805\n6           perform ~~          perform                0.185 0.033  5.663\n7         negtone_c ~~        negtone_c                0.219 0.050  4.428\n8           dysfunc ~~          dysfunc                0.136 0.000     NA\n9           dysfunc ~~         negexp_c               -0.001 0.000     NA\n10          dysfunc ~~ negtone_negexp_c               -0.003 0.000     NA\n11         negexp_c ~~         negexp_c                0.291 0.000     NA\n12         negexp_c ~~ negtone_negexp_c                0.046 0.000     NA\n13 negtone_negexp_c ~~ negtone_negexp_c                0.072 0.000     NA\n14          perform ~1                                -0.032 0.060 -0.531\n15        negtone_c ~1                                -0.021 0.059 -0.367\n16          dysfunc ~1                                 0.035 0.000     NA\n17         negexp_c ~1                                 0.000 0.000     NA\n18 negtone_negexp_c ~1                                 0.024 0.000     NA\n19            b_low :=  b1+b3*(-0.5437)         b_low -0.150 0.223 -0.675\n20           b_mean :=          b1+b3*0        b_mean -0.431 0.128 -3.375\n21           b_high :=     b1+b3*0.5437        b_high -0.713 0.135 -5.271\n22           ab_low :=          a*b_low        ab_low -0.093 0.153 -0.611\n23          ab_mean :=         a*b_mean       ab_mean -0.267 0.118 -2.267\n24          ab_high :=         a*b_high       ab_high -0.442 0.161 -2.739\n25    index_Mod_Med :=             a*b3 index_Mod_Med -0.320 0.190 -1.683\n   pvalue ci.lower ci.upper\n1   0.055   -0.035    0.716\n2   0.001   -0.675   -0.163\n3   0.659   -0.234    0.155\n4   0.034   -1.051   -0.087\n5   0.005    0.234    1.085\n6   0.000    0.137    0.262\n7   0.000    0.144    0.345\n8      NA    0.136    0.136\n9      NA   -0.001   -0.001\n10     NA   -0.003   -0.003\n11     NA    0.291    0.291\n12     NA    0.046    0.046\n13     NA    0.072    0.072\n14  0.596   -0.154    0.082\n15  0.714   -0.140    0.099\n16     NA    0.035    0.035\n17     NA    0.000    0.000\n18     NA    0.024    0.024\n19  0.499   -0.510    0.346\n20  0.001   -0.675   -0.163\n21  0.000   -1.098   -0.520\n22  0.541   -0.435    0.182\n23  0.023   -0.585   -0.098\n24  0.006   -0.817   -0.168\n25  0.092   -0.839   -0.058",
    "crumbs": [
      "Keith's",
      "Path Modeling",
      "Interactions"
    ]
  },
  {
    "objectID": "contents/setup.html",
    "href": "contents/setup.html",
    "title": "환경설정",
    "section": "",
    "text": "R 다운로드 및 설치\n\nWindows인 경우 &gt; Download R for Windows &gt; base\nMac인 경우 &gt; Download R for macOS &gt; Apple silicon 또는 Intel Macs 선택\n\nRStudio 다운로드 및 설치\n\n2: Install RStudio",
    "crumbs": [
      "R tutorial",
      "Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#r-및-rstudio-설치",
    "href": "contents/setup.html#r-및-rstudio-설치",
    "title": "환경설정",
    "section": "",
    "text": "R 다운로드 및 설치\n\nWindows인 경우 &gt; Download R for Windows &gt; base\nMac인 경우 &gt; Download R for macOS &gt; Apple silicon 또는 Intel Macs 선택\n\nRStudio 다운로드 및 설치\n\n2: Install RStudio",
    "crumbs": [
      "R tutorial",
      "Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#rstudio-소개",
    "href": "contents/setup.html#rstudio-소개",
    "title": "환경설정",
    "section": "RStudio 소개",
    "text": "RStudio 소개\n4개의 패널로 구성\nProject 단위로 분석\n\n시작시 project을 새로 만들거나 불러와서 실행: filename.Rproj 형태로 저장\nFile &gt; New Project 또는 +박스그림 버튼 &gt; New Directory &gt; New Project\n\nDirectory name, Sub directory\n\n\nWorking directory\n\nproject에서 참조하는 최상위 폴더\n하위폴더 지시: 예) data/file.sav\n\nR script 생성, 저장\nRStudio 닫기, 열기\n\nWorkspace 저장 vs. R script 저장\nWorkspace save/load: .Rdata 형태로 저장\n\nSession\n\nRestart R\n\n\n환경설정: Tools &gt; Global Options\n“Save workspace to .RData on exit” 옵션: 종료시 working space 자동 저장\nCode\n\nsoft-wrap R source files\nUse native pipe operator\n\nAppearance\n\nZoom: 전체 보기 줌\nEdiotr font: Cascadia Mono (Win), Menlo (Mac)\nEditor font size: 글자 크기\nTheme: Tomorrow Night??",
    "crumbs": [
      "R tutorial",
      "Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#패키지의-설치",
    "href": "contents/setup.html#패키지의-설치",
    "title": "환경설정",
    "section": "패키지의 설치",
    "text": "패키지의 설치\n\n# 메뉴를 통한 설치\n\n# 명령어를 통한 설치\ninstall.packages(\"name\")\n\n# 수업에서 필요한 기본 패키지\ninstall.packages(\"tidyverse\") # 패키지들의 패키지\n\n## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.7     ✔ dplyr   1.0.9\n## ✔ tidyr   1.2.0     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1\n\n# 패키지들 간의 함수의 충돌에 대해서... mask\n\n# 추가 패키지\ninstall.packages(c(\"mosaicData\", \"palmerpenguins\")) # c(): combine items\n\n# 패키지 로드: 필요한 패키지는 세션마다 시행해야 함\nlibrary(\"name\")\n    e.g. library(tidyverse)",
    "crumbs": [
      "R tutorial",
      "Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#단축키",
    "href": "contents/setup.html#단축키",
    "title": "환경설정",
    "section": "단축키",
    "text": "단축키\n\n자동완성: tab\n\n현재 라인 실행: Ctrl+Enter (Win)   |   Command+Return (Mac)\nassignment operator (&lt;-) 입력: Alt+- (Win)   |   Option+- (Mac)\npipe operator (%&gt;%) 입력: Ctrl+Shift+M (Win)   |   Shift+Command+M (Mac)\nconsol에서 화살표 키\ncopy, paste\nundo, redo: Ctrl+Z / Ctrl+Shift+Z (Win)   |   Command+Z / Command+Shift+Z (Mac)\nCopy Lines Up/Down: Shift+Alt+Up/Down (Win)   |   Option+Command+Up or Down (Mac)\n\n단축키 변경: Tools &gt;&gt; modify keyboard shortcuts: e.g. pipe operator: Alt+.",
    "crumbs": [
      "R tutorial",
      "Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#도움말",
    "href": "contents/setup.html#도움말",
    "title": "환경설정",
    "section": "도움말",
    "text": "도움말\nhelp() 또는 ?\ne.g. help(factor), ?factor",
    "crumbs": [
      "R tutorial",
      "Setup"
    ]
  },
  {
    "objectID": "contents/tidyverse.html",
    "href": "contents/tidyverse.html",
    "title": "Tidyverse",
    "section": "",
    "text": "함수들: print(), glimpse(), summary(), count()\n() 안에 들어가는 것을 argument라고 부름\n\nlibrary(tidyverse)\n\ncps &lt;- as_tibble(mosaicData::CPS85) # mosaicData package의 CPS85 데이터셋\nprint(cps) # print 생략!\n\n# A tibble: 534 x 11\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1   9      10 W     M     NH       NS    Married    27 Not      43 const   \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales   \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales   \n4  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n5  15      12 W     M     NH       NS    Married    40 Union    58 const   \n6   9      16 W     F     NH       NS    Married    27 Not      49 clerical\n# i 528 more rows\n\n\n\n\n\n\n\n\nprint()\n\n\n\n강의 노트에서 print()를 쓰는 것은 jupyter notebook에서 data frame을 표시하는 방식때문이므로 무시하셔도 됩니다.\n\n\n보통 print()없이 데이터 프레임을 살펴보지만, print()을 이용하면, 표시되는 방식을 조정해서 볼 수 있음.\n\nprint(cps, n = 3) # 처음 3개 행\n\n# A tibble: 534 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector\n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n1   9      10 W     M     NH       NS    Married    27 Not      43 const \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales \n# … with 531 more rows\n\n\n\n\n\n\n\n\ntip: print() 옵션\n\n\n\n\n\nprint(tibble, n = 10, width = Inf) # 10개의 rows와 모든 columns\n기본 셋팅을 변경하려면\noptions(tibble.print_min = 10, tibble.width = Inf)\nColumns/변수들이 많은 경우 화면에서 다음과 같이 축약되어 나오는데, 이를 다 보려면\nprint(nycflights13::flights) # nycflights13 패키지의 flights 데이터\n# # A tibble: 336,776 × 19\n#    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;       &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n# 1  2013     1     1      517         515       2     830     819      11 UA     \n# 2  2013     1     1      533         529       4     850     830      20 UA     \n# 3  2013     1     1      542         540       2     923     850      33 AA     \n# 4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n# 5  2013     1     1      554         600      -6     812     837     -25 DL     \n# 6  2013     1     1      554         558      -4     740     728      12 UA     \n# # … with 336,770 more rows, 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;,\n# #   origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,\n# #   minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, and abbreviated variable names\n# #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\nprint(nycflights13::flights, n = 3, width = Inf) # 가로 열의 개수: Inf (모든 열)\n# # A tibble: 336,776 × 19\n#    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n# 1  2013     1     1      517            515         2      830            819\n# 2  2013     1     1      533            529         4      850            830\n# 3  2013     1     1      542            540         2      923            850\n#   arr_delay carrier flight tailnum origin dest  air_time distance  hour minute\n#       &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n# 1        11 UA        1545 N14228  EWR    IAH        227     1400     5     15\n# 2        20 UA        1714 N24211  LGA    IAH        227     1416     5     29\n# 3        33 AA        1141 N619AA  JFK    MIA        160     1089     5     40\n#   time_hour          \n#   &lt;dttm&gt;             \n# 1 2013-01-01 05:00:00\n# 2 2013-01-01 05:00:00\n# 3 2013-01-01 05:00:00\n# # … with 336,773 more rows\n\n\n\n많은 변수들을 간략히 보는 방법으로는 glimpse()\n\nglimpse(cps)\n\nRows: 534\nColumns: 11\n$ wage     &lt;dbl&gt; 9.00, 5.50, 3.80, 10.50, 15.00, 9.00, 9.57, 15.00, 11.00, 5.0…\n$ educ     &lt;int&gt; 10, 12, 12, 12, 12, 16, 12, 14, 8, 12, 17, 17, 14, 14, 12, 14…\n$ race     &lt;fct&gt; W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, NW, NW, W,…\n$ sex      &lt;fct&gt; M, M, F, F, M, F, F, M, M, F, M, M, M, M, M, M, M, M, M, M, F…\n$ hispanic &lt;fct&gt; NH, NH, NH, NH, NH, NH, NH, NH, NH, NH, Hisp, NH, Hisp, NH, N…\n$ south    &lt;fct&gt; NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, N…\n$ married  &lt;fct&gt; Married, Married, Single, Married, Married, Married, Married,…\n$ exper    &lt;int&gt; 27, 20, 4, 29, 40, 27, 5, 22, 42, 14, 18, 3, 4, 14, 35, 0, 7,…\n$ union    &lt;fct&gt; Not, Not, Not, Not, Union, Not, Union, Not, Not, Not, Not, No…\n$ age      &lt;int&gt; 43, 38, 22, 47, 58, 49, 23, 42, 56, 32, 41, 26, 24, 34, 53, 2…\n$ sector   &lt;fct&gt; const, sales, sales, clerical, const, clerical, service, sale…\n\n\n\n\n\n\n\n\nTip\n\n\n\n엑셀 스프레드시트처럼 보는 방법은\nEnvironment 패널에 보이는 cps 데이터셋 맨 끝에 네모난 마크를 클릭하거나,\nview(cps)\n\n\n변수들에 대한 통계치 요약 summary()\n\nsummary(cps)\n\n      wage             educ       race     sex     hispanic   south   \n Min.   : 1.000   Min.   : 2.00   NW: 67   F:245   Hisp: 27   NS:378  \n 1st Qu.: 5.250   1st Qu.:12.00   W :467   M:289   NH  :507   S :156  \n Median : 7.780   Median :12.00                                       \n Mean   : 9.024   Mean   :13.02                                       \n 3rd Qu.:11.250   3rd Qu.:15.00                                       \n Max.   :44.500   Max.   :18.00                                       \n                                                                      \n    married        exper         union          age             sector   \n Married:350   Min.   : 0.00   Not  :438   Min.   :18.00   prof    :105  \n Single :184   1st Qu.: 8.00   Union: 96   1st Qu.:28.00   clerical: 97  \n               Median :15.00               Median :35.00   service : 83  \n               Mean   :17.82               Mean   :36.83   manuf   : 68  \n               3rd Qu.:26.00               3rd Qu.:44.00   other   : 68  \n               Max.   :55.00               Max.   :64.00   manag   : 55  \n                                                           (Other) : 58  \n\n\n카테고리별 개수를 세주는 count()\nNumber(수)에 대해서도 적용 가능: ex. educ 수준 2, 3, … 18 각각에 대해서\n\ncps |&gt;  # pipe operator: alt + . (option + .)\n    count(sector) |&gt;\n    print() # 생략해도 됨\n\n# A tibble: 8 × 2\n  sector       n\n  &lt;fct&gt;    &lt;int&gt;\n1 clerical    97\n2 const       20\n3 manag       55\n4 manuf       68\n5 other       68\n6 prof       105\n7 sales       38\n8 service     83\n\n\n\ncps |&gt;\n    count(sex, married) |&gt;\n    print()\n\n# A tibble: 4 × 3\n  sex   married     n\n  &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n1 F     Married   162\n2 F     Single     83\n3 M     Married   188\n4 M     Single    101\n\n\n\n\n\n\n\n\nPipe operator\n\n\n\n|&gt; 또는 %&gt;% (’then’의 의미로…)\nx |&gt; f(y) # f(x, y),\nx |&gt; f(y) |&gt; g(z) # g(f(x, y), z)\nsummary(cps) 는 다음과 같음\ncps |&gt;\n    summary()\ncount(cps, sector)는 다음과 같음\ncps |&gt; \n    count(sector)",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/tidyverse.html#inspecting-data",
    "href": "contents/tidyverse.html#inspecting-data",
    "title": "Tidyverse",
    "section": "",
    "text": "함수들: print(), glimpse(), summary(), count()\n() 안에 들어가는 것을 argument라고 부름\n\nlibrary(tidyverse)\n\ncps &lt;- as_tibble(mosaicData::CPS85) # mosaicData package의 CPS85 데이터셋\nprint(cps) # print 생략!\n\n# A tibble: 534 x 11\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1   9      10 W     M     NH       NS    Married    27 Not      43 const   \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales   \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales   \n4  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n5  15      12 W     M     NH       NS    Married    40 Union    58 const   \n6   9      16 W     F     NH       NS    Married    27 Not      49 clerical\n# i 528 more rows\n\n\n\n\n\n\n\n\nprint()\n\n\n\n강의 노트에서 print()를 쓰는 것은 jupyter notebook에서 data frame을 표시하는 방식때문이므로 무시하셔도 됩니다.\n\n\n보통 print()없이 데이터 프레임을 살펴보지만, print()을 이용하면, 표시되는 방식을 조정해서 볼 수 있음.\n\nprint(cps, n = 3) # 처음 3개 행\n\n# A tibble: 534 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector\n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n1   9      10 W     M     NH       NS    Married    27 Not      43 const \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales \n# … with 531 more rows\n\n\n\n\n\n\n\n\ntip: print() 옵션\n\n\n\n\n\nprint(tibble, n = 10, width = Inf) # 10개의 rows와 모든 columns\n기본 셋팅을 변경하려면\noptions(tibble.print_min = 10, tibble.width = Inf)\nColumns/변수들이 많은 경우 화면에서 다음과 같이 축약되어 나오는데, 이를 다 보려면\nprint(nycflights13::flights) # nycflights13 패키지의 flights 데이터\n# # A tibble: 336,776 × 19\n#    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;       &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n# 1  2013     1     1      517         515       2     830     819      11 UA     \n# 2  2013     1     1      533         529       4     850     830      20 UA     \n# 3  2013     1     1      542         540       2     923     850      33 AA     \n# 4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n# 5  2013     1     1      554         600      -6     812     837     -25 DL     \n# 6  2013     1     1      554         558      -4     740     728      12 UA     \n# # … with 336,770 more rows, 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;,\n# #   origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,\n# #   minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, and abbreviated variable names\n# #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\nprint(nycflights13::flights, n = 3, width = Inf) # 가로 열의 개수: Inf (모든 열)\n# # A tibble: 336,776 × 19\n#    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n# 1  2013     1     1      517            515         2      830            819\n# 2  2013     1     1      533            529         4      850            830\n# 3  2013     1     1      542            540         2      923            850\n#   arr_delay carrier flight tailnum origin dest  air_time distance  hour minute\n#       &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n# 1        11 UA        1545 N14228  EWR    IAH        227     1400     5     15\n# 2        20 UA        1714 N24211  LGA    IAH        227     1416     5     29\n# 3        33 AA        1141 N619AA  JFK    MIA        160     1089     5     40\n#   time_hour          \n#   &lt;dttm&gt;             \n# 1 2013-01-01 05:00:00\n# 2 2013-01-01 05:00:00\n# 3 2013-01-01 05:00:00\n# # … with 336,773 more rows\n\n\n\n많은 변수들을 간략히 보는 방법으로는 glimpse()\n\nglimpse(cps)\n\nRows: 534\nColumns: 11\n$ wage     &lt;dbl&gt; 9.00, 5.50, 3.80, 10.50, 15.00, 9.00, 9.57, 15.00, 11.00, 5.0…\n$ educ     &lt;int&gt; 10, 12, 12, 12, 12, 16, 12, 14, 8, 12, 17, 17, 14, 14, 12, 14…\n$ race     &lt;fct&gt; W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, NW, NW, W,…\n$ sex      &lt;fct&gt; M, M, F, F, M, F, F, M, M, F, M, M, M, M, M, M, M, M, M, M, F…\n$ hispanic &lt;fct&gt; NH, NH, NH, NH, NH, NH, NH, NH, NH, NH, Hisp, NH, Hisp, NH, N…\n$ south    &lt;fct&gt; NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, N…\n$ married  &lt;fct&gt; Married, Married, Single, Married, Married, Married, Married,…\n$ exper    &lt;int&gt; 27, 20, 4, 29, 40, 27, 5, 22, 42, 14, 18, 3, 4, 14, 35, 0, 7,…\n$ union    &lt;fct&gt; Not, Not, Not, Not, Union, Not, Union, Not, Not, Not, Not, No…\n$ age      &lt;int&gt; 43, 38, 22, 47, 58, 49, 23, 42, 56, 32, 41, 26, 24, 34, 53, 2…\n$ sector   &lt;fct&gt; const, sales, sales, clerical, const, clerical, service, sale…\n\n\n\n\n\n\n\n\nTip\n\n\n\n엑셀 스프레드시트처럼 보는 방법은\nEnvironment 패널에 보이는 cps 데이터셋 맨 끝에 네모난 마크를 클릭하거나,\nview(cps)\n\n\n변수들에 대한 통계치 요약 summary()\n\nsummary(cps)\n\n      wage             educ       race     sex     hispanic   south   \n Min.   : 1.000   Min.   : 2.00   NW: 67   F:245   Hisp: 27   NS:378  \n 1st Qu.: 5.250   1st Qu.:12.00   W :467   M:289   NH  :507   S :156  \n Median : 7.780   Median :12.00                                       \n Mean   : 9.024   Mean   :13.02                                       \n 3rd Qu.:11.250   3rd Qu.:15.00                                       \n Max.   :44.500   Max.   :18.00                                       \n                                                                      \n    married        exper         union          age             sector   \n Married:350   Min.   : 0.00   Not  :438   Min.   :18.00   prof    :105  \n Single :184   1st Qu.: 8.00   Union: 96   1st Qu.:28.00   clerical: 97  \n               Median :15.00               Median :35.00   service : 83  \n               Mean   :17.82               Mean   :36.83   manuf   : 68  \n               3rd Qu.:26.00               3rd Qu.:44.00   other   : 68  \n               Max.   :55.00               Max.   :64.00   manag   : 55  \n                                                           (Other) : 58  \n\n\n카테고리별 개수를 세주는 count()\nNumber(수)에 대해서도 적용 가능: ex. educ 수준 2, 3, … 18 각각에 대해서\n\ncps |&gt;  # pipe operator: alt + . (option + .)\n    count(sector) |&gt;\n    print() # 생략해도 됨\n\n# A tibble: 8 × 2\n  sector       n\n  &lt;fct&gt;    &lt;int&gt;\n1 clerical    97\n2 const       20\n3 manag       55\n4 manuf       68\n5 other       68\n6 prof       105\n7 sales       38\n8 service     83\n\n\n\ncps |&gt;\n    count(sex, married) |&gt;\n    print()\n\n# A tibble: 4 × 3\n  sex   married     n\n  &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n1 F     Married   162\n2 F     Single     83\n3 M     Married   188\n4 M     Single    101\n\n\n\n\n\n\n\n\nPipe operator\n\n\n\n|&gt; 또는 %&gt;% (’then’의 의미로…)\nx |&gt; f(y) # f(x, y),\nx |&gt; f(y) |&gt; g(z) # g(f(x, y), z)\nsummary(cps) 는 다음과 같음\ncps |&gt;\n    summary()\ncount(cps, sector)는 다음과 같음\ncps |&gt; \n    count(sector)",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/tidyverse.html#rows",
    "href": "contents/tidyverse.html#rows",
    "title": "Tidyverse",
    "section": "Rows",
    "text": "Rows\n행에 적용되는 함수들\nfilter(), arrange(), distinct()\n\nfilter()\n조건에 맞는 행을 선택\n\nConditional operators:\n&gt;, &gt;=, &lt;, &lt;=,\n== (equal to), != (not equal to)\n& (and) | (or)\n! (not)\n%in% (includes)\n\n\n# 임금(wage)가 10이상인 사람들\ncps |&gt;\n    filter(wage &gt;= 10) |&gt;\n    print()\n\n# A tibble: 184 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n2  15      12 W     M     NH       NS    Married    40 Union    58 const   \n3  15      14 W     M     NH       NS    Single     22 Not      42 sales   \n4  11       8 W     M     NH       NS    Married    42 Not      56 manuf   \n5  25.0    17 W     M     Hisp     NS    Married    18 Not      41 prof    \n6  20.4    17 W     M     NH       NS    Single      3 Not      26 prof    \n# … with 178 more rows\n\n\n\n# 임금(wage)가 10이상이고 여성(F)들\ncps |&gt;\n    filter(wage &gt;= 10 & sex == \"F\") |&gt;\n    print()\n\n# A tibble: 62 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n2  11.2    17 NW    F     NH       NS    Married    32 Not      55 clerical\n3  25.0    17 W     F     NH       NS    Single      5 Not      28 prof    \n4  12.6    17 W     F     NH       NS    Married    13 Not      36 manag   \n5  11.7    16 W     F     NH       NS    Single     42 Not      64 clerical\n6  12.5    15 W     F     NH       NS    Married     6 Not      27 clerical\n# … with 56 more rows\n\n\n\n# 간부급(management)과 전문직(professional)에 종사하는 사람들\ncps |&gt;\n    filter(sector == \"manag\" | sector == \"prof\") |&gt;\n    print()\n\n# A tibble: 160 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector\n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n1  25.0    17 W     M     Hisp     NS    Married    18 Not      41 prof  \n2  20.4    17 W     M     NH       NS    Single      3 Not      26 prof  \n3  10      16 W     M     Hisp     NS    Married     7 Union    29 manag \n4  15      16 NW    M     NH       NS    Married    26 Union    48 manag \n5  25.0    17 W     F     NH       NS    Single      5 Not      28 prof  \n6  10      14 W     M     NH       NS    Married    22 Not      42 prof  \n# … with 154 more rows\n\n\n다음과 같이 편리하게 %in%을 이용하여 여러 항목을 포함하는, 즉 |와 ==를 합친 조건문을 생성\n즉, include인지 판별\n\n# A shorter way to select sectors for management or professional\ncps |&gt;\n    filter(sector %in% c(\"manag\", \"prof\")) |&gt;\n    print()\n\n# A tibble: 160 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector\n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n1  25.0    17 W     M     Hisp     NS    Married    18 Not      41 prof  \n2  20.4    17 W     M     NH       NS    Single      3 Not      26 prof  \n3  10      16 W     M     Hisp     NS    Married     7 Union    29 manag \n4  15      16 NW    M     NH       NS    Married    26 Union    48 manag \n5  25.0    17 W     F     NH       NS    Single      5 Not      28 prof  \n6  10      14 W     M     NH       NS    Married    22 Not      42 prof  \n# … with 154 more rows\n\n\n\n\n\n\n\n\nImportant\n\n\n\nfilter()로 얻은 데이터 프레임은 원래 데이터 프레임을 수정하는 것이 아니므로 계속 사용하려면 저장해야 함\n이후 모든 함수들에 대해서도 마찬가지\nprestige &lt;- cps |&gt;\n    filter(sector %in% c(\"manag\", \"prof\"))\n\nprestige\n#    wage  educ race  sex   hispanic south married exper union   age sector\n#   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n# 1  25.0    17 W     M     Hisp     NS    Married    18 Not      41 prof  \n# 2  20.4    17 W     M     NH       NS    Single      3 Not      26 prof  \n# 3  10      16 W     M     Hisp     NS    Married     7 Union    29 manag \n# ...\n\n\n\n\n\n\n\n\nTip\n\n\n\n잦은 실수들\ncps |&gt;\n    filter(sex = \"F\") # \"==\" vs. \"=\"\ncps |&gt;\n    filter(sector == \"manage\" | \"prof\") # | 전후 모두 완결된 조건문 필요\n\n\n\n\narrange()\nColumn의 값을 기준으로 row를 정렬\n\n# 교육정도(educ)와 임금(wage)에 따라 오름차순으로 정렬\ncps |&gt;\n    arrange(educ, wage) |&gt;\n    print(n = 10)\n\n# A tibble: 534 × 11\n    wage  educ race  sex   hispanic south married exper union   age sector \n   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;  \n 1  3.75     2 W     M     Hisp     NS    Single     16 Not      24 service\n 2  7        3 W     M     Hisp     S     Married    55 Not      64 manuf  \n 3  6        4 W     M     NH       NS    Married    54 Not      64 service\n 4 14        5 W     M     NH       S     Married    44 Not      55 const  \n 5  3        6 W     F     Hisp     NS    Married    43 Union    55 manuf  \n 6  4.62     6 NW    F     NH       S     Single     33 Not      45 manuf  \n 7  5.75     6 W     M     NH       S     Married    45 Not      57 manuf  \n 8  3.35     7 W     M     NH       S     Married    43 Not      56 manuf  \n 9  4.5      7 W     M     Hisp     S     Married    14 Not      27 service\n10  6        7 W     F     NH       S     Married    15 Not      28 manuf  \n# … with 524 more rows\n\n\ndesc()을 이용하면 내림차순으로 정렬\n\n# educ을 내림차순으로 정렬\ncps |&gt;\n    arrange(desc(educ)) |&gt;\n    print(n = 10)\n\n# A tibble: 534 × 11\n    wage  educ race  sex   hispanic south married exper union   age sector\n   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n 1 15       18 W     M     NH       NS    Married    12 Not      36 prof  \n 2 14.0     18 W     F     NH       NS    Married    14 Not      38 manag \n 3 13.5     18 W     M     NH       NS    Married    14 Union    38 prof  \n 4 20       18 W     F     NH       NS    Married    19 Not      43 manag \n 5  7       18 W     M     NH       NS    Married    33 Not      57 prof  \n 6 11.2     18 W     M     NH       NS    Married    19 Not      43 prof  \n 7  5.71    18 W     M     NH       NS    Married     3 Not      27 prof  \n 8 18       18 W     M     NH       NS    Married    15 Not      39 prof  \n 9 19       18 W     M     NH       NS    Single     13 Not      37 manag \n10 22.8     18 W     F     NH       NS    Single     37 Not      61 prof  \n# … with 524 more rows\n\n\narrange()와 filter()를 함께 사용하여 좀 더 복잡한 문제를 해결할 수 있음\n\n# 높은 지위의 섹터에서 일하는 사람들 중 임금이 상위에 있는 사람들\ncps |&gt;\n    filter(sector == \"manage\" | sector == \"prof\") |&gt;\n    arrange(desc(wage)) |&gt;\n    print()\n\n# A tibble: 105 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector\n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n1  25.0    17 W     M     Hisp     NS    Married    18 Not      41 prof  \n2  25.0    17 W     F     NH       NS    Single      5 Not      28 prof  \n3  25.0    17 W     M     NH       NS    Married    31 Not      54 prof  \n4  25.0    16 W     F     NH       S     Single      5 Not      27 prof  \n5  23.2    17 NW    F     NH       NS    Married    25 Union    48 prof  \n6  22.8    18 W     F     NH       NS    Single     37 Not      61 prof  \n# … with 99 more rows\n\n\n\n\ndistinct()**\n유티크한 조합들을 리스트\n\ncps |&gt;\n    distinct(sector, sex) |&gt;\n    print()\n\n# A tibble: 15 × 2\n   sector   sex  \n   &lt;fct&gt;    &lt;fct&gt;\n 1 const    M    \n 2 sales    M    \n 3 sales    F    \n 4 clerical F    \n 5 service  F    \n 6 manuf    M    \n 7 prof     M    \n 8 service  M    \n 9 other    M    \n10 clerical M    \n11 manag    M    \n12 prof     F    \n13 manag    F    \n14 manuf    F    \n15 other    F",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/tidyverse.html#columns",
    "href": "contents/tidyverse.html#columns",
    "title": "Tidyverse",
    "section": "Columns",
    "text": "Columns\n열에 적용되는 함수들\nmutate(), select(), rename()\n\nmutate()\nColumns/변수들로부터 값을 계산하여 새로운 변수를 만듦\n\ntips &lt;- as_tibble(reshape::tips) # reshpae 패키지 안에 tips 데이터셋\ntips |&gt; print()\n\n# A tibble: 244 x 7\n  total_bill   tip sex    smoker day   time    size\n       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;  &lt;int&gt;\n1       17.0  1.01 Female No     Sun   Dinner     2\n2       10.3  1.66 Male   No     Sun   Dinner     3\n3       21.0  3.5  Male   No     Sun   Dinner     3\n4       23.7  3.31 Male   No     Sun   Dinner     2\n5       24.6  3.61 Female No     Sun   Dinner     4\n6       25.3  4.71 Male   No     Sun   Dinner     4\n# i 238 more rows\n\n\n\ntips |&gt;\n    mutate(\n        tip_pct = tip / total_bill * 100,\n        tip_pct_per = tip_pct / size\n    ) |&gt;\n    print()\n\n# A tibble: 244 × 9\n  total_bill   tip sex    smoker day   time    size tip_pct tip_pct_per\n       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;  &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1       17.0  1.01 Female No     Sun   Dinner     2    5.94        2.97\n2       10.3  1.66 Male   No     Sun   Dinner     3   16.1         5.35\n3       21.0  3.5  Male   No     Sun   Dinner     3   16.7         5.55\n4       23.7  3.31 Male   No     Sun   Dinner     2   14.0         6.99\n5       24.6  3.61 Female No     Sun   Dinner     4   14.7         3.67\n6       25.3  4.71 Male   No     Sun   Dinner     4   18.6         4.66\n# … with 238 more rows\n\n\n\n\nselect()\nColumns/변수를 선택\n\ntips |&gt;\n    select(total_bill, tip, day, time) |&gt;\n    print()\n\n# A tibble: 244 × 4\n  total_bill   tip day   time  \n       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; \n1       17.0  1.01 Sun   Dinner\n2       10.3  1.66 Sun   Dinner\n3       21.0  3.5  Sun   Dinner\n4       23.7  3.31 Sun   Dinner\n5       24.6  3.61 Sun   Dinner\n6       25.3  4.71 Sun   Dinner\n# … with 238 more rows\n\n\n\n# tip에서 smoker까지, 그리고 size columns 선택\ntips |&gt;\n    select(tip:smoker, size) |&gt;  # select(2:4, 7)처럼 number로 선택가능\n    print()\n\n# A tibble: 244 × 4\n    tip sex    smoker  size\n  &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;int&gt;\n1  1.01 Female No         2\n2  1.66 Male   No         3\n3  3.5  Male   No         3\n4  3.31 Male   No         2\n5  3.61 Female No         4\n6  4.71 Male   No         4\n# … with 238 more rows\n\n\n\n# sex에서 day까지 columns은 제외하고\ntips |&gt;\n    select(!sex:day) |&gt; # !: not\n    print()\n\n# A tibble: 244 × 4\n  total_bill   tip time    size\n       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;int&gt;\n1       17.0  1.01 Dinner     2\n2       10.3  1.66 Dinner     3\n3       21.0  3.5  Dinner     3\n4       23.7  3.31 Dinner     2\n5       24.6  3.61 Dinner     4\n6       25.3  4.71 Dinner     4\n# … with 238 more rows\n\n\n\n# factor 타입의 변수들만 선택: 함수를 이용\ntips |&gt;\n    select(where(is.factor)) |&gt;  # 다른 함수들: is.numeric, is.character\n    print()\n\n# A tibble: 244 × 4\n  sex    smoker day   time  \n  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt; \n1 Female No     Sun   Dinner\n2 Male   No     Sun   Dinner\n3 Male   No     Sun   Dinner\n4 Male   No     Sun   Dinner\n5 Female No     Sun   Dinner\n6 Male   No     Sun   Dinner\n# … with 238 more rows\n\n\n다양한 select()의 선택방법은 ?select로 help참고\n예를 들어, starts_with(\"abc\")는 abc로 시작하는 열의 이름을 가진 열들\n\n\n\n\n\n\nNote\n\n\n\nBase R에서 행과 열의 선택과 비교하면,\ncps[2:5, c(\"wage\", \"married\")] # 2~5행과 wage, married열\n# # A tibble: 4 × 2\n#    wage married\n#   &lt;dbl&gt; &lt;fct&gt;  \n# 1   5.5 Married\n# 2   3.8 Single \n# 3  10.5 Married\n# 4  15   Married\n\ncps |&gt; \n    select(wage, married) |&gt; \n    slice(2:5) # 행을 선택\n\n\n\n\nrelocate()\nColumns의 순서를 변경\n\ntips |&gt; print(n = 2)\n\n# A tibble: 244 x 7\n  total_bill   tip sex    smoker day   time    size\n       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;  &lt;int&gt;\n1       17.0  1.01 Female No     Sun   Dinner     2\n2       10.3  1.66 Male   No     Sun   Dinner     3\n# i 242 more rows\n\n\n\ntips |&gt; \n    relocate(day, time) |&gt;  # day, time을 맨 앞으로 이동\n    print(n = 2)\n\n# A tibble: 244 x 7\n  day   time   total_bill   tip sex    smoker  size\n  &lt;fct&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;int&gt;\n1 Sun   Dinner       17.0  1.01 Female No         2\n2 Sun   Dinner       10.3  1.66 Male   No         3\n# i 242 more rows\n\n\n\ntips |&gt; \n    relocate(sex:time, tip) |&gt;  # sex부터 time까지와 tip을 맨 앞으로 이동\n    print(n = 2)\n\n# A tibble: 244 x 7\n  sex    smoker day   time     tip total_bill  size\n  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1 Female No     Sun   Dinner  1.01       17.0     2\n2 Male   No     Sun   Dinner  1.66       10.3     3\n# i 242 more rows\n\n\n\ntips |&gt; \n    relocate(day:size, .after = tip) |&gt;   # .before: 앞에, .after: 뒤에\n    print(n = 2)\n\n# A tibble: 244 x 7\n  total_bill   tip day   time    size sex    smoker\n       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; \n1       17.0  1.01 Sun   Dinner     2 Female No    \n2       10.3  1.66 Sun   Dinner     3 Male   No    \n# i 242 more rows\n\n\n\n\nrename()\nColumns의 이름을 변경\n\ncps |&gt;\n    rename(education = educ, marital = married) |&gt; # new = old\n    print()\n\n# A tibble: 534 × 11\n   wage education race  sex   hispanic south marital exper union   age sector  \n  &lt;dbl&gt;     &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1   9          10 W     M     NH       NS    Married    27 Not      43 const   \n2   5.5        12 W     M     NH       NS    Married    20 Not      38 sales   \n3   3.8        12 W     F     NH       NS    Single      4 Not      22 sales   \n4  10.5        12 W     F     NH       NS    Married    29 Not      47 clerical\n5  15          12 W     M     NH       NS    Married    40 Union    58 const   \n6   9          16 W     F     NH       NS    Married    27 Not      49 clerical\n# … with 528 more rows\n\n\n변수를 select할 때 동시에 이름도 바꿀 수 있음\n\ncps |&gt;\n    select(education = educ, marital = married) |&gt; # new = old\n    print()\n\n# A tibble: 534 × 2\n  education marital\n      &lt;int&gt; &lt;fct&gt;  \n1        10 Married\n2        12 Married\n3        12 Single \n4        12 Married\n5        12 Married\n6        16 Married\n# … with 528 more rows",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/tidyverse.html#groups",
    "href": "contents/tidyverse.html#groups",
    "title": "Tidyverse",
    "section": "Groups",
    "text": "Groups\n분석에서는 자주 카테고리별로 데이터를 나누어 통계치를 계산하곤 하는데,\ngroup_by()와 summarise()의 두 함수를 함께 사용하여 가장 자주 사용하게 됨\n\ngroup_by()\n데이터셋을 분석을 위해 의미있는 그룹으로 나눔\n다음은 성별로 데이터셋을 나눈 것인데, 실제 데이터를 수정하는 것은 아니고, 내부적으로 grouping되어 있음.\n맨 위 줄에 보면 Groups:  sex [2]로 표시되어 grouped data frame임을 명시함\n\ncps |&gt;\n    group_by(sex) |&gt; \n    print()\n\n# A tibble: 534 × 11\n# Groups:   sex [2]\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1   9      10 W     M     NH       NS    Married    27 Not      43 const   \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales   \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales   \n4  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n5  15      12 W     M     NH       NS    Married    40 Union    58 const   \n6   9      16 W     F     NH       NS    Married    27 Not      49 clerical\n# … with 528 more rows\n\n\n\n\nsummarise()\nsummarize()와 동일\ngroup별로 통계치를 구해 하나의 행으로 산출\n\n# 남녀별로 임금의 평균을 구함\ncps |&gt;\n    group_by(sex) |&gt;\n    summarise(\n        avg_wage = mean(wage, na.rm = TRUE),  # mean(): 평균, na.rm: NA를 remove할 것인가\n        n = n()  # n(): 개수\n    ) |&gt;\n    print()\n\n# A tibble: 2 × 3\n  sex   avg_wage     n\n  &lt;fct&gt;    &lt;dbl&gt; &lt;int&gt;\n1 F         7.88   245\n2 M         9.99   289\n\n\n2개 이상의 변수들로 grouping할 수 있음\n\ncps |&gt;\n    group_by(sex, married) |&gt;\n    summarize(\n        ave_wage = mean(wage),\n        sd_wage = sd(wage)) |&gt;\n    print()\n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 4 × 4\n# Groups:   sex [2]\n  sex   married ave_wage sd_wage\n  &lt;fct&gt; &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n1 F     Married     7.68    3.73\n2 F     Single      8.26    6.23\n3 M     Married    10.9     5.35\n4 M     Single      8.35    4.78\n\n\n이때, 결과 데이터 프레임은 sex로 grouping되어 있음.\ngrouping을 해제하려면 ungroup()이 필요함.\n그렇지 않으면, 저 결과는 sex로 grouped data frame임\n\nUseful summary functions\n자세한 사항은 R for Data Science/Data transformation\n\nMeasures of location: mean(), median()\nMeasures of spread: sd(), IQR(), mad()\nMeasures of rank: min(), max(), quantile(x, 0.25)\nMeasures of position: min_rank(), first(), nth(x, 2), last()\nMeasures of count: count(), n_distinct()",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/tidyverse.html#missing",
    "href": "contents/tidyverse.html#missing",
    "title": "Tidyverse",
    "section": "Missing",
    "text": "Missing\nR에서 missing values (결측치)는 NA로 표시\nNaN (not a number)는 주로 계산 결과로 나오는데, 예들 들어 0으로 나눌 때처럼, R에서는 NA로 취급되니 크게 신경쓰지 않아도 됨. 자세한 사항은 R for Data Science/Missing values 참고\nNA는 다음과 같은 성질을 지님\nNA &gt; 5\n#&gt; [1] NA\n10 == NA\n#&gt; [1] NA\nNA + 10\n#&gt; [1] NA\nNA / 2\n#&gt; [1] NA\nNA == NA\n#&gt; [1] NA\n\nx &lt;- NA\nis.na(x)\n#&gt; [1] TRUE\nNA는 filter()안의 조건문의 참거짓에 상관없이 모두 제외함\n\n실제로 조건문의 결과는 TRUE, FALSE로 이루어지짐\n\ndf &lt;- tibble(\n        one = c(1, NA, 3, 4, 2, NA), \n        two = c(2, 5, 3, NA, 10, NA), \n        three = c(\"a\", \"a\", \"a\", \"a\", \"b\", \"b\")\n    )\ndf\n#     one   two three\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n# 1     1     2 a    \n# 2    NA     5 a    \n# 3     3     3 a    \n# 4     4    NA a    \n# 5     2    10 b    \n# 6    NA    NA b    \n\nfilter(df, one &gt; 1)\n#     one   two three\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n# 1     3     3 a    \n# 2     4    NA a    \n# 3     2    10 b\n\n# NA를 포함하고자 할 때,\nfilter(df, one &gt; 1 | is.na(one))\n#     one   two three\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n# 1    NA     5 a    \n# 2     3     3 a    \n# 3     4    NA a    \n# 4     2    10 b    \n# 5    NA    NA b\n\n# NA를 포함하지 않은 행들만\nfilter(df, !is.na(one))\nfilter(df, !is.na(one) & !is.na(two)) # one, two 열에 모두 NA가 없는 행들만\n\nna.omit(df) # NA가 하나라도 있는 행은 모두 제거, 보통 결측치를 조심스럽게 대체한 후 사용\n#     one   two three\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n# 1     1     2 a    \n# 2     3     3 a    \n# 3     2    10 b \n\n# 함수 중에 NA를 직접 처리하는 경우들이 많음\nmean(df$one)\n## [1] NA\n\nmean(df$one, na.rm = TRUE) # NA removed\n## [1] 2.5\nna.rm = TRUE로 얻은 계산값에서 몇 개의 데이터로 계산되었는지 알기 위해서는\ndf |&gt; \n    group_by(three) |&gt; \n    summarise(\n        ave = mean(two, na.rm = TRUE), \n        n = n(), \n        n_notna = sum(!is.na(two))  # TRUE는 1로, FALSE는 0으로 계산됨\n    )\n#   three   ave     n n_notna\n#   &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;   &lt;int&gt;\n# 1 a      3.33     4       3\n# 2 b     10        2       1",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/tidyverse.html#summary",
    "href": "contents/tidyverse.html#summary",
    "title": "Tidyverse",
    "section": "Summary",
    "text": "Summary\n다음 dplyr 패키지의 기본 verb 함수들로 데이터를 가공하면서 필요한 통계치를 구함\n\n조건에 맞는 행들(관측치)만 필터링: filter()\n열을 재정렬: arrange()\n변수들의 선택: select()\n변수들과 함수들을 이용하여 새로운 변수를 생성: mutate()\n원하는 요약 통계치를 간추림: summarise()",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "contents/fit-index.html",
    "href": "contents/fit-index.html",
    "title": "Fit Indices",
    "section": "",
    "text": "다음에 설명하는 방법은 SEM의 보고 표준(Appelbaum et al., 2018)과 일치하며, 연구자가 과거보다 모델 적합도에 대해 더 많은 정보를 보고하도록 요구합니다:\n\n동시 추정 방법을 사용하는 경우 모델 카이제곱 을 자유도 및 p 값과 함께 보고합니다. 결과 변수가 이 분법적이고 이원 로지스틱 또는 확률 회귀 방법을 사용 하여 경로 공분산을 추정하는 경우와 같이 일부 분석에 서는 모델 카이제곱을 사용할 수 없는 경우도 있지만( 예는 Muthén과 Muthén(1998-2017, 3장) 참조), 이러한 경우는 예외적인 경우에 해당합니다.\n모델이 적합도 테스트에 실패하면 (a) 직접 그렇 게 말하고, 표본 크기에 관계없이 (b) 해당 모델을 10% 확률로 거부합니다. 다음으로, (c) 부적합의 크기와 가 능한 원인을 모두 진단합니다(국부적 적합도 검사). 그 근거는 실패를 설명하는 통계적으로 유의미하지만 약 간의 모델 데이터 불일치를 감지하는 것입니다. 이는 대규모 샘플에서 발생할 가능성이 가장 높습니다. 모델 을 거부하기로 한 초기 결정은 철회할 수 있지만, 관찰 된 모델-데이터 불일치가 실제로 중요하지 않은 이유에 대한 설명과 함께 국소 적합도 증거에 근거해서만 철회할 수 있습니다.\n모델이 정확한 적합도 테스트를 통과한 경우에도 로 컬 적합도를 검사해야 합니다. 그 이유는 통계적으로 유의 미하지는 않지만 모델에 의문을 제기할 만큼 큰 모델 데이 터 불일치를 감지하기 위해서입니다. 이는 작은 샘플에서 발생할 가능성이 높습니다. 지역 적합도에 대한 증거가 상 당한 불일치를 나타내는 경우, 카이제곱 테스트를 통과했 더라도 모델을 거부해야 합니다.\n본문에상관관계,표준화또는정규화된잔차등의 잔차 행렬을 보고합니다.원고를 작성합니다. 모델이 너무 커서 직접 설명하기 어려운 경우에는 (a) 부록 자료에 표를 제공하고 (b) 원고에 큰 잔류물의 위치 및 징후와 같은 잔류물의 패턴을 설명합니다. 모델이 어떻게 잘못 지정될 수 있는지 이해하는 데 진단적 가치가 있을 수 있는 패 턴을 찾습니다. 잔류에 대한 정보가 없는 결과 보고 는 불완전합니다. 안타깝게도 이 영역에서 불완전한 보고는 예외가 아니라 일반적입니다. 예를 들어, 조 직 관리 분야에서 발표된 144개의 SEM 연구를 검토 한 결과, 잔차가 언급된 연구는 약 17%에 불과했습 니다(Zhang et al., 2021).\n대략적인 적합도 지수 값을 보고하는 경우 이장의 앞부분에서 설명한 최소 집합에 대한 값을 포함하세요. 그러나 이러한 글로벌 적합도 통계에 대해 고정 또는 동적 임계값에만 의존하여 모델을 유지하는 것을 정당화하려고 해서는 안 됩니다. 특히 모델이 적합도 테스트에 실패하고 잔차의 패턴이 사소하지 않은 사양 오류를 시사하는 경우 특히 그렇습니다.\n초기 모델을 재특정하는 경우 그 근거를 설명 하세요. 또한 잔차와 같은 진단 통계가 재수정에서 어떤 역할을 했는지 설명해야 합니다. 즉, 모델에 대 한 수치 결과, 관련 이론 및 원래 모델에 대한 수정 사항 간의 연관성을 지적합니다(3장). 재조정된 모델 이 여전히 적합도 테스트에 실패하는 경우, 모델과 데이터의 불일치가 정말 미미하다는 것을 입증하고, 그렇지 않은 경우 모델에 대한 유의미한 공분산 증거 가 없다는 것을 입증하는 데 소홀히 한 것입니다.\n모델 적합도에 대한 통계적 증거는 중요하지만 모델 유지 여부를 결정하는 유일한 요소는 아닙니다. 예를 들어, 연구 문제를 고려할 때 매개변수 추정치 가 합리적이어야 합니다. 특정 데이터 집합에 잘 맞는 모델보다는 동일한 인과 프로세스에 의해 생성되는 향후의 데이터 집합에 잘 맞을 가능성이 합리적인 모델 을 선호해야 합니다. 이는 거의 모든 임의의 데이터에 잠재적으로 적합할 수 있는 복잡하고 과도하게 매개변수가 설정된 모델의 경우 특히 그렇습니다. 이러한 모델은 (1) 더 간결한 모델보다 위조 가능성이 적고 (2) 샘 플과 설정의 변화에 따라 생성될 가능성이 적습니다(Preacher et al., 2013).\n특정 모델을 유지하는 경우, 연구자는 해당 모델 이 동등하거나 거의 동등한 버전보다 선호되어야 하는 이유를 설명해야 합니다. 동일한 데이터를 정확히 또는 거의 비슷하게 설명합니 다. 이 단계는 통계보다 훨씬 더 논리적이며, 향후 연구 에서 심각한 경쟁 모델을 구별하기 위해 무엇을 할 수 있는지 설명하는 것도 포함됩니다. 동등하거나 거의 동 등한 모델에 대한 완전한 보고는 드물기 때문에, 양심 적인 독자는 이 문제를 해결함으로써 자신의 SEM 분 석을 실제로 구별할 수 있습니다. 동등한 버전의 구조 모델 생성 및 평가는 다음 장에서 다룹니다.\n모델이 유지되지 않는 경우 분석에서 테스트한 이론에 대한 의미를 설명하려면 학자로서의 기술이 필 요합니다. 결국, 유지 모델이 있든 없든 간에 진정한 영광은 최 선을 다해 논리적으로 끝까지 철저한 평가 프로세스를 따르는 데서 비롯됩니다. 시인 랄프 왈도 에머슨은 이 를 이렇게 표현했습니다: 잘한 일의 보상은 그것을 해 냈다는 것입니다(Mikis, 2012, 294쪽).\n\n결국, 유지 모델이 있든 없든 간에 진정한 영광은 최 선을 다해 논리적으로 끝까지 철저한 평가 프로세스를 따르는 데서 비롯됩니다. 시인 랄프 왈도 에머슨은 이 를 이렇게 표현했습니다: 잘한 일의 보상은 그것을 해 냈다는 것입니다(Mikis, 2012, 294쪽)."
  },
  {
    "objectID": "contents/fit-index.html#적합성-평가를-위한-권장-접근-방식",
    "href": "contents/fit-index.html#적합성-평가를-위한-권장-접근-방식",
    "title": "Fit Indices",
    "section": "",
    "text": "다음에 설명하는 방법은 SEM의 보고 표준(Appelbaum et al., 2018)과 일치하며, 연구자가 과거보다 모델 적합도에 대해 더 많은 정보를 보고하도록 요구합니다:\n\n동시 추정 방법을 사용하는 경우 모델 카이제곱 을 자유도 및 p 값과 함께 보고합니다. 결과 변수가 이 분법적이고 이원 로지스틱 또는 확률 회귀 방법을 사용 하여 경로 공분산을 추정하는 경우와 같이 일부 분석에 서는 모델 카이제곱을 사용할 수 없는 경우도 있지만( 예는 Muthén과 Muthén(1998-2017, 3장) 참조), 이러한 경우는 예외적인 경우에 해당합니다.\n모델이 적합도 테스트에 실패하면 (a) 직접 그렇 게 말하고, 표본 크기에 관계없이 (b) 해당 모델을 10% 확률로 거부합니다. 다음으로, (c) 부적합의 크기와 가 능한 원인을 모두 진단합니다(국부적 적합도 검사). 그 근거는 실패를 설명하는 통계적으로 유의미하지만 약 간의 모델 데이터 불일치를 감지하는 것입니다. 이는 대규모 샘플에서 발생할 가능성이 가장 높습니다. 모델 을 거부하기로 한 초기 결정은 철회할 수 있지만, 관찰 된 모델-데이터 불일치가 실제로 중요하지 않은 이유에 대한 설명과 함께 국소 적합도 증거에 근거해서만 철회할 수 있습니다.\n모델이 정확한 적합도 테스트를 통과한 경우에도 로 컬 적합도를 검사해야 합니다. 그 이유는 통계적으로 유의 미하지는 않지만 모델에 의문을 제기할 만큼 큰 모델 데이 터 불일치를 감지하기 위해서입니다. 이는 작은 샘플에서 발생할 가능성이 높습니다. 지역 적합도에 대한 증거가 상 당한 불일치를 나타내는 경우, 카이제곱 테스트를 통과했 더라도 모델을 거부해야 합니다.\n본문에상관관계,표준화또는정규화된잔차등의 잔차 행렬을 보고합니다.원고를 작성합니다. 모델이 너무 커서 직접 설명하기 어려운 경우에는 (a) 부록 자료에 표를 제공하고 (b) 원고에 큰 잔류물의 위치 및 징후와 같은 잔류물의 패턴을 설명합니다. 모델이 어떻게 잘못 지정될 수 있는지 이해하는 데 진단적 가치가 있을 수 있는 패 턴을 찾습니다. 잔류에 대한 정보가 없는 결과 보고 는 불완전합니다. 안타깝게도 이 영역에서 불완전한 보고는 예외가 아니라 일반적입니다. 예를 들어, 조 직 관리 분야에서 발표된 144개의 SEM 연구를 검토 한 결과, 잔차가 언급된 연구는 약 17%에 불과했습 니다(Zhang et al., 2021).\n대략적인 적합도 지수 값을 보고하는 경우 이장의 앞부분에서 설명한 최소 집합에 대한 값을 포함하세요. 그러나 이러한 글로벌 적합도 통계에 대해 고정 또는 동적 임계값에만 의존하여 모델을 유지하는 것을 정당화하려고 해서는 안 됩니다. 특히 모델이 적합도 테스트에 실패하고 잔차의 패턴이 사소하지 않은 사양 오류를 시사하는 경우 특히 그렇습니다.\n초기 모델을 재특정하는 경우 그 근거를 설명 하세요. 또한 잔차와 같은 진단 통계가 재수정에서 어떤 역할을 했는지 설명해야 합니다. 즉, 모델에 대 한 수치 결과, 관련 이론 및 원래 모델에 대한 수정 사항 간의 연관성을 지적합니다(3장). 재조정된 모델 이 여전히 적합도 테스트에 실패하는 경우, 모델과 데이터의 불일치가 정말 미미하다는 것을 입증하고, 그렇지 않은 경우 모델에 대한 유의미한 공분산 증거 가 없다는 것을 입증하는 데 소홀히 한 것입니다.\n모델 적합도에 대한 통계적 증거는 중요하지만 모델 유지 여부를 결정하는 유일한 요소는 아닙니다. 예를 들어, 연구 문제를 고려할 때 매개변수 추정치 가 합리적이어야 합니다. 특정 데이터 집합에 잘 맞는 모델보다는 동일한 인과 프로세스에 의해 생성되는 향후의 데이터 집합에 잘 맞을 가능성이 합리적인 모델 을 선호해야 합니다. 이는 거의 모든 임의의 데이터에 잠재적으로 적합할 수 있는 복잡하고 과도하게 매개변수가 설정된 모델의 경우 특히 그렇습니다. 이러한 모델은 (1) 더 간결한 모델보다 위조 가능성이 적고 (2) 샘 플과 설정의 변화에 따라 생성될 가능성이 적습니다(Preacher et al., 2013).\n특정 모델을 유지하는 경우, 연구자는 해당 모델 이 동등하거나 거의 동등한 버전보다 선호되어야 하는 이유를 설명해야 합니다. 동일한 데이터를 정확히 또는 거의 비슷하게 설명합니 다. 이 단계는 통계보다 훨씬 더 논리적이며, 향후 연구 에서 심각한 경쟁 모델을 구별하기 위해 무엇을 할 수 있는지 설명하는 것도 포함됩니다. 동등하거나 거의 동 등한 모델에 대한 완전한 보고는 드물기 때문에, 양심 적인 독자는 이 문제를 해결함으로써 자신의 SEM 분 석을 실제로 구별할 수 있습니다. 동등한 버전의 구조 모델 생성 및 평가는 다음 장에서 다룹니다.\n모델이 유지되지 않는 경우 분석에서 테스트한 이론에 대한 의미를 설명하려면 학자로서의 기술이 필 요합니다. 결국, 유지 모델이 있든 없든 간에 진정한 영광은 최 선을 다해 논리적으로 끝까지 철저한 평가 프로세스를 따르는 데서 비롯됩니다. 시인 랄프 왈도 에머슨은 이 를 이렇게 표현했습니다: 잘한 일의 보상은 그것을 해 냈다는 것입니다(Mikis, 2012, 294쪽).\n\n결국, 유지 모델이 있든 없든 간에 진정한 영광은 최 선을 다해 논리적으로 끝까지 철저한 평가 프로세스를 따르는 데서 비롯됩니다. 시인 랄프 왈도 에머슨은 이 를 이렇게 표현했습니다: 잘한 일의 보상은 그것을 해 냈다는 것입니다(Mikis, 2012, 294쪽)."
  },
  {
    "objectID": "contents/fit-index.html#잔여물-검사를-위한-팁",
    "href": "contents/fit-index.html#잔여물-검사를-위한-팁",
    "title": "Fit Indices",
    "section": "잔여물 검사를 위한 팁",
    "text": "잔여물 검사를 위한 팁\n잔차에 대해 보고하는 것이 중요하지만, 카이제곱 검정 결과 및 근사 적합도 지수 값과 마찬가지로 잔차의 크 기와 모델 오정의 유형 또는 양 사이에는 신뢰할 수 있 거나 신뢰할 수 있는 연관성이 없다는 것을 알아야 합 니다. 예를 들어, 상대적으로 작은 상관관계 잔차로 표 시되는 구체화 오류의 정도는 경미할 수도 있지만 심각 할 수도 있습니다. 한 가지 이유는 다음 장에서 정의하 는 수정 지수를 비롯한 잔차 및 기타 진단 통계의 값 자 체가 오특정에 의해 영향을 받기 때문입니다. 의학에 비유하자면, 특정 질병에 대한 진단 검사가 해당 질병 을 앓고 있는 환자에게는 정확도가 떨어질 수 있습니다 . 두 번째 이유는 모델 한 부분의 잘못된 지정이 모델의 다른 부분의 추정치를 왜곡하는 전체 추정에서의 오류전파입니다. 세 번째는 잔차는 동일하지만 인과 관계의 모순된 패턴을 갖는 동등 모델입니다. 그러나 우리는 일 반적으로 모형의 어느 부분이 잘못된 것인지 미리 알 수 없기 때문에 잔차가 우리에게 무엇을 알려주는지 정확 히 이해하기 어려울 수 있습니다. 잔차의 패턴을 검사하는 것이 때때로 도움이 될 수 있 습니다.rXY &gt;0인한쌍의변수X와Y가간접인과경로 로만 연결되어 있다고 가정해 보겠습니다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "강사: 조성균\nemail: sk.cho@snu.ac.kr\n수업시간: 월 7:00 ~ 9:50PM\n면담 시간: 수업 후\nWebsite: sem.modellings.art\n과제: Notice\n질문: Communicate/Ask",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#강의-정보",
    "href": "index.html#강의-정보",
    "title": "Welcome",
    "section": "",
    "text": "강사: 조성균\nemail: sk.cho@snu.ac.kr\n수업시간: 월 7:00 ~ 9:50PM\n면담 시간: 수업 후\nWebsite: sem.modellings.art\n과제: Notice\n질문: Communicate/Ask",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#강의-개요",
    "href": "index.html#강의-개요",
    "title": "Welcome",
    "section": "강의 개요",
    "text": "강의 개요\n심리통계에서 배운 회귀분석을 기반으로 하여, 잠재변수(latent variable)을 포함해 여러 변수들 간의 관계에 대한 가설에 대한 통계적 검증을 위한 구조모형을 다룹니다. 이는 확인적 요인 분석 (confirmatory factor analysis, CFA)과 경로분석 (path analysis)을 통합한 구조방정식모형 (Structural equation model, SEM)의 프레임워크를 포함하며, 인과관계의 대한 추론을 위해 SEM을 활용하는 방법과 그 한계에 대해 다룹니다. 본 수업에서는 R의 SEM 패키지들을 활용해 실습을 하며, 다양한 실제 사례를 통해 분석의 결과를 통계 이론의 이해를 바탕으로 올바로 해석할 수 있는 능력을 갖춥니다.\n\n교재 및 R 코드\n\n\n다중회귀분석과 구조방정식모형분석 - 다중회귀분석을 넘어 (3판), 2024 by Timothy Z. Keith (지은이), 노석준 (옮긴이)\n원제: Multiple Regression and Beyond: An Introduction to Multiple Regression and Structural Equation Modeling, Third Edition (2019)\n저자 웹사이트: https://tzkeith.com\n실습을 위한 참고 홈페이지\n\nR Cookbook for Structural Equation Modeling by Ge Jiang\nGithub repository\n\n\n\n\n\n\n\n참고 도서\n\n\nPrinciples and Practice of Structural Equation Modeling (5e), 2023 by Rex B. Kline\nR code 및 부가 자료 링크\nR 참고 도서\nR for Data Science (2e) by Hadley Wickham and Garrett Grolemund",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#수업-활동",
    "href": "index.html#수업-활동",
    "title": "Welcome",
    "section": "수업 활동",
    "text": "수업 활동\n출석 (10%), 일반과제 (30%), 중간고사 대체 과제 (30%), 기말고사 (30%)",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "contents/diagnostics.html#linearity",
    "href": "contents/diagnostics.html#linearity",
    "title": "Diagnostics",
    "section": "Linearity",
    "text": "Linearity\n\ntrendlines &lt;- function(data, mapping, ...){\n  ggplot(data = data, mapping = mapping) + \n  geom_smooth(method = loess, se = FALSE, color = \"orange\", ...)\n}\nhw_mean |&gt; select(-Female, -ethnic) |&gt; \n  GGally::ggpairs(lower = list(continuous = trendlines))\n\n\n\n\n\n\n\n\n\nlibrary(car)\nmod &lt;- lm(eng92 ~ hw10, data=hw_mean)\ncrPlots(mod)",
    "crumbs": [
      "Keith's",
      "Diagnostics"
    ]
  },
  {
    "objectID": "contents/chap18.html#initial-model",
    "href": "contents/chap18.html#initial-model",
    "title": "Chapter 18. Multigroup Models, Panel Models, Dangers and Assumptions",
    "section": "Initial Model",
    "text": "Initial Model\n\n\n\nLoad the dataset\nhw &lt;- haven::read_sav(\"data/chap 18 latent var SEM 2/HW latent matrix.sav\")\nhwcov &lt;- hw[c(2:15), c(3:16)] |&gt; \n  as.matrix() |&gt; \n  lav_matrix_vechr(diagonal = TRUE) |&gt; \n  getCov(names = hw$varname_[2:15], sds = hw[16, 3:16]  |&gt; as.double())\n\n# generate a dataset with the same covariance matrix\nset.seed(123)\nhw_sim &lt;- semTools::kd(hwcov, n = 1000, type = \"exact\")\n\n# replace Minority values to 1 if it &gt; 0.3, otherwise 0\nhw_sim &lt;- hw_sim |&gt; \n  mutate(Minority = ifelse(Minority &gt; 0.3, 1, 0))\n\n\n\nhw_model &lt;- \"\n  # measurement model\n  EthnicMinor =~ Minority\n  Famback =~ parocc + bypared + byfaminc\n  PrevAch =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  HW =~ hw10 + hw_8\n  Grades =~ eng_12 + math_12 + sci_12 + ss_12\n\n  # residual covariacne/variance\n  Minority ~~ 0.0099*Minority\n  bytxrstd ~~ eng_12\n  bytxmstd ~~ math_12\n  bytxsstd ~~ sci_12\n  bytxhstd ~~ ss_12\n\n  # structural model\n  PrevAch ~ b1*Famback + b2*EthnicMinor\n  Grades ~ b3*PrevAch + b4*HW\n  HW ~ b5*PrevAch + b6*Famback + b7*EthnicMinor\n\n  # indirect effects; famback &gt; homework &gt; grades\n  ind_fam := b4*b6\n  # indirect effects; ethnicminor &gt; homework &gt; grades\n  ind_ethinic := b4*b7\n\"\n\nhw_fit &lt;- sem(hw_model, sample.cov = hwcov, sample.nobs = 1000)\nsummary(hw_fit, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 277 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        39\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                               204.654\n  Degrees of freedom                                66\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              8392.044\n  Degrees of freedom                                91\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.983\n  Tucker-Lewis Index (TLI)                       0.977\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33331.251\n  Loglikelihood unrestricted model (H1)     -33228.924\n                                                      \n  Akaike (AIC)                               66740.502\n  Bayesian (BIC)                             66931.905\n  Sample-size adjusted Bayesian (SABIC)      66808.039\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.046\n  90 Percent confidence interval - lower         0.039\n  90 Percent confidence interval - upper         0.053\n  P-value H_0: RMSEA &lt;= 0.050                    0.825\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  EthnicMinor =~                                                        \n    Minority          1.000                               0.434    0.975\n  Famback =~                                                            \n    parocc            1.000                              16.760    0.776\n    bypared           0.062    0.003   21.618    0.000    1.032    0.805\n    byfaminc          0.100    0.005   19.224    0.000    1.683    0.667\n  PrevAch =~                                                            \n    bytxrstd          1.000                               8.806    0.855\n    bytxmstd          0.997    0.030   33.754    0.000    8.782    0.844\n    bytxsstd          0.990    0.030   33.537    0.000    8.718    0.846\n    bytxhstd          0.967    0.029   32.925    0.000    8.513    0.837\n  HW =~                                                                 \n    hw10              1.000                               1.126    0.592\n    hw_8              0.453    0.060    7.553    0.000    0.510    0.451\n  Grades =~                                                             \n    eng_12            1.000                               2.471    0.924\n    math_12           0.896    0.024   37.832    0.000    2.213    0.820\n    sci_12            0.957    0.022   43.705    0.000    2.364    0.878\n    ss_12             1.062    0.022   48.218    0.000    2.625    0.914\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  PrevAch ~                                                             \n    Famback   (b1)    0.278    0.020   13.656    0.000    0.529    0.529\n    EthnicMnr (b2)   -1.774    0.645   -2.749    0.006   -0.087   -0.087\n  Grades ~                                                              \n    PrevAch   (b3)    0.145    0.012   12.580    0.000    0.518    0.518\n    HW        (b4)    0.601    0.132    4.569    0.000    0.274    0.274\n  HW ~                                                                  \n    PrevAch   (b5)    0.053    0.008    6.643    0.000    0.413    0.413\n    Famback   (b6)    0.013    0.004    3.122    0.002    0.198    0.198\n    EthnicMnr (b7)    0.281    0.123    2.293    0.022    0.108    0.108\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.704    0.248    2.844    0.004    0.704    0.128\n .bytxmstd ~~                                                           \n   .math_12           2.856    0.342    8.350    0.000    2.856    0.331\n .bytxsstd ~~                                                           \n   .sci_12            0.920    0.285    3.227    0.001    0.920    0.130\n .bytxhstd ~~                                                           \n   .ss_12             0.533    0.277    1.927    0.054    0.533    0.082\n  EthnicMinor ~~                                                        \n    Famback          -2.136    0.277   -7.709    0.000   -0.294   -0.294\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Minority          0.010                               0.010    0.050\n   .parocc          185.127   13.017   14.222    0.000  185.127    0.397\n   .bypared           0.580    0.046   12.697    0.000    0.580    0.353\n   .byfaminc          3.528    0.193   18.276    0.000    3.528    0.555\n   .bytxrstd         28.555    1.718   16.625    0.000   28.555    0.269\n   .bytxmstd         31.233    1.821   17.153    0.000   31.233    0.288\n   .bytxsstd         30.165    1.767   17.067    0.000   30.165    0.284\n   .bytxhstd         30.864    1.774   17.399    0.000   30.864    0.299\n   .hw10              2.352    0.202   11.639    0.000    2.352    0.650\n   .hw_8              1.018    0.058   17.562    0.000    1.018    0.797\n   .eng_12            1.052    0.074   14.115    0.000    1.052    0.147\n   .math_12           2.378    0.122   19.545    0.000    2.378    0.327\n   .sci_12            1.653    0.094   17.625    0.000    1.653    0.228\n   .ss_12             1.364    0.090   15.133    0.000    1.364    0.165\n    EthnicMinor       0.188    0.009   21.242    0.000    1.000    1.000\n    Famback         280.913   21.604   13.003    0.000    1.000    1.000\n   .PrevAch          53.163    3.514   15.130    0.000    0.686    0.686\n   .HW                0.915    0.182    5.020    0.000    0.722    0.722\n   .Grades            3.150    0.202   15.604    0.000    0.516    0.516\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    ind_fam           0.008    0.003    2.727    0.006    0.054    0.054\n    ind_ethinic       0.169    0.079    2.138    0.032    0.030    0.030\n\n\n\n비표준화 결과\n어떤 단위로 측정되었는가?\nGrades: 0 (an F average) to 12 (A+)\nHomework:\n\n\n\n가족배경이 성적에 미치는 효과\n소수인종 변수를 통제한 상태에서\nAll (direct/indirect) paths from family background to grades: total effect\n\n# Using the simulated data\nhw_fit_sim &lt;- sem(hw_model, data = hw_sim)\n\n\nlibrary(manymome)\n# All indirect paths from family background to grades: total effect\npaths &lt;- all_indirect_paths(hw_fit_sim, x = \"Famback\", y = \"Grades\")\npaths |&gt; print()\n\nCall: \nall_indirect_paths(fit = hw_fit_sim, x = \"Famback\", y = \"Grades\")\nPath(s): \n  path                              \n1 Famback -&gt; PrevAch -&gt; HW -&gt; Grades\n2 Famback -&gt; PrevAch -&gt; Grades      \n3 Famback -&gt; HW -&gt; Grades           \n\n\n\nind_est_std &lt;- many_indirect_effects(paths,\n  fit = hw_fit_sim, R = 1000,\n  boot_ci = TRUE, boot_type = \"bc\",\n  standardized_x = TRUE,\n  standardized_y = TRUE\n)\nind_est_std\n\n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100% elapsed=16s  \n\n\n\n==  Indirect Effect(s) (Both x-variable(s) and y-variable(s) Standardized)  ==\n                                     std CI.lo CI.hi Sig\nFamback -&gt; PrevAch -&gt; HW -&gt; Grades 0.059 0.034 0.108 Sig\nFamback -&gt; PrevAch -&gt; Grades       0.284 0.229 0.336 Sig\nFamback -&gt; HW -&gt; Grades            0.050 0.013 0.098 Sig\n\n - [CI.lo to CI.hi] are 95.0% bias-corrected confidence intervals by\n   nonparametric bootstrapping with 1000 samples.\n - std: The standardized indirect effects.\n \n\n\n\n# total effect (all indirects + directs) from family background to grades\nind_est_std[[1]] + ind_est_std[[2]] + ind_est_std[[3]]\n\n\n== Indirect Effect (Both ‘Famback’ and ‘Grades’ Standardized) ==\n                                                        \n Path:                Famback -&gt; PrevAch -&gt; HW -&gt; Grades\n Path:                Famback -&gt; PrevAch -&gt; Grades      \n Path:                Famback -&gt; HW -&gt; Grades           \n Function of Effects: 0.392                             \n 95.0% Bootstrap CI:  [0.342 to 0.442]                  \n\nComputation of the Function of Effects:\n ((Famback-&gt;PrevAch-&gt;HW-&gt;Grades)\n+(Famback-&gt;PrevAch-&gt;Grades))\n+(Famback-&gt;HW-&gt;Grades) \n\n\nBias-corrected confidence interval formed by nonparametric bootstrapping with\n1000 bootstrap samples.\n\n\n특정 간접효과를 테스트하려면,\n\nindirect_effect(\n  fit = hw_fit_sim,\n  x = \"Famback\",\n  y = \"Grades\",\n  m = c(\"HW\"),\n  boot_out = out_med,\n  standardized_x = TRUE,\n  standardized_y = TRUE\n)\n\n# ==  Indirect Effect(s) (Both x-variable(s) and y-variable(s) Standardized)  ==\n#   std CI.lo CI.hi Sig\n# famback -&gt; prevach -&gt; hw -&gt; grades 0.060 0.034 0.105 Sig\n# famback -&gt; prevach -&gt; grades       0.274 0.222 0.325 Sig\n# famback -&gt; hw -&gt; grades            0.054 0.018 0.102 Sig\n\n\n== Indirect Effect (Both ‘Famback’ and ‘Grades’ Standardized) ==\n                                         \n Path:            Famback -&gt; HW -&gt; Grades\n Indirect Effect: 0.050                  \n\nComputation Formula:\n  (b.HW~Famback)*(b.Grades~HW)*sd_Famback/sd_Grades\n\nComputation:\n  (0.01251)*(0.58685)*(16.70928)/(2.47203)\n\nCoefficients of Component Paths:\n       Path Coefficient\n HW~Famback      0.0125\n  Grades~HW      0.5868\n\nNOTE:\n- The effects of the component paths are from the model, not standardized.\n\n\n\n\n소수인종이 성적에 미치는 영향\n가족배경 변수를 통제한 상태에서\nAll (direct/indirect) paths from minority to grades: total effect\n소수인종 변수가 카테고리 변수(더미)이므로 해석에 유의\n\n# All indirect paths from minority to grades: total effect\npaths2 &lt;- all_indirect_paths(hw_fit_sim,\n  x = \"EthnicMinor\",\n  y = \"Grades\"\n)\npaths2 |&gt; print()\n\nCall: \nall_indirect_paths(fit = hw_fit_sim, x = \"EthnicMinor\", y = \"Grades\")\nPath(s): \n  path                                  \n1 EthnicMinor -&gt; PrevAch -&gt; HW -&gt; Grades\n2 EthnicMinor -&gt; PrevAch -&gt; Grades      \n3 EthnicMinor -&gt; HW -&gt; Grades           \n\n\n\nind_est_std2 &lt;- many_indirect_effects(paths2,\n  fit = hw_fit_sim, R = 1000,\n  boot_ci = TRUE, boot_type = \"bc\",\n  standardized_x = FALSE,  # categorical variable\n  standardized_y = TRUE\n)\nind_est_std2\n\n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100% elapsed=13s  \n\n\n\n==  Indirect Effect(s) (y-variable(s) Standardized)  ==\n                                          ind  CI.lo CI.hi Sig\nEthnicMinor -&gt; PrevAch -&gt; HW -&gt; Grades -0.013 -0.040 0.001    \nEthnicMinor -&gt; PrevAch -&gt; Grades       -0.061 -0.144 0.013    \nEthnicMinor -&gt; HW -&gt; Grades             0.036 -0.017 0.110    \n\n - [CI.lo to CI.hi] are 95.0% bias-corrected confidence intervals by\n   nonparametric bootstrapping with 1000 samples.\n - std: The partially standardized indirect effects. \n - y-variable(s) standardized.\n \n\n\n\n# total effect (all indirects + directs) from minority to grades\nind_est_std2[[1]] + ind_est_std2[[2]] + ind_est_std2[[3]]\n\n\n== Indirect Effect (‘Grades’ Standardized) ==\n                                                            \n Path:                EthnicMinor -&gt; PrevAch -&gt; HW -&gt; Grades\n Path:                EthnicMinor -&gt; PrevAch -&gt; Grades      \n Path:                EthnicMinor -&gt; HW -&gt; Grades           \n Function of Effects: -0.037                                \n 95.0% Bootstrap CI:  [-0.146 to 0.073]                     \n\nComputation of the Function of Effects:\n ((EthnicMinor-&gt;PrevAch-&gt;HW-&gt;Grades)\n+(EthnicMinor-&gt;PrevAch-&gt;Grades))\n+(EthnicMinor-&gt;HW-&gt;Grades) \n\n\nBias-corrected confidence interval formed by nonparametric bootstrapping with\n1000 bootstrap samples.\n\n\n\n\nUsing cSEM\n\nhw_model_c &lt;- \"\n  EthnicMinor &lt;~ Minority\n  Famback =~ bypared + byfaminc + parocc\n  PrevAch =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  HW =~ hw_8 + hw10\n  Grades =~ eng_12 + math_12 + sci_12 + ss_12\n\n  PrevAch ~ Famback + EthnicMinor\n  Grades ~ PrevAch + HW\n  HW ~ PrevAch + Famback + EthnicMinor\n\"\nhw_raw_imp &lt;- VIM::kNN(hw_raw, k = 10) # kNN imputation\ncsem_fit &lt;- cSEM::csem(.data = hw_raw_imp, .model = hw_model_c, .resample_method = \"bootstrap\")\ncSEM::summarize(csem_fit) |&gt; print()\n\nERROR: Error: 객체 'hw_raw'를 찾을 수 없습니다\n\nError: 객체 'hw_raw'를 찾을 수 없습니다\nTraceback:\n\n1. check_data(data)\n2. .handleSimpleError(function (cnd) \n . {\n .     watcher$capture_plot_and_output()\n .     cnd &lt;- sanitize_call(cnd)\n .     watcher$push(cnd)\n .     switch(on_error, continue = invokeRestart(\"eval_continue\"), \n .         stop = invokeRestart(\"eval_stop\"), error = invokeRestart(\"eval_error\", \n .             cnd))\n . }, \"객체 'hw_raw'를 찾을 수 없습니다\", base::quote(eval(expr, \n .     envir)))",
    "crumbs": [
      "Keith's",
      "Multigroup Models"
    ]
  },
  {
    "objectID": "contents/chap18.html#competing-models",
    "href": "contents/chap18.html#competing-models",
    "title": "Chapter 18. Multigroup Models, Panel Models, Dangers and Assumptions",
    "section": "Competing models",
    "text": "Competing models\n\n\n\n# 5. Measurement model\nhw_model_cfa &lt;- \"\n  EthnicMinor =~ Minority\n  Famback =~ bypared + byfaminc + parocc\n  PrevAch =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  HW =~ hw_8 + hw10\n  Grades =~ eng_12 + math_12 + sci_12 + ss_12\n\n  Minority ~~ 0.0099*Minority\n  bytxrstd ~~ eng_12\n  bytxmstd ~~ math_12\n  bytxsstd ~~ sci_12\n  bytxhstd ~~ ss_12\n\"\nhw_fit_cfa &lt;- cfa(hw_model_cfa, sample.cov = hwcov, sample.nobs = 1000)\n\n\ncompareFit(hw_fit, hw_fit_cfa) |&gt; summary() |&gt; print()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n           Df   AIC   BIC  Chisq Chisq diff     RMSEA Df diff Pr(&gt;Chisq)\nhw_fit_cfa 64 66742 66944 202.47                                        \nhw_fit     66 66741 66932 204.65     2.1889 0.0097196       2     0.3347\n\n####################### Model Fit Indices ###########################\n              chisq df pvalue rmsea   cfi   tli  srmr        aic        bic\nhw_fit_cfa 202.465† 64   .000 .047  .983† .976  .029† 66742.313  66943.531 \nhw_fit     204.654  66   .000 .046† .983  .977† .029  66740.502† 66931.905†\n\n################## Differences in Fit Indices #######################\n                    df  rmsea cfi   tli  srmr    aic     bic\nhw_fit - hw_fit_cfa  2 -0.001   0 0.001 0.001 -1.811 -11.627\n\nThe following lavaan models were compared:\n    hw_fit_cfa\n    hw_fit\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.\n\n\nModification indices\n\nmodindices(hw_fit, sort = TRUE) |&gt; subset(mi &gt; 10) |&gt; print()\n\n            lhs op      rhs     mi    epc sepc.lv sepc.all sepc.nox\n86           HW =~ bytxmstd 42.221  4.562   2.326    0.223    0.223\n98       Grades =~ bytxmstd 37.990  0.734   1.814    0.174    0.174\n103    Minority ~~  bypared 23.209  0.067   0.067    0.889    0.889\n47  EthnicMinor =~  bypared 22.716  0.395   0.171    0.133    0.133\n158    bytxmstd ~~ bytxhstd 21.288 -6.514  -6.514   -0.210   -0.210\n48  EthnicMinor =~ byfaminc 18.841 -0.725  -0.314   -0.125   -0.125\n104    Minority ~~ byfaminc 18.752 -0.123  -0.123   -0.657   -0.657\n99       Grades =~ bytxsstd 14.693 -0.455  -1.123   -0.109   -0.109\n167    bytxsstd ~~   eng_12 14.085 -1.009  -1.009   -0.179   -0.179\n187     math_12 ~~   sci_12 13.040  0.277   0.277    0.140    0.140\n82           HW =~  bypared 11.355  0.358   0.182    0.142    0.142\n186      eng_12 ~~    ss_12 10.345  0.285   0.285    0.238    0.238\n\n\nResiduals\nKeith’s figure 18.10: normalized residuals\n\n# standardized residuals\nresid_z &lt;- residuals(hw_fit, type = \"standardized.mplus\")$cov\ncorrplot::corrplot(resid_z, method = 'number', type = 'lower', is.corr = FALSE)\n\n\n\n\n\n\n\n\n\n# correlation residuals\nresid_cor &lt;- residuals(hw_fit, type = \"cor\")$cov\ncorrplot::corrplot(resid_cor, method = 'number', type = 'lower', is.corr = FALSE)",
    "crumbs": [
      "Keith's",
      "Multigroup Models"
    ]
  },
  {
    "objectID": "contents/chap18.html#panel-models",
    "href": "contents/chap18.html#panel-models",
    "title": "Chapter 18. Multigroup Models, Panel Models, Dangers and Assumptions",
    "section": "Panel Models",
    "text": "Panel Models\n성취도가 locus of control에 미치는 영향과 locus of control이 성취도에 미치는 종단적 영향을 파악하기 위한 모형; 주 목적은 효과의 방향성에 대한 이해\n\n\n\n\n\n\n\nBYS44B 나는 내 인생이 나아가는 방향을 충분히 통제하지 못한다(그림에서 control8)\nBYS44C 내 인생에서 성공을 위해서는 노력보다 행운이 더 중요하다(luck8)\nBYS44F 내가 앞서 나가려고 할 때마다 무언가 또는 누군가가 나를 막는다 (stops8)\nBYS44M 내 인생에서 일어나는 일에는 기회와 행운이 매우 중요하다(chance8)\n\n\n\n논의: 각 시점 내에서 인과관계는 설정하지 않았음. Why?\n\n\nLoad the dataset\nvars &lt;- c(\"Sex\", \"BySES\", \"control8\", \"luck8\", \"stops8\", \"chance8\", \"contro10\", \"luck10\", \"stops10\", \"chance10\", \"contro12\", \"luck12\", \"stops12\", \"chance12\", \"feel8\", \"worth8\", \"do8\", \"sat8\", \"feel10\", \"worth10\", \"do10\", \"sat10\", \"feel12\", \"worth12\", \"do12\", \"sat12\", \"read8\", \"math8\", \"scienc8\", \"social8\", \"read10\", \"math10\", \"scienc10\", \"social10\", \"read12\", \"math12\", \"scienc12\", \"social12\")\nloc_txt &lt;- read_tsv(\"data/chap 18 latent var SEM 2/sc locus ach matrix n12k.txt\", col_names = vars)\n\nloc_mean &lt;- loc_txt[40, ] |&gt; as.double()\nloc_sds &lt;- loc_txt[39, ] |&gt; as.double()\nloc_cor &lt;- loc_txt[1:38, ] |&gt; as.matrix()\nloc_cov &lt;- loc_cor |&gt; \n  lav_matrix_vechr(diagonal = TRUE) |&gt;\n  getCov(names = vars, sds = loc_sds)\n\n\n\nloc_model &lt;- \"\n  # measurement model\n  Locus8 =~ control8 + luck8 + stops8 + chance8\n  Locus10 =~ contro10 + luck10 + stops10 + chance10\n  Locus12 =~ contro12 + luck12 + stops12 + chance12\n\n  Ach8 =~ read8 + math8 + scienc8 + social8\n  Ach10 =~ read10 + math10 + scienc10 + social10\n  Ach12 =~ read12 + math12 + scienc12 + social12\n\n  # structural model\n  Locus12 ~ Locus10 + Ach10\n  Ach12 ~ Ach10 + Locus10\n\n  Locus10 ~ Locus8 + Ach8\n  Ach10 ~ Ach8 + Locus8\n\n  Locus8 ~ Sex + BySES\n  Ach8 ~ Sex + BySES\n\n  Sex ~~ 0*BySES\n\n  # correlated disturbances at each time point\n  Locus8 ~~ Ach8\n  Locus10 ~~ Ach10\n  Locus12 ~~ Ach12\n\n  # correlated errors items over time\n  control8 ~~ contro10\n  control8 ~~ contro12\n  contro10 ~~ contro12\n  luck8 ~~ luck10\n  luck8 ~~ luck12\n  luck10 ~~ luck12\n  stops8 ~~ stops10\n  stops8 ~~ stops12\n  stops10 ~~ stops12\n  chance8 ~~ chance10\n  chance8 ~~ chance12\n  chance10 ~~ chance12\n\n  read8 ~~ read10\n  read8 ~~ read12\n  read10 ~~ read12\n  math8 ~~ math10\n  math8 ~~ math12\n  math10 ~~ math12\n  scienc8 ~~ scienc10\n  scienc8 ~~ scienc12\n  scienc10 ~~ scienc12\n  social8 ~~ social10\n  social8 ~~ social12\n  social10 ~~ social12\n\"\nloc_fit &lt;- sem(loc_model, sample.cov = loc_cov, sample.nobs = 12572)\nsummary(loc_fit, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 286 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        89\n\n  Number of observations                         12572\n\nModel Test User Model:\n                                                      \n  Test statistic                              8204.529\n  Degrees of freedom                               262\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                            224320.606\n  Degrees of freedom                               325\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.965\n  Tucker-Lewis Index (TLI)                       0.956\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)            -652454.746\n  Loglikelihood unrestricted model (H1)    -648352.482\n                                                      \n  Akaike (AIC)                             1305087.492\n  Bayesian (BIC)                           1305749.584\n  Sample-size adjusted Bayesian (SABIC)    1305466.751\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.049\n  90 Percent confidence interval - lower         0.048\n  90 Percent confidence interval - upper         0.050\n  P-value H_0: RMSEA &lt;= 0.050                    0.946\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.044\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Locus8 =~                                                             \n    control8          1.000                               0.357    0.441\n    luck8             1.248    0.035   35.417    0.000    0.446    0.605\n    stops8            0.929    0.030   31.099    0.000    0.332    0.439\n    chance8           1.555    0.043   35.948    0.000    0.555    0.624\n  Locus10 =~                                                            \n    contro10          1.000                               0.355    0.457\n    luck10            1.281    0.033   39.181    0.000    0.455    0.663\n    stops10           0.892    0.027   33.620    0.000    0.317    0.451\n    chance10          1.459    0.037   39.484    0.000    0.518    0.657\n  Locus12 =~                                                            \n    contro12          1.000                               0.373    0.473\n    luck12            1.343    0.031   43.051    0.000    0.500    0.719\n    stops12           0.951    0.025   37.709    0.000    0.354    0.505\n    chance12          1.489    0.035   43.061    0.000    0.555    0.694\n  Ach8 =~                                                               \n    read8             1.000                               8.564    0.842\n    math8             1.023    0.008  121.261    0.000    8.763    0.851\n    scienc8           1.019    0.008  120.649    0.000    8.729    0.855\n    social8           1.005    0.008  118.952    0.000    8.605    0.846\n  Ach10 =~                                                              \n    read10            1.000                               8.695    0.863\n    math10            1.021    0.007  136.374    0.000    8.877    0.877\n    scienc10          1.042    0.008  136.616    0.000    9.064    0.885\n    social10          1.006    0.008  130.757    0.000    8.744    0.864\n  Ach12 =~                                                              \n    read12            1.000                               8.607    0.857\n    math12            1.038    0.008  135.026    0.000    8.932    0.883\n    scienc12          1.036    0.008  133.000    0.000    8.921    0.881\n    social12          1.016    0.008  130.088    0.000    8.744    0.869\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Locus12 ~                                                             \n    Locus10           0.564    0.020   28.477    0.000    0.537    0.537\n    Ach10             0.006    0.000   12.038    0.000    0.136    0.136\n  Ach12 ~                                                               \n    Ach10             0.950    0.006  151.543    0.000    0.959    0.959\n    Locus10           0.321    0.107    3.000    0.003    0.013    0.013\n  Locus10 ~                                                             \n    Locus8            0.521    0.021   24.766    0.000    0.524    0.524\n    Ach8              0.004    0.001    7.883    0.000    0.103    0.103\n  Ach10 ~                                                               \n    Ach8              0.945    0.007  132.611    0.000    0.931    0.931\n    Locus8            0.818    0.142    5.745    0.000    0.034    0.034\n  Locus8 ~                                                              \n    Sex               0.023    0.008    2.926    0.003    0.063    0.032\n    BySES             0.126    0.006   22.788    0.000    0.353    0.283\n  Ach8 ~                                                                \n    Sex              -0.540    0.135   -3.994    0.000   -0.063   -0.032\n    BySES             5.788    0.090   64.430    0.000    0.676    0.542\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Sex ~~                                                                \n    BySES             0.000                               0.000    0.000\n .Locus8 ~~                                                             \n   .Ach8              0.982    0.038   25.985    0.000    0.399    0.399\n .Locus10 ~~                                                            \n   .Ach10             0.149    0.013   11.154    0.000    0.185    0.185\n .Locus12 ~~                                                            \n   .Ach12             0.108    0.011    9.890    0.000    0.160    0.160\n .control8 ~~                                                           \n   .contro10          0.077    0.005   15.574    0.000    0.077    0.154\n   .contro12          0.065    0.005   13.194    0.000    0.065    0.129\n .contro10 ~~                                                           \n   .contro12          0.089    0.005   18.824    0.000    0.089    0.186\n .luck8 ~~                                                              \n   .luck10            0.026    0.004    7.223    0.000    0.026    0.086\n   .luck12            0.025    0.003    7.467    0.000    0.025    0.089\n .luck10 ~~                                                             \n   .luck12            0.025    0.003    7.839    0.000    0.025    0.101\n .stops8 ~~                                                             \n   .stops10           0.089    0.004   21.178    0.000    0.089    0.210\n   .stops12           0.067    0.004   16.658    0.000    0.067    0.164\n .stops10 ~~                                                            \n   .stops12           0.099    0.004   25.969    0.000    0.099    0.263\n .chance8 ~~                                                            \n   .chance10          0.076    0.005   15.238    0.000    0.076    0.185\n   .chance12          0.074    0.005   15.669    0.000    0.074    0.185\n .chance10 ~~                                                           \n   .chance12          0.068    0.004   15.871    0.000    0.068    0.199\n .read8 ~~                                                              \n   .read10           12.691    0.336   37.725    0.000   12.691    0.456\n   .read12            9.783    0.327   29.894    0.000    9.783    0.345\n .read10 ~~                                                             \n   .read12           11.906    0.319   37.334    0.000   11.906    0.453\n .math8 ~~                                                              \n   .math10           18.040    0.351   51.470    0.000   18.040    0.686\n   .math12           15.389    0.332   46.385    0.000   15.389    0.598\n .math10 ~~                                                             \n   .math12           18.056    0.328   55.035    0.000   18.056    0.782\n .scienc8 ~~                                                            \n   .scienc10          4.344    0.303   14.353    0.000    4.344    0.171\n   .scienc12          4.285    0.300   14.284    0.000    4.285    0.168\n .scienc10 ~~                                                           \n   .scienc12          7.300    0.290   25.213    0.000    7.300    0.318\n .social8 ~~                                                            \n   .social10          7.607    0.323   23.573    0.000    7.607    0.275\n   .social12          5.962    0.310   19.204    0.000    5.962    0.221\n .social10 ~~                                                           \n   .social12          7.351    0.301   24.438    0.000    7.351    0.290\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .control8          0.528    0.008   70.109    0.000    0.528    0.806\n   .luck8             0.344    0.006   56.216    0.000    0.344    0.634\n   .stops8            0.460    0.007   70.237    0.000    0.460    0.807\n   .chance8           0.484    0.009   54.421    0.000    0.484    0.611\n   .contro10          0.477    0.007   70.646    0.000    0.477    0.791\n   .luck10            0.264    0.005   51.766    0.000    0.264    0.560\n   .stops10           0.392    0.006   71.020    0.000    0.392    0.796\n   .chance10          0.352    0.007   53.246    0.000    0.352    0.568\n   .contro12          0.482    0.007   71.233    0.000    0.482    0.776\n   .luck12            0.234    0.005   47.379    0.000    0.234    0.483\n   .stops12           0.366    0.005   69.721    0.000    0.366    0.745\n   .chance12          0.331    0.006   51.512    0.000    0.331    0.518\n   .read8            29.988    0.464   64.565    0.000   29.988    0.290\n   .math8            29.338    0.461   63.667    0.000   29.338    0.276\n   .scienc8          28.105    0.454   61.900    0.000   28.105    0.269\n   .social8          29.485    0.463   63.690    0.000   29.485    0.285\n   .read10           25.818    0.397   65.026    0.000   25.818    0.255\n   .math10           23.568    0.372   63.324    0.000   23.568    0.230\n   .scienc10         22.844    0.377   60.589    0.000   22.844    0.218\n   .social10         26.025    0.405   64.281    0.000   26.025    0.254\n   .read12           26.760    0.411   65.092    0.000   26.760    0.265\n   .math12           22.605    0.369   61.267    0.000   22.605    0.221\n   .scienc12         23.044    0.380   60.675    0.000   23.044    0.225\n   .social12         24.674    0.392   62.900    0.000   24.674    0.244\n    Sex               0.250    0.003   79.284    0.000    0.250    1.000\n    BySES             0.643    0.008   79.284    0.000    0.643    1.000\n   .Locus8            0.117    0.006   21.190    0.000    0.919    0.919\n   .Locus10           0.084    0.004   21.806    0.000    0.664    0.664\n   .Locus12           0.088    0.004   23.341    0.000    0.636    0.636\n   .Ach8             51.746    0.913   56.670    0.000    0.705    0.705\n   .Ach10             7.732    0.194   39.803    0.000    0.102    0.102\n   .Ach12             5.139    0.139   37.013    0.000    0.069    0.069",
    "crumbs": [
      "Keith's",
      "Multigroup Models"
    ]
  },
  {
    "objectID": "contents/chap18.html#multi-group",
    "href": "contents/chap18.html#multi-group",
    "title": "Chapter 18. Multigroup Models, Panel Models, Dangers and Assumptions",
    "section": "Multi-group",
    "text": "Multi-group\n\n\nLoad the dataset\n# generate a dataset with the same covariance matrix\nlibrary(readxl)\nhw_mg_minor &lt;- read_xls(\"data/chap 18 latent var SEM 2/minority matrix.xls\")\nhw_mg_white &lt;- read_xls(\"data/chap 18 latent var SEM 2/white matrix.xls\")\n\n# For minority group\nhw_mg_minor_cov &lt;- hw_mg_minor[c(2:14), c(3:15)] |&gt;\n  as.matrix() |&gt;\n  lav_matrix_vechr(diagonal = TRUE) |&gt;\n  getCov(names = hw_mg_minor$varname_[2:14], sds = hw_mg_minor[16, 3:15] |&gt; as.double())\nmean_minor &lt;- hw_mg_minor[15, 3:15] |&gt; as.double()\n\n# For white group\nhw_mg_white_cov &lt;- hw_mg_white[c(2:14), c(3:15)] |&gt;\n  as.matrix() |&gt;\n  lav_matrix_vechr(diagonal = TRUE) |&gt;\n  getCov(names = hw_mg_white$varname_[2:14], sds = hw_mg_white[16, 3:15] |&gt; as.double())\nmean_white &lt;- hw_mg_white[15, 3:15] |&gt; as.double()\n\n# simulate the data\nhw_sim_minor &lt;- semTools::kd(hw_mg_minor_cov, n = 274, type = \"exact\") |&gt;\n  sweep(2, mean_minor, FUN = \"+\")\nhw_sim_white &lt;- semTools::kd(hw_mg_white_cov, n = 751, type = \"exact\") |&gt;\n  sweep(2, mean_white, FUN = \"+\")\n\nhw_multigroup &lt;- bind_rows(\n  hw_sim_minor |&gt; mutate(group = \"minority\"),\n  hw_sim_white |&gt; mutate(group = \"white\")\n)\n\ncolnames(hw_multigroup) &lt;- tolower(colnames(hw_multigroup))\n\n\n\n# Fit the model\nhw_model_mg &lt;- \"\n  famback =~ parocc + bypared + byfaminc\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw10 + hw_8\n  grades =~ eng_12 + math_12 + sci_12 + ss_12\n\n  bytxrstd ~~ eng_12\n  bytxmstd ~~ math_12\n  bytxsstd ~~ sci_12\n  bytxhstd ~~ ss_12\n\n  prevach ~ famback\n  grades ~ prevach + hw\n  hw ~ prevach + famback\n\"\n\nhw_fit_mg &lt;- sem(hw_model_mg, data = hw_multigroup, group = \"group\")\nsummary(hw_fit_mg, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 427 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        96\n\n  Number of observations per group:                   \n    minority                                       274\n    white                                          751\n\nModel Test User Model:\n                                                      \n  Test statistic                               223.178\n  Degrees of freedom                               112\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    minority                                    95.607\n    white                                      127.571\n\nModel Test Baseline Model:\n\n  Test statistic                              8290.390\n  Degrees of freedom                               156\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.986\n  Tucker-Lewis Index (TLI)                       0.981\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33497.854\n  Loglikelihood unrestricted model (H1)     -33386.266\n                                                      \n  Akaike (AIC)                               67187.709\n  Bayesian (BIC)                             67661.224\n  Sample-size adjusted Bayesian (SABIC)      67356.318\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.044\n  90 Percent confidence interval - lower         0.036\n  90 Percent confidence interval - upper         0.052\n  P-value H_0: RMSEA &lt;= 0.050                    0.876\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [minority]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              19.007    0.793\n    bypared           0.059    0.005   11.771    0.000    1.112    0.801\n    byfaminc          0.103    0.010   10.666    0.000    1.961    0.692\n  prevach =~                                                            \n    bytxrstd          1.000                               8.514    0.859\n    bytxmstd          1.058    0.057   18.530    0.000    9.010    0.869\n    bytxsstd          0.951    0.057   16.576    0.000    8.100    0.815\n    bytxhstd          0.942    0.057   16.552    0.000    8.016    0.813\n  hw =~                                                                 \n    hw10              1.000                               0.544    0.296\n    hw_8              0.791    0.195    4.062    0.000    0.430    0.378\n  grades =~                                                             \n    eng_12            1.000                               2.559    0.929\n    math_12           0.895    0.044   20.520    0.000    2.292    0.833\n    sci_12            0.923    0.040   22.949    0.000    2.362    0.877\n    ss_12             1.049    0.042   24.764    0.000    2.686    0.902\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.263    0.032    8.278    0.000    0.586    0.586\n  grades ~                                                              \n    prevach           3.094   62.062    0.050    0.960   10.294   10.294\n    hw              -45.100  964.398   -0.047    0.963   -9.583   -9.583\n  hw ~                                                                  \n    prevach           0.065    0.014    4.594    0.000    1.012    1.012\n    famback          -0.000    0.004   -0.047    0.962   -0.006   -0.006\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.191    0.456    0.418    0.676    0.191    0.037\n .bytxmstd ~~                                                           \n   .math_12           2.386    0.613    3.895    0.000    2.386    0.305\n .bytxsstd ~~                                                           \n   .sci_12            0.289    0.551    0.524    0.600    0.289    0.039\n .bytxhstd ~~                                                           \n   .ss_12             1.140    0.573    1.989    0.047    1.140    0.155\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           43.085    1.448   29.755    0.000   43.085    1.798\n   .bypared           2.849    0.084   33.953    0.000    2.849    2.051\n   .byfaminc          8.757    0.171   51.130    0.000    8.757    3.089\n   .bytxrstd         48.537    0.599   81.056    0.000   48.537    4.897\n   .bytxmstd         49.808    0.626   79.526    0.000   49.808    4.804\n   .bytxsstd         47.959    0.600   79.907    0.000   47.959    4.827\n   .bytxhstd         48.160    0.596   80.823    0.000   48.160    4.883\n   .hw10              3.214    0.111   29.011    0.000    3.214    1.753\n   .hw_8              1.718    0.069   24.980    0.000    1.718    1.509\n   .eng_12            5.820    0.166   34.973    0.000    5.820    2.113\n   .math_12           5.385    0.166   32.389    0.000    5.385    1.957\n   .sci_12            5.590    0.163   34.372    0.000    5.590    2.076\n   .ss_12             5.895    0.180   32.785    0.000    5.895    1.981\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          213.211   29.517    7.223    0.000  213.211    0.371\n   .bypared           0.691    0.099    6.993    0.000    0.691    0.358\n   .byfaminc          4.193    0.447    9.378    0.000    4.193    0.522\n   .bytxrstd         25.756    3.009    8.560    0.000   25.756    0.262\n   .bytxmstd         26.302    3.181    8.268    0.000   26.302    0.245\n   .bytxsstd         33.096    3.475    9.525    0.000   33.096    0.335\n   .bytxhstd         33.025    3.448    9.577    0.000   33.025    0.339\n   .hw10              3.068    0.281   10.933    0.000    3.068    0.912\n   .hw_8              1.112    0.114    9.775    0.000    1.112    0.857\n   .eng_12            1.039    0.148    7.029    0.000    1.039    0.137\n   .math_12           2.321    0.230   10.076    0.000    2.321    0.306\n   .sci_12            1.670    0.181    9.241    0.000    1.670    0.230\n   .ss_12             1.647    0.195    8.433    0.000    1.647    0.186\n    famback         361.277   51.153    7.063    0.000    1.000    1.000\n   .prevach          47.575    6.080    7.825    0.000    0.656    0.656\n   .hw               -0.005    0.103   -0.047    0.963   -0.016   -0.016\n   .grades           13.700  210.287    0.065    0.948    2.092    2.092\n\n\nGroup 2 [white]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              14.207    0.718\n    bypared           0.072    0.004   16.238    0.000    1.025    0.842\n    byfaminc          0.094    0.007   14.113    0.000    1.336    0.593\n  prevach =~                                                            \n    bytxrstd          1.000                               8.583    0.844\n    bytxmstd          0.997    0.036   28.022    0.000    8.560    0.835\n    bytxsstd          0.987    0.035   28.103    0.000    8.475    0.842\n    bytxhstd          0.970    0.035   27.524    0.000    8.325    0.833\n  hw =~                                                                 \n    hw10              1.000                               1.280    0.664\n    hw_8              0.389    0.063    6.181    0.000    0.498    0.441\n  grades =~                                                             \n    eng_12            1.000                               2.419    0.920\n    math_12           0.901    0.028   32.154    0.000    2.179    0.815\n    sci_12            0.972    0.026   37.566    0.000    2.352    0.878\n    ss_12             1.065    0.026   41.677    0.000    2.577    0.917\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.299    0.027   11.063    0.000    0.495    0.495\n  grades ~                                                              \n    prevach           0.156    0.012   13.222    0.000    0.555    0.555\n    hw                0.468    0.114    4.118    0.000    0.248    0.248\n  hw ~                                                                  \n    prevach           0.046    0.009    4.917    0.000    0.307    0.307\n    famback           0.022    0.006    3.658    0.000    0.239    0.239\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.860    0.289    2.971    0.003    0.860    0.153\n .bytxmstd ~~                                                           \n   .math_12           2.953    0.400    7.387    0.000    2.953    0.338\n .bytxsstd ~~                                                           \n   .sci_12            1.139    0.327    3.483    0.000    1.139    0.164\n .bytxhstd ~~                                                           \n   .ss_12             0.349    0.310    1.127    0.260    0.349    0.056\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           54.883    0.722   76.051    0.000   54.883    2.775\n   .bypared           3.335    0.044   75.101    0.000    3.335    2.740\n   .byfaminc         10.348    0.082  125.757    0.000   10.348    4.589\n   .bytxrstd         53.260    0.371  143.506    0.000   53.260    5.237\n   .bytxmstd         53.559    0.374  143.173    0.000   53.559    5.224\n   .bytxsstd         53.335    0.367  145.260    0.000   53.335    5.301\n   .bytxhstd         52.956    0.365  145.243    0.000   52.956    5.300\n   .hw10              3.443    0.070   48.980    0.000    3.443    1.787\n   .hw_8              1.733    0.041   42.046    0.000    1.733    1.534\n   .eng_12            6.409    0.096   66.808    0.000    6.409    2.438\n   .math_12           5.823    0.098   59.670    0.000    5.823    2.177\n   .sci_12            6.088    0.098   62.269    0.000    6.088    2.272\n   .ss_12             6.612    0.103   64.449    0.000    6.612    2.352\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          189.274   14.140   13.386    0.000  189.274    0.484\n   .bypared           0.431    0.056    7.687    0.000    0.431    0.291\n   .byfaminc          3.299    0.198   16.701    0.000    3.299    0.649\n   .bytxrstd         29.775    2.045   14.556    0.000   29.775    0.288\n   .bytxmstd         31.816    2.137   14.890    0.000   31.816    0.303\n   .bytxsstd         29.426    2.010   14.640    0.000   29.426    0.291\n   .bytxhstd         30.535    2.047   14.917    0.000   30.535    0.306\n   .hw10              2.073    0.282    7.355    0.000    2.073    0.559\n   .hw_8              1.028    0.067   15.451    0.000    1.028    0.806\n   .eng_12            1.060    0.085   12.496    0.000    1.060    0.153\n   .math_12           2.401    0.141   17.014    0.000    2.401    0.336\n   .sci_12            1.645    0.108   15.261    0.000    1.645    0.229\n   .ss_12             1.262    0.099   12.770    0.000    1.262    0.160\n    famback         201.844   20.408    9.890    0.000    1.000    1.000\n   .prevach          55.591    4.259   13.054    0.000    0.755    0.755\n   .hw                1.271    0.275    4.623    0.000    0.776    0.776\n   .grades            3.005    0.215   13.978    0.000    0.514    0.514\n\n\n\n\nlavInspect(hw_fit_mg, \"cov.lv\") |&gt; print()\ninspect(hw_fit_mg, what = \"est\") |&gt; print()\n\n$minority\n         fambck  prevch      hw  grades\nfamback 361.281                        \nprevach  94.886  72.495                \nhw        6.070   4.668   0.296        \ngrades   19.872  13.811   1.106   6.550\n\n$white\n         fambck  prevch      hw  grades\nfamback 201.842                        \nprevach  60.406  73.668                \nhw        7.113   4.677   1.638        \ngrades   12.780  13.715   1.498   5.852\n\n$minority\n$minority$lambda\n         fambck prevch    hw grades\nparocc    1.000  0.000 0.000  0.000\nbypared   0.059  0.000 0.000  0.000\nbyfaminc  0.103  0.000 0.000  0.000\nbytxrstd  0.000  1.000 0.000  0.000\nbytxmstd  0.000  1.058 0.000  0.000\nbytxsstd  0.000  0.951 0.000  0.000\nbytxhstd  0.000  0.942 0.000  0.000\nhw10      0.000  0.000 1.000  0.000\nhw_8      0.000  0.000 0.791  0.000\neng_12    0.000  0.000 0.000  1.000\nmath_12   0.000  0.000 0.000  0.895\nsci_12    0.000  0.000 0.000  0.923\nss_12     0.000  0.000 0.000  1.049\n\n$minority$theta\n          parocc  bypard  byfmnc  bytxrs  bytxms  bytxss  bytxhs    hw10\nparocc   213.210                                                        \nbypared    0.000   0.691                                                \nbyfaminc   0.000   0.000   4.193                                        \nbytxrstd   0.000   0.000   0.000  25.756                                \nbytxmstd   0.000   0.000   0.000   0.000  26.302                        \nbytxsstd   0.000   0.000   0.000   0.000   0.000  33.096                \nbytxhstd   0.000   0.000   0.000   0.000   0.000   0.000  33.025        \nhw10       0.000   0.000   0.000   0.000   0.000   0.000   0.000   3.068\nhw_8       0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.000\neng_12     0.000   0.000   0.000   0.191   0.000   0.000   0.000   0.000\nmath_12    0.000   0.000   0.000   0.000   2.386   0.000   0.000   0.000\nsci_12     0.000   0.000   0.000   0.000   0.000   0.289   0.000   0.000\nss_12      0.000   0.000   0.000   0.000   0.000   0.000   1.140   0.000\n            hw_8  eng_12  mth_12  sci_12   ss_12\nparocc                                          \nbypared                                         \nbyfaminc                                        \nbytxrstd                                        \nbytxmstd                                        \nbytxsstd                                        \nbytxhstd                                        \nhw10                                            \nhw_8       1.112                                \neng_12     0.000   1.039                        \nmath_12    0.000   0.000   2.321                \nsci_12     0.000   0.000   0.000   1.670        \nss_12      0.000   0.000   0.000   0.000   1.647\n\n$minority$psi\n         fambck  prevch      hw  grades\nfamback 361.281                        \nprevach   0.000  47.575                \nhw        0.000   0.000  -0.005        \ngrades    0.000   0.000   0.000  13.806\n\n$minority$beta\n        fambck prevch      hw grades\nfamback  0.000  0.000   0.000      0\nprevach  0.263  0.000   0.000      0\nhw       0.000  0.065   0.000      0\ngrades   0.000  3.125 -45.575      0\n\n$minority$nu\n         intrcp\nparocc   43.085\nbypared   2.849\nbyfaminc  8.757\nbytxrstd 48.537\nbytxmstd 49.808\nbytxsstd 47.959\nbytxhstd 48.160\nhw10      3.214\nhw_8      1.718\neng_12    5.820\nmath_12   5.385\nsci_12    5.590\nss_12     5.895\n\n$minority$alpha\n        intrcp\nfamback      0\nprevach      0\nhw           0\ngrades       0\n\n\n$white\n$white$lambda\n         fambck prevch    hw grades\nparocc    1.000  0.000 0.000  0.000\nbypared   0.072  0.000 0.000  0.000\nbyfaminc  0.094  0.000 0.000  0.000\nbytxrstd  0.000  1.000 0.000  0.000\nbytxmstd  0.000  0.997 0.000  0.000\nbytxsstd  0.000  0.987 0.000  0.000\nbytxhstd  0.000  0.970 0.000  0.000\nhw10      0.000  0.000 1.000  0.000\nhw_8      0.000  0.000 0.389  0.000\neng_12    0.000  0.000 0.000  1.000\nmath_12   0.000  0.000 0.000  0.901\nsci_12    0.000  0.000 0.000  0.972\nss_12     0.000  0.000 0.000  1.065\n\n$white$theta\n          parocc  bypard  byfmnc  bytxrs  bytxms  bytxss  bytxhs    hw10\nparocc   189.275                                                        \nbypared    0.000   0.431                                                \nbyfaminc   0.000   0.000   3.299                                        \nbytxrstd   0.000   0.000   0.000  29.775                                \nbytxmstd   0.000   0.000   0.000   0.000  31.816                        \nbytxsstd   0.000   0.000   0.000   0.000   0.000  29.426                \nbytxhstd   0.000   0.000   0.000   0.000   0.000   0.000  30.535        \nhw10       0.000   0.000   0.000   0.000   0.000   0.000   0.000   2.073\nhw_8       0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.000\neng_12     0.000   0.000   0.000   0.860   0.000   0.000   0.000   0.000\nmath_12    0.000   0.000   0.000   0.000   2.953   0.000   0.000   0.000\nsci_12     0.000   0.000   0.000   0.000   0.000   1.139   0.000   0.000\nss_12      0.000   0.000   0.000   0.000   0.000   0.000   0.349   0.000\n            hw_8  eng_12  mth_12  sci_12   ss_12\nparocc                                          \nbypared                                         \nbyfaminc                                        \nbytxrstd                                        \nbytxmstd                                        \nbytxsstd                                        \nbytxhstd                                        \nhw10                                            \nhw_8       1.028                                \neng_12     0.000   1.060                        \nmath_12    0.000   0.000   2.401                \nsci_12     0.000   0.000   0.000   1.645        \nss_12      0.000   0.000   0.000   0.000   1.262\n\n$white$psi\n         fambck  prevch      hw  grades\nfamback 201.842                        \nprevach   0.000  55.591                \nhw        0.000   0.000   1.271        \ngrades    0.000   0.000   0.000   3.005\n\n$white$beta\n        fambck prevch    hw grades\nfamback  0.000  0.000 0.000      0\nprevach  0.299  0.000 0.000      0\nhw       0.022  0.046 0.000      0\ngrades   0.000  0.156 0.468      0\n\n$white$nu\n         intrcp\nparocc   54.883\nbypared   3.335\nbyfaminc 10.348\nbytxrstd 53.260\nbytxmstd 53.559\nbytxsstd 53.335\nbytxhstd 52.956\nhw10      3.443\nhw_8      1.733\neng_12    6.409\nmath_12   5.823\nsci_12    6.088\nss_12     6.612\n\n$white$alpha\n        intrcp\nfamback      0\nprevach      0\nhw           0\ngrades       0\n\n\n\n\n\n# contrains factor loadings\nsem_fit_mg2 &lt;- sem(hw_model_mg,\n  data = hw_multigroup,\n  group = \"group\",\n  group.equal = c(\"loadings\")\n)\n\nsummary(sem_fit_mg2, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 381 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        96\n  Number of equality constraints                     9\n\n  Number of observations per group:                   \n    minority                                       274\n    white                                          751\n\nModel Test User Model:\n                                                      \n  Test statistic                               238.780\n  Degrees of freedom                               121\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    minority                                   106.831\n    white                                      131.949\n\nModel Test Baseline Model:\n\n  Test statistic                              8290.390\n  Degrees of freedom                               156\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.986\n  Tucker-Lewis Index (TLI)                       0.981\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33505.655\n  Loglikelihood unrestricted model (H1)     -33386.266\n                                                      \n  Akaike (AIC)                               67185.311\n  Bayesian (BIC)                             67614.434\n  Sample-size adjusted Bayesian (SABIC)      67338.113\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.044\n  90 Percent confidence interval - lower         0.035\n  90 Percent confidence interval - upper         0.052\n  P-value H_0: RMSEA &lt;= 0.050                    0.902\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.033\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [minority]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              17.929    0.760\n    bypared (.p2.)    0.067    0.003   20.083    0.000    1.207    0.846\n    byfamnc (.p3.)    0.096    0.006   17.481    0.000    1.727    0.629\n  prevach =~                                                            \n    bytxrst           1.000                               8.530    0.860\n    bytxmst (.p5.)    1.014    0.030   33.540    0.000    8.647    0.854\n    bytxsst (.p6.)    0.978    0.030   32.678    0.000    8.340    0.826\n    bytxhst (.p7.)    0.962    0.030   32.151    0.000    8.209    0.823\n  hw =~                                                                 \n    hw10              1.000                               0.678    0.361\n    hw_8    (.p9.)    0.468    0.061    7.685    0.000    0.317    0.286\n  grades =~                                                             \n    eng_12            1.000                               2.527    0.926\n    math_12 (.11.)    0.900    0.024   38.162    0.000    2.274    0.831\n    sci_12  (.12.)    0.958    0.022   44.037    0.000    2.421    0.884\n    ss_12   (.13.)    1.061    0.022   48.511    0.000    2.681    0.902\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.282    0.031    9.079    0.000    0.592    0.592\n  grades ~                                                              \n    prevach          -2.947    0.483   -6.106    0.000   -9.947   -9.947\n    hw               39.821    2.381   16.727    0.000   10.691   10.691\n  hw ~                                                                  \n    prevach           0.078    0.011    6.990    0.000    0.986    0.986\n    famback           0.000    0.000    0.806    0.420    0.006    0.006\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.222    0.456    0.487    0.626    0.222    0.042\n .bytxmstd ~~                                                           \n   .math_12           2.426    0.616    3.937    0.000    2.426    0.302\n .bytxsstd ~~                                                           \n   .sci_12            0.347    0.552    0.628    0.530    0.347    0.047\n .bytxhstd ~~                                                           \n   .ss_12             1.099    0.571    1.925    0.054    1.099    0.151\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           43.085    1.425   30.231    0.000   43.085    1.826\n   .bypared           2.849    0.086   33.058    0.000    2.849    1.997\n   .byfaminc          8.757    0.166   52.839    0.000    8.757    3.192\n   .bytxrstd         48.537    0.599   80.980    0.000   48.537    4.892\n   .bytxmstd         49.808    0.612   81.393    0.000   49.808    4.917\n   .bytxsstd         47.959    0.610   78.615    0.000   47.959    4.749\n   .bytxhstd         48.160    0.603   79.915    0.000   48.160    4.828\n   .hw10              3.214    0.114   28.309    0.000    3.214    1.710\n   .hw_8              1.718    0.067   25.611    0.000    1.718    1.547\n   .eng_12            5.820    0.165   35.298    0.000    5.820    2.132\n   .math_12           5.385    0.165   32.569    0.000    5.385    1.968\n   .sci_12            5.590    0.165   33.777    0.000    5.590    2.041\n   .ss_12             5.895    0.180   32.823    0.000    5.895    1.983\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          235.082   27.675    8.494    0.000  235.082    0.422\n   .bypared           0.578    0.096    6.031    0.000    0.578    0.284\n   .byfaminc          4.543    0.442   10.284    0.000    4.543    0.604\n   .bytxrstd         25.676    2.946    8.715    0.000   25.676    0.261\n   .bytxmstd         27.839    3.136    8.878    0.000   27.839    0.271\n   .bytxsstd         32.420    3.423    9.471    0.000   32.420    0.318\n   .bytxhstd         32.120    3.371    9.528    0.000   32.120    0.323\n   .hw10              3.072    0.265   11.610    0.000    3.072    0.870\n   .hw_8              1.133    0.097   11.671    0.000    1.133    0.918\n   .eng_12            1.064    0.143    7.437    0.000    1.064    0.143\n   .math_12           2.320    0.228   10.161    0.000    2.320    0.310\n   .sci_12            1.643    0.179    9.178    0.000    1.643    0.219\n   .ss_12             1.652    0.192    8.612    0.000    1.652    0.187\n    famback         321.434   39.202    8.199    0.000    1.000    1.000\n   .prevach          47.246    5.437    8.690    0.000    0.649    0.649\n   .hw                0.010    0.005    1.917    0.055    0.021    0.021\n   .grades          -11.683    6.164   -1.895    0.058   -1.830   -1.830\n\n\nGroup 2 [white]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              14.625    0.735\n    bypared (.p2.)    0.067    0.003   20.083    0.000    0.984    0.818\n    byfamnc (.p3.)    0.096    0.006   17.481    0.000    1.409    0.617\n  prevach =~                                                            \n    bytxrst           1.000                               8.580    0.844\n    bytxmst (.p5.)    1.014    0.030   33.540    0.000    8.698    0.841\n    bytxsst (.p6.)    0.978    0.030   32.678    0.000    8.389    0.839\n    bytxhst (.p7.)    0.962    0.030   32.151    0.000    8.258    0.830\n  hw =~                                                                 \n    hw10              1.000                               1.177    0.615\n    hw_8    (.p9.)    0.468    0.061    7.685    0.000    0.550    0.484\n  grades =~                                                             \n    eng_12            1.000                               2.431    0.921\n    math_12 (.11.)    0.900    0.024   38.162    0.000    2.187    0.816\n    sci_12  (.12.)    0.958    0.022   44.037    0.000    2.329    0.875\n    ss_12   (.13.)    1.061    0.022   48.511    0.000    2.579    0.917\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.292    0.025   11.434    0.000    0.497    0.497\n  grades ~                                                              \n    prevach           0.156    0.012   13.103    0.000    0.551    0.551\n    hw                0.513    0.121    4.234    0.000    0.249    0.249\n  hw ~                                                                  \n    prevach           0.045    0.009    5.024    0.000    0.326    0.326\n    famback           0.019    0.005    3.546    0.000    0.239    0.239\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.850    0.289    2.938    0.003    0.850    0.152\n .bytxmstd ~~                                                           \n   .math_12           2.951    0.400    7.377    0.000    2.951    0.340\n .bytxsstd ~~                                                           \n   .sci_12            1.141    0.327    3.491    0.000    1.141    0.163\n .bytxhstd ~~                                                           \n   .ss_12             0.361    0.310    1.163    0.245    0.361    0.058\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           54.883    0.726   75.580    0.000   54.883    2.758\n   .bypared           3.335    0.044   75.936    0.000    3.335    2.771\n   .byfaminc         10.348    0.083  124.155    0.000   10.348    4.530\n   .bytxrstd         53.260    0.371  143.520    0.000   53.260    5.237\n   .bytxmstd         53.559    0.378  141.840    0.000   53.559    5.176\n   .bytxsstd         53.335    0.365  146.123    0.000   53.335    5.332\n   .bytxhstd         52.956    0.363  145.852    0.000   52.956    5.322\n   .hw10              3.443    0.070   49.289    0.000    3.443    1.799\n   .hw_8              1.733    0.042   41.718    0.000    1.733    1.522\n   .eng_12            6.409    0.096   66.567    0.000    6.409    2.429\n   .math_12           5.823    0.098   59.527    0.000    5.823    2.172\n   .sci_12            6.088    0.097   62.700    0.000    6.088    2.288\n   .ss_12             6.612    0.103   64.415    0.000    6.612    2.351\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          182.115   13.639   13.352    0.000  182.115    0.460\n   .bypared           0.480    0.050    9.615    0.000    0.480    0.331\n   .byfaminc          3.233    0.196   16.479    0.000    3.233    0.620\n   .bytxrstd         29.799    2.030   14.681    0.000   29.799    0.288\n   .bytxmstd         31.427    2.125   14.792    0.000   31.427    0.293\n   .bytxsstd         29.674    1.994   14.880    0.000   29.674    0.297\n   .bytxhstd         30.813    2.036   15.133    0.000   30.813    0.311\n   .hw10              2.279    0.229    9.959    0.000    2.279    0.622\n   .hw_8              0.993    0.067   14.787    0.000    0.993    0.766\n   .eng_12            1.053    0.084   12.506    0.000    1.053    0.151\n   .math_12           2.401    0.141   17.038    0.000    2.401    0.334\n   .sci_12            1.655    0.107   15.444    0.000    1.655    0.234\n   .ss_12             1.261    0.098   12.866    0.000    1.261    0.159\n    famback         213.888   19.110   11.192    0.000    1.000    1.000\n   .prevach          55.447    4.061   13.654    0.000    0.753    0.753\n   .hw                1.051    0.209    5.032    0.000    0.759    0.759\n   .grades            3.031    0.213   14.223    0.000    0.513    0.513\n\n\n\n\n# compare the two models\nlavTestLRT(sem_fit_mg, sem_fit_mg2) |&gt; print()\n\n\nChi-Squared Difference Test\n\n             Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)  \nsem_fit_mg  112 67188 67661 223.18                                         \nsem_fit_mg2 121 67185 67614 238.78     15.603 0.037835       9    0.07565 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# constrains the effect of homework\nhw_model_mg3 &lt;- \"\n  famback =~ parocc + bypared + byfaminc\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw10 + hw_8\n  grades =~ eng_12 + math_12 + sci_12 + ss_12\n\n  bytxrstd ~~ eng_12\n  bytxmstd ~~ math_12\n  bytxsstd ~~ sci_12\n  bytxhstd ~~ ss_12\n\n  prevach ~ famback\n  grades ~ prevach + c(h, h)*hw\n  hw ~ prevach + famback\n\"\n\nsem_fit_mg3 &lt;- sem(hw_model_mg3,\n  data = hw_multigroup,\n  estimator = \"ML\",\n  group = \"group\",\n  group.equal = c(\"loadings\")\n)\n\nsummary(sem_fit_mg3, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 311 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        96\n  Number of equality constraints                    10\n\n  Number of observations per group:                   \n    minority                                       274\n    white                                          751\n\nModel Test User Model:\n                                                      \n  Test statistic                               236.984\n  Degrees of freedom                               122\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    minority                                   104.664\n    white                                      132.321\n\nModel Test Baseline Model:\n\n  Test statistic                              8290.390\n  Degrees of freedom                               156\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.986\n  Tucker-Lewis Index (TLI)                       0.982\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33504.758\n  Loglikelihood unrestricted model (H1)     -33386.266\n                                                      \n  Akaike (AIC)                               67181.515\n  Bayesian (BIC)                             67605.706\n  Sample-size adjusted Bayesian (SABIC)      67332.561\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.043\n  90 Percent confidence interval - lower         0.035\n  90 Percent confidence interval - upper         0.051\n  P-value H_0: RMSEA &lt;= 0.050                    0.924\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.033\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [minority]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              17.948    0.761\n    bypared (.p2.)    0.067    0.003   20.086    0.000    1.207    0.845\n    byfamnc (.p3.)    0.096    0.006   17.489    0.000    1.728    0.630\n  prevach =~                                                            \n    bytxrst           1.000                               8.528    0.859\n    bytxmst (.p5.)    1.014    0.030   33.531    0.000    8.647    0.854\n    bytxsst (.p6.)    0.978    0.030   32.663    0.000    8.339    0.825\n    bytxhst (.p7.)    0.963    0.030   32.151    0.000    8.210    0.823\n  hw =~                                                                 \n    hw10              1.000                               0.991    0.528\n    hw_8    (.p9.)    0.475    0.061    7.816    0.000    0.471    0.423\n  grades =~                                                             \n    eng_12            1.000                               2.522    0.926\n    math_12 (.11.)    0.900    0.024   38.175    0.000    2.269    0.830\n    sci_12  (.12.)    0.958    0.022   44.034    0.000    2.416    0.883\n    ss_12   (.13.)    1.061    0.022   48.518    0.000    2.675    0.901\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.283    0.031    9.132    0.000    0.595    0.595\n  grades ~                                                              \n    prevach           0.146    0.019    7.856    0.000    0.493    0.493\n    hw         (h)    0.548    0.121    4.519    0.000    0.216    0.216\n  hw ~                                                                  \n    prevach           0.076    0.015    5.030    0.000    0.655    0.655\n    famback           0.001    0.007    0.094    0.925    0.012    0.012\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.202    0.456    0.443    0.658    0.202    0.039\n .bytxmstd ~~                                                           \n   .math_12           2.427    0.616    3.941    0.000    2.427    0.302\n .bytxsstd ~~                                                           \n   .sci_12            0.342    0.553    0.619    0.536    0.342    0.047\n .bytxhstd ~~                                                           \n   .ss_12             1.079    0.570    1.892    0.059    1.079    0.148\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           43.085    1.425   30.228    0.000   43.085    1.826\n   .bypared           2.849    0.086   33.036    0.000    2.849    1.996\n   .byfaminc          8.757    0.166   52.837    0.000    8.757    3.192\n   .bytxrstd         48.537    0.600   80.939    0.000   48.537    4.890\n   .bytxmstd         49.808    0.612   81.404    0.000   49.808    4.918\n   .bytxsstd         47.959    0.610   78.584    0.000   47.959    4.747\n   .bytxhstd         48.160    0.602   79.937    0.000   48.160    4.829\n   .hw10              3.214    0.114   28.318    0.000    3.214    1.711\n   .hw_8              1.718    0.067   25.554    0.000    1.718    1.544\n   .eng_12            5.820    0.165   35.367    0.000    5.820    2.137\n   .math_12           5.385    0.165   32.611    0.000    5.385    1.970\n   .sci_12            5.590    0.165   33.832    0.000    5.590    2.044\n   .ss_12             5.895    0.179   32.880    0.000    5.895    1.986\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          234.497   27.663    8.477    0.000  234.497    0.421\n   .bypared           0.581    0.096    6.047    0.000    0.581    0.285\n   .byfaminc          4.539    0.442   10.279    0.000    4.539    0.603\n   .bytxrstd         25.812    2.954    8.738    0.000   25.812    0.262\n   .bytxmstd         27.801    3.130    8.882    0.000   27.801    0.271\n   .bytxsstd         32.520    3.429    9.484    0.000   32.520    0.319\n   .bytxhstd         32.045    3.363    9.527    0.000   32.045    0.322\n   .hw10              2.548    0.310    8.213    0.000    2.548    0.722\n   .hw_8              1.017    0.101   10.048    0.000    1.017    0.821\n   .eng_12            1.062    0.143    7.424    0.000    1.062    0.143\n   .math_12           2.320    0.228   10.158    0.000    2.320    0.311\n   .sci_12            1.644    0.179    9.177    0.000    1.644    0.220\n   .ss_12             1.651    0.192    8.606    0.000    1.651    0.187\n    famback         322.146   39.268    8.204    0.000    1.000    1.000\n   .prevach          46.966    5.417    8.671    0.000    0.646    0.646\n   .hw                0.552    0.226    2.444    0.015    0.562    0.562\n   .grades            3.626    0.380    9.533    0.000    0.570    0.570\n\n\nGroup 2 [white]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              14.631    0.735\n    bypared (.p2.)    0.067    0.003   20.086    0.000    0.984    0.818\n    byfamnc (.p3.)    0.096    0.006   17.489    0.000    1.409    0.617\n  prevach =~                                                            \n    bytxrst           1.000                               8.579    0.844\n    bytxmst (.p5.)    1.014    0.030   33.531    0.000    8.699    0.841\n    bytxsst (.p6.)    0.978    0.030   32.663    0.000    8.389    0.839\n    bytxhst (.p7.)    0.963    0.030   32.151    0.000    8.260    0.830\n  hw =~                                                                 \n    hw10              1.000                               1.159    0.606\n    hw_8    (.p9.)    0.475    0.061    7.816    0.000    0.551    0.484\n  grades =~                                                             \n    eng_12            1.000                               2.433    0.921\n    math_12 (.11.)    0.900    0.024   38.175    0.000    2.190    0.816\n    sci_12  (.12.)    0.958    0.022   44.034    0.000    2.331    0.875\n    ss_12   (.13.)    1.061    0.022   48.518    0.000    2.581    0.917\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.291    0.025   11.429    0.000    0.497    0.497\n  grades ~                                                              \n    prevach           0.154    0.012   12.970    0.000    0.543    0.543\n    hw         (h)    0.548    0.121    4.519    0.000    0.261    0.261\n  hw ~                                                                  \n    prevach           0.045    0.009    5.044    0.000    0.330    0.330\n    famback           0.019    0.005    3.546    0.000    0.240    0.240\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.851    0.289    2.944    0.003    0.851    0.152\n .bytxmstd ~~                                                           \n   .math_12           2.950    0.400    7.375    0.000    2.950    0.340\n .bytxsstd ~~                                                           \n   .sci_12            1.141    0.327    3.492    0.000    1.141    0.163\n .bytxhstd ~~                                                           \n   .ss_12             0.362    0.310    1.168    0.243    0.362    0.058\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           54.883    0.726   75.571    0.000   54.883    2.758\n   .bypared           3.335    0.044   75.949    0.000    3.335    2.771\n   .byfaminc         10.348    0.083  124.151    0.000   10.348    4.530\n   .bytxrstd         53.260    0.371  143.540    0.000   53.260    5.238\n   .bytxmstd         53.559    0.378  141.817    0.000   53.559    5.175\n   .bytxsstd         53.335    0.365  146.139    0.000   53.335    5.333\n   .bytxhstd         52.956    0.363  145.841    0.000   52.956    5.322\n   .hw10              3.443    0.070   49.350    0.000    3.443    1.801\n   .hw_8              1.733    0.042   41.712    0.000    1.733    1.522\n   .eng_12            6.409    0.096   66.521    0.000    6.409    2.427\n   .math_12           5.823    0.098   59.483    0.000    5.823    2.171\n   .sci_12            6.088    0.097   62.661    0.000    6.088    2.287\n   .ss_12             6.612    0.103   64.363    0.000    6.612    2.349\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          182.019   13.641   13.343    0.000  182.019    0.460\n   .bypared           0.480    0.050    9.631    0.000    0.480    0.332\n   .byfaminc          3.232    0.196   16.477    0.000    3.232    0.620\n   .bytxrstd         29.797    2.030   14.681    0.000   29.797    0.288\n   .bytxmstd         31.440    2.126   14.791    0.000   31.440    0.294\n   .bytxsstd         29.662    1.994   14.876    0.000   29.662    0.297\n   .bytxhstd         30.798    2.036   15.128    0.000   30.798    0.311\n   .hw10              2.312    0.223   10.360    0.000    2.312    0.632\n   .hw_8              0.993    0.067   14.829    0.000    0.993    0.766\n   .eng_12            1.052    0.084   12.505    0.000    1.052    0.151\n   .math_12           2.401    0.141   17.037    0.000    2.401    0.334\n   .sci_12            1.656    0.107   15.446    0.000    1.656    0.234\n   .ss_12             1.261    0.098   12.869    0.000    1.261    0.159\n    famback         214.077   19.121   11.196    0.000    1.000    1.000\n   .prevach          55.441    4.061   13.652    0.000    0.753    0.753\n   .hw                1.014    0.201    5.045    0.000    0.755    0.755\n   .grades            3.015    0.214   14.100    0.000    0.509    0.509\n\n\n\n\n# compare the two models\nlavTestLRT(sem_fit_mg2, sem_fit_mg3) |&gt; print()\n\n\nChi-Squared Difference Test\n\n             Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nsem_fit_mg2 121 67185 67614 238.78                                    \nsem_fit_mg3 122 67182 67606 236.98    -1.7954     0       1          1\n\n\n\n# constrain all the effects\nsem_fit_mg4 &lt;- sem(hw_model_mg,\n  data = hw_multigroup,\n  estimator = \"ML\",\n  group = \"group\",\n  group.equal = c(\"loadings\", \"regressions\")\n)\nsummary(sem_fit_mg4, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 283 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        96\n  Number of equality constraints                    14\n\n  Number of observations per group:                   \n    minority                                       274\n    white                                          751\n\nModel Test User Model:\n                                                      \n  Test statistic                               241.795\n  Degrees of freedom                               126\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    minority                                   108.582\n    white                                      133.213\n\nModel Test Baseline Model:\n\n  Test statistic                              8290.390\n  Degrees of freedom                               156\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.986\n  Tucker-Lewis Index (TLI)                       0.982\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33507.163\n  Loglikelihood unrestricted model (H1)     -33386.266\n                                                      \n  Akaike (AIC)                               67178.326\n  Bayesian (BIC)                             67582.787\n  Sample-size adjusted Bayesian (SABIC)      67322.346\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.042\n  90 Percent confidence interval - lower         0.034\n  90 Percent confidence interval - upper         0.050\n  P-value H_0: RMSEA &lt;= 0.050                    0.942\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.033\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [minority]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              17.852    0.758\n    bypared (.p2.)    0.067    0.003   20.053    0.000    1.201    0.844\n    byfamnc (.p3.)    0.096    0.006   17.465    0.000    1.717    0.627\n  prevach =~                                                            \n    bytxrst           1.000                               8.569    0.861\n    bytxmst (.p5.)    1.014    0.030   33.525    0.000    8.687    0.854\n    bytxsst (.p6.)    0.978    0.030   32.671    0.000    8.380    0.828\n    bytxhst (.p7.)    0.962    0.030   32.138    0.000    8.248    0.825\n  hw =~                                                                 \n    hw10              1.000                               1.008    0.540\n    hw_8    (.p9.)    0.460    0.060    7.647    0.000    0.463    0.414\n  grades =~                                                             \n    eng_12            1.000                               2.540    0.927\n    math_12 (.11.)    0.900    0.024   38.191    0.000    2.286    0.832\n    sci_12  (.12.)    0.958    0.022   44.053    0.000    2.433    0.884\n    ss_12   (.13.)    1.061    0.022   48.543    0.000    2.695    0.903\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback (.18.)    0.288    0.021   13.859    0.000    0.600    0.600\n  grades ~                                                              \n    prevach (.19.)    0.152    0.011   13.289    0.000    0.512    0.512\n    hw      (.20.)    0.548    0.122    4.485    0.000    0.217    0.217\n  hw ~                                                                  \n    prevach (.21.)    0.053    0.008    6.710    0.000    0.450    0.450\n    famback (.22.)    0.013    0.004    3.088    0.002    0.236    0.236\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.205    0.456    0.450    0.652    0.205    0.039\n .bytxmstd ~~                                                           \n   .math_12           2.437    0.618    3.942    0.000    2.437    0.302\n .bytxsstd ~~                                                           \n   .sci_12            0.358    0.552    0.648    0.517    0.358    0.049\n .bytxhstd ~~                                                           \n   .ss_12             1.061    0.570    1.862    0.063    1.061    0.146\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           43.085    1.423   30.270    0.000   43.085    1.829\n   .bypared           2.849    0.086   33.131    0.000    2.849    2.002\n   .byfaminc          8.757    0.166   52.887    0.000    8.757    3.195\n   .bytxrstd         48.537    0.601   80.719    0.000   48.537    4.876\n   .bytxmstd         49.808    0.615   81.054    0.000   49.808    4.897\n   .bytxsstd         47.959    0.612   78.401    0.000   47.959    4.736\n   .bytxhstd         48.160    0.604   79.727    0.000   48.160    4.816\n   .hw10              3.214    0.113   28.488    0.000    3.214    1.721\n   .hw_8              1.718    0.068   25.404    0.000    1.718    1.535\n   .eng_12            5.820    0.166   35.148    0.000    5.820    2.123\n   .math_12           5.385    0.166   32.448    0.000    5.385    1.960\n   .sci_12            5.590    0.166   33.635    0.000    5.590    2.032\n   .ss_12             5.895    0.180   32.692    0.000    5.895    1.975\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          236.422   27.644    8.553    0.000  236.422    0.426\n   .bypared           0.582    0.095    6.150    0.000    0.582    0.287\n   .byfaminc          4.563    0.443   10.306    0.000    4.563    0.607\n   .bytxrstd         25.638    2.949    8.694    0.000   25.638    0.259\n   .bytxmstd         28.001    3.152    8.883    0.000   28.001    0.271\n   .bytxsstd         32.304    3.418    9.451    0.000   32.304    0.315\n   .bytxhstd         31.959    3.362    9.505    0.000   31.959    0.320\n   .hw10              2.473    0.314    7.883    0.000    2.473    0.709\n   .hw_8              1.039    0.103   10.123    0.000    1.039    0.829\n   .eng_12            1.061    0.143    7.419    0.000    1.061    0.141\n   .math_12           2.322    0.228   10.161    0.000    2.322    0.308\n   .sci_12            1.648    0.179    9.184    0.000    1.648    0.218\n   .ss_12             1.648    0.192    8.601    0.000    1.648    0.185\n    famback         318.690   38.517    8.274    0.000    1.000    1.000\n   .prevach          46.956    5.362    8.757    0.000    0.639    0.639\n   .hw                0.624    0.237    2.631    0.009    0.615    0.615\n   .grades            3.608    0.379    9.517    0.000    0.559    0.559\n\n\nGroup 2 [white]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              14.660    0.736\n    bypared (.p2.)    0.067    0.003   20.053    0.000    0.987    0.819\n    byfamnc (.p3.)    0.096    0.006   17.465    0.000    1.410    0.617\n  prevach =~                                                            \n    bytxrst           1.000                               8.563    0.843\n    bytxmst (.p5.)    1.014    0.030   33.525    0.000    8.680    0.840\n    bytxsst (.p6.)    0.978    0.030   32.671    0.000    8.373    0.838\n    bytxhst (.p7.)    0.962    0.030   32.138    0.000    8.241    0.829\n  hw =~                                                                 \n    hw10              1.000                               1.174    0.613\n    hw_8    (.p9.)    0.460    0.060    7.647    0.000    0.540    0.475\n  grades =~                                                             \n    eng_12            1.000                               2.428    0.921\n    math_12 (.11.)    0.900    0.024   38.191    0.000    2.185    0.816\n    sci_12  (.12.)    0.958    0.022   44.053    0.000    2.326    0.875\n    ss_12   (.13.)    1.061    0.022   48.543    0.000    2.576    0.917\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback (.18.)    0.288    0.021   13.859    0.000    0.494    0.494\n  grades ~                                                              \n    prevach (.19.)    0.152    0.011   13.289    0.000    0.535    0.535\n    hw      (.20.)    0.548    0.122    4.485    0.000    0.265    0.265\n  hw ~                                                                  \n    prevach (.21.)    0.053    0.008    6.710    0.000    0.386    0.386\n    famback (.22.)    0.013    0.004    3.088    0.002    0.166    0.166\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.848    0.289    2.933    0.003    0.848    0.151\n .bytxmstd ~~                                                           \n   .math_12           2.946    0.400    7.373    0.000    2.946    0.339\n .bytxsstd ~~                                                           \n   .sci_12            1.144    0.327    3.502    0.000    1.144    0.163\n .bytxhstd ~~                                                           \n   .ss_12             0.369    0.310    1.187    0.235    0.369    0.059\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           54.883    0.727   75.536    0.000   54.883    2.756\n   .bypared           3.335    0.044   75.852    0.000    3.335    2.768\n   .byfaminc         10.348    0.083  124.116    0.000   10.348    4.529\n   .bytxrstd         53.260    0.371  143.714    0.000   53.260    5.244\n   .bytxmstd         53.559    0.377  142.084    0.000   53.559    5.185\n   .bytxsstd         53.335    0.365  146.302    0.000   53.335    5.339\n   .bytxhstd         52.956    0.363  146.011    0.000   52.956    5.328\n   .hw10              3.443    0.070   49.266    0.000    3.443    1.798\n   .hw_8              1.733    0.041   41.802    0.000    1.733    1.525\n   .eng_12            6.409    0.096   66.645    0.000    6.409    2.432\n   .math_12           5.823    0.098   59.578    0.000    5.823    2.174\n   .sci_12            6.088    0.097   62.768    0.000    6.088    2.290\n   .ss_12             6.612    0.103   64.482    0.000    6.612    2.353\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          181.557   13.673   13.279    0.000  181.557    0.458\n   .bypared           0.478    0.050    9.557    0.000    0.478    0.329\n   .byfaminc          3.232    0.196   16.467    0.000    3.232    0.619\n   .bytxrstd         29.827    2.030   14.693    0.000   29.827    0.289\n   .bytxmstd         31.370    2.121   14.790    0.000   31.370    0.294\n   .bytxsstd         29.695    1.995   14.887    0.000   29.695    0.298\n   .bytxhstd         30.875    2.038   15.149    0.000   30.875    0.313\n   .hw10              2.290    0.229   10.003    0.000    2.290    0.624\n   .hw_8              0.999    0.066   15.027    0.000    0.999    0.774\n   .eng_12            1.052    0.084   12.502    0.000    1.052    0.151\n   .math_12           2.401    0.141   17.038    0.000    2.401    0.335\n   .sci_12            1.656    0.107   15.448    0.000    1.656    0.234\n   .ss_12             1.262    0.098   12.872    0.000    1.262    0.160\n    famback         214.915   19.108   11.247    0.000    1.000    1.000\n   .prevach          55.460    4.038   13.734    0.000    0.756    0.756\n   .hw                1.048    0.209    5.005    0.000    0.760    0.760\n   .grades            3.011    0.214   14.059    0.000    0.511    0.511\n\n\n\n\n# compare the two models\nlavTestLRT(sem_fit_mg3, sem_fit_mg4) |&gt; print()\nlavTestLRT(sem_fit_mg2, sem_fit_mg4) |&gt; print()\n\n\nChi-Squared Difference Test\n\n             Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)\nsem_fit_mg3 122 67182 67606 236.98                                       \nsem_fit_mg4 126 67178 67583 241.79     4.8107 0.019887       4     0.3073\n\nChi-Squared Difference Test\n\n             Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nsem_fit_mg2 121 67185 67614 238.78                                    \nsem_fit_mg4 126 67178 67583 241.79     3.0153     0       5     0.6976\n\n\n\n# constrain all the effects & errors\nhw_model_mg5 &lt;- \"\n  famback =~ parocc + bypared + byfaminc\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw10 + hw_8\n  grades =~ eng_12 + math_12 + sci_12 + ss_12\n\n  bytxrstd ~~ c(en, en)*eng_12\n  bytxmstd ~~ c(ma, ma)*math_12\n  bytxsstd ~~ c(sc, sc)*sci_12\n  bytxhstd ~~ c(ss, ss)*ss_12\n\n  prevach ~ famback\n  grades ~ prevach + hw\n  hw ~ prevach + famback\n\"\n\nsem_fit_mg5 &lt;- sem(hw_model_mg5,\n  data = hw_multigroup,\n  estimator = \"ML\",\n  group = \"group\",\n  group.equal = c(\n    \"loadings\", \"regressions\",\n    \"residuals\", \"lv.variances\"\n  )\n)\n\nsummary(sem_fit_mg5, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 184 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        96\n  Number of equality constraints                    35\n\n  Number of observations per group:                   \n    minority                                       274\n    white                                          751\n\nModel Test User Model:\n                                                      \n  Test statistic                               289.323\n  Degrees of freedom                               147\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    minority                                   141.348\n    white                                      147.975\n\nModel Test Baseline Model:\n\n  Test statistic                              8290.390\n  Degrees of freedom                               156\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.983\n  Tucker-Lewis Index (TLI)                       0.981\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33530.927\n  Loglikelihood unrestricted model (H1)     -33386.266\n                                                      \n  Akaike (AIC)                               67183.854\n  Bayesian (BIC)                             67484.733\n  Sample-size adjusted Bayesian (SABIC)      67290.991\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.043\n  90 Percent confidence interval - lower         0.036\n  90 Percent confidence interval - upper         0.051\n  P-value H_0: RMSEA &lt;= 0.050                    0.927\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.051\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [minority]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              15.679    0.747\n    bypared (.p2.)    0.067    0.003   20.248    0.000    1.043    0.824\n    byfamnc (.p3.)    0.097    0.005   17.694    0.000    1.514    0.625\n  prevach =~                                                            \n    bytxrst           1.000                               8.554    0.847\n    bytxmst (.p5.)    1.013    0.030   33.355    0.000    8.665    0.843\n    bytxsst (.p6.)    0.980    0.030   32.601    0.000    8.385    0.836\n    bytxhst (.p7.)    0.963    0.030   32.072    0.000    8.239    0.828\n  hw =~                                                                 \n    hw10              1.000                               1.130    0.594\n    hw_8    (.p9.)    0.457    0.060    7.599    0.000    0.516    0.456\n  grades =~                                                             \n    eng_12            1.000                               2.459    0.923\n    math_12 (.11.)    0.899    0.024   38.183    0.000    2.211    0.820\n    sci_12  (.12.)    0.958    0.022   44.034    0.000    2.356    0.878\n    ss_12   (.13.)    1.060    0.022   48.459    0.000    2.608    0.912\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback (.18.)    0.287    0.021   13.854    0.000    0.526    0.526\n  grades ~                                                              \n    prevach (.19.)    0.150    0.012   12.896    0.000    0.523    0.523\n    hw      (.20.)    0.570    0.127    4.495    0.000    0.262    0.262\n  hw ~                                                                  \n    prevach (.21.)    0.052    0.008    6.619    0.000    0.395    0.395\n    famback (.22.)    0.014    0.004    3.169    0.002    0.190    0.190\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12    (en)    0.692    0.245    2.825    0.005    0.692    0.126\n .bytxmstd ~~                                                           \n   .math_12   (ma)    2.813    0.337    8.356    0.000    2.813    0.330\n .bytxsstd ~~                                                           \n   .sci_12    (sc)    0.941    0.281    3.344    0.001    0.941    0.133\n .bytxhstd ~~                                                           \n   .ss_12     (ss)    0.543    0.274    1.982    0.048    0.543    0.083\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           43.085    1.267   33.997    0.000   43.085    2.054\n   .bypared           2.849    0.076   37.273    0.000    2.849    2.252\n   .byfaminc          8.757    0.146   59.809    0.000    8.757    3.613\n   .bytxrstd         48.537    0.610   79.598    0.000   48.537    4.809\n   .bytxmstd         49.808    0.621   80.194    0.000   49.808    4.845\n   .bytxsstd         47.959    0.606   79.127    0.000   47.959    4.780\n   .bytxhstd         48.160    0.601   80.108    0.000   48.160    4.840\n   .hw10              3.214    0.115   27.973    0.000    3.214    1.690\n   .hw_8              1.718    0.068   25.130    0.000    1.718    1.518\n   .eng_12            5.820    0.161   36.152    0.000    5.820    2.184\n   .math_12           5.385    0.163   33.072    0.000    5.385    1.998\n   .sci_12            5.590    0.162   34.475    0.000    5.590    2.083\n   .ss_12             5.895    0.173   34.146    0.000    5.895    2.063\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc  (.23.)  194.226   13.021   14.916    0.000  194.226    0.441\n   .bypared (.24.)    0.513    0.048   10.687    0.000    0.513    0.320\n   .byfamnc (.25.)    3.580    0.188   19.087    0.000    3.580    0.609\n   .bytxrst (.26.)   28.719    1.704   16.853    0.000   28.719    0.282\n   .bytxmst (.27.)   30.622    1.793   17.074    0.000   30.622    0.290\n   .bytxsst (.28.)   30.349    1.743   17.410    0.000   30.349    0.302\n   .bytxhst (.29.)   31.142    1.761   17.689    0.000   31.142    0.314\n   .hw10    (.30.)    2.341    0.201   11.627    0.000    2.341    0.647\n   .hw_8    (.31.)    1.015    0.058   17.575    0.000    1.015    0.792\n   .eng_12  (.32.)    1.053    0.074   14.287    0.000    1.053    0.148\n   .math_12 (.33.)    2.376    0.120   19.758    0.000    2.376    0.327\n   .sci_12  (.34.)    1.653    0.093   17.821    0.000    1.653    0.229\n   .ss_12   (.35.)    1.368    0.089   15.354    0.000    1.368    0.167\n    famback (.36.)  245.842   20.005   12.289    0.000    1.000    1.000\n   .prevach (.37.)   52.921    3.490   15.164    0.000    0.723    0.723\n   .hw      (.38.)    0.931    0.182    5.109    0.000    0.729    0.729\n   .grades  (.39.)    3.161    0.198   15.985    0.000    0.523    0.523\n\n\nGroup 2 [white]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              15.679    0.747\n    bypared (.p2.)    0.067    0.003   20.248    0.000    1.043    0.824\n    byfamnc (.p3.)    0.097    0.005   17.694    0.000    1.514    0.625\n  prevach =~                                                            \n    bytxrst           1.000                               8.554    0.847\n    bytxmst (.p5.)    1.013    0.030   33.355    0.000    8.665    0.843\n    bytxsst (.p6.)    0.980    0.030   32.601    0.000    8.385    0.836\n    bytxhst (.p7.)    0.963    0.030   32.072    0.000    8.239    0.828\n  hw =~                                                                 \n    hw10              1.000                               1.130    0.594\n    hw_8    (.p9.)    0.457    0.060    7.599    0.000    0.516    0.456\n  grades =~                                                             \n    eng_12            1.000                               2.459    0.923\n    math_12 (.11.)    0.899    0.024   38.183    0.000    2.211    0.820\n    sci_12  (.12.)    0.958    0.022   44.034    0.000    2.356    0.878\n    ss_12   (.13.)    1.060    0.022   48.459    0.000    2.608    0.912\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback (.18.)    0.287    0.021   13.854    0.000    0.526    0.526\n  grades ~                                                              \n    prevach (.19.)    0.150    0.012   12.896    0.000    0.523    0.523\n    hw      (.20.)    0.570    0.127    4.495    0.000    0.262    0.262\n  hw ~                                                                  \n    prevach (.21.)    0.052    0.008    6.619    0.000    0.395    0.395\n    famback (.22.)    0.014    0.004    3.169    0.002    0.190    0.190\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12    (en)    0.692    0.245    2.825    0.005    0.692    0.126\n .bytxmstd ~~                                                           \n   .math_12   (ma)    2.813    0.337    8.356    0.000    2.813    0.330\n .bytxsstd ~~                                                           \n   .sci_12    (sc)    0.941    0.281    3.344    0.001    0.941    0.133\n .bytxhstd ~~                                                           \n   .ss_12     (ss)    0.543    0.274    1.982    0.048    0.543    0.083\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           54.883    0.765   71.697    0.000   54.883    2.616\n   .bypared           3.335    0.046   72.240    0.000    3.335    2.636\n   .byfaminc         10.348    0.088  117.008    0.000   10.348    4.270\n   .bytxrstd         53.260    0.368  144.601    0.000   53.260    5.277\n   .bytxmstd         53.559    0.375  142.766    0.000   53.559    5.210\n   .bytxsstd         53.335    0.366  145.684    0.000   53.335    5.316\n   .bytxhstd         52.956    0.363  145.831    0.000   52.956    5.321\n   .hw10              3.443    0.069   49.606    0.000    3.443    1.810\n   .hw_8              1.733    0.041   41.954    0.000    1.733    1.531\n   .eng_12            6.409    0.097   65.911    0.000    6.409    2.405\n   .math_12           5.823    0.098   59.205    0.000    5.823    2.160\n   .sci_12            6.088    0.098   62.161    0.000    6.088    2.268\n   .ss_12             6.612    0.104   63.401    0.000    6.612    2.314\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc  (.23.)  194.226   13.021   14.916    0.000  194.226    0.441\n   .bypared (.24.)    0.513    0.048   10.687    0.000    0.513    0.320\n   .byfamnc (.25.)    3.580    0.188   19.087    0.000    3.580    0.609\n   .bytxrst (.26.)   28.719    1.704   16.853    0.000   28.719    0.282\n   .bytxmst (.27.)   30.622    1.793   17.074    0.000   30.622    0.290\n   .bytxsst (.28.)   30.349    1.743   17.410    0.000   30.349    0.302\n   .bytxhst (.29.)   31.142    1.761   17.689    0.000   31.142    0.314\n   .hw10    (.30.)    2.341    0.201   11.627    0.000    2.341    0.647\n   .hw_8    (.31.)    1.015    0.058   17.575    0.000    1.015    0.792\n   .eng_12  (.32.)    1.053    0.074   14.287    0.000    1.053    0.148\n   .math_12 (.33.)    2.376    0.120   19.758    0.000    2.376    0.327\n   .sci_12  (.34.)    1.653    0.093   17.821    0.000    1.653    0.229\n   .ss_12   (.35.)    1.368    0.089   15.354    0.000    1.368    0.167\n    famback (.36.)  245.842   20.005   12.289    0.000    1.000    1.000\n   .prevach (.37.)   52.921    3.490   15.164    0.000    0.723    0.723\n   .hw      (.38.)    0.931    0.182    5.109    0.000    0.729    0.729\n   .grades  (.39.)    3.161    0.198   15.985    0.000    0.523    0.523\n\n\n\n\n# compare the two models\nlavTestLRT(sem_fit_mg4, sem_fit_mg5) |&gt; print()\n\n\nChi-Squared Difference Test\n\n             Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)    \nsem_fit_mg4 126 67178 67583 241.79                                           \nsem_fit_mg5 147 67184 67485 289.32     47.528 0.049647      21  0.0007971 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Keith's",
      "Multigroup Models"
    ]
  },
  {
    "objectID": "contents/chap18.html#multigroup-models",
    "href": "contents/chap18.html#multigroup-models",
    "title": "Chapter 18. Multigroup Models, Panel Models, Dangers and Assumptions",
    "section": "Multigroup Models",
    "text": "Multigroup Models\n카테고리 변수의 레벨에 따라 잠재변수의 효과가 다른지에 대한 검증; 상호작용 효과\n연속인 (잠재)변수 간의 상호작용에 대해서는 22장에서 다룸\n \nEstimation\n그룹별 공분산 행렬로부터 얻어지는 fit function들의 (표본 수) 가중치 평균을 fit function으로 사용\nML estimation의 경우\n\n각 그룹 j에 대해, \\(F_{ML}^j = log|\\Sigma(\\theta)| + tr(S\\Sigma^{-1}(\\theta)) - log|S| - (p + q)\\)\n\\(\\displaystyle F_{ML} = \\sum_j \\frac{N_j}{N} F_{ML}^j\\)\n\n\n\n\nLoad the dataset\n# generate a dataset with the same covariance matrix\nlibrary(readxl)\nhw_mg_minor &lt;- read_xls(\"data/chap 18 latent var SEM 2/minority matrix.xls\")\nhw_mg_white &lt;- read_xls(\"data/chap 18 latent var SEM 2/white matrix.xls\")\n\n# For minority group\nhw_mg_minor_cov &lt;- hw_mg_minor[c(2:14), c(3:15)] |&gt;\n  as.matrix() |&gt;\n  lav_matrix_vechr(diagonal = TRUE) |&gt;\n  getCov(names = hw_mg_minor$varname_[2:14], sds = hw_mg_minor[16, 3:15] |&gt; as.double())\ncolnames(hw_mg_minor_cov) &lt;- tolower(colnames(hw_mg_minor_cov))\nrownames(hw_mg_minor_cov) &lt;- tolower(rownames(hw_mg_minor_cov))\nmean_minor &lt;- hw_mg_minor[15, 3:15] |&gt; as.double()\n\n# For white group\nhw_mg_white_cov &lt;- hw_mg_white[c(2:14), c(3:15)] |&gt;\n  as.matrix() |&gt;\n  lav_matrix_vechr(diagonal = TRUE) |&gt;\n  getCov(names = hw_mg_white$varname_[2:14], sds = hw_mg_white[16, 3:15] |&gt; as.double())\ncolnames(hw_mg_white_cov) &lt;- tolower(colnames(hw_mg_white_cov))\nrownames(hw_mg_white_cov) &lt;- tolower(rownames(hw_mg_white_cov))\nmean_white &lt;- hw_mg_white[15, 3:15] |&gt; as.double()\n\n# simulate the data\nset.seed(123) # for reproducibility\nhw_sim_minor &lt;- semTools::kd(hw_mg_minor_cov, n = 274, type = \"exact\") |&gt;\n  sweep(2, mean_minor, FUN = \"+\")\nhw_sim_white &lt;- semTools::kd(hw_mg_white_cov, n = 751, type = \"exact\") |&gt;\n  sweep(2, mean_white, FUN = \"+\")\n\nset.seed(131)\nhw_multigroup &lt;- bind_rows(\n  hw_sim_minor |&gt; mutate(group = \"minority\"),\n  hw_sim_white |&gt; mutate(group = \"white\")\n) |&gt; sample_n(900)\n\n\n\nhw_multigroup |&gt; head() |&gt; print()\n\n   bypared  byfaminc   parocc bytxrstd bytxmstd bytxsstd bytxhstd     hw_8     hw10\n1 3.477102 10.665843 38.43281 46.96390 57.89953 50.71309 45.52944 2.879256 6.703532\n2 3.298204 10.055041 46.10124 63.11553 65.43042 46.32358 52.61707 2.718312 5.034043\n3 3.392463  9.906801 48.69503 61.66536 62.68831 67.51536 62.09874 2.149325 5.855915\n4 2.522122  8.551281 49.65279 56.15432 54.94386 44.65248 47.67488 2.947018 5.570204\n5 5.575341 10.891556 64.23261 58.78616 63.80637 57.38648 60.86180 3.403386 4.031776\n6 5.192402 10.667251 68.82314 56.87892 54.70169 54.10558 51.51500 5.503236 3.788413\n     eng_12  math_12   sci_12     ss_12    group\n1  8.661664 7.910145 7.247774  8.123688    white\n2  6.632333 4.591556 6.289245  7.601705 minority\n3  9.070309 5.404964 9.544480  8.503456 minority\n4 10.935536 7.104243 9.076028 11.466477    white\n5  5.652511 2.516713 3.941772  8.302535 minority\n6  7.828620 4.378463 6.640992  8.810365 minority\n\n\n\n\n\n\n\n\nUsing covariance matrices\n\n\n\nhw_fit_mg &lt;- sem(hw_model_mg, \n  sample.cov = list(hw_mg_white_cov, hw_mg_minor_cov),\n  sample.mean = list(mean_white, mean_minor),\n  sample.nobs = list(751, 274))\n\n\nSimulated data (N=900)를 이용\n\n# Fit the model\nhw_model_mg &lt;- \"\n  famback =~ parocc + bypared + byfaminc\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw10 + hw_8\n  grades =~ eng_12 + math_12 + sci_12 + ss_12\n\n  bytxrstd ~~ eng_12\n  bytxmstd ~~ math_12\n  bytxsstd ~~ sci_12\n  bytxhstd ~~ ss_12\n\n  prevach ~ famback\n  grades ~ prevach + hw\n  hw ~ prevach + famback\n\"\nhw_fit_mg &lt;- sem(hw_model_mg, data = hw_multigroup, group = \"group\")\nsummary(hw_fit_mg, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 402 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        96\n\n  Number of observations per group:                   \n    white                                          665\n    minority                                       235\n\nModel Test User Model:\n                                                      \n  Test statistic                               194.017\n  Degrees of freedom                               112\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    white                                      116.568\n    minority                                    77.449\n\nModel Test Baseline Model:\n\n  Test statistic                              7292.337\n  Degrees of freedom                               156\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.989\n  Tucker-Lewis Index (TLI)                       0.984\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -29423.810\n  Loglikelihood unrestricted model (H1)     -29326.802\n                                                      \n  Akaike (AIC)                               59039.621\n  Bayesian (BIC)                             59500.651\n  Sample-size adjusted Bayesian (SABIC)      59195.771\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.040\n  90 Percent confidence interval - lower         0.031\n  90 Percent confidence interval - upper         0.050\n  P-value H_0: RMSEA &lt;= 0.050                    0.955\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [white]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              14.608    0.727\n    bypared           0.071    0.004   15.757    0.000    1.035    0.842\n    byfaminc          0.096    0.007   13.683    0.000    1.397    0.605\n  prevach =~                                                            \n    bytxrstd          1.000                               8.625    0.849\n    bytxmstd          0.986    0.037   26.563    0.000    8.509    0.834\n    bytxsstd          0.967    0.037   26.491    0.000    8.342    0.837\n    bytxhstd          0.960    0.037   26.134    0.000    8.278    0.834\n  hw =~                                                                 \n    hw10              1.000                               1.278    0.660\n    hw_8              0.381    0.068    5.606    0.000    0.487    0.433\n  grades =~                                                             \n    eng_12            1.000                               2.454    0.924\n    math_12           0.888    0.030   29.950    0.000    2.179    0.808\n    sci_12            0.967    0.027   35.958    0.000    2.373    0.879\n    ss_12             1.062    0.027   39.818    0.000    2.606    0.918\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.297    0.028   10.658    0.000    0.504    0.504\n  grades ~                                                              \n    prevach           0.161    0.012   12.909    0.000    0.567    0.567\n    hw                0.444    0.120    3.686    0.000    0.231    0.231\n  hw ~                                                                  \n    prevach           0.042    0.010    4.237    0.000    0.285    0.285\n    famback           0.023    0.006    3.669    0.000    0.259    0.259\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.862    0.303    2.842    0.004    0.862    0.158\n .bytxmstd ~~                                                           \n   .math_12           3.018    0.432    6.985    0.000    3.018    0.337\n .bytxsstd ~~                                                           \n   .sci_12            1.349    0.348    3.876    0.000    1.349    0.193\n .bytxhstd ~~                                                           \n   .ss_12             0.094    0.328    0.288    0.774    0.094    0.015\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           54.813    0.780   70.297    0.000   54.813    2.726\n   .bypared           3.321    0.048   69.655    0.000    3.321    2.701\n   .byfaminc         10.328    0.090  115.341    0.000   10.328    4.473\n   .bytxrstd         53.051    0.394  134.605    0.000   53.051    5.220\n   .bytxmstd         53.420    0.396  135.035    0.000   53.420    5.236\n   .bytxsstd         53.243    0.386  137.807    0.000   53.243    5.344\n   .bytxhstd         52.914    0.385  137.469    0.000   52.914    5.331\n   .hw10              3.418    0.075   45.506    0.000    3.418    1.765\n   .hw_8              1.723    0.044   39.523    0.000    1.723    1.533\n   .eng_12            6.410    0.103   62.243    0.000    6.410    2.414\n   .math_12           5.793    0.105   55.391    0.000    5.793    2.148\n   .sci_12            6.089    0.105   58.201    0.000    6.089    2.257\n   .ss_12             6.621    0.110   60.106    0.000    6.621    2.331\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          190.906   15.208   12.553    0.000  190.906    0.472\n   .bypared           0.441    0.059    7.501    0.000    0.441    0.292\n   .byfaminc          3.379    0.216   15.625    0.000    3.379    0.634\n   .bytxrstd         28.897    2.135   13.538    0.000   28.897    0.280\n   .bytxmstd         31.677    2.252   14.066    0.000   31.677    0.304\n   .bytxsstd         29.676    2.122   13.983    0.000   29.676    0.299\n   .bytxhstd         30.003    2.141   14.011    0.000   30.003    0.305\n   .hw10              2.119    0.310    6.835    0.000    2.119    0.565\n   .hw_8              1.027    0.071   14.548    0.000    1.027    0.812\n   .eng_12            1.031    0.090   11.518    0.000    1.031    0.146\n   .math_12           2.526    0.156   16.171    0.000    2.526    0.347\n   .sci_12            1.648    0.115   14.386    0.000    1.648    0.226\n   .ss_12             1.275    0.106   12.025    0.000    1.275    0.158\n    famback         213.400   22.443    9.509    0.000    1.000    1.000\n   .prevach          55.537    4.493   12.361    0.000    0.746    0.746\n   .hw                1.269    0.301    4.213    0.000    0.777    0.777\n   .grades            3.107    0.232   13.370    0.000    0.516    0.516\n\n\nGroup 2 [minority]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              18.455    0.788\n    bypared           0.057    0.005   10.629    0.000    1.054    0.792\n    byfaminc          0.105    0.011    9.719    0.000    1.941    0.690\n  prevach =~                                                            \n    bytxrstd          1.000                               8.852    0.877\n    bytxmstd          1.022    0.058   17.572    0.000    9.046    0.864\n    bytxsstd          0.897    0.058   15.372    0.000    7.936    0.803\n    bytxhstd          0.883    0.057   15.505    0.000    7.818    0.804\n  hw =~                                                                 \n    hw10              1.000                               0.802    0.442\n    hw_8              0.731    0.181    4.045    0.000    0.586    0.512\n  grades =~                                                             \n    eng_12            1.000                               2.555    0.936\n    math_12           0.905    0.047   19.150    0.000    2.312    0.831\n    sci_12            0.909    0.042   21.428    0.000    2.322    0.875\n    ss_12             1.037    0.045   22.915    0.000    2.649    0.895\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.281    0.037    7.662    0.000    0.587    0.587\n  grades ~                                                              \n    prevach           0.131    0.042    3.099    0.002    0.455    0.455\n    hw                0.768    0.603    1.272    0.203    0.241    0.241\n  hw ~                                                                  \n    prevach           0.062    0.015    4.044    0.000    0.681    0.681\n    famback           0.001    0.006    0.225    0.822    0.031    0.031\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.257    0.474    0.543    0.587    0.257    0.055\n .bytxmstd ~~                                                           \n   .math_12           2.350    0.683    3.441    0.001    2.350    0.288\n .bytxsstd ~~                                                           \n   .sci_12            0.287    0.600    0.478    0.632    0.287    0.038\n .bytxhstd ~~                                                           \n   .ss_12             1.420    0.629    2.257    0.024    1.420    0.186\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           43.630    1.528   28.555    0.000   43.630    1.863\n   .bypared           2.823    0.087   32.534    0.000    2.823    2.122\n   .byfaminc          8.887    0.183   48.462    0.000    8.887    3.161\n   .bytxrstd         48.458    0.659   73.578    0.000   48.458    4.800\n   .bytxmstd         49.773    0.683   72.871    0.000   49.773    4.754\n   .bytxsstd         48.064    0.645   74.522    0.000   48.064    4.861\n   .bytxhstd         48.249    0.634   76.060    0.000   48.249    4.962\n   .hw10              3.207    0.118   27.095    0.000    3.207    1.768\n   .hw_8              1.731    0.075   23.172    0.000    1.731    1.512\n   .eng_12            5.785    0.178   32.494    0.000    5.785    2.120\n   .math_12           5.439    0.182   29.961    0.000    5.439    1.954\n   .sci_12            5.620    0.173   32.455    0.000    5.620    2.117\n   .ss_12             5.939    0.193   30.769    0.000    5.939    2.007\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          208.041   31.044    6.701    0.000  208.041    0.379\n   .bypared           0.658    0.100    6.583    0.000    0.658    0.372\n   .byfaminc          4.137    0.480    8.621    0.000    4.137    0.523\n   .bytxrstd         23.579    3.192    7.387    0.000   23.579    0.231\n   .bytxmstd         27.811    3.569    7.793    0.000   27.811    0.254\n   .bytxsstd         34.775    3.867    8.994    0.000   34.775    0.356\n   .bytxhstd         33.441    3.720    8.988    0.000   33.441    0.354\n   .hw10              2.649    0.305    8.677    0.000    2.649    0.805\n   .hw_8              0.967    0.132    7.306    0.000    0.967    0.738\n   .eng_12            0.919    0.152    6.041    0.000    0.919    0.123\n   .math_12           2.400    0.256    9.370    0.000    2.400    0.310\n   .sci_12            1.657    0.192    8.631    0.000    1.657    0.235\n   .ss_12             1.739    0.216    8.063    0.000    1.739    0.199\n    famback         340.576   52.807    6.449    0.000    1.000    1.000\n   .prevach          51.378    6.931    7.413    0.000    0.656    0.656\n   .hw                0.329    0.187    1.762    0.078    0.511    0.511\n   .grades            3.794    0.484    7.841    0.000    0.581    0.581\n\n\n\n\nMeasurement Constraints\n두 그룹에게 잠재변수가 동일한 의미를 가지는가?; 20장에서 자세히 다룸\n그룹 간에 잠재변수/측정모형의 동일성(invariance)을 검정하기 위해 factor loadings를 그룹 간에 동일하게 제약; (weak invariance)\n제약에 대한 lavaan 문법: lavaan 문서\nHS.model &lt;- ' visual  =~ x1 + x2 + c(v3,v3)*x3\n              textual =~ x4 + x5 + x6\n              speed   =~ x7 + x8 + x9 '\n또는 다음 예에서 처럼 group.equal = c(\"loadings\") 키워드를 이용\n결과표에서 .p1, .p2, …으로 동일하게 제약된 factor loadings를 확인할 수 있음\n\n# contrains factor loadings\nhw_fit_mg2 &lt;- sem(hw_model_mg,\n  data = hw_multigroup,\n  group = \"group\",\n  group.equal = c(\"loadings\")\n)\nsummary(hw_fit_mg2, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 401 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        96\n  Number of equality constraints                     9\n\n  Number of observations per group:                   \n    white                                          665\n    minority                                       235\n\nModel Test User Model:\n                                                      \n  Test statistic                               209.165\n  Degrees of freedom                               121\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    white                                      120.642\n    minority                                    88.523\n\nModel Test Baseline Model:\n\n  Test statistic                              7292.337\n  Degrees of freedom                               156\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.988\n  Tucker-Lewis Index (TLI)                       0.984\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -29431.384\n  Loglikelihood unrestricted model (H1)     -29326.802\n                                                      \n  Akaike (AIC)                               59036.769\n  Bayesian (BIC)                             59454.577\n  Sample-size adjusted Bayesian (SABIC)      59178.279\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.040\n  90 Percent confidence interval - lower         0.031\n  90 Percent confidence interval - upper         0.049\n  P-value H_0: RMSEA &lt;= 0.050                    0.963\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.033\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [white]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              15.002    0.741\n    bypared (.p2.)    0.066    0.003   19.026    0.000    0.996    0.819\n    byfamnc (.p3.)    0.098    0.006   16.614    0.000    1.468    0.628\n  prevach =~                                                            \n    bytxrst           1.000                               8.691    0.851\n    bytxmst (.p5.)    0.995    0.031   31.773    0.000    8.648    0.840\n    bytxsst (.p6.)    0.948    0.031   30.716    0.000    8.235    0.833\n    bytxhst (.p7.)    0.939    0.031   30.463    0.000    8.159    0.829\n  hw =~                                                                 \n    hw10              1.000                               1.181    0.613\n    hw_8    (.p9.)    0.454    0.064    7.059    0.000    0.536    0.473\n  grades =~                                                             \n    eng_12            1.000                               2.467    0.925\n    math_12 (.11.)    0.893    0.025   35.559    0.000    2.202    0.811\n    sci_12  (.12.)    0.950    0.023   41.891    0.000    2.344    0.876\n    ss_12   (.13.)    1.056    0.023   46.042    0.000    2.604    0.917\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.293    0.027   11.005    0.000    0.505    0.505\n  grades ~                                                              \n    prevach           0.160    0.012   12.855    0.000    0.564    0.564\n    hw                0.480    0.128    3.762    0.000    0.230    0.230\n  hw ~                                                                  \n    prevach           0.041    0.009    4.335    0.000    0.302    0.302\n    famback           0.021    0.006    3.600    0.000    0.263    0.263\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.854    0.304    2.813    0.005    0.854    0.157\n .bytxmstd ~~                                                           \n   .math_12           2.997    0.432    6.941    0.000    2.997    0.338\n .bytxsstd ~~                                                           \n   .sci_12            1.351    0.348    3.884    0.000    1.351    0.191\n .bytxhstd ~~                                                           \n   .ss_12             0.117    0.328    0.357    0.721    0.117    0.019\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           54.813    0.785   69.859    0.000   54.813    2.709\n   .bypared           3.321    0.047   70.439    0.000    3.321    2.732\n   .byfaminc         10.328    0.091  113.892    0.000   10.328    4.417\n   .bytxrstd         53.051    0.396  133.951    0.000   53.051    5.194\n   .bytxmstd         53.420    0.399  133.769    0.000   53.420    5.187\n   .bytxsstd         53.243    0.384  138.823    0.000   53.243    5.383\n   .bytxhstd         52.914    0.382  138.555    0.000   52.914    5.373\n   .hw10              3.418    0.075   45.757    0.000    3.418    1.774\n   .hw_8              1.723    0.044   39.250    0.000    1.723    1.522\n   .eng_12            6.410    0.103   61.994    0.000    6.410    2.404\n   .math_12           5.793    0.105   55.031    0.000    5.793    2.134\n   .sci_12            6.089    0.104   58.703    0.000    6.089    2.276\n   .ss_12             6.621    0.110   60.139    0.000    6.621    2.332\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          184.342   14.742   12.505    0.000  184.342    0.450\n   .bypared           0.486    0.053    9.178    0.000    0.486    0.329\n   .byfaminc          3.312    0.215   15.409    0.000    3.312    0.606\n   .bytxrstd         28.778    2.122   13.559    0.000   28.778    0.276\n   .bytxmstd         31.265    2.239   13.962    0.000   31.265    0.295\n   .bytxsstd         29.999    2.109   14.224    0.000   29.999    0.307\n   .bytxhstd         30.411    2.129   14.283    0.000   30.411    0.314\n   .hw10              2.317    0.251    9.215    0.000    2.317    0.624\n   .hw_8              0.994    0.071   14.040    0.000    0.994    0.776\n   .eng_12            1.023    0.089   11.512    0.000    1.023    0.144\n   .math_12           2.519    0.156   16.154    0.000    2.519    0.342\n   .sci_12            1.659    0.114   14.572    0.000    1.659    0.232\n   .ss_12             1.277    0.105   12.145    0.000    1.277    0.158\n    famback         225.055   21.210   10.611    0.000    1.000    1.000\n   .prevach          56.253    4.348   12.938    0.000    0.745    0.745\n   .hw                1.059    0.229    4.621    0.000    0.759    0.759\n   .grades            3.140    0.230   13.622    0.000    0.516    0.516\n\n\nGroup 2 [minority]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              17.351    0.753\n    bypared (.p2.)    0.066    0.003   19.026    0.000    1.152    0.841\n    byfamnc (.p3.)    0.098    0.006   16.614    0.000    1.698    0.625\n  prevach =~                                                            \n    bytxrst           1.000                               8.701    0.872\n    bytxmst (.p5.)    0.995    0.031   31.773    0.000    8.658    0.847\n    bytxsst (.p6.)    0.948    0.031   30.716    0.000    8.245    0.817\n    bytxhst (.p7.)    0.939    0.031   30.463    0.000    8.169    0.821\n  hw =~                                                                 \n    hw10              1.000                               0.979    0.530\n    hw_8    (.p9.)    0.454    0.064    7.059    0.000    0.444    0.396\n  grades =~                                                             \n    eng_12            1.000                               2.523    0.933\n    math_12 (.11.)    0.893    0.025   35.559    0.000    2.252    0.823\n    sci_12  (.12.)    0.950    0.023   41.891    0.000    2.397    0.883\n    ss_12   (.13.)    1.056    0.023   46.042    0.000    2.664    0.897\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.295    0.035    8.332    0.000    0.589    0.589\n  grades ~                                                              \n    prevach           0.118    0.043    2.772    0.006    0.407    0.407\n    hw                0.826    0.515    1.602    0.109    0.321    0.321\n  hw ~                                                                  \n    prevach           0.068    0.016    4.369    0.000    0.602    0.602\n    famback           0.006    0.008    0.722    0.470    0.098    0.098\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.339    0.473    0.717    0.473    0.339    0.071\n .bytxmstd ~~                                                           \n   .math_12           2.433    0.689    3.529    0.000    2.433    0.288\n .bytxsstd ~~                                                           \n   .sci_12            0.356    0.602    0.591    0.555    0.356    0.048\n .bytxhstd ~~                                                           \n   .ss_12             1.376    0.628    2.191    0.028    1.376    0.184\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           43.630    1.503   29.034    0.000   43.630    1.894\n   .bypared           2.823    0.089   31.592    0.000    2.823    2.061\n   .byfaminc          8.887    0.177   50.153    0.000    8.887    3.272\n   .bytxrstd         48.458    0.651   74.428    0.000   48.458    4.855\n   .bytxmstd         49.773    0.666   74.681    0.000   49.773    4.872\n   .bytxsstd         48.064    0.659   72.971    0.000   48.064    4.760\n   .bytxhstd         48.249    0.649   74.326    0.000   48.249    4.849\n   .hw10              3.207    0.121   26.602    0.000    3.207    1.735\n   .hw_8              1.731    0.073   23.657    0.000    1.731    1.543\n   .eng_12            5.785    0.176   32.793    0.000    5.785    2.139\n   .math_12           5.439    0.179   30.460    0.000    5.439    1.987\n   .sci_12            5.620    0.177   31.721    0.000    5.620    2.069\n   .ss_12             5.939    0.194   30.648    0.000    5.939    1.999\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          229.612   28.952    7.931    0.000  229.612    0.433\n   .bypared           0.550    0.097    5.667    0.000    0.550    0.293\n   .byfaminc          4.495    0.472    9.519    0.000    4.495    0.609\n   .bytxrstd         23.913    3.099    7.716    0.000   23.913    0.240\n   .bytxmstd         29.426    3.529    8.339    0.000   29.426    0.282\n   .bytxsstd         33.978    3.820    8.895    0.000   33.978    0.333\n   .bytxhstd         32.303    3.655    8.837    0.000   32.303    0.326\n   .hw10              2.457    0.354    6.941    0.000    2.457    0.719\n   .hw_8              1.060    0.113    9.396    0.000    1.060    0.843\n   .eng_12            0.946    0.146    6.467    0.000    0.946    0.129\n   .math_12           2.422    0.255    9.512    0.000    2.422    0.323\n   .sci_12            1.630    0.191    8.541    0.000    1.630    0.221\n   .ss_12             1.730    0.212    8.156    0.000    1.730    0.196\n    famback         301.059   39.678    7.588    0.000    1.000    1.000\n   .prevach          49.415    6.115    8.081    0.000    0.653    0.653\n   .hw                0.536    0.281    1.909    0.056    0.558    0.558\n   .grades            3.562    0.491    7.252    0.000    0.559    0.559\n\n\n\n\n# compare the two models\ncompareFit(hw_fit_mg, hw_fit_mg2) |&gt; summary() |&gt; print()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n            Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)  \nhw_fit_mg  112 59040 59501 194.02                                        \nhw_fit_mg2 121 59037 59455 209.16     15.148 0.03896       9    0.08696 .\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n####################### Model Fit Indices ###########################\n              chisq  df pvalue rmsea   cfi   tli  srmr        aic        bic\nhw_fit_mg  194.017† 112   .000 .040  .989† .984  .029† 59039.621  59500.651 \nhw_fit_mg2 209.165  121   .000 .040† .988  .984† .033  59036.769† 59454.577†\n\n################## Differences in Fit Indices #######################\n                       df rmsea    cfi tli  srmr    aic     bic\nhw_fit_mg2 - hw_fit_mg  9     0 -0.001   0 0.004 -2.852 -46.074\n\nThe following lavaan models were compared:\n    hw_fit_mg\n    hw_fit_mg2\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.\n\n\n\n\nEffect of Homework Across Groups\n경로 hw → grades가 그룹 간에 동일한가?\n\n# constrains the effect of homework\nhw_model_mg3 &lt;- \"\n  famback =~ parocc + bypared + byfaminc\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw10 + hw_8\n  grades =~ eng_12 + math_12 + sci_12 + ss_12\n\n  bytxrstd ~~ eng_12\n  bytxmstd ~~ math_12\n  bytxsstd ~~ sci_12\n  bytxhstd ~~ ss_12\n\n  prevach ~ famback\n  grades ~ prevach + c(h, h)*hw\n  hw ~ prevach + famback\n\"\nhw_fit_mg3 &lt;- sem(hw_model_mg3,\n  data = hw_multigroup,\n  group = \"group\",\n  group.equal = c(\"loadings\")\n)\n\n\n# compare the models\ncompareFit(hw_fit_mg, hw_fit_mg2, hw_fit_mg3) |&gt; summary() |&gt; print()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n            Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)  \nhw_fit_mg  112 59040 59501 194.02                                        \nhw_fit_mg2 121 59037 59455 209.16    15.1476 0.03896       9    0.08696 .\nhw_fit_mg3 122 59035 59448 209.76     0.5932 0.00000       1    0.44117  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n####################### Model Fit Indices ###########################\n              chisq  df pvalue rmsea   cfi   tli  srmr        aic        bic\nhw_fit_mg  194.017† 112   .000 .040  .989† .984  .029† 59039.621  59500.651 \nhw_fit_mg2 209.165  121   .000 .040  .988  .984  .033  59036.769  59454.577 \nhw_fit_mg3 209.758  122   .000 .040† .988  .984† .033  59035.362† 59448.368†\n\n################## Differences in Fit Indices #######################\n                        df rmsea    cfi tli  srmr    aic     bic\nhw_fit_mg2 - hw_fit_mg   9     0 -0.001   0 0.004 -2.852 -46.074\nhw_fit_mg3 - hw_fit_mg2  1     0  0.000   0 0.000 -1.407  -6.209\n\nThe following lavaan models were compared:\n    hw_fit_mg\n    hw_fit_mg2\n    hw_fit_mg3\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.\n\n\n\n\nAll Effects Across Groups\n\n# constrain all the effects\nhw_fit_mg4 &lt;- sem(hw_model_mg,\n  data = hw_multigroup,\n  group = \"group\",\n  group.equal = c(\"loadings\", \"regressions\")\n)\n\n\n# compare the models\ncompareFit(hw_fit_mg, hw_fit_mg2, hw_fit_mg3, hw_fit_mg4) |&gt; summary() |&gt; print()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n            Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)  \nhw_fit_mg  112 59040 59501 194.02                                        \nhw_fit_mg2 121 59037 59455 209.16    15.1476 0.03896       9    0.08696 .\nhw_fit_mg3 122 59035 59448 209.76     0.5932 0.00000       1    0.44117  \nhw_fit_mg4 126 59031 59425 213.11     3.3504 0.00000       4    0.50099  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n####################### Model Fit Indices ###########################\n              chisq  df pvalue rmsea   cfi   tli  srmr        aic        bic\nhw_fit_mg  194.017† 112   .000 .040  .989† .984  .029† 59039.621  59500.651 \nhw_fit_mg2 209.165  121   .000 .040  .988  .984  .033  59036.769  59454.577 \nhw_fit_mg3 209.758  122   .000 .040  .988  .984  .033  59035.362  59448.368 \nhw_fit_mg4 213.108  126   .000 .039† .988  .985† .034  59030.712† 59424.509†\n\n################## Differences in Fit Indices #######################\n                        df  rmsea    cfi   tli  srmr    aic     bic\nhw_fit_mg2 - hw_fit_mg   9  0.000 -0.001 0.000 0.004 -2.852 -46.074\nhw_fit_mg3 - hw_fit_mg2  1  0.000  0.000 0.000 0.000 -1.407  -6.209\nhw_fit_mg4 - hw_fit_mg3  4 -0.001  0.000 0.001 0.001 -4.650 -23.859\n\nThe following lavaan models were compared:\n    hw_fit_mg\n    hw_fit_mg2\n    hw_fit_mg3\n    hw_fit_mg4\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.\n\n\n\n\nAll Effects and Errors Across Groups\n\n# constrain all the effects & errors\nhw_model_mg5 &lt;- \"\n  famback =~ parocc + bypared + byfaminc\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw10 + hw_8\n  grades =~ eng_12 + math_12 + sci_12 + ss_12\n\n  bytxrstd ~~ c(en, en)*eng_12\n  bytxmstd ~~ c(ma, ma)*math_12\n  bytxsstd ~~ c(sc, sc)*sci_12\n  bytxhstd ~~ c(ss, ss)*ss_12\n\n  prevach ~ famback\n  grades ~ prevach + hw\n  hw ~ prevach + famback\n\"\nhw_fit_mg5 &lt;- sem(hw_model_mg5,\n  data = hw_multigroup,\n  group = \"group\",\n  group.equal = c(\n    \"loadings\", \"regressions\",\n    \"residuals\", \"lv.variances\"\n  )\n)\n\n\n# compare the models\ncompareFit(hw_fit_mg, hw_fit_mg2, hw_fit_mg3, hw_fit_mg4, hw_fit_mg5) |&gt; summary() |&gt; print()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n            Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)  \nhw_fit_mg  112 59040 59501 194.02                                        \nhw_fit_mg2 121 59037 59455 209.16     15.148 0.03896       9    0.08696 .\nhw_fit_mg3 122 59035 59448 209.76      0.593 0.00000       1    0.44117  \nhw_fit_mg4 126 59031 59425 213.11      3.350 0.00000       4    0.50099  \nhw_fit_mg5 147 59023 59316 247.26     34.155 0.03731      21    0.03488 *\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n####################### Model Fit Indices ###########################\n              chisq  df pvalue rmsea   cfi   tli  srmr        aic        bic\nhw_fit_mg  194.017† 112   .000 .040  .989† .984  .029† 59039.621  59500.651 \nhw_fit_mg2 209.165  121   .000 .040  .988  .984  .033  59036.769  59454.577 \nhw_fit_mg3 209.758  122   .000 .040  .988  .984  .033  59035.362  59448.368 \nhw_fit_mg4 213.108  126   .000 .039  .988  .985  .034  59030.712  59424.509 \nhw_fit_mg5 247.263  147   .000 .039† .986  .985† .044  59022.867† 59315.813†\n\n################## Differences in Fit Indices #######################\n                        df  rmsea    cfi   tli  srmr    aic      bic\nhw_fit_mg2 - hw_fit_mg   9  0.000 -0.001 0.000 0.004 -2.852  -46.074\nhw_fit_mg3 - hw_fit_mg2  1  0.000  0.000 0.000 0.000 -1.407   -6.209\nhw_fit_mg4 - hw_fit_mg3  4 -0.001  0.000 0.001 0.001 -4.650  -23.859\nhw_fit_mg5 - hw_fit_mg4 21  0.000 -0.002 0.000 0.010 -7.845 -108.695\n\nThe following lavaan models were compared:\n    hw_fit_mg\n    hw_fit_mg2\n    hw_fit_mg3\n    hw_fit_mg4\n    hw_fit_mg5\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.",
    "crumbs": [
      "Keith's",
      "Multigroup Models"
    ]
  },
  {
    "objectID": "contents/chap19.html#estimation-of-means-and-intercepts-in-single-group-sem-models",
    "href": "contents/chap19.html#estimation-of-means-and-intercepts-in-single-group-sem-models",
    "title": "Chapter 19. Latent Means in SEM",
    "section": "Estimation of Means and Intercepts in Single Group SEM Models",
    "text": "Estimation of Means and Intercepts in Single Group SEM Models\n\n\nhw_model &lt;- \"\n  Famback =~ parocc + byfaminc + bypared\n  Prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  HW =~ f2s25f2 + f1s36a2\n  Grades =~ eng92 + math92 + sci92 + soc92\n\n  bytxrstd ~~ eng92\n  bytxmstd ~~ math92\n  bytxsstd ~~ sci92\n  bytxhstd ~~ soc92\n\n  Prevach ~ Famback\n  Grades ~ Prevach + HW\n  HW ~ Prevach + Famback\n\"\nsem_fit &lt;- sem(hw_model,\n  data = hw_mean,\n  meanstructure = TRUE,\n  missing = \"FIML\"  # for mising data\n)\nsummary(sem_fit, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 171 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        48\n\n  Number of observations                          1000\n  Number of missing patterns                        42\n\nModel Test User Model:\n                                                      \n  Test statistic                               113.358\n  Degrees of freedom                                56\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              7792.886\n  Degrees of freedom                                78\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.993\n  Tucker-Lewis Index (TLI)                       0.990\n                                                      \n  Robust Comparative Fit Index (CFI)             0.993\n  Robust Tucker-Lewis Index (TLI)                0.990\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -32104.831\n  Loglikelihood unrestricted model (H1)             NA\n                                                      \n  Akaike (AIC)                               64305.662\n  Bayesian (BIC)                             64541.235\n  Sample-size adjusted Bayesian (SABIC)      64388.784\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.032\n  90 Percent confidence interval - lower         0.023\n  90 Percent confidence interval - upper         0.040\n  P-value H_0: RMSEA &lt;= 0.050                    1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n                                                      \n  Robust RMSEA                                   0.032\n  90 Percent confidence interval - lower         0.023\n  90 Percent confidence interval - upper         0.041\n  P-value H_0: Robust RMSEA &lt;= 0.050             1.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.023\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Famback =~                                                            \n    parocc            1.000                              15.210    0.710\n    byfaminc          0.124    0.006   19.348    0.000    1.891    0.728\n    bypared           0.069    0.003   20.632    0.000    1.049    0.834\n  Prevach =~                                                            \n    bytxrstd          1.000                               8.532    0.852\n    bytxmstd          0.988    0.030   32.651    0.000    8.432    0.853\n    bytxsstd          0.960    0.031   30.702    0.000    8.189    0.819\n    bytxhstd          0.942    0.030   31.597    0.000    8.039    0.830\n  HW =~                                                                 \n    f2s25f2           1.000                               1.171    0.592\n    f1s36a2           0.994    0.103    9.671    0.000    1.164    0.688\n  Grades =~                                                             \n    eng92             1.000                               2.436    0.915\n    math92            0.876    0.025   34.958    0.000    2.135    0.812\n    sci92             0.961    0.023   41.780    0.000    2.341    0.884\n    soc92             1.049    0.023   45.416    0.000    2.555    0.908\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Prevach ~                                                             \n    Famback           0.325    0.022   14.540    0.000    0.579    0.579\n  Grades ~                                                              \n    Prevach           0.140    0.011   12.973    0.000    0.490    0.490\n    HW                0.655    0.101    6.491    0.000    0.315    0.315\n  HW ~                                                                  \n    Prevach           0.045    0.008    5.612    0.000    0.331    0.331\n    Famback           0.018    0.005    3.986    0.000    0.235    0.235\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng92             0.486    0.256    1.900    0.057    0.486    0.086\n .bytxmstd ~~                                                           \n   .math92            1.573    0.320    4.915    0.000    1.573    0.199\n .bytxsstd ~~                                                           \n   .sci92             0.459    0.294    1.563    0.118    0.459    0.064\n .bytxhstd ~~                                                           \n   .soc92             0.266    0.278    0.956    0.339    0.266    0.042\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           51.388    0.679   75.669    0.000   51.388    2.400\n   .byfaminc          9.841    0.083  118.414    0.000    9.841    3.792\n   .bypared           3.128    0.040   78.568    0.000    3.128    2.485\n   .bytxrstd         51.257    0.320  160.386    0.000   51.257    5.120\n   .bytxmstd         51.493    0.316  163.072    0.000   51.493    5.206\n   .bytxsstd         51.179    0.320  160.149    0.000   51.179    5.117\n   .bytxhstd         51.373    0.310  165.931    0.000   51.373    5.302\n   .f2s25f2           3.280    0.066   49.942    0.000    3.280    1.659\n   .f1s36a2           2.481    0.055   45.366    0.000    2.481    1.465\n   .eng92             6.074    0.084   71.892    0.000    6.074    2.281\n   .math92            5.482    0.083   65.676    0.000    5.482    2.086\n   .sci92             5.770    0.084   68.653    0.000    5.770    2.177\n   .soc92             6.207    0.089   69.614    0.000    6.207    2.207\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          227.271   13.266   17.132    0.000  227.271    0.496\n   .byfaminc          3.161    0.198   15.973    0.000    3.161    0.469\n   .bypared           0.483    0.044   10.967    0.000    0.483    0.305\n   .bytxrstd         27.413    1.704   16.091    0.000   27.413    0.274\n   .bytxmstd         26.728    1.669   16.017    0.000   26.728    0.273\n   .bytxsstd         32.977    1.881   17.536    0.000   32.977    0.330\n   .bytxhstd         29.242    1.709   17.107    0.000   29.242    0.312\n   .f2s25f2           2.537    0.178   14.290    0.000    2.537    0.649\n   .f1s36a2           1.510    0.149   10.158    0.000    1.510    0.527\n   .eng92             1.155    0.080   14.479    0.000    1.155    0.163\n   .math92            2.349    0.122   19.249    0.000    2.349    0.340\n   .sci92             1.541    0.092   16.744    0.000    1.541    0.219\n   .soc92             1.383    0.091   15.197    0.000    1.383    0.175\n    Famback         231.337   19.948   11.597    0.000    1.000    1.000\n   .Prevach          48.379    3.317   14.583    0.000    0.665    0.665\n   .HW                1.021    0.154    6.644    0.000    0.745    0.745\n   .Grades            3.063    0.200   15.324    0.000    0.516    0.516",
    "crumbs": [
      "Keith's",
      "Latent Means"
    ]
  },
  {
    "objectID": "contents/chap19.html#잠재-평균-간의-차이를-검정하는-두-가지-방법",
    "href": "contents/chap19.html#잠재-평균-간의-차이를-검정하는-두-가지-방법",
    "title": "Chapter 19. Latent Means in SEM",
    "section": "잠재 평균 간의 차이를 검정하는 두 가지 방법",
    "text": "잠재 평균 간의 차이를 검정하는 두 가지 방법\n\n더미 변수의 회귀계수를 통해 평균의 차이를 추정하는 방법: 예. 성별에 대한 주효과에 대한 검증\n\nMultigroup SEM을 통해 추정하는 방법: 예. 성별과 과제의 상호작용 효과에 대한 검증\n\n\n잠재변수의 intercept를 추정하는 방법: 주효과와 상호작용 효과를 동시에 추정할 수 있음\n(MG-MACS; multi-group mean and covariance structures)",
    "crumbs": [
      "Keith's",
      "Latent Means"
    ]
  },
  {
    "objectID": "contents/chap19.html#single-groupdummy-variable-approach",
    "href": "contents/chap19.html#single-groupdummy-variable-approach",
    "title": "Chapter 19. Latent Means in SEM",
    "section": "Single Group/Dummy Variable Approach",
    "text": "Single Group/Dummy Variable Approach\n\nhotflash_model &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre + Group  # Group: numeric variable!\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\"\nhotflash_fit &lt;- sem(hotflash_model, data = hotflash)\nsummary(hotflash_fit, \n  standardized = \"std.nox\", # dummy 변수에 대해서는 표준화하지 않기 위함\n  fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 161 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Number of observations                            96\n\nModel Test User Model:\n                                                      \n  Test statistic                                 3.501\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.174\n\nModel Test Baseline Model:\n\n  Test statistic                               244.790\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.994\n  Tucker-Lewis Index (TLI)                       0.968\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1466.241\n  Loglikelihood unrestricted model (H1)             NA\n                                                      \n  Akaike (AIC)                                2956.482\n  Bayesian (BIC)                              2987.254\n  Sample-size adjusted Bayesian (SABIC)       2949.365\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.088\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.239\n  P-value H_0: RMSEA &lt;= 0.050                    0.245\n  P-value H_0: RMSEA &gt;= 0.080                    0.649\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.082\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.nox\n  hf_pre =~                                                    \n    int1              1.000                               0.932\n    HF1               0.260    0.112    2.323    0.020    0.439\n  hf_post =~                                                   \n    int2              1.000                               0.927\n    HF2               0.310    0.037    8.480    0.000    0.659\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.nox\n  hf_post ~                                                    \n    hf_pre            0.487    0.195    2.500    0.012    0.441\n    Group           -28.615    3.149   -9.087    0.000   -1.386\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.nox\n .HF1 ~~                                                       \n   .HF2              60.277   10.820    5.571    0.000    0.831\n .int1 ~~                                                      \n   .int2            -24.643   55.046   -0.448    0.654   -0.405\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.nox\n   .int1             52.754  179.207    0.294    0.768    0.131\n   .HF1              98.604   17.851    5.524    0.000    0.807\n   .int2             70.301   51.321    1.370    0.171    0.142\n   .HF2              53.415    9.107    5.865    0.000    0.565\n    hf_pre          349.972  190.044    1.842    0.066    1.000\n   .hf_post         138.752   35.569    3.901    0.000    0.325\n\n\n\n\n효과의 유의성 검정\n\n무선할당이 적절히 이루어졌는지 확인\n최면 치료의 효과가 유의한지 확인\n\n\n\n# free covariance between pretest and group\nhotflash_model2 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre + Group\n  hf_pre ~ Group  # free the parameter\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\n  # 또는 hf_pre ~~ Group\n\"\nhotflash_fit2 &lt;- sem(hotflash_model2, data = hotflash)\nparameterEstimates(hotflash_fit2, standardized = TRUE) |&gt; subset(rhs == \"Group\") |&gt; print()\n\n       lhs op   rhs     est    se      z pvalue ci.lower ci.upper std.lv std.all std.nox\n6  hf_post  ~ Group -27.460 3.413 -8.047   0.00  -34.149  -20.771 -1.286  -0.643  -1.286\n7   hf_pre  ~ Group  -7.485 3.982 -1.880   0.06  -15.289    0.318 -0.428  -0.214  -0.428\n16   Group ~~ Group   0.250 0.000     NA     NA    0.250    0.250  0.250   1.000   0.250\n\n\n\nlavTestLRT(hotflash_fit2, hotflash_fit) |&gt; print()\n\n\nChi-Squared Difference Test\n\n              Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)  \nhotflash_fit2  1 2955.1 2988.4 0.0881                                        \nhotflash_fit   2 2956.5 2987.2 3.5007     3.4126 0.15853       1     0.0647 .\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n\n\n# no effect of group on post-test\nhotflash_model3 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre + 0*Group  # fix the parameter to 0\n  hf_pre ~ Group  # free the parameter\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\"\nhotflash_fit3 &lt;- sem(hotflash_model3, data = hotflash)\n\n\ncompareFit(hotflash_fit3, hotflash_fit2) |&gt; summary() |&gt; print()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n              Df    AIC    BIC   Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nhotflash_fit2  1 2955.1 2988.4  0.0881                                          \nhotflash_fit3  2 2969.2 3000.0 16.2486      16.16 0.39739       1   5.82e-05 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n####################### Model Fit Indices ###########################\n                chisq df pvalue rmsea    cfi    tli  srmr       aic       bic\nhotflash_fit2   .088†  1   .767 .000† 1.000† 1.039† .009† 2955.070† 2988.406†\nhotflash_fit3 16.249   2   .000 .272   .939   .697  .113  2969.230  3000.002 \n\n################## Differences in Fit Indices #######################\n                              df rmsea    cfi    tli  srmr   aic    bic\nhotflash_fit3 - hotflash_fit2  1 0.272 -0.061 -0.342 0.104 14.16 11.596\n\nThe following lavaan models were compared:\n    hotflash_fit2\n    hotflash_fit3\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.",
    "crumbs": [
      "Keith's",
      "Latent Means"
    ]
  },
  {
    "objectID": "contents/chap19.html#sem의-평균-구조mean-structure",
    "href": "contents/chap19.html#sem의-평균-구조mean-structure",
    "title": "Chapter 19. Latent Means in SEM",
    "section": "SEM의 평균 구조(Mean Structure)",
    "text": "SEM의 평균 구조(Mean Structure)\n예를 들어, 잠재변수 \\(\\xi_1\\)이 잠재변수 \\(\\eta_1\\)에 영향을 주는 모형에 대해,\n\n\nStructural model:\n\\(\\eta = \\alpha + \\gamma*\\xi + \\zeta\\)\nMeasurement model:\n\\(X_1 = \\nu_{X_1} + \\xi + \\delta_1\\)\n\\(X_2 = \\nu_{X_2} + \\lambda_{2} *\\xi + \\delta_2\\)\n\\(Y_1 = \\nu_{Y_1} + \\eta + \\epsilon_1\\)\n\\(Y_2 = \\nu_{Y_2} + \\lambda_{4}* \\eta + \\epsilon_2\\)\n(회귀계수 \\(\\lambda\\): factor loading, 요인부하량, \\(\\alpha, \\nu\\): 절편)\n\n\n\n\n각 변수의 평균들은 \\(\\xi\\)의 평균 \\(E(\\xi) = m\\)이라 하면,\n\\(E(\\eta) = \\alpha + \\gamma*m\\),\n\\(E(X_1) = \\nu_{X_1} + m\\),  \\(E(X_2) = \\nu_{X_2} + \\lambda_{2}*m\\),\n\\(E(Y_1) = \\nu_{Y_1} + \\alpha + \\gamma*m\\),  \\(E(Y_2) = \\nu_{Y_2} + \\lambda_{4}*(\\alpha + \\gamma*m)\\)\n새로운 파라미터 \\(m, \\alpha, \\nu_{X_1}, \\nu_{X_2}, \\nu_{Y_1}, \\nu_{Y_2}\\)를 추정하려면, 6개의 정보가 더 필요하나 변수 4개의 평균 정보만 추가되므로 2개의 파라미터의 제약이 필요함.\n\n잠재변수들의 평균을 0으로 설정: \\(E(\\xi) = 0, ~\\alpha = 0\\)으로 설정; 이때, \\(E(\\nu) = 0\\)이 됨\n잠재변수들의 원점(0)의 위치를 각각 \\(X_1\\), \\(Y_1\\)과 동일하게 설정\n\n절편을 0으로 고정하면 됨 (잠재변수의 scale은 이미 각각 \\(X_1, Y_1\\)으로 설정되어 있음)\n\\(\\nu_{X_1} = 0\\); 이때 \\(E(X_1) = E(\\xi)\\), 즉 잠재변수 \\(\\xi\\)의 평균이 \\(X_1\\)의 평균과 동일하게 됨\n\\(\\nu_{Y_1} = 0\\); 이때 \\(E(Y_1) = E(\\eta)\\)이 됨, 즉 잠재변수 \\(\\eta\\)의 평균이 \\(Y_1\\)의 평균과 동일하게 됨\n\n\nMulti-group SEM에서 그룹 간에 절편에 대한 동일성을 가정하면, identified 될 수 있음.\n\n\n\n\n\n\n변수 \\(X\\)의 평균의 의미: 예측변수 \\(I=(1, 1, ..., 1)\\)에 대한 회귀계수인 절편; lm(y ~ 1)\n절편(intercept)을 통해 평균(mean)을 추정하게 되고\n선형함수에 의해 \\(X\\)(들)의 평균(벡터)은 \\(Y\\)의 평균으로 예측됨!\nLatent의 평균은 보통 0으로 설정하므로, factor(latent)의 절편은 곧 indicator의 평균이 됨.\n\n\n\n파라미터인 절편의 갯수가 늘어나는 만큼 평균에 대한 정보도 늘어나기 때문에,\n평균 구조(mean structure)를 추정하더라도 모형의 자유도나 적합도, 모수 등에 변화는 없음.\nSEM diagram에서의 표시\n\n \n예를 들어,\n\n\nFIGURE 9.1. Recursive path model of illness with unstandardized parameters for the covariance structure and the mean structure. Estimates for the mean structure are shown in boldface. Values for exercise and hardy (exogenous variables) are means, and values for fitness, stress, and illness (endogenous variables) are intercepts.\nSource: Kline, R. B. (2023). Principles and practice of structural equation modeling (5e)\n\n\n\nLoad data\nhw_mean &lt;- haven::read_sav(\"data/chap 19 latent means/Homework means.sav\")\n\n\n가령, 다음과 같은 모형에 대한 절편을 포함한 파라미터를 추정한다면,\n\n기본적으로 잠재변수의 평균은 0으로 설정; default\nIndicator에 대한 절편을 0으로 고정하고, 잠재변수의 평균을 추정하려면 직접 지정해야 함\n\n\n\nintercept_model &lt;- \"\n  Grades =~ eng92 + math92 + sci92 + soc92\n  HW =~ f2s25f2 + f1s36a2\n  Grades ~ HW\n\"\nintercept_fit &lt;- sem(intercept_model, data = hw_mean,  meanstructure = TRUE)\ninspect(intercept_fit, what = \"estimates\") |&gt; print()\n\n$lambda\n        Grades    HW\neng92    1.000 0.000\nmath92   0.882 0.000\nsci92    0.970 0.000\nsoc92    1.032 0.000\nf2s25f2  0.000 1.000\nf1s36a2  0.000 1.049\n\n$theta\n        eng92 math92 sci92 soc92 f2s252 f1s362\neng92   1.052                                 \nmath92  0.000  2.366                          \nsci92   0.000  0.000 1.494                    \nsoc92   0.000  0.000 0.000 1.330              \nf2s25f2 0.000  0.000 0.000 0.000  2.538       \nf1s36a2 0.000  0.000 0.000 0.000  0.000  1.407\n\n$psi\n       Grades    HW\nGrades  3.671      \nHW      0.000 1.278\n\n$beta\n       Grades    HW\nGrades      0 0.958\nHW          0 0.000\n\n$nu\n        intrcp\neng92    6.512\nmath92   5.842\nsci92    6.163\nsoc92    6.657\nf2s25f2  3.410\nf1s36a2  2.584\n\n$alpha\n       intrcp\nGrades      0\nHW          0\n\n\n\n\nparameterEstimates(intercept_fit) |&gt; subset(op == \"~1\") |&gt; print()\n\n       lhs op rhs   est    se      z pvalue ci.lower ci.upper\n16   eng92 ~1     6.512 0.082 79.004      0    6.350    6.673\n17  math92 ~1     5.842 0.084 69.495      0    5.677    6.006\n18   sci92 ~1     6.163 0.084 73.804      0    6.000    6.327\n19   soc92 ~1     6.657 0.086 76.961      0    6.487    6.826\n20 f2s25f2 ~1     3.410 0.066 51.434      0    3.280    3.540\n21 f1s36a2 ~1     2.584 0.057 45.382      0    2.472    2.696\n22  Grades ~1     0.000 0.000     NA     NA    0.000    0.000\n23      HW ~1     0.000 0.000     NA     NA    0.000    0.000\n\n\n\nsummary(intercept_fit, \n  fit.measures = FALSE,\n  standardized = TRUE,\n  remove.unused = FALSE, # keep the unused parameters\n) |&gt; print()\n\nlavaan 0.6-19 ended normally after 40 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        19\n\n                                                  Used       Total\n  Number of observations                           868        1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                20.993\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.007\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Grades =~                                                             \n    eng92             1.000                               2.201    0.906\n    math92            0.882    0.029   30.161    0.000    1.941    0.784\n    sci92             0.970    0.026   36.828    0.000    2.135    0.868\n    soc92             1.032    0.027   38.924    0.000    2.272    0.892\n  HW =~                                                                 \n    f2s25f2           1.000                               1.130    0.579\n    f1s36a2           1.049    0.130    8.089    0.000    1.186    0.707\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Grades ~                                                              \n    HW                0.958    0.108    8.873    0.000    0.492    0.492\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .eng92             6.512    0.082   79.004    0.000    6.512    2.682\n   .math92            5.842    0.084   69.495    0.000    5.842    2.359\n   .sci92             6.163    0.084   73.804    0.000    6.163    2.505\n   .soc92             6.657    0.086   76.961    0.000    6.657    2.612\n   .f2s25f2           3.410    0.066   51.434    0.000    3.410    1.746\n   .f1s36a2           2.584    0.057   45.382    0.000    2.584    1.540\n   .Grades            0.000                               0.000    0.000\n    HW                0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .eng92             1.052    0.080   13.149    0.000    1.052    0.178\n   .math92            2.366    0.130   18.271    0.000    2.366    0.386\n   .sci92             1.494    0.095   15.780    0.000    1.494    0.247\n   .soc92             1.330    0.093   14.324    0.000    1.330    0.205\n   .f2s25f2           2.538    0.195   13.033    0.000    2.538    0.665\n   .f1s36a2           1.407    0.180    7.798    0.000    1.407    0.500\n   .Grades            3.671    0.261   14.082    0.000    0.758    0.758\n    HW                1.278    0.204    6.252    0.000    1.000    1.000\n\n\n\nIndicator의 대한 절편을 0으로 고정하고, 잠재변수의 평균을 추정하려면 직접 지정!\n\nintercept_model2 &lt;- \"\n  Grades =~ eng92 + math92 + sci92 + soc92\n  HW =~ f2s25f2 + f1s36a2\n  Grades ~ HW\n\n  # specify the intercepts\n  Grades ~ 1\n  HW ~ 1\n  eng92 ~ 0*1\n  f2s25f2 ~ 0*1\n\"\nintercept_fit2 &lt;- sem(intercept_model2, data = hw_mean,  meanstructure = TRUE)\ninspect(intercept_fit2, what = \"estimates\") |&gt; print()\n\n$lambda\n        Grades    HW\neng92    1.000 0.000\nmath92   0.882 0.000\nsci92    0.970 0.000\nsoc92    1.032 0.000\nf2s25f2  0.000 1.000\nf1s36a2  0.000 1.049\n\n$theta\n        eng92 math92 sci92 soc92 f2s252 f1s362\neng92   1.052                                 \nmath92  0.000  2.366                          \nsci92   0.000  0.000 1.494                    \nsoc92   0.000  0.000 0.000 1.330              \nf2s25f2 0.000  0.000 0.000 0.000  2.538       \nf1s36a2 0.000  0.000 0.000 0.000  0.000  1.407\n\n$psi\n       Grades    HW\nGrades  3.671      \nHW      0.000 1.278\n\n$beta\n       Grades    HW\nGrades      0 0.958\nHW          0 0.000\n\n$nu\n        intrcp\neng92    0.000\nmath92   0.100\nsci92   -0.154\nsoc92   -0.066\nf2s25f2  0.000\nf1s36a2 -0.995\n\n$alpha\n       intrcp\nGrades  3.244\nHW      3.410\n\n\n\n\nsummary(intercept_fit2, \n  fit.measures = FALSE,\n  standardized = TRUE,\n  remove.unused = FALSE, # keep the unused parameters\n) |&gt; print()\n\nlavaan 0.6-19 ended normally after 63 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        19\n\n                                                  Used       Total\n  Number of observations                           868        1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                20.993\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.007\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Grades =~                                                             \n    eng92             1.000                               2.201    0.906\n    math92            0.882    0.029   30.161    0.000    1.941    0.784\n    sci92             0.970    0.026   36.828    0.000    2.135    0.868\n    soc92             1.032    0.027   38.924    0.000    2.272    0.892\n  HW =~                                                                 \n    f2s25f2           1.000                               1.130    0.579\n    f1s36a2           1.049    0.130    8.089    0.000    1.186    0.707\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Grades ~                                                              \n    HW                0.958    0.108    8.873    0.000    0.492    0.492\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Grades            3.244    0.379    8.555    0.000    1.474    1.474\n    HW                3.410    0.066   51.434    0.000    3.017    3.017\n   .eng92             0.000                               0.000    0.000\n   .f2s25f2           0.000                               0.000    0.000\n   .math92            0.100    0.200    0.500    0.617    0.100    0.040\n   .sci92            -0.154    0.180   -0.857    0.392   -0.154   -0.063\n   .soc92            -0.066    0.181   -0.367    0.714   -0.066   -0.026\n   .f1s36a2          -0.995    0.448   -2.221    0.026   -0.995   -0.593\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .eng92             1.052    0.080   13.149    0.000    1.052    0.178\n   .math92            2.366    0.130   18.271    0.000    2.366    0.386\n   .sci92             1.494    0.095   15.780    0.000    1.494    0.247\n   .soc92             1.330    0.093   14.324    0.000    1.330    0.205\n   .f2s25f2           2.538    0.195   13.033    0.000    2.538    0.665\n   .f1s36a2           1.407    0.180    7.798    0.000    1.407    0.500\n   .Grades            3.671    0.261   14.082    0.000    0.758    0.758\n    HW                1.278    0.204    6.252    0.000    1.000    1.000",
    "crumbs": [
      "Keith's",
      "Latent Means"
    ]
  },
  {
    "objectID": "contents/chap19.html#single-group-sem-models",
    "href": "contents/chap19.html#single-group-sem-models",
    "title": "Chapter 19. Latent Means in SEM",
    "section": "Single Group SEM Models",
    "text": "Single Group SEM Models\nEstimation of Means and Intercepts in Single Group SEM Models\n\n\nhw_model &lt;- \"\n  Famback =~ parocc + byfaminc + bypared\n  Prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  HW =~ f2s25f2 + f1s36a2\n  Grades =~ eng92 + math92 + sci92 + soc92\n\n  bytxrstd ~~ eng92\n  bytxmstd ~~ math92\n  bytxsstd ~~ sci92\n  bytxhstd ~~ soc92\n\n  Prevach ~ Famback\n  Grades ~ Prevach + HW\n  HW ~ Prevach + Famback\n\"\nsem_fit &lt;- sem(hw_model,\n  data = hw_mean,\n  meanstructure = TRUE,\n  missing = \"FIML\"  # for mising data\n)\nsummary(sem_fit, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 171 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        48\n\n  Number of observations                          1000\n  Number of missing patterns                        42\n\nModel Test User Model:\n                                                      \n  Test statistic                               113.358\n  Degrees of freedom                                56\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              7792.886\n  Degrees of freedom                                78\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.993\n  Tucker-Lewis Index (TLI)                       0.990\n                                                      \n  Robust Comparative Fit Index (CFI)             0.993\n  Robust Tucker-Lewis Index (TLI)                0.990\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -32104.831\n  Loglikelihood unrestricted model (H1)             NA\n                                                      \n  Akaike (AIC)                               64305.662\n  Bayesian (BIC)                             64541.235\n  Sample-size adjusted Bayesian (SABIC)      64388.784\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.032\n  90 Percent confidence interval - lower         0.023\n  90 Percent confidence interval - upper         0.040\n  P-value H_0: RMSEA &lt;= 0.050                    1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n                                                      \n  Robust RMSEA                                   0.032\n  90 Percent confidence interval - lower         0.023\n  90 Percent confidence interval - upper         0.041\n  P-value H_0: Robust RMSEA &lt;= 0.050             1.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.023\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Famback =~                                                            \n    parocc            1.000                              15.210    0.710\n    byfaminc          0.124    0.006   19.348    0.000    1.891    0.728\n    bypared           0.069    0.003   20.632    0.000    1.049    0.834\n  Prevach =~                                                            \n    bytxrstd          1.000                               8.532    0.852\n    bytxmstd          0.988    0.030   32.651    0.000    8.432    0.853\n    bytxsstd          0.960    0.031   30.702    0.000    8.189    0.819\n    bytxhstd          0.942    0.030   31.597    0.000    8.039    0.830\n  HW =~                                                                 \n    f2s25f2           1.000                               1.171    0.592\n    f1s36a2           0.994    0.103    9.671    0.000    1.164    0.688\n  Grades =~                                                             \n    eng92             1.000                               2.436    0.915\n    math92            0.876    0.025   34.958    0.000    2.135    0.812\n    sci92             0.961    0.023   41.780    0.000    2.341    0.884\n    soc92             1.049    0.023   45.416    0.000    2.555    0.908\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Prevach ~                                                             \n    Famback           0.325    0.022   14.540    0.000    0.579    0.579\n  Grades ~                                                              \n    Prevach           0.140    0.011   12.973    0.000    0.490    0.490\n    HW                0.655    0.101    6.491    0.000    0.315    0.315\n  HW ~                                                                  \n    Prevach           0.045    0.008    5.612    0.000    0.331    0.331\n    Famback           0.018    0.005    3.986    0.000    0.235    0.235\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng92             0.486    0.256    1.900    0.057    0.486    0.086\n .bytxmstd ~~                                                           \n   .math92            1.573    0.320    4.915    0.000    1.573    0.199\n .bytxsstd ~~                                                           \n   .sci92             0.459    0.294    1.563    0.118    0.459    0.064\n .bytxhstd ~~                                                           \n   .soc92             0.266    0.278    0.956    0.339    0.266    0.042\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           51.388    0.679   75.669    0.000   51.388    2.400\n   .byfaminc          9.841    0.083  118.414    0.000    9.841    3.792\n   .bypared           3.128    0.040   78.568    0.000    3.128    2.485\n   .bytxrstd         51.257    0.320  160.386    0.000   51.257    5.120\n   .bytxmstd         51.493    0.316  163.072    0.000   51.493    5.206\n   .bytxsstd         51.179    0.320  160.149    0.000   51.179    5.117\n   .bytxhstd         51.373    0.310  165.931    0.000   51.373    5.302\n   .f2s25f2           3.280    0.066   49.942    0.000    3.280    1.659\n   .f1s36a2           2.481    0.055   45.366    0.000    2.481    1.465\n   .eng92             6.074    0.084   71.892    0.000    6.074    2.281\n   .math92            5.482    0.083   65.676    0.000    5.482    2.086\n   .sci92             5.770    0.084   68.653    0.000    5.770    2.177\n   .soc92             6.207    0.089   69.614    0.000    6.207    2.207\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          227.271   13.266   17.132    0.000  227.271    0.496\n   .byfaminc          3.161    0.198   15.973    0.000    3.161    0.469\n   .bypared           0.483    0.044   10.967    0.000    0.483    0.305\n   .bytxrstd         27.413    1.704   16.091    0.000   27.413    0.274\n   .bytxmstd         26.728    1.669   16.017    0.000   26.728    0.273\n   .bytxsstd         32.977    1.881   17.536    0.000   32.977    0.330\n   .bytxhstd         29.242    1.709   17.107    0.000   29.242    0.312\n   .f2s25f2           2.537    0.178   14.290    0.000    2.537    0.649\n   .f1s36a2           1.510    0.149   10.158    0.000    1.510    0.527\n   .eng92             1.155    0.080   14.479    0.000    1.155    0.163\n   .math92            2.349    0.122   19.249    0.000    2.349    0.340\n   .sci92             1.541    0.092   16.744    0.000    1.541    0.219\n   .soc92             1.383    0.091   15.197    0.000    1.383    0.175\n    Famback         231.337   19.948   11.597    0.000    1.000    1.000\n   .Prevach          48.379    3.317   14.583    0.000    0.665    0.665\n   .HW                1.021    0.154    6.644    0.000    0.745    0.745\n   .Grades            3.063    0.200   15.324    0.000    0.516    0.516",
    "crumbs": [
      "Keith's",
      "Latent Means"
    ]
  },
  {
    "objectID": "contents/chap21.html#unconditional-growth-model",
    "href": "contents/chap21.html#unconditional-growth-model",
    "title": "Chapter 21. Latent Growth Models",
    "section": "Unconditional growth model",
    "text": "Unconditional growth model\n\n\n\n\n\n\n\n\n\n\n\n\n\n각 indicator들이 두 잠재변수(\\(\\xi_1, ~\\xi_2\\))에 의해 예측되는 모형\n\nFactor loading에 대한 설정\nIndicator에 대한 intercept는 0으로 설정\n\n\\(math1 = 0\\cdot Slope + 1\\cdot Intercept + 0\\)\n\\(math2 = 1\\cdot Slope + 1\\cdot Intercept + 0\\)\n\\(math3 = 2\\cdot Slope + 1\\cdot Intercept + 0\\)\n\\(math4 = 3\\cdot Slope + 1\\cdot Intercept + 0\\)\n\\(math5 = 4\\cdot Slope + 1\\cdot Intercept + 0\\)\n\\(math = Slope\\cdot X + Intercept\\cdot 1\\),   \\(X = 0, 1, 2, 3, 4\\)\n\n\n\n\n\n# Unconditional growth model\nlgm_math_model &lt;- '\n  # latents\n  Initial =~ 1*math1 + 1*math2 + 1*math3 + 1*math4 + 1*math5\n  Growth =~ 0*math1 + 1*math2 + 2*math3 + 3*math4 + 4*math5\n  \n  # intercepts\n  Initial ~ 1\n  Growth ~ 1\n  \n  # residual (co)variances\n  Growth ~~ Initial\n  \n  math1 ~~ a*math1\n  math2 ~~ a*math2\n  math3 ~~ a*math3\n  math4 ~~ a*math4\n  math5 ~~ a*math5\n\n  # not neeed when using growth() function instead sem()\n  math1 ~ 0*1\n  math2 ~ 0*1\n  math3 ~ 0*1\n  math4 ~ 0*1\n  math5 ~ 0*1\n  # same as \"math1 + math2 + math3 + math4 + math5 ~ 0*1\"\n'\nlgm_math &lt;- sem(lgm_math_model, data = math_growth)\nsummary(lgm_math, \n        remove.unused = FALSE, # keep the unused parameters\n        standardized = TRUE, \n        fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 62 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n  Number of equality constraints                     4\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                80.454\n  Degrees of freedom                                14\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              9369.171\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.993\n  Tucker-Lewis Index (TLI)                       0.995\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -14661.484\n  Loglikelihood unrestricted model (H1)     -14621.257\n                                                      \n  Akaike (AIC)                               29334.968\n  Bayesian (BIC)                             29364.414\n  Sample-size adjusted Bayesian (SABIC)      29345.358\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.069\n  90 Percent confidence interval - lower         0.055\n  90 Percent confidence interval - upper         0.084\n  P-value H_0: RMSEA &lt;= 0.050                    0.015\n  P-value H_0: RMSEA &gt;= 0.080                    0.115\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.017\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial =~                                                            \n    math1             1.000                               9.010    0.969\n    math2             1.000                               9.010    0.895\n    math3             1.000                               9.010    0.797\n    math4             1.000                               9.010    0.699\n    math5             1.000                               9.010    0.613\n  Growth =~                                                             \n    math1             0.000                               0.000    0.000\n    math2             1.000                               2.414    0.240\n    math3             2.000                               4.828    0.427\n    math4             3.000                               7.241    0.562\n    math5             4.000                               9.655    0.657\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial ~~                                                            \n    Growth            4.559    0.741    6.156    0.000    0.210    0.210\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Initial          70.467    0.290  242.668    0.000    7.821    7.821\n    Growth            8.877    0.080  111.410    0.000    3.678    3.678\n   .math1             0.000                               0.000    0.000\n   .math2             0.000                               0.000    0.000\n   .math3             0.000                               0.000    0.000\n   .math4             0.000                               0.000    0.000\n   .math5             0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .math1      (a)    5.228    0.135   38.730    0.000    5.228    0.060\n   .math2      (a)    5.228    0.135   38.730    0.000    5.228    0.052\n   .math3      (a)    5.228    0.135   38.730    0.000    5.228    0.041\n   .math4      (a)    5.228    0.135   38.730    0.000    5.228    0.031\n   .math5      (a)    5.228    0.135   38.730    0.000    5.228    0.024\n    Initial          81.187    3.772   21.524    0.000    1.000    1.000\n    Growth            5.826    0.284   20.496    0.000    1.000    1.000\n\n\n\nsem() 대신 growth()를 이용하면,\n\n# Unconditional growth model\nlgm_math_model &lt;- '\n  # latents\n  Initial =~ 1*math1 + 1*math2 + 1*math3 + 1*math4 + 1*math5\n  Growth =~ 0*math1 + 1*math2 + 2*math3 + 3*math4 + 4*math5\n  \n  # intercepts\n  Initial ~ 1\n  Growth ~ 1\n  \n  # residual (co)variances\n  Growth ~~ Initial\n  \n  math1 ~~ a*math1\n  math2 ~~ a*math2\n  math3 ~~ a*math3\n  math4 ~~ a*math4\n  math5 ~~ a*math5\n'\nlgm_math &lt;- growth(lgm_math_model, data = math_growth)\nsummary(lgm_math, \n        remove.unused = FALSE, # keep the unused parameters\n        standardized = TRUE, \n        fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 62 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n  Number of equality constraints                     4\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                80.454\n  Degrees of freedom                                14\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              9369.171\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.993\n  Tucker-Lewis Index (TLI)                       0.995\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -14661.484\n  Loglikelihood unrestricted model (H1)     -14621.257\n                                                      \n  Akaike (AIC)                               29334.968\n  Bayesian (BIC)                             29364.414\n  Sample-size adjusted Bayesian (SABIC)      29345.358\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.069\n  90 Percent confidence interval - lower         0.055\n  90 Percent confidence interval - upper         0.084\n  P-value H_0: RMSEA &lt;= 0.050                    0.015\n  P-value H_0: RMSEA &gt;= 0.080                    0.115\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.017\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial =~                                                            \n    math1             1.000                               9.010    0.969\n    math2             1.000                               9.010    0.895\n    math3             1.000                               9.010    0.797\n    math4             1.000                               9.010    0.699\n    math5             1.000                               9.010    0.613\n  Growth =~                                                             \n    math1             0.000                               0.000    0.000\n    math2             1.000                               2.414    0.240\n    math3             2.000                               4.828    0.427\n    math4             3.000                               7.241    0.562\n    math5             4.000                               9.655    0.657\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial ~~                                                            \n    Growth            4.559    0.741    6.156    0.000    0.210    0.210\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Initial          70.467    0.290  242.668    0.000    7.821    7.821\n    Growth            8.877    0.080  111.410    0.000    3.678    3.678\n   .math1             0.000                               0.000    0.000\n   .math2             0.000                               0.000    0.000\n   .math3             0.000                               0.000    0.000\n   .math4             0.000                               0.000    0.000\n   .math5             0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .math1      (a)    5.228    0.135   38.730    0.000    5.228    0.060\n   .math2      (a)    5.228    0.135   38.730    0.000    5.228    0.052\n   .math3      (a)    5.228    0.135   38.730    0.000    5.228    0.041\n   .math4      (a)    5.228    0.135   38.730    0.000    5.228    0.031\n   .math5      (a)    5.228    0.135   38.730    0.000    5.228    0.024\n    Initial          81.187    3.772   21.524    0.000    1.000    1.000\n    Growth            5.826    0.284   20.496    0.000    1.000    1.000\n\n\n\nmath1, math2, math3, math4, math5에 대해 모형이 예측하는 평균은?\n\nNo equal residual variances\n\n\n# Unconditional growth model\nlgm_math_model_unequal &lt;- '\n  # latents\n  Initial =~ 1*math1 + 1*math2 + 1*math3 + 1*math4 + 1*math5\n  Growth =~ 0*math1 + 1*math2 + 2*math3 + 3*math4 + 4*math5\n  \n  # intercepts\n  Initial ~ 1\n  Growth ~ 1\n  \n  # residual (co)variances\n  Growth ~~ Initial\n'\nlgm_math_unequal &lt;- growth(lgm_math_model_unequal, data = math_growth)\nsummary(lgm_math_unequal, \n        remove.unused = FALSE, # keep the unused parameters\n        standardized = TRUE, \n        fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 160 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                68.644\n  Degrees of freedom                                10\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              9369.171\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.994\n  Tucker-Lewis Index (TLI)                       0.994\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -14655.579\n  Loglikelihood unrestricted model (H1)     -14621.257\n                                                      \n  Akaike (AIC)                               29331.158\n  Bayesian (BIC)                             29380.235\n  Sample-size adjusted Bayesian (SABIC)      29348.475\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.077\n  90 Percent confidence interval - lower         0.060\n  90 Percent confidence interval - upper         0.094\n  P-value H_0: RMSEA &lt;= 0.050                    0.005\n  P-value H_0: RMSEA &gt;= 0.080                    0.394\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.018\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial =~                                                            \n    math1             1.000                               9.004    0.959\n    math2             1.000                               9.004    0.895\n    math3             1.000                               9.004    0.796\n    math4             1.000                               9.004    0.699\n    math5             1.000                               9.004    0.613\n  Growth =~                                                             \n    math1             0.000                               0.000    0.000\n    math2             1.000                               2.395    0.238\n    math3             2.000                               4.790    0.423\n    math4             3.000                               7.185    0.558\n    math5             4.000                               9.580    0.652\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial ~~                                                            \n    Growth            4.729    0.741    6.380    0.000    0.219    0.219\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Initial          70.540    0.291  242.420    0.000    7.834    7.834\n    Growth            8.853    0.079  111.512    0.000    3.696    3.696\n   .math1             0.000                               0.000    0.000\n   .math2             0.000                               0.000    0.000\n   .math3             0.000                               0.000    0.000\n   .math4             0.000                               0.000    0.000\n   .math5             0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .math1             7.064    0.620   11.396    0.000    7.064    0.080\n   .math2             4.833    0.349   13.845    0.000    4.833    0.048\n   .math3             5.072    0.289   17.577    0.000    5.072    0.040\n   .math4             4.889    0.357   13.708    0.000    4.889    0.029\n   .math5             4.984    0.599    8.313    0.000    4.984    0.023\n    Initial          81.071    3.790   21.392    0.000    1.000    1.000\n    Growth            5.736    0.283   20.243    0.000    1.000    1.000\n\n\n\n\ncompareFit(lgm_math, lgm_math_unequal) |&gt; summary() |&gt; print()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n                 Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)  \nlgm_math_unequal 10 29331 29380 68.644                                         \nlgm_math         14 29335 29364 80.454      11.81 0.044187       4    0.01882 *\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n####################### Model Fit Indices ###########################\n                   chisq df pvalue rmsea   cfi   tli  srmr        aic        bic\nlgm_math_unequal 68.644† 10   .000 .077  .994† .994  .018  29331.158† 29380.235 \nlgm_math         80.454  14   .000 .069† .993  .995† .017† 29334.968  29364.414†\n\n################## Differences in Fit Indices #######################\n                            df  rmsea    cfi   tli   srmr  aic     bic\nlgm_math - lgm_math_unequal  4 -0.008 -0.001 0.001 -0.002 3.81 -15.821\n\nThe following lavaan models were compared:\n    lgm_math_unequal\n    lgm_math\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.",
    "crumbs": [
      "Keith's",
      "Latent Growth Models"
    ]
  },
  {
    "objectID": "contents/chap21.html#conditional-growth-model",
    "href": "contents/chap21.html#conditional-growth-model",
    "title": "Chapter 21. Latent Growth Models",
    "section": "Conditional growth model",
    "text": "Conditional growth model\nUnstandardized Estimates\n Standardized Estimates\n\n두드러진 점들\n\nFemale: Initial, Growth에 모두 유의한 차이가 없음 (각각, 0.04, 0.05)\nCognitive: 모두에서 큰 효과가 있음 (각각, 0.43, 0.41)\nAge: Initial로는 효과가 없는 반면, Growth에서 반대로(음수) 효과가 나타남; 유치원 입학을 늦춘 이유는?\nInitial ~ Growth의 상관관계가 예측변수들에 의해 다 설명되었음; r=0.21에서 (residual) r=0.007\n한편, Initial과 Growth의 residual variance(\\(1-R^2\\))는 각각 0.782, 0.779로 Initial과 Growth의 변량을 많이 설명하지는 못함.\n\n\n# Conditional growth model\nlgm_math_cond_model &lt;- '\n  # latents\n  Initial =~ 1*math1 + 1*math2 + 1*math3 + 1*math4 + 1*math5\n  Growth =~ 0*math1 + 1*math2 + 2*math3 + 3*math4 + 4*math5\n  \n  # intercepts\n  Initial + Growth ~ 1\n  sex + ParEd + Cognitive + age ~ 1\n\n  # an alterative syntax for the intercepts\n  # Initial ~ 1\n  # Growth ~ 1\n  # sex ~ 1\n  # ParEd ~ 1\n  # Cognitive ~ 1\n  # age ~ 1\n  \n  # residual (co)variances\n  Growth ~~ Initial\n  ParEd ~~ Cognitive\n  \n  math1 ~~ a*math1\n  math2 ~~ a*math2\n  math3 ~~ a*math3\n  math4 ~~ a*math4\n  math5 ~~ a*math5\n  \n  # regression\n  Initial ~ sex +  ParEd + Cognitive + age\n  Growth ~ sex + ParEd + Cognitive + age\n'\nlgm_math_cond &lt;- growth(lgm_math_cond_model, data = math_growth)\nsummary(lgm_math_cond, \n        remove.unused = FALSE, # keep the unused parameters\n        standardized = TRUE, \n        fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 138 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        27\n  Number of equality constraints                     4\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                92.768\n  Degrees of freedom                                31\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              9839.753\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.994\n  Tucker-Lewis Index (TLI)                       0.993\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23160.430\n  Loglikelihood unrestricted model (H1)     -23114.046\n                                                      \n  Akaike (AIC)                               46366.859\n  Bayesian (BIC)                             46479.738\n  Sample-size adjusted Bayesian (SABIC)      46406.688\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.045\n  90 Percent confidence interval - lower         0.034\n  90 Percent confidence interval - upper         0.055\n  P-value H_0: RMSEA &lt;= 0.050                    0.788\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.012\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial =~                                                            \n    math1             1.000                               9.013    0.969\n    math2             1.000                               9.013    0.895\n    math3             1.000                               9.013    0.796\n    math4             1.000                               9.013    0.698\n    math5             1.000                               9.013    0.612\n  Growth =~                                                             \n    math1             0.000                               0.000    0.000\n    math2             1.000                               2.418    0.240\n    math3             2.000                               4.835    0.427\n    math4             3.000                               7.253    0.562\n    math5             4.000                               9.671    0.657\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial ~                                                             \n    sex               0.777    0.516    1.505    0.132    0.086    0.043\n    ParEd             0.887    0.187    4.738    0.000    0.098    0.137\n    Cognitive         0.256    0.017   14.709    0.000    0.028    0.426\n    age              -0.051    0.127   -0.400    0.689   -0.006   -0.011\n  Growth ~                                                              \n    sex               0.246    0.143    1.723    0.085    0.102    0.051\n    ParEd             0.160    0.052    3.097    0.002    0.066    0.092\n    Cognitive         0.067    0.005   13.871    0.000    0.028    0.413\n    age              -0.201    0.035   -5.736    0.000   -0.083   -0.169\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .Initial ~~                                                            \n   .Growth            0.112    0.583    0.192    0.847    0.007    0.007\n  ParEd ~~                                                              \n    Cognitive         2.940    0.667    4.411    0.000    2.940    0.141\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Initial          33.402    9.322    3.583    0.000    3.706    3.706\n   .Growth           13.249    2.573    5.149    0.000    5.480    5.480\n    sex               0.507    0.016   32.069    0.000    0.507    1.014\n    ParEd            16.133    0.044  366.346    0.000   16.133   11.585\n    Cognitive       101.001    0.474  213.113    0.000  101.001    6.739\n    age              68.518    0.064 1068.229    0.000   68.518   33.780\n   .math1             0.000                               0.000    0.000\n   .math2             0.000                               0.000    0.000\n   .math3             0.000                               0.000    0.000\n   .math4             0.000                               0.000    0.000\n   .math5             0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .math1      (a)    5.228    0.135   38.730    0.000    5.228    0.060\n   .math2      (a)    5.228    0.135   38.730    0.000    5.228    0.052\n   .math3      (a)    5.228    0.135   38.730    0.000    5.228    0.041\n   .math4      (a)    5.228    0.135   38.730    0.000    5.228    0.031\n   .math5      (a)    5.228    0.135   38.730    0.000    5.228    0.024\n    sex               0.250    0.011   22.361    0.000    0.250    1.000\n    ParEd             1.939    0.087   22.361    0.000    1.939    1.000\n    Cognitive       224.611   10.045   22.361    0.000  224.611    1.000\n    age               4.114    0.184   22.361    0.000    4.114    1.000\n   .Initial          63.501    2.981   21.300    0.000    0.782    0.782\n   .Growth            4.554    0.227   20.023    0.000    0.779    0.779",
    "crumbs": [
      "Keith's",
      "Latent Growth Models"
    ]
  },
  {
    "objectID": "contents/chap21.html#conditional-growth-model---revised",
    "href": "contents/chap21.html#conditional-growth-model---revised",
    "title": "Chapter 21. Latent Growth Models",
    "section": "Conditional growth model - revised",
    "text": "Conditional growth model - revised\n\n# Conditional growth model - revised\nlgm_math_cond_model2 &lt;- '\n  # latents\n  Initial =~ 1*math1 + 1*math2 + 1*math3 + 1*math4 + 1*math5\n  Growth =~ 0*math1 + 1*math2 + 2*math3 + math4 + math5\n  \n  # intercepts\n  Initial + Growth ~ 1\n  sex + ParEd + Cognitive + age ~ 1\n    \n  # residual (co)variances\n  Growth ~~ 0*Initial # removed covariance\n  ParEd ~~ Cognitive\n  \n  math1 ~~ a*math1\n  math2 ~~ a*math2\n  math3 ~~ a*math3\n  math4 ~~ a*math4\n  math5 ~~ a*math5\n  \n  # regression\n  Initial ~ ParEd + Cognitive  # remove sex, age\n  Growth ~ ParEd + Cognitive + age  # remove sex\n'\nlgm_math_cond2 &lt;- growth(lgm_math_cond_model2, data = math_growth)\nsummary(lgm_math_cond2, \n        remove.unused = FALSE, # keep the unused parameters\n        standardized = TRUE, \n        fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 127 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        25\n  Number of equality constraints                     4\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                43.024\n  Degrees of freedom                                33\n  P-value (Chi-square)                           0.114\n\nModel Test Baseline Model:\n\n  Test statistic                              9839.753\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.999\n  Tucker-Lewis Index (TLI)                       0.999\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23135.558\n  Loglikelihood unrestricted model (H1)     -23114.046\n                                                      \n  Akaike (AIC)                               46313.115\n  Bayesian (BIC)                             46416.178\n  Sample-size adjusted Bayesian (SABIC)      46349.481\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.017\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.031\n  P-value H_0: RMSEA &lt;= 0.050                    1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.017\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial =~                                                            \n    math1             1.000                               9.001    0.970\n    math2             1.000                               9.001    0.894\n    math3             1.000                               9.001    0.791\n    math4             1.000                               9.001    0.700\n    math5             1.000                               9.001    0.613\n  Growth =~                                                             \n    math1             0.000                               0.000    0.000\n    math2             1.000                               2.507    0.249\n    math3             2.000                               5.015    0.441\n    math4             2.905    0.013  217.475    0.000    7.284    0.566\n    math5             3.871    0.018  220.189    0.000    9.706    0.661\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial ~                                                             \n    ParEd             0.883    0.188    4.710    0.000    0.098    0.137\n    Cognitive         0.254    0.017   14.564    0.000    0.028    0.423\n  Growth ~                                                              \n    ParEd             0.166    0.054    3.088    0.002    0.066    0.092\n    Cognitive         0.069    0.005   13.800    0.000    0.027    0.412\n    age              -0.208    0.036   -5.701    0.000   -0.083   -0.168\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .Initial ~~                                                            \n   .Growth            0.000                               0.000    0.000\n  ParEd ~~                                                              \n    Cognitive         2.940    0.667    4.411    0.000    2.940    0.141\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Initial          30.346    3.289    9.226    0.000    3.371    3.371\n   .Growth           13.795    2.667    5.172    0.000    5.502    5.502\n    sex               0.507    0.016   32.069    0.000    0.507    1.014\n    ParEd            16.133    0.044  366.346    0.000   16.133   11.585\n    Cognitive       101.001    0.474  213.113    0.000  101.001    6.739\n    age              68.518    0.064 1068.229    0.000   68.518   33.780\n   .math1             0.000                               0.000    0.000\n   .math2             0.000                               0.000    0.000\n   .math3             0.000                               0.000    0.000\n   .math4             0.000                               0.000    0.000\n   .math5             0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .math1      (a)    5.131    0.132   38.771    0.000    5.131    0.060\n   .math2      (a)    5.131    0.132   38.771    0.000    5.131    0.051\n   .math3      (a)    5.131    0.132   38.771    0.000    5.131    0.040\n   .math4      (a)    5.131    0.132   38.771    0.000    5.131    0.031\n   .math5      (a)    5.131    0.132   38.771    0.000    5.131    0.024\n    ParEd             1.939    0.087   22.361    0.000    1.939    1.000\n    Cognitive       224.611   10.045   22.361    0.000  224.611    1.000\n    age               4.114    0.184   22.361    0.000    4.114    1.000\n    sex               0.250    0.011   22.361    0.000    0.250    1.000\n   .Initial          63.715    2.981   21.374    0.000    0.786    0.786\n   .Growth            4.921    0.249   19.797    0.000    0.783    0.783\n\n\n\n\nCompare the two models\n\n# Compare the two models\ncompareFit(lgm_math_cond, lgm_math_cond2, nested = FALSE) |&gt; summary() |&gt; print()\n\n####################### Model Fit Indices ###########################\n                 chisq df pvalue rmsea    cfi    tli  srmr        aic        bic\nlgm_math_cond  92.768  31   .000 .045   .994   .993  .012† 46366.859  46479.738 \nlgm_math_cond2 43.024† 33   .114 .017† 0.999† 0.999† .017  46313.115† 46416.178†\n\nThe following lavaan models were compared:\n    lgm_math_cond\n    lgm_math_cond2\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.",
    "crumbs": [
      "Keith's",
      "Latent Growth Models"
    ]
  },
  {
    "objectID": "contents/chap21.html#compare-the-two-models",
    "href": "contents/chap21.html#compare-the-two-models",
    "title": "Chapter 21. Latent Growth Models",
    "section": "Compare the two models",
    "text": "Compare the two models\n\n# Compare the two models\nlavTestLRT(lgm_math_cond, lgm_math_cond2) |&gt; print()\n\n\nChi-Squared Difference Test\n\n               Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nlgm_math_cond  31 46367 46480 92.768                                    \nlgm_math_cond2 33 46313 46416 43.024    -49.744     0       2          1",
    "crumbs": [
      "Keith's",
      "Latent Growth Models"
    ]
  },
  {
    "objectID": "contents/chap21.html#conditional-growth-model---free-slopes",
    "href": "contents/chap21.html#conditional-growth-model---free-slopes",
    "title": "Chapter 21. Latent Growth Models",
    "section": "Conditional growth model - free slopes",
    "text": "Conditional growth model - free slopes\n\n# Conditional growth model - free slopes\nlgm_math_cond_model3 &lt;- '\n  # latents\n  Initial =~ 1*math1 + 1*math2 + 1*math3 + 1*math4 + 1*math5\n  Growth =~ 0*math1 + 1*math2 + 2*math3 + NA*math4 + NA*math5  # free slopes of math4 and math5\n  \n  # intercepts\n  Initial + Growth ~ 1\n  sex + ParEd + Cognitive + age ~ 1\n  \n  # residual (co)variances\n  Growth ~~ 0*Initial # removed covariance\n  ParEd ~~ Cognitive\n  \n  math1 ~~ a*math1\n  math2 ~~ a*math2\n  math3 ~~ a*math3\n  math4 ~~ a*math4\n  math5 ~~ a*math5\n  \n  # regression\n  Initial ~ ParEd + Cognitive  # remove sex, age\n  Growth ~ ParEd + Cognitive + age  # remove sex\n'\nlgm_math_cond3 &lt;- growth(lgm_math_cond_model3, data = math_growth)\nsummary(lgm_math_cond3, \n        remove.unused = FALSE, # keep the unused parameters\n        standardized = TRUE, \n        fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 127 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        25\n  Number of equality constraints                     4\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                43.024\n  Degrees of freedom                                33\n  P-value (Chi-square)                           0.114\n\nModel Test Baseline Model:\n\n  Test statistic                              9839.753\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.999\n  Tucker-Lewis Index (TLI)                       0.999\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23135.558\n  Loglikelihood unrestricted model (H1)     -23114.046\n                                                      \n  Akaike (AIC)                               46313.115\n  Bayesian (BIC)                             46416.178\n  Sample-size adjusted Bayesian (SABIC)      46349.481\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.017\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.031\n  P-value H_0: RMSEA &lt;= 0.050                    1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.017\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial =~                                                            \n    math1             1.000                               9.001    0.970\n    math2             1.000                               9.001    0.894\n    math3             1.000                               9.001    0.791\n    math4             1.000                               9.001    0.700\n    math5             1.000                               9.001    0.613\n  Growth =~                                                             \n    math1             0.000                               0.000    0.000\n    math2             1.000                               2.507    0.249\n    math3             2.000                               5.015    0.441\n    math4             2.905    0.013  217.475    0.000    7.284    0.566\n    math5             3.871    0.018  220.189    0.000    9.706    0.661\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial ~                                                             \n    ParEd             0.883    0.188    4.710    0.000    0.098    0.137\n    Cognitive         0.254    0.017   14.564    0.000    0.028    0.423\n  Growth ~                                                              \n    ParEd             0.166    0.054    3.088    0.002    0.066    0.092\n    Cognitive         0.069    0.005   13.800    0.000    0.027    0.412\n    age              -0.208    0.036   -5.701    0.000   -0.083   -0.168\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .Initial ~~                                                            \n   .Growth            0.000                               0.000    0.000\n  ParEd ~~                                                              \n    Cognitive         2.940    0.667    4.411    0.000    2.940    0.141\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Initial          30.346    3.289    9.226    0.000    3.371    3.371\n   .Growth           13.795    2.667    5.172    0.000    5.502    5.502\n    sex               0.507    0.016   32.069    0.000    0.507    1.014\n    ParEd            16.133    0.044  366.346    0.000   16.133   11.585\n    Cognitive       101.001    0.474  213.113    0.000  101.001    6.739\n    age              68.518    0.064 1068.229    0.000   68.518   33.780\n   .math1             0.000                               0.000    0.000\n   .math2             0.000                               0.000    0.000\n   .math3             0.000                               0.000    0.000\n   .math4             0.000                               0.000    0.000\n   .math5             0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .math1      (a)    5.131    0.132   38.771    0.000    5.131    0.060\n   .math2      (a)    5.131    0.132   38.771    0.000    5.131    0.051\n   .math3      (a)    5.131    0.132   38.771    0.000    5.131    0.040\n   .math4      (a)    5.131    0.132   38.771    0.000    5.131    0.031\n   .math5      (a)    5.131    0.132   38.771    0.000    5.131    0.024\n    ParEd             1.939    0.087   22.361    0.000    1.939    1.000\n    Cognitive       224.611   10.045   22.361    0.000  224.611    1.000\n    age               4.114    0.184   22.361    0.000    4.114    1.000\n    sex               0.250    0.011   22.361    0.000    0.250    1.000\n   .Initial          63.715    2.981   21.374    0.000    0.786    0.786\n   .Growth            4.921    0.249   19.797    0.000    0.783    0.783\n\n\n\n\nCompare the two models\n\n# Compare the two models\ncompareFit(lgm_math_cond, lgm_math_cond3, nested = FALSE) |&gt; summary() |&gt; print()\n\n####################### Model Fit Indices ###########################\n                 chisq df pvalue rmsea    cfi    tli  srmr        aic        bic\nlgm_math_cond  92.768  31   .000 .045   .994   .993  .012† 46366.859  46479.738 \nlgm_math_cond3 43.024† 33   .114 .017† 0.999† 0.999† .017  46313.115† 46416.178†\n\nThe following lavaan models were compared:\n    lgm_math_cond\n    lgm_math_cond3\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.",
    "crumbs": [
      "Keith's",
      "Latent Growth Models"
    ]
  },
  {
    "objectID": "contents/chap21.html#the-early-childhood-longitudinal-study-kindergarten-class-of-1998-99-ecls-k-website",
    "href": "contents/chap21.html#the-early-childhood-longitudinal-study-kindergarten-class-of-1998-99-ecls-k-website",
    "title": "Chapter 21. Latent Growth Models",
    "section": "The Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K): website",
    "text": "The Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K): website\n\nThe test is designed to measure “conceptual knowledge, procedural knowledge, and problem solving” with math items ranging from simple (number knowledge) to advanced (algebra) (DiPerna, Lei, & Reid, 2007, p. 372)\n\n\n\nmath_growth &lt;- haven::read_sav(\"data/chap 21 latent growth models/math growth final.sav\")\nmath_growth |&gt; print()\n\n# A tibble: 1,000 × 9\n  math1 math2 math3 math4 math5   sex   age ParEd Cognitive\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1  57.4  65.8  78.7  86.0  93.8     1  68.2    16        95\n2  81.4  87.9  94.2 104.  111.      1  70.8    15       104\n3  74.7  82.0  95.5 109.  120.      0  65.4    15       107\n4  71.1  80.4  88.2  97.8 102.      1  68.9    14       121\n5  70.1  79.1  85.2  92.1 102.      1  69.8    17       112\n6  71.7  84.2  86.2  88.2  94.9     0  67.5    19        69\n# ℹ 994 more rows\n\n\n\n# 분포 확인: 특히 age\nresults &lt;- MVN::mvn(data = math_growth, univariatePlot = \"histogram\") # normality histogram",
    "crumbs": [
      "Keith's",
      "Latent Growth Models"
    ]
  },
  {
    "objectID": "contents/chap21.html#the-early-childhood-longitudinal-study-kindergarten-class-of-1998-99-ecls-k",
    "href": "contents/chap21.html#the-early-childhood-longitudinal-study-kindergarten-class-of-1998-99-ecls-k",
    "title": "Chapter 21. Latent Growth Models",
    "section": "The Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K)",
    "text": "The Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K)\nNCES website\n\nThe test is designed to measure “conceptual knowledge, procedural knowledge, and problem solving” with math items ranging from simple (number knowledge) to advanced (algebra) (DiPerna, Lei, & Reid, 2007, p. 372)\n\n\n\nmath_growth &lt;- haven::read_sav(\"data/chap 21 latent growth models/math growth final.sav\")\nmath_growth |&gt; print()\n\n# A tibble: 1,000 × 9\n  math1 math2 math3 math4 math5   sex   age ParEd Cognitive\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1  57.4  65.8  78.7  86.0  93.8     1  68.2    16        95\n2  81.4  87.9  94.2 104.  111.      1  70.8    15       104\n3  74.7  82.0  95.5 109.  120.      0  65.4    15       107\n4  71.1  80.4  88.2  97.8 102.      1  68.9    14       121\n5  70.1  79.1  85.2  92.1 102.      1  69.8    17       112\n6  71.7  84.2  86.2  88.2  94.9     0  67.5    19        69\n# ℹ 994 more rows\n\n\n\n# 분포 확인: 특히 age\nresults &lt;- MVN::mvn(data = math_growth, univariatePlot = \"histogram\") # normality histogram",
    "crumbs": [
      "Keith's",
      "Latent Growth Models"
    ]
  }
]