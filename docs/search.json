[
  {
    "objectID": "contents/baser.html",
    "href": "contents/baser.html",
    "title": "Base R",
    "section": "",
    "text": "90년대에 통계 분석을 위해 개발된 R 언어와 대비하여, 좀 더 직관적이고 효율적인 데이터 분석을 위해 새로운 문법이 R내의 패키지 형태로 구현되었는데 이 새로운 생태계 안의 패키지들의 모임이 Tidyverse라는 이름하에 발전하고 있음: Tidyverse\n이 패키지들은 design philosophy, grammar, data structures를 공유하며 유기적으로 작동됨.\n기존 R의 문법과는 상당한 차이가 있어 단점도 지적되고 있고, 소위 base-R을 고수하는 사람들과 tidyverse를 기본으로 사용하는 사람들이 나뉘어 있다고 알려져 있음.\n아마도 빠르게 발전하고 있는 tidyverse/tidymodel 생태계의 언어들이 기본으로 자리잡지 않을까 함.\n본 강의에서는 주로 tidyverse의 언어로만 분석하고자 함.",
    "crumbs": [
      "R tutorial",
      "Base R"
    ]
  },
  {
    "objectID": "contents/baser.html#r의-데이터-구조와-변수-타입",
    "href": "contents/baser.html#r의-데이터-구조와-변수-타입",
    "title": "Base R",
    "section": "R의 데이터 구조와 변수 타입",
    "text": "R의 데이터 구조와 변수 타입\n주로 vector (벡터)와 data frame (데이터프레임)을 다룸\n\nSource: R in Action by Rob Kabacoff\nData frame의 예\n\n각 column이 하나의 variable (변수)를 구성하고, 한가지 타입의 데이터로 이루어짐\n\n각 Row가 하나의 observation (관측치)을 구성함.\n\n이러한 형태를 갖춘 데이터를 tidy라고도 부르며, 이를 벗어난 형태의 경우 가공이 필요함.\nex. “m23”: male이고 23세임을 나타내는 표기도 있음\n\n\nlibrary(tidyverse)\n\ncps &lt;- mosaicData::CPS85 # mosaicData package의 CPS85 데이터셋\nhead(cps) |&gt; print() # print()는 생략할 것!\n\n  wage educ race sex hispanic south married exper union age   sector\n1  9.0   10    W   M       NH    NS Married    27   Not  43    const\n2  5.5   12    W   M       NH    NS Married    20   Not  38    sales\n3  3.8   12    W   F       NH    NS  Single     4   Not  22    sales\n4 10.5   12    W   F       NH    NS Married    29   Not  47 clerical\n5 15.0   12    W   M       NH    NS Married    40 Union  58    const\n6  9.0   16    W   F       NH    NS Married    27   Not  49 clerical\n\n\n\ncps &lt;- as_tibble(cps) # tibble vs. data.frame\nhead(cps) |&gt; print()\n\n# A tibble: 6 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1   9      10 W     M     NH       NS    Married    27 Not      43 const   \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales   \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales   \n4  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n5  15      12 W     M     NH       NS    Married    40 Union    58 const   \n6   9      16 W     F     NH       NS    Married    27 Not      49 clerical\n\n\n\n# Dataset의 설명\nhelp(CPS85, package=\"mosaicData\") # 또는\n?mosaicData::CPS85",
    "crumbs": [
      "R tutorial",
      "Base R"
    ]
  },
  {
    "objectID": "contents/baser.html#vector",
    "href": "contents/baser.html#vector",
    "title": "Base R",
    "section": "Vector",
    "text": "Vector\n한 가지 타입으로만 구성: 숫자 (numeric), 문자 (character), 논리형 (logical), factor, etc\n\nvar &lt;- c(1, 2, 5, 3, 6, -2, 4) # 변수에 assign: '=' 대신 '&lt;-'\nnm &lt;- c(\"one\", \"two\", \"three\")\ntf &lt;- c(TRUE, TRUE, TRUE, FALSE, TRUE, FALSE)\n\n# 타입/클래스 확인\nclass(var)\n## [1] \"double\"\n\nclass(nm)\n## [1] \"character\"\n\nclass(tf)\n## [1] \"logical\"\n\n\n원소의 추출 및 대체\n다음은 원소를 추출, 대체하는 R의 native한 방식임\n수업에서는 뒤에서 다룰 tidyverse 문법을 주로 활용할 것임\nVector의 경우\n\nvar\n## [1]  1  2  5  3  6 -2  4\n\nvar[3]\n## [1] 5\n\nvar[c(1, 3, 5)]\n## [1] 1 5 6\n\nvar[2:6] # \":\"\" slicing: c(2, 3, 4, 5, 6)\n## [1]  2  5  3  6 -2\n\nvar[c(1, 3:5)] # 혼합\n## [1] 1 5 3 6\n\nvar[-c(1, 3)] # \"-\"는 제외라는 의미\n## [1]  2  3  6 -2  4\n\nc(10, var, 100, 101) # 추가\n##  [1]  10   1   2   5   3   6  -2   4 100 101\n\nvar[2] &lt;- 55 # 대체\n## var\n## [1]  1 55  5  3  6 -2  4\n\nvar[c(2, 5)] &lt;- c(200, 500) # 대체\n## var\n## [1]   1 200   5   3 500  -2   4\n\n# numeric 벡터의 연산: recycling rule\n1:5 * 2\n## [1]  2  4  6  8 10\n\nc(1, 3, 5) - 5\n## [1] -4 -2  0\n\nc(2, 4, 6) / 2\n## [1] 1 2 3\n\nc(1, 3) * c(2, 4)\n## [1]  2 12\n\nc(1, 3) - c(2, 4)\n## [1] -1 -1",
    "crumbs": [
      "R tutorial",
      "Base R"
    ]
  },
  {
    "objectID": "contents/baser.html#factor",
    "href": "contents/baser.html#factor",
    "title": "Base R",
    "section": "Factor",
    "text": "Factor\nVector로서 명목변수(카테고리)를 다룸\npatientID &lt;- c(1, 2, 1, 3)\ndiabetes &lt;- c(\"Type1\", \"Type2\", \"Type1\", \"Type1\")\nstatus &lt;- c(\"Poor\", \"Improved\", \"Excellent\", \"Poor\")\n\n# factor로 변환: 알파벳 순서로 levels의 순서가 정해짐\nfactor(patientID)\n## [1] 1 2 1 3\n## Levels: 1 2 3\n\nfactor(diabetes)\n## [1] Type1 Type2 Type1 Type1\n## Levels: Type1 Type2\n\nfactor(status, order = TRUE) # order를 표시\n## [1] Poor      Improved  Excellent Poor     \n## Levels: Excellent &lt; Improved &lt; Poor\n\n# 구체적으로 표시하는 것을 추천: 지정한 성분 순서대로 levels의 순서가 정해짐\nfactor(status, levels = c(\"Poor\", \"Improved\", \"Excellent\"),\n                                         order = TRUE)\n## [1] Poor      Improved  Excellent Poor     \n## Levels: Poor &lt; Improved &lt; Excellent\n\n# order가 없을시\nfactor(status, levels = c(\"Poor\", \"Improved\", \"Excellent\"))\n## [1] Poor      Improved  Excellent Poor     \n## Levels: Poor Improved Excellent\n\n# 대표적으로 성별을 코딩할 때: 숫자대신 레이블로 표시\nsex &lt;- c(1, 2, 1, 1, 1, 2, 2, 1)\nfactor(sex, levels = c(1, 2), labels = c(\"Male\", \"Female\"))\n## [1] Male   Female Male   Male   Male   Female Female Male  \n## Levels: Male Female\n\nsex_fct &lt;- factor(sex, levels = c(1, 2), labels = c(\"Male\", \"Female\"))\n\nlevels(sex) # 레벨 확인\n## NULL\nlevels(sex_fct) # 레벨 확인\n## [1] \"Male\"   \"Female\"\n\nsex\n## [1] 1 2 1 1 1 2 2 1\nsex_fct\n## [1] Male   Female Male   Male   Male   Female Female Male  \n## Levels: Male Female",
    "crumbs": [
      "R tutorial",
      "Base R"
    ]
  },
  {
    "objectID": "contents/baser.html#data-frame",
    "href": "contents/baser.html#data-frame",
    "title": "Base R",
    "section": "Data Frame",
    "text": "Data Frame\n\n데이터 프레임의 구성\n# 벡터들로부터 데이터 프레임 구성\npatientID &lt;- c(1, 2, 3, 4)\nage &lt;- c(25, 34, 28, 52)\ndiabetes &lt;- c(\"Type1\", \"Type2\", \"Type1\", \"Type1\")\nstatus &lt;- c(\"Poor\", \"Improved\", \"Excellent\", \"Poor\")\n\npatientdata &lt;- data.frame(patientID, age, diabetes, status)\n\npatientdata\n##   patientID age diabetes    status\n## 1         1  25    Type1      Poor\n## 2         2  34    Type2  Improved\n## 3         3  28    Type1 Excellent\n## 4         4  52    Type1      Poor\n\nmidterm &lt;- data.frame(english = c(90, 80, 60, 70),\n                      math = c(50, 60, 100, 20),\n                      class = c(1, 1, 2, 2))\nmidterm\n##   english math class\n## 1      90   50     1\n## 2      80   60     1\n## 3      60  100     2\n## 4      70   20     2\n\n\n원소의 추출 및 대체\n# 원소의 추출\npatientdata[1:2] # 변수의 열을 지정\n##   patientID age\n## 1         1  25\n## 2         2  34\n## 3         3  28\n## 4         4  52\n\npatientdata[c(\"diabetes\", \"status\")] # 열 이름을 지정\n##   diabetes    status\n## 1    Type1      Poor\n## 2    Type2  Improved\n## 3    Type1 Excellent\n## 4    Type1      Poor\n\npatientdata[c(1, 3), c(\"age\", \"status\")] # 행과 열을 모두 지정\n##   age    status\n## 1  25      Poor\n## 3  28 Excellent\n\npatientdata[c(1, 3), c(2, 4)]\n##   age    status\n## 1  25      Poor\n## 3  28 Excellent\n\npatientdata[, 1:2] # patientdata[1:2]과 동일, 빈칸은 모든 행을 의미\n##   patientID age\n## 1         1  25\n## 2         2  34\n## 3         3  28\n## 4         4  52\n\npatientdata[1:2, ] # 빈칸은 모든 열을 의미\n##   patientID age diabetes   status\n## 1         1  25    Type1     Poor\n## 2         2  34    Type2 Improved\n\npatientdata[-1] # 열 제외\n##   age diabetes    status\n## 1  25    Type1      Poor\n## 2  34    Type2  Improved\n## 3  28    Type1 Excellent\n## 4  52    Type1      Poor\n\npatientdata[-c(1, 3)] # 열 제외\n##   age    status\n## 1  25      Poor\n## 2  34  Improved\n## 3  28 Excellent\n## 4  52      Poor\n\npatientdata[-c(1:2), 2:4] # 행 제외 & 열 선택\n##   age diabetes    status\n## 3  28    Type1 Excellent\n## 4  52    Type1      Poor\n\n\n# 변수/열의 성분을 벡터로 추출: $ 또는 [[ ]]을 이용\npatientdata$age # $를 이용\n## [1] 25 34 28 52\n\nclass(patientdata$age) # numeric vector임을 확인\n## [1] \"numeric\"\n\npatientdata[[\"age\"]] # patientdata$age과 동일, [[ ]] doule bracket을 이용해 벡터로 추출\n## [1] 25 34 28 52\n\npatientdata[[2]] # 열의 위치를 이용해도 동일한 추출\n## [1] 25 34 28 52\n\npatientdata[\"age\"] # [ ] single bracket은 열을 선택하는 것으로 데이터 프레임으로 추출\n##   age\n## 1  25\n## 2  34\n## 3  28\n## 4  52\n\npatientdata[2] # 2번째 열을 추출; patientdata[\"age\"]과 동일\n##   age\n## 1  25\n## 2  34\n## 3  28\n## 4  52\n\n\n데이터의 추가 및 대체\n# 데이터 추가\npatientdata$gender &lt;- c(1, 1, 2, 2) \n\npatientdata\n##   patientID age diabetes    status gender\n## 1         1  25    Type1      Poor      1\n## 2         2  34    Type2  Improved      1\n## 3         3  28    Type1 Excellent      2\n## 4         4  52    Type1      Poor      2\n\n# 데이터 대체\npatientdata[c(1,3), \"age\"] # 혼동: 원칙적으로 데이터프레임으로 추출되어야하나 벡터로 추출됨\n## [1] 25 28\n\npatientdata[c(1,3), \"age\"] &lt;- c(88, 99)\npatientdata\n##   patientID age diabetes    status gender\n## 1         1  88    Type1      Poor      1\n## 2         2  34    Type2  Improved      1\n## 3         3  99    Type1 Excellent      2\n## 4         4  52    Type1      Poor      2\n\n# 참고\nrow.names(patientdata) # 데이터 프레임의 행 이름\n## [1] \"1\" \"2\" \"3\" \"4\"\n\nrow.names(patientdata) &lt;- c(\"a\", \"b\", \"c\", \"d\")\npatientdata\n##   patientID age diabetes    status gender\n## a         1  88    Type1      Poor      1\n## b         2  34    Type2  Improved      1\n## c         3  99    Type1 Excellent      2\n## d         4  52    Type1      Poor      2",
    "crumbs": [
      "R tutorial",
      "Base R"
    ]
  },
  {
    "objectID": "contents/baser.html#tibble",
    "href": "contents/baser.html#tibble",
    "title": "Base R",
    "section": "Tibble",
    "text": "Tibble\n기존 data.frame의 단점을 보안한 tidyverse에서 기본이 되는 데이터 형식\n\nData frame vs. tibble\nPrinting의 차이\ncps &lt;- mosaicData::CPS85 # data.frame\ncps\n#   wage educ race sex hispanic south married exper union age   sector\n# 1  9.0   10    W   M       NH    NS Married    27   Not  43    const\n# 2  5.5   12    W   M       NH    NS Married    20   Not  38    sales\n# 3  3.8   12    W   F       NH    NS  Single     4   Not  22    sales\n# 4 10.5   12    W   F       NH    NS Married    29   Not  47 clerical\n# 5 15.0   12    W   M       NH    NS Married    40 Union  58    const\n# 6  9.0   16    W   F       NH    NS Married    27   Not  49 clerical\n...\n\ncps_tibble &lt;- as_tibble(cps)\ncps_tibble\n# # A tibble: 534 × 11\n#    wage  educ race  sex   hispanic south married exper union   age sector  \n#   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n# 1   9      10 W     M     NH       NS    Married    27 Not      43 const   \n# 2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales   \n# 3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales   \n# 4  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n# 5  15      12 W     M     NH       NS    Married    40 Union    58 const   \n# 6   9      16 W     F     NH       NS    Married    27 Not      49 clerical\n# # … with 528 more rows\n그 외의 차이는 R for Data Science/10.3 Tibbles vs. data.frame을 참고",
    "crumbs": [
      "R tutorial",
      "Base R"
    ]
  },
  {
    "objectID": "contents/notice.html#중간시험-대체-과제",
    "href": "contents/notice.html#중간시험-대체-과제",
    "title": "Notice",
    "section": "중간시험 대체 과제",
    "text": "중간시험 대체 과제",
    "crumbs": [
      "Notice"
    ]
  },
  {
    "objectID": "contents/notice.html#기말시험",
    "href": "contents/notice.html#기말시험",
    "title": "Notice",
    "section": "기말시험",
    "text": "기말시험",
    "crumbs": [
      "Notice"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "contents/tidyverse.html",
    "href": "contents/tidyverse.html",
    "title": "Tidyverse",
    "section": "",
    "text": "함수들: print(), glimpse(), summary(), count()\n() 안에 들어가는 것을 argument라고 부름\n\nlibrary(tidyverse)\n\ncps &lt;- as_tibble(mosaicData::CPS85) # mosaicData package의 CPS85 데이터셋\nprint(cps) # print 생략!\n\n# A tibble: 534 x 11\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1   9      10 W     M     NH       NS    Married    27 Not      43 const   \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales   \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales   \n4  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n5  15      12 W     M     NH       NS    Married    40 Union    58 const   \n6   9      16 W     F     NH       NS    Married    27 Not      49 clerical\n# i 528 more rows\n\n\n\n\n\n\n\n\nprint()\n\n\n\n강의 노트에서 print()를 쓰는 것은 jupyter notebook에서 data frame을 표시하는 방식때문이므로 무시하셔도 됩니다.\n\n\n보통 print()없이 데이터 프레임을 살펴보지만, print()을 이용하면, 표시되는 방식을 조정해서 볼 수 있음.\n\nprint(cps, n = 3) # 처음 3개 행\n\n# A tibble: 534 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector\n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n1   9      10 W     M     NH       NS    Married    27 Not      43 const \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales \n# … with 531 more rows\n\n\n\n\n\n\n\n\ntip: print() 옵션\n\n\n\n\n\nprint(tibble, n = 10, width = Inf) # 10개의 rows와 모든 columns\n기본 셋팅을 변경하려면\noptions(tibble.print_min = 10, tibble.width = Inf)\nColumns/변수들이 많은 경우 화면에서 다음과 같이 축약되어 나오는데, 이를 다 보려면\nprint(nycflights13::flights) # nycflights13 패키지의 flights 데이터\n# # A tibble: 336,776 × 19\n#    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;       &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n# 1  2013     1     1      517         515       2     830     819      11 UA     \n# 2  2013     1     1      533         529       4     850     830      20 UA     \n# 3  2013     1     1      542         540       2     923     850      33 AA     \n# 4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n# 5  2013     1     1      554         600      -6     812     837     -25 DL     \n# 6  2013     1     1      554         558      -4     740     728      12 UA     \n# # … with 336,770 more rows, 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;,\n# #   origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,\n# #   minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, and abbreviated variable names\n# #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\nprint(nycflights13::flights, n = 3, width = Inf) # 가로 열의 개수: Inf (모든 열)\n# # A tibble: 336,776 × 19\n#    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n# 1  2013     1     1      517            515         2      830            819\n# 2  2013     1     1      533            529         4      850            830\n# 3  2013     1     1      542            540         2      923            850\n#   arr_delay carrier flight tailnum origin dest  air_time distance  hour minute\n#       &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n# 1        11 UA        1545 N14228  EWR    IAH        227     1400     5     15\n# 2        20 UA        1714 N24211  LGA    IAH        227     1416     5     29\n# 3        33 AA        1141 N619AA  JFK    MIA        160     1089     5     40\n#   time_hour          \n#   &lt;dttm&gt;             \n# 1 2013-01-01 05:00:00\n# 2 2013-01-01 05:00:00\n# 3 2013-01-01 05:00:00\n# # … with 336,773 more rows\n\n\n\n많은 변수들을 간략히 보는 방법으로는 glimpse()\n\nglimpse(cps)\n\nRows: 534\nColumns: 11\n$ wage     &lt;dbl&gt; 9.00, 5.50, 3.80, 10.50, 15.00, 9.00, 9.57, 15.00, 11.00, 5.0…\n$ educ     &lt;int&gt; 10, 12, 12, 12, 12, 16, 12, 14, 8, 12, 17, 17, 14, 14, 12, 14…\n$ race     &lt;fct&gt; W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, NW, NW, W,…\n$ sex      &lt;fct&gt; M, M, F, F, M, F, F, M, M, F, M, M, M, M, M, M, M, M, M, M, F…\n$ hispanic &lt;fct&gt; NH, NH, NH, NH, NH, NH, NH, NH, NH, NH, Hisp, NH, Hisp, NH, N…\n$ south    &lt;fct&gt; NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, N…\n$ married  &lt;fct&gt; Married, Married, Single, Married, Married, Married, Married,…\n$ exper    &lt;int&gt; 27, 20, 4, 29, 40, 27, 5, 22, 42, 14, 18, 3, 4, 14, 35, 0, 7,…\n$ union    &lt;fct&gt; Not, Not, Not, Not, Union, Not, Union, Not, Not, Not, Not, No…\n$ age      &lt;int&gt; 43, 38, 22, 47, 58, 49, 23, 42, 56, 32, 41, 26, 24, 34, 53, 2…\n$ sector   &lt;fct&gt; const, sales, sales, clerical, const, clerical, service, sale…\n\n\n\n\n\n\n\n\nTip\n\n\n\n엑셀 스프레드시트처럼 보는 방법은\nEnvironment 패널에 보이는 cps 데이터셋 맨 끝에 네모난 마크를 클릭하거나,\nview(cps)\n\n\n변수들에 대한 통계치 요약 summary()\n\nsummary(cps)\n\n      wage             educ       race     sex     hispanic   south   \n Min.   : 1.000   Min.   : 2.00   NW: 67   F:245   Hisp: 27   NS:378  \n 1st Qu.: 5.250   1st Qu.:12.00   W :467   M:289   NH  :507   S :156  \n Median : 7.780   Median :12.00                                       \n Mean   : 9.024   Mean   :13.02                                       \n 3rd Qu.:11.250   3rd Qu.:15.00                                       \n Max.   :44.500   Max.   :18.00                                       \n                                                                      \n    married        exper         union          age             sector   \n Married:350   Min.   : 0.00   Not  :438   Min.   :18.00   prof    :105  \n Single :184   1st Qu.: 8.00   Union: 96   1st Qu.:28.00   clerical: 97  \n               Median :15.00               Median :35.00   service : 83  \n               Mean   :17.82               Mean   :36.83   manuf   : 68  \n               3rd Qu.:26.00               3rd Qu.:44.00   other   : 68  \n               Max.   :55.00               Max.   :64.00   manag   : 55  \n                                                           (Other) : 58  \n\n\n카테고리별 개수를 세주는 count()\nNumber(수)에 대해서도 적용 가능: ex. educ 수준 2, 3, … 18 각각에 대해서\n\ncps |&gt;  # pipe operator: alt + . (option + .)\n    count(sector) |&gt;\n    print() # 생략해도 됨\n\n# A tibble: 8 × 2\n  sector       n\n  &lt;fct&gt;    &lt;int&gt;\n1 clerical    97\n2 const       20\n3 manag       55\n4 manuf       68\n5 other       68\n6 prof       105\n7 sales       38\n8 service     83\n\n\n\ncps |&gt;\n    count(sex, married) |&gt;\n    print()\n\n# A tibble: 4 × 3\n  sex   married     n\n  &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n1 F     Married   162\n2 F     Single     83\n3 M     Married   188\n4 M     Single    101\n\n\n\n\n\n\n\n\nPipe operator\n\n\n\n|&gt; 또는 %&gt;% (’then’의 의미로…)\nx |&gt; f(y) # f(x, y),\nx |&gt; f(y) |&gt; g(z) # g(f(x, y), z)\nsummary(cps) 는 다음과 같음\ncps |&gt;\n    summary()\ncount(cps, sector)는 다음과 같음\ncps |&gt; \n    count(sector)",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/tidyverse.html#inspecting-data",
    "href": "contents/tidyverse.html#inspecting-data",
    "title": "Tidyverse",
    "section": "",
    "text": "함수들: print(), glimpse(), summary(), count()\n() 안에 들어가는 것을 argument라고 부름\n\nlibrary(tidyverse)\n\ncps &lt;- as_tibble(mosaicData::CPS85) # mosaicData package의 CPS85 데이터셋\nprint(cps) # print 생략!\n\n# A tibble: 534 x 11\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1   9      10 W     M     NH       NS    Married    27 Not      43 const   \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales   \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales   \n4  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n5  15      12 W     M     NH       NS    Married    40 Union    58 const   \n6   9      16 W     F     NH       NS    Married    27 Not      49 clerical\n# i 528 more rows\n\n\n\n\n\n\n\n\nprint()\n\n\n\n강의 노트에서 print()를 쓰는 것은 jupyter notebook에서 data frame을 표시하는 방식때문이므로 무시하셔도 됩니다.\n\n\n보통 print()없이 데이터 프레임을 살펴보지만, print()을 이용하면, 표시되는 방식을 조정해서 볼 수 있음.\n\nprint(cps, n = 3) # 처음 3개 행\n\n# A tibble: 534 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector\n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n1   9      10 W     M     NH       NS    Married    27 Not      43 const \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales \n# … with 531 more rows\n\n\n\n\n\n\n\n\ntip: print() 옵션\n\n\n\n\n\nprint(tibble, n = 10, width = Inf) # 10개의 rows와 모든 columns\n기본 셋팅을 변경하려면\noptions(tibble.print_min = 10, tibble.width = Inf)\nColumns/변수들이 많은 경우 화면에서 다음과 같이 축약되어 나오는데, 이를 다 보려면\nprint(nycflights13::flights) # nycflights13 패키지의 flights 데이터\n# # A tibble: 336,776 × 19\n#    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;       &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n# 1  2013     1     1      517         515       2     830     819      11 UA     \n# 2  2013     1     1      533         529       4     850     830      20 UA     \n# 3  2013     1     1      542         540       2     923     850      33 AA     \n# 4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n# 5  2013     1     1      554         600      -6     812     837     -25 DL     \n# 6  2013     1     1      554         558      -4     740     728      12 UA     \n# # … with 336,770 more rows, 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;,\n# #   origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,\n# #   minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, and abbreviated variable names\n# #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\nprint(nycflights13::flights, n = 3, width = Inf) # 가로 열의 개수: Inf (모든 열)\n# # A tibble: 336,776 × 19\n#    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n# 1  2013     1     1      517            515         2      830            819\n# 2  2013     1     1      533            529         4      850            830\n# 3  2013     1     1      542            540         2      923            850\n#   arr_delay carrier flight tailnum origin dest  air_time distance  hour minute\n#       &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n# 1        11 UA        1545 N14228  EWR    IAH        227     1400     5     15\n# 2        20 UA        1714 N24211  LGA    IAH        227     1416     5     29\n# 3        33 AA        1141 N619AA  JFK    MIA        160     1089     5     40\n#   time_hour          \n#   &lt;dttm&gt;             \n# 1 2013-01-01 05:00:00\n# 2 2013-01-01 05:00:00\n# 3 2013-01-01 05:00:00\n# # … with 336,773 more rows\n\n\n\n많은 변수들을 간략히 보는 방법으로는 glimpse()\n\nglimpse(cps)\n\nRows: 534\nColumns: 11\n$ wage     &lt;dbl&gt; 9.00, 5.50, 3.80, 10.50, 15.00, 9.00, 9.57, 15.00, 11.00, 5.0…\n$ educ     &lt;int&gt; 10, 12, 12, 12, 12, 16, 12, 14, 8, 12, 17, 17, 14, 14, 12, 14…\n$ race     &lt;fct&gt; W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, NW, NW, W,…\n$ sex      &lt;fct&gt; M, M, F, F, M, F, F, M, M, F, M, M, M, M, M, M, M, M, M, M, F…\n$ hispanic &lt;fct&gt; NH, NH, NH, NH, NH, NH, NH, NH, NH, NH, Hisp, NH, Hisp, NH, N…\n$ south    &lt;fct&gt; NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, N…\n$ married  &lt;fct&gt; Married, Married, Single, Married, Married, Married, Married,…\n$ exper    &lt;int&gt; 27, 20, 4, 29, 40, 27, 5, 22, 42, 14, 18, 3, 4, 14, 35, 0, 7,…\n$ union    &lt;fct&gt; Not, Not, Not, Not, Union, Not, Union, Not, Not, Not, Not, No…\n$ age      &lt;int&gt; 43, 38, 22, 47, 58, 49, 23, 42, 56, 32, 41, 26, 24, 34, 53, 2…\n$ sector   &lt;fct&gt; const, sales, sales, clerical, const, clerical, service, sale…\n\n\n\n\n\n\n\n\nTip\n\n\n\n엑셀 스프레드시트처럼 보는 방법은\nEnvironment 패널에 보이는 cps 데이터셋 맨 끝에 네모난 마크를 클릭하거나,\nview(cps)\n\n\n변수들에 대한 통계치 요약 summary()\n\nsummary(cps)\n\n      wage             educ       race     sex     hispanic   south   \n Min.   : 1.000   Min.   : 2.00   NW: 67   F:245   Hisp: 27   NS:378  \n 1st Qu.: 5.250   1st Qu.:12.00   W :467   M:289   NH  :507   S :156  \n Median : 7.780   Median :12.00                                       \n Mean   : 9.024   Mean   :13.02                                       \n 3rd Qu.:11.250   3rd Qu.:15.00                                       \n Max.   :44.500   Max.   :18.00                                       \n                                                                      \n    married        exper         union          age             sector   \n Married:350   Min.   : 0.00   Not  :438   Min.   :18.00   prof    :105  \n Single :184   1st Qu.: 8.00   Union: 96   1st Qu.:28.00   clerical: 97  \n               Median :15.00               Median :35.00   service : 83  \n               Mean   :17.82               Mean   :36.83   manuf   : 68  \n               3rd Qu.:26.00               3rd Qu.:44.00   other   : 68  \n               Max.   :55.00               Max.   :64.00   manag   : 55  \n                                                           (Other) : 58  \n\n\n카테고리별 개수를 세주는 count()\nNumber(수)에 대해서도 적용 가능: ex. educ 수준 2, 3, … 18 각각에 대해서\n\ncps |&gt;  # pipe operator: alt + . (option + .)\n    count(sector) |&gt;\n    print() # 생략해도 됨\n\n# A tibble: 8 × 2\n  sector       n\n  &lt;fct&gt;    &lt;int&gt;\n1 clerical    97\n2 const       20\n3 manag       55\n4 manuf       68\n5 other       68\n6 prof       105\n7 sales       38\n8 service     83\n\n\n\ncps |&gt;\n    count(sex, married) |&gt;\n    print()\n\n# A tibble: 4 × 3\n  sex   married     n\n  &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n1 F     Married   162\n2 F     Single     83\n3 M     Married   188\n4 M     Single    101\n\n\n\n\n\n\n\n\nPipe operator\n\n\n\n|&gt; 또는 %&gt;% (’then’의 의미로…)\nx |&gt; f(y) # f(x, y),\nx |&gt; f(y) |&gt; g(z) # g(f(x, y), z)\nsummary(cps) 는 다음과 같음\ncps |&gt;\n    summary()\ncount(cps, sector)는 다음과 같음\ncps |&gt; \n    count(sector)",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/tidyverse.html#rows",
    "href": "contents/tidyverse.html#rows",
    "title": "Tidyverse",
    "section": "Rows",
    "text": "Rows\n행에 적용되는 함수들\nfilter(), arrange(), distinct()\n\nfilter()\n조건에 맞는 행을 선택\n\nConditional operators:\n&gt;, &gt;=, &lt;, &lt;=,\n== (equal to), != (not equal to)\n& (and) | (or)\n! (not)\n%in% (includes)\n\n\n# 임금(wage)가 10이상인 사람들\ncps |&gt;\n    filter(wage &gt;= 10) |&gt;\n    print()\n\n# A tibble: 184 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n2  15      12 W     M     NH       NS    Married    40 Union    58 const   \n3  15      14 W     M     NH       NS    Single     22 Not      42 sales   \n4  11       8 W     M     NH       NS    Married    42 Not      56 manuf   \n5  25.0    17 W     M     Hisp     NS    Married    18 Not      41 prof    \n6  20.4    17 W     M     NH       NS    Single      3 Not      26 prof    \n# … with 178 more rows\n\n\n\n# 임금(wage)가 10이상이고 여성(F)들\ncps |&gt;\n    filter(wage &gt;= 10 & sex == \"F\") |&gt;\n    print()\n\n# A tibble: 62 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n2  11.2    17 NW    F     NH       NS    Married    32 Not      55 clerical\n3  25.0    17 W     F     NH       NS    Single      5 Not      28 prof    \n4  12.6    17 W     F     NH       NS    Married    13 Not      36 manag   \n5  11.7    16 W     F     NH       NS    Single     42 Not      64 clerical\n6  12.5    15 W     F     NH       NS    Married     6 Not      27 clerical\n# … with 56 more rows\n\n\n\n# 간부급(management)과 전문직(professional)에 종사하는 사람들\ncps |&gt;\n    filter(sector == \"manag\" | sector == \"prof\") |&gt;\n    print()\n\n# A tibble: 160 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector\n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n1  25.0    17 W     M     Hisp     NS    Married    18 Not      41 prof  \n2  20.4    17 W     M     NH       NS    Single      3 Not      26 prof  \n3  10      16 W     M     Hisp     NS    Married     7 Union    29 manag \n4  15      16 NW    M     NH       NS    Married    26 Union    48 manag \n5  25.0    17 W     F     NH       NS    Single      5 Not      28 prof  \n6  10      14 W     M     NH       NS    Married    22 Not      42 prof  \n# … with 154 more rows\n\n\n다음과 같이 편리하게 %in%을 이용하여 여러 항목을 포함하는, 즉 |와 ==를 합친 조건문을 생성\n즉, include인지 판별\n\n# A shorter way to select sectors for management or professional\ncps |&gt;\n    filter(sector %in% c(\"manag\", \"prof\")) |&gt;\n    print()\n\n# A tibble: 160 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector\n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n1  25.0    17 W     M     Hisp     NS    Married    18 Not      41 prof  \n2  20.4    17 W     M     NH       NS    Single      3 Not      26 prof  \n3  10      16 W     M     Hisp     NS    Married     7 Union    29 manag \n4  15      16 NW    M     NH       NS    Married    26 Union    48 manag \n5  25.0    17 W     F     NH       NS    Single      5 Not      28 prof  \n6  10      14 W     M     NH       NS    Married    22 Not      42 prof  \n# … with 154 more rows\n\n\n\n\n\n\n\n\nImportant\n\n\n\nfilter()로 얻은 데이터 프레임은 원래 데이터 프레임을 수정하는 것이 아니므로 계속 사용하려면 저장해야 함\n이후 모든 함수들에 대해서도 마찬가지\nprestige &lt;- cps |&gt;\n    filter(sector %in% c(\"manag\", \"prof\"))\n\nprestige\n#    wage  educ race  sex   hispanic south married exper union   age sector\n#   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n# 1  25.0    17 W     M     Hisp     NS    Married    18 Not      41 prof  \n# 2  20.4    17 W     M     NH       NS    Single      3 Not      26 prof  \n# 3  10      16 W     M     Hisp     NS    Married     7 Union    29 manag \n# ...\n\n\n\n\n\n\n\n\nTip\n\n\n\n잦은 실수들\ncps |&gt;\n    filter(sex = \"F\") # \"==\" vs. \"=\"\ncps |&gt;\n    filter(sector == \"manage\" | \"prof\") # | 전후 모두 완결된 조건문 필요\n\n\n\n\narrange()\nColumn의 값을 기준으로 row를 정렬\n\n# 교육정도(educ)와 임금(wage)에 따라 오름차순으로 정렬\ncps |&gt;\n    arrange(educ, wage) |&gt;\n    print(n = 10)\n\n# A tibble: 534 × 11\n    wage  educ race  sex   hispanic south married exper union   age sector \n   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;  \n 1  3.75     2 W     M     Hisp     NS    Single     16 Not      24 service\n 2  7        3 W     M     Hisp     S     Married    55 Not      64 manuf  \n 3  6        4 W     M     NH       NS    Married    54 Not      64 service\n 4 14        5 W     M     NH       S     Married    44 Not      55 const  \n 5  3        6 W     F     Hisp     NS    Married    43 Union    55 manuf  \n 6  4.62     6 NW    F     NH       S     Single     33 Not      45 manuf  \n 7  5.75     6 W     M     NH       S     Married    45 Not      57 manuf  \n 8  3.35     7 W     M     NH       S     Married    43 Not      56 manuf  \n 9  4.5      7 W     M     Hisp     S     Married    14 Not      27 service\n10  6        7 W     F     NH       S     Married    15 Not      28 manuf  \n# … with 524 more rows\n\n\ndesc()을 이용하면 내림차순으로 정렬\n\n# educ을 내림차순으로 정렬\ncps |&gt;\n    arrange(desc(educ)) |&gt;\n    print(n = 10)\n\n# A tibble: 534 × 11\n    wage  educ race  sex   hispanic south married exper union   age sector\n   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n 1 15       18 W     M     NH       NS    Married    12 Not      36 prof  \n 2 14.0     18 W     F     NH       NS    Married    14 Not      38 manag \n 3 13.5     18 W     M     NH       NS    Married    14 Union    38 prof  \n 4 20       18 W     F     NH       NS    Married    19 Not      43 manag \n 5  7       18 W     M     NH       NS    Married    33 Not      57 prof  \n 6 11.2     18 W     M     NH       NS    Married    19 Not      43 prof  \n 7  5.71    18 W     M     NH       NS    Married     3 Not      27 prof  \n 8 18       18 W     M     NH       NS    Married    15 Not      39 prof  \n 9 19       18 W     M     NH       NS    Single     13 Not      37 manag \n10 22.8     18 W     F     NH       NS    Single     37 Not      61 prof  \n# … with 524 more rows\n\n\narrange()와 filter()를 함께 사용하여 좀 더 복잡한 문제를 해결할 수 있음\n\n# 높은 지위의 섹터에서 일하는 사람들 중 임금이 상위에 있는 사람들\ncps |&gt;\n    filter(sector == \"manage\" | sector == \"prof\") |&gt;\n    arrange(desc(wage)) |&gt;\n    print()\n\n# A tibble: 105 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector\n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n1  25.0    17 W     M     Hisp     NS    Married    18 Not      41 prof  \n2  25.0    17 W     F     NH       NS    Single      5 Not      28 prof  \n3  25.0    17 W     M     NH       NS    Married    31 Not      54 prof  \n4  25.0    16 W     F     NH       S     Single      5 Not      27 prof  \n5  23.2    17 NW    F     NH       NS    Married    25 Union    48 prof  \n6  22.8    18 W     F     NH       NS    Single     37 Not      61 prof  \n# … with 99 more rows\n\n\n\n\ndistinct()**\n유티크한 조합들을 리스트\n\ncps |&gt;\n    distinct(sector, sex) |&gt;\n    print()\n\n# A tibble: 15 × 2\n   sector   sex  \n   &lt;fct&gt;    &lt;fct&gt;\n 1 const    M    \n 2 sales    M    \n 3 sales    F    \n 4 clerical F    \n 5 service  F    \n 6 manuf    M    \n 7 prof     M    \n 8 service  M    \n 9 other    M    \n10 clerical M    \n11 manag    M    \n12 prof     F    \n13 manag    F    \n14 manuf    F    \n15 other    F",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/tidyverse.html#columns",
    "href": "contents/tidyverse.html#columns",
    "title": "Tidyverse",
    "section": "Columns",
    "text": "Columns\n열에 적용되는 함수들\nmutate(), select(), rename()\n\nmutate()\nColumns/변수들로부터 값을 계산하여 새로운 변수를 만듦\n\ntips &lt;- as_tibble(reshape::tips) # reshpae 패키지 안에 tips 데이터셋\ntips |&gt; print()\n\n# A tibble: 244 x 7\n  total_bill   tip sex    smoker day   time    size\n       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;  &lt;int&gt;\n1       17.0  1.01 Female No     Sun   Dinner     2\n2       10.3  1.66 Male   No     Sun   Dinner     3\n3       21.0  3.5  Male   No     Sun   Dinner     3\n4       23.7  3.31 Male   No     Sun   Dinner     2\n5       24.6  3.61 Female No     Sun   Dinner     4\n6       25.3  4.71 Male   No     Sun   Dinner     4\n# i 238 more rows\n\n\n\ntips |&gt;\n    mutate(\n        tip_pct = tip / total_bill * 100,\n        tip_pct_per = tip_pct / size\n    ) |&gt;\n    print()\n\n# A tibble: 244 × 9\n  total_bill   tip sex    smoker day   time    size tip_pct tip_pct_per\n       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;  &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1       17.0  1.01 Female No     Sun   Dinner     2    5.94        2.97\n2       10.3  1.66 Male   No     Sun   Dinner     3   16.1         5.35\n3       21.0  3.5  Male   No     Sun   Dinner     3   16.7         5.55\n4       23.7  3.31 Male   No     Sun   Dinner     2   14.0         6.99\n5       24.6  3.61 Female No     Sun   Dinner     4   14.7         3.67\n6       25.3  4.71 Male   No     Sun   Dinner     4   18.6         4.66\n# … with 238 more rows\n\n\n\n\nselect()\nColumns/변수를 선택\n\ntips |&gt;\n    select(total_bill, tip, day, time) |&gt;\n    print()\n\n# A tibble: 244 × 4\n  total_bill   tip day   time  \n       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; \n1       17.0  1.01 Sun   Dinner\n2       10.3  1.66 Sun   Dinner\n3       21.0  3.5  Sun   Dinner\n4       23.7  3.31 Sun   Dinner\n5       24.6  3.61 Sun   Dinner\n6       25.3  4.71 Sun   Dinner\n# … with 238 more rows\n\n\n\n# tip에서 smoker까지, 그리고 size columns 선택\ntips |&gt;\n    select(tip:smoker, size) |&gt;  # select(2:4, 7)처럼 number로 선택가능\n    print()\n\n# A tibble: 244 × 4\n    tip sex    smoker  size\n  &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;int&gt;\n1  1.01 Female No         2\n2  1.66 Male   No         3\n3  3.5  Male   No         3\n4  3.31 Male   No         2\n5  3.61 Female No         4\n6  4.71 Male   No         4\n# … with 238 more rows\n\n\n\n# sex에서 day까지 columns은 제외하고\ntips |&gt;\n    select(!sex:day) |&gt; # !: not\n    print()\n\n# A tibble: 244 × 4\n  total_bill   tip time    size\n       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;int&gt;\n1       17.0  1.01 Dinner     2\n2       10.3  1.66 Dinner     3\n3       21.0  3.5  Dinner     3\n4       23.7  3.31 Dinner     2\n5       24.6  3.61 Dinner     4\n6       25.3  4.71 Dinner     4\n# … with 238 more rows\n\n\n\n# factor 타입의 변수들만 선택: 함수를 이용\ntips |&gt;\n    select(where(is.factor)) |&gt;  # 다른 함수들: is.numeric, is.character\n    print()\n\n# A tibble: 244 × 4\n  sex    smoker day   time  \n  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt; \n1 Female No     Sun   Dinner\n2 Male   No     Sun   Dinner\n3 Male   No     Sun   Dinner\n4 Male   No     Sun   Dinner\n5 Female No     Sun   Dinner\n6 Male   No     Sun   Dinner\n# … with 238 more rows\n\n\n다양한 select()의 선택방법은 ?select로 help참고\n예를 들어, starts_with(\"abc\")는 abc로 시작하는 열의 이름을 가진 열들\n\n\n\n\n\n\nNote\n\n\n\nBase R에서 행과 열의 선택과 비교하면,\ncps[2:5, c(\"wage\", \"married\")] # 2~5행과 wage, married열\n# # A tibble: 4 × 2\n#    wage married\n#   &lt;dbl&gt; &lt;fct&gt;  \n# 1   5.5 Married\n# 2   3.8 Single \n# 3  10.5 Married\n# 4  15   Married\n\ncps |&gt; \n    select(wage, married) |&gt; \n    slice(2:5) # 행을 선택\n\n\n\n\nrelocate()\nColumns의 순서를 변경\n\ntips |&gt; print(n = 2)\n\n# A tibble: 244 x 7\n  total_bill   tip sex    smoker day   time    size\n       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;  &lt;int&gt;\n1       17.0  1.01 Female No     Sun   Dinner     2\n2       10.3  1.66 Male   No     Sun   Dinner     3\n# i 242 more rows\n\n\n\ntips |&gt; \n    relocate(day, time) |&gt;  # day, time을 맨 앞으로 이동\n    print(n = 2)\n\n# A tibble: 244 x 7\n  day   time   total_bill   tip sex    smoker  size\n  &lt;fct&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;int&gt;\n1 Sun   Dinner       17.0  1.01 Female No         2\n2 Sun   Dinner       10.3  1.66 Male   No         3\n# i 242 more rows\n\n\n\ntips |&gt; \n    relocate(sex:time, tip) |&gt;  # sex부터 time까지와 tip을 맨 앞으로 이동\n    print(n = 2)\n\n# A tibble: 244 x 7\n  sex    smoker day   time     tip total_bill  size\n  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1 Female No     Sun   Dinner  1.01       17.0     2\n2 Male   No     Sun   Dinner  1.66       10.3     3\n# i 242 more rows\n\n\n\ntips |&gt; \n    relocate(day:size, .after = tip) |&gt;   # .before: 앞에, .after: 뒤에\n    print(n = 2)\n\n# A tibble: 244 x 7\n  total_bill   tip day   time    size sex    smoker\n       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; \n1       17.0  1.01 Sun   Dinner     2 Female No    \n2       10.3  1.66 Sun   Dinner     3 Male   No    \n# i 242 more rows\n\n\n\n\nrename()\nColumns의 이름을 변경\n\ncps |&gt;\n    rename(education = educ, marital = married) |&gt; # new = old\n    print()\n\n# A tibble: 534 × 11\n   wage education race  sex   hispanic south marital exper union   age sector  \n  &lt;dbl&gt;     &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1   9          10 W     M     NH       NS    Married    27 Not      43 const   \n2   5.5        12 W     M     NH       NS    Married    20 Not      38 sales   \n3   3.8        12 W     F     NH       NS    Single      4 Not      22 sales   \n4  10.5        12 W     F     NH       NS    Married    29 Not      47 clerical\n5  15          12 W     M     NH       NS    Married    40 Union    58 const   \n6   9          16 W     F     NH       NS    Married    27 Not      49 clerical\n# … with 528 more rows\n\n\n변수를 select할 때 동시에 이름도 바꿀 수 있음\n\ncps |&gt;\n    select(education = educ, marital = married) |&gt; # new = old\n    print()\n\n# A tibble: 534 × 2\n  education marital\n      &lt;int&gt; &lt;fct&gt;  \n1        10 Married\n2        12 Married\n3        12 Single \n4        12 Married\n5        12 Married\n6        16 Married\n# … with 528 more rows",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/tidyverse.html#groups",
    "href": "contents/tidyverse.html#groups",
    "title": "Tidyverse",
    "section": "Groups",
    "text": "Groups\n분석에서는 자주 카테고리별로 데이터를 나누어 통계치를 계산하곤 하는데,\ngroup_by()와 summarise()의 두 함수를 함께 사용하여 가장 자주 사용하게 됨\n\ngroup_by()\n데이터셋을 분석을 위해 의미있는 그룹으로 나눔\n다음은 성별로 데이터셋을 나눈 것인데, 실제 데이터를 수정하는 것은 아니고, 내부적으로 grouping되어 있음.\n맨 위 줄에 보면 Groups:  sex [2]로 표시되어 grouped data frame임을 명시함\n\ncps |&gt;\n    group_by(sex) |&gt; \n    print()\n\n# A tibble: 534 × 11\n# Groups:   sex [2]\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1   9      10 W     M     NH       NS    Married    27 Not      43 const   \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales   \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales   \n4  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n5  15      12 W     M     NH       NS    Married    40 Union    58 const   \n6   9      16 W     F     NH       NS    Married    27 Not      49 clerical\n# … with 528 more rows\n\n\n\n\nsummarise()\nsummarize()와 동일\ngroup별로 통계치를 구해 하나의 행으로 산출\n\n# 남녀별로 임금의 평균을 구함\ncps |&gt;\n    group_by(sex) |&gt;\n    summarise(\n        avg_wage = mean(wage, na.rm = TRUE),  # mean(): 평균, na.rm: NA를 remove할 것인가\n        n = n()  # n(): 개수\n    ) |&gt;\n    print()\n\n# A tibble: 2 × 3\n  sex   avg_wage     n\n  &lt;fct&gt;    &lt;dbl&gt; &lt;int&gt;\n1 F         7.88   245\n2 M         9.99   289\n\n\n2개 이상의 변수들로 grouping할 수 있음\n\ncps |&gt;\n    group_by(sex, married) |&gt;\n    summarize(\n        ave_wage = mean(wage),\n        sd_wage = sd(wage)) |&gt;\n    print()\n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 4 × 4\n# Groups:   sex [2]\n  sex   married ave_wage sd_wage\n  &lt;fct&gt; &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n1 F     Married     7.68    3.73\n2 F     Single      8.26    6.23\n3 M     Married    10.9     5.35\n4 M     Single      8.35    4.78\n\n\n이때, 결과 데이터 프레임은 sex로 grouping되어 있음.\ngrouping을 해제하려면 ungroup()이 필요함.\n그렇지 않으면, 저 결과는 sex로 grouped data frame임\n\nUseful summary functions\n자세한 사항은 R for Data Science/Data transformation\n\nMeasures of location: mean(), median()\nMeasures of spread: sd(), IQR(), mad()\nMeasures of rank: min(), max(), quantile(x, 0.25)\nMeasures of position: min_rank(), first(), nth(x, 2), last()\nMeasures of count: count(), n_distinct()",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/tidyverse.html#missing",
    "href": "contents/tidyverse.html#missing",
    "title": "Tidyverse",
    "section": "Missing",
    "text": "Missing\nR에서 missing values (결측치)는 NA로 표시\nNaN (not a number)는 주로 계산 결과로 나오는데, 예들 들어 0으로 나눌 때처럼, R에서는 NA로 취급되니 크게 신경쓰지 않아도 됨. 자세한 사항은 R for Data Science/Missing values 참고\nNA는 다음과 같은 성질을 지님\nNA &gt; 5\n#&gt; [1] NA\n10 == NA\n#&gt; [1] NA\nNA + 10\n#&gt; [1] NA\nNA / 2\n#&gt; [1] NA\nNA == NA\n#&gt; [1] NA\n\nx &lt;- NA\nis.na(x)\n#&gt; [1] TRUE\nNA는 filter()안의 조건문의 참거짓에 상관없이 모두 제외함\n\n실제로 조건문의 결과는 TRUE, FALSE로 이루어지짐\n\ndf &lt;- tibble(\n        one = c(1, NA, 3, 4, 2, NA), \n        two = c(2, 5, 3, NA, 10, NA), \n        three = c(\"a\", \"a\", \"a\", \"a\", \"b\", \"b\")\n    )\ndf\n#     one   two three\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n# 1     1     2 a    \n# 2    NA     5 a    \n# 3     3     3 a    \n# 4     4    NA a    \n# 5     2    10 b    \n# 6    NA    NA b    \n\nfilter(df, one &gt; 1)\n#     one   two three\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n# 1     3     3 a    \n# 2     4    NA a    \n# 3     2    10 b\n\n# NA를 포함하고자 할 때,\nfilter(df, one &gt; 1 | is.na(one))\n#     one   two three\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n# 1    NA     5 a    \n# 2     3     3 a    \n# 3     4    NA a    \n# 4     2    10 b    \n# 5    NA    NA b\n\n# NA를 포함하지 않은 행들만\nfilter(df, !is.na(one))\nfilter(df, !is.na(one) & !is.na(two)) # one, two 열에 모두 NA가 없는 행들만\n\nna.omit(df) # NA가 하나라도 있는 행은 모두 제거, 보통 결측치를 조심스럽게 대체한 후 사용\n#     one   two three\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n# 1     1     2 a    \n# 2     3     3 a    \n# 3     2    10 b \n\n# 함수 중에 NA를 직접 처리하는 경우들이 많음\nmean(df$one)\n## [1] NA\n\nmean(df$one, na.rm = TRUE) # NA removed\n## [1] 2.5\nna.rm = TRUE로 얻은 계산값에서 몇 개의 데이터로 계산되었는지 알기 위해서는\ndf |&gt; \n    group_by(three) |&gt; \n    summarise(\n        ave = mean(two, na.rm = TRUE), \n        n = n(), \n        n_notna = sum(!is.na(two))  # TRUE는 1로, FALSE는 0으로 계산됨\n    )\n#   three   ave     n n_notna\n#   &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;   &lt;int&gt;\n# 1 a      3.33     4       3\n# 2 b     10        2       1",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/tidyverse.html#summary",
    "href": "contents/tidyverse.html#summary",
    "title": "Tidyverse",
    "section": "Summary",
    "text": "Summary\n다음 dplyr 패키지의 기본 verb 함수들로 데이터를 가공하면서 필요한 통계치를 구함\n\n조건에 맞는 행들(관측치)만 필터링: filter()\n열을 재정렬: arrange()\n변수들의 선택: select()\n변수들과 함수들을 이용하여 새로운 변수를 생성: mutate()\n원하는 요약 통계치를 간추림: summarise()",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/import.html",
    "href": "contents/import.html",
    "title": "Import",
    "section": "",
    "text": "자세한 데이터 import에 대해서는 링크",
    "crumbs": [
      "R tutorial",
      "Import"
    ]
  },
  {
    "objectID": "contents/import.html#text-files-csv",
    "href": "contents/import.html#text-files-csv",
    "title": "Import",
    "section": "Text files: csv",
    "text": "Text files: csv\nreadr 패키지(tidyverse에 포함)\nread_csv(), write_csv()\n\nR 기본 함수 read.csv()를 개선\n다양한 옵션은 ?read_csv, ?write_csv 참고\n\n\ncsv 파일 읽기\naltruism.csv 파일 링크\n\nlibrary(tidyverse)\n\nhelping &lt;- read_csv(\"data/altruism.csv\")\nhelping |&gt; print()\n\n# A tibble: 120 × 12\n      id pho_1 pho_2 pho_3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1   250    95    95    95     1  2004      80      NA      80      80      70\n 2    32    58    62    NA     0  2003      62      58      59      57      56\n 3   109   100    50    50    NA  2003      90      51      51      51      52\n 4   209    77    77    64     1  2004      66      72      88      82      67\n 5    94    77    50    77     1  2003     100     100     100      51      78\n 6   260   100    75   100     0  2004     100      60      70      55      70\n 7   258    77    94    86     1  2004      91      93      85      91      73\n 8   244    90    68    20     0  2004      67      66      31      67      63\n 9   180   100    79    77     0  2003      61      51      30      51      51\n10   182    75    50    64     1  2003      80      80      70      65      70\n# … with 110 more rows, and 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nread_csv()의 자주 사용되는 옵션\nread_csv(\"data/file.csv\", skip = 2) # 첫 2절 스킵\nread_csv(\"data/file.csv\", na = \".\") # 결측치가 .으로 기록된 파일\n\n\n\n\ncsv 파일 쓰기\nwrite_csv(): 단, 쓰기를 하면서 변수 타입 소멸\n\nwrite_csv(helping, file=\"data/helping_new.csv\")",
    "crumbs": [
      "R tutorial",
      "Import"
    ]
  },
  {
    "objectID": "contents/import.html#excel-spreadsheets",
    "href": "contents/import.html#excel-spreadsheets",
    "title": "Import",
    "section": "Excel spreadsheets",
    "text": "Excel spreadsheets\nreadxl package\nread_excel(), read_xlsx(), read_xls()\n\n엑셀 파일 읽기\nstduents.xlsx 파일 링크\n\nlibrary(readxl) # install.packages(\"readxl\")\n\nstud &lt;- read_xlsx(\"data/students.xlsx\")\nstud |&gt; print()\n\n# A tibble: 1,000 × 93\n  stu_id sch_id sstratid   sex  race ethnic bys42a bys42b bys44a bys44b bys44c\n   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 124966   1249        1     2     4      1      3      4      2      4      4\n2 124972   1249        1     1     4      1      4      5      1      3      3\n3 175551   1755        1     2     3      0     NA      3      2      3      3\n4 180660   1806        1     1     4      1      2     NA      1      4      4\n5 180672   1806        1     2     4      1      2      3      1      4      3\n6 298885   2988        2     1     3      0      5      4      2      3      3\n# … with 994 more rows, and 82 more variables: bys44d &lt;dbl&gt;, bys44e &lt;dbl&gt;,\n#   bys44f &lt;dbl&gt;, bys44g &lt;dbl&gt;, bys44h &lt;dbl&gt;, bys44i &lt;dbl&gt;, bys44j &lt;dbl&gt;,\n#   bys44k &lt;dbl&gt;, bys44l &lt;dbl&gt;, bys44m &lt;dbl&gt;, bys48a &lt;dbl&gt;, bys48b &lt;dbl&gt;,\n#   bys79a &lt;dbl&gt;, byfamsiz &lt;dbl&gt;, famcomp &lt;dbl&gt;, bygrads &lt;dbl&gt;, byses &lt;dbl&gt;,\n#   byfaminc &lt;dbl&gt;, parocc &lt;dbl&gt;, bytxrstd &lt;dbl&gt;, bytxmstd &lt;dbl&gt;,\n#   bytxsstd &lt;dbl&gt;, bytxhstd &lt;dbl&gt;, bypared &lt;dbl&gt;, bytests &lt;dbl&gt;,\n#   par_inv &lt;dbl&gt;, f1s36a1 &lt;dbl&gt;, f1s36a2 &lt;dbl&gt;, f1s36b1 &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nNote\n\n\n\nSpecify sheet either by position or by name\nread_excel(\"salaries.xlsx\", sheet = 2) # The default is sheet = 1\nread_excel(\"salaries.xlsx\", sheet = \"personnel\")",
    "crumbs": [
      "R tutorial",
      "Import"
    ]
  },
  {
    "objectID": "contents/import.html#statistical-packages",
    "href": "contents/import.html#statistical-packages",
    "title": "Import",
    "section": "Statistical packages",
    "text": "Statistical packages\nSPSS의 데이터: read_sav()\nstudents-shorter.sav 파일 링크\n\nlibrary(haven) # install.packages(\"haven\")\n\nstud_spss &lt;- read_sav(\"data/students-shorter.sav\")\nstud_spss |&gt; print()\n\n# A tibble: 1,000 x 93\n   stu_id    sch_id   sstratid sex     race    ethnic  bys42a   bys42b   bys44a \n   &lt;dbl+lbl&gt; &lt;dbl+lb&gt; &lt;dbl+lb&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt; &lt;dbl+lb&gt; &lt;dbl+l&gt;\n 1 124966    1249     1        2 [Fem~ 4 [Whi~ 1 [whi~  3 [2-3~  4 [3-4~ 2 [Agr~\n 2 124972    1249     1        1 [Mal~ 4 [Whi~ 1 [whi~  4 [3-4~  5 [4-5~ 1 [Str~\n 3 175551    1755     1        2 [Fem~ 3 [Bla~ 0 [blk~ NA        3 [2-3~ 2 [Agr~\n 4 180660    1806     1        1 [Mal~ 4 [Whi~ 1 [whi~  2 [1-2~ NA       1 [Str~\n 5 180672    1806     1        2 [Fem~ 4 [Whi~ 1 [whi~  2 [1-2~  3 [2-3~ 1 [Str~\n 6 298885    2988     2        1 [Mal~ 3 [Bla~ 0 [blk~  5 [4-5~  4 [3-4~ 2 [Agr~\n 7 604419    6044     6        2 [Fem~ 4 [Whi~ 1 [whi~  4 [3-4~  5 [4-5~ 2 [Agr~\n 8 605355    6053     6        2 [Fem~ 4 [Whi~ 1 [whi~  2 [1-2~  3 [2-3~ 1 [Str~\n 9 605377    6053     6        2 [Fem~ 4 [Whi~ 1 [whi~  3 [2-3~  5 [4-5~ 2 [Agr~\n10 637529    6375     6        1 [Mal~ 3 [Bla~ 0 [blk~  5 [4-5~  6 [Ove~ 2 [Agr~\n# i 990 more rows\n# i 84 more variables: bys44b &lt;dbl+lbl&gt;, bys44c &lt;dbl+lbl&gt;, bys44d &lt;dbl+lbl&gt;,\n#   bys44e &lt;dbl+lbl&gt;, bys44f &lt;dbl+lbl&gt;, bys44g &lt;dbl+lbl&gt;, bys44h &lt;dbl+lbl&gt;,\n#   bys44i &lt;dbl+lbl&gt;, bys44j &lt;dbl+lbl&gt;, bys44k &lt;dbl+lbl&gt;, bys44l &lt;dbl+lbl&gt;,\n#   bys44m &lt;dbl+lbl&gt;, bys48a &lt;dbl+lbl&gt;, bys48b &lt;dbl+lbl&gt;, bys79a &lt;dbl+lbl&gt;,\n#   byfamsiz &lt;dbl+lbl&gt;, famcomp &lt;dbl+lbl&gt;, bygrads &lt;dbl+lbl&gt;, byses &lt;dbl+lbl&gt;,\n#   byfaminc &lt;dbl+lbl&gt;, parocc &lt;dbl&gt;, bytxrstd &lt;dbl+lbl&gt;, ...\n\n\n\nstud_spss |&gt;\n    select(ethnic) |&gt;\n    print()\n\n# A tibble: 1,000 x 1\n  ethnic            \n  &lt;dbl+lbl&gt;         \n1 1 [white-asian]   \n2 1 [white-asian]   \n3 0 [blk,namer,hisp]\n4 1 [white-asian]   \n5 1 [white-asian]   \n6 0 [blk,namer,hisp]\n# i 994 more rows\n\n\nlabelled 데이터 참고\n\ninstall.packages(\"labelled\")\nlibrary(labelled)\n\n\n# labelled 변수를 factor로 변환\nstud_spss |&gt;\n    unlabelled() |&gt;\n    print()\n\n# A tibble: 1,000 x 93\n  stu_id sch_id sstratid sex    race   ethnic bys42a bys42b bys44a bys44b bys44c\n   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; \n1 124966   1249        1 Female White~ white~ 2-3 h~ 3-4 h~ Agree  Stron~ Stron~\n2 124972   1249        1 Male   White~ white~ 3-4 h~ 4-5 h~ Stron~ Disag~ Disag~\n3 175551   1755        1 Female Black~ blk,n~ NA     2-3 h~ Agree  Disag~ Disag~\n4 180660   1806        1 Male   White~ white~ 1-2 h~ NA     Stron~ Stron~ Stron~\n5 180672   1806        1 Female White~ white~ 1-2 h~ 2-3 h~ Stron~ Stron~ Disag~\n6 298885   2988        2 Male   Black~ blk,n~ 4-5 h~ 3-4 h~ Agree  Disag~ Disag~\n# i 994 more rows\n# i 82 more variables: bys44d &lt;fct&gt;, bys44e &lt;fct&gt;, bys44f &lt;fct&gt;, bys44g &lt;fct&gt;,\n#   bys44h &lt;fct&gt;, bys44i &lt;fct&gt;, bys44j &lt;fct&gt;, bys44k &lt;fct&gt;, bys44l &lt;fct&gt;,\n#   bys44m &lt;fct&gt;, bys48a &lt;fct&gt;, bys48b &lt;fct&gt;, bys79a &lt;fct&gt;, byfamsiz &lt;fct&gt;,\n#   famcomp &lt;fct&gt;, bygrads &lt;dbl&gt;, byses &lt;dbl&gt;, byfaminc &lt;fct&gt;, parocc &lt;dbl&gt;,\n#   bytxrstd &lt;dbl&gt;, bytxmstd &lt;dbl&gt;, bytxsstd &lt;dbl&gt;, bytxhstd &lt;dbl&gt;,\n#   bypared &lt;fct&gt;, bytests &lt;dbl&gt;, par_inv &lt;dbl&gt;, f1s36a1 &lt;fct&gt;, ...\n\n\nLabels 제거하기\n\nstud &lt;- stud_spss |&gt;\n    remove_val_labels()\nstud |&gt; print()\n\n# A tibble: 1,000 x 93\n  stu_id sch_id sstratid   sex  race ethnic bys42a bys42b bys44a bys44b bys44c\n   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 124966   1249        1     2     4      1      3      4      2      4      4\n2 124972   1249        1     1     4      1      4      5      1      3      3\n3 175551   1755        1     2     3      0     NA      3      2      3      3\n4 180660   1806        1     1     4      1      2     NA      1      4      4\n5 180672   1806        1     2     4      1      2      3      1      4      3\n6 298885   2988        2     1     3      0      5      4      2      3      3\n# i 994 more rows\n# i 82 more variables: bys44d &lt;dbl&gt;, bys44e &lt;dbl&gt;, bys44f &lt;dbl&gt;, bys44g &lt;dbl&gt;,\n#   bys44h &lt;dbl&gt;, bys44i &lt;dbl&gt;, bys44j &lt;dbl&gt;, bys44k &lt;dbl&gt;, bys44l &lt;dbl&gt;,\n#   bys44m &lt;dbl&gt;, bys48a &lt;dbl&gt;, bys48b &lt;dbl&gt;, bys79a &lt;dbl&gt;, byfamsiz &lt;dbl&gt;,\n#   famcomp &lt;dbl&gt;, bygrads &lt;dbl&gt;, byses &lt;dbl&gt;, byfaminc &lt;dbl&gt;, parocc &lt;dbl&gt;,\n#   bytxrstd &lt;dbl&gt;, bytxmstd &lt;dbl&gt;, bytxsstd &lt;dbl&gt;, bytxhstd &lt;dbl&gt;,\n#   bypared &lt;dbl&gt;, bytests &lt;dbl&gt;, par_inv &lt;dbl&gt;, f1s36a1 &lt;dbl&gt;, ...",
    "crumbs": [
      "R tutorial",
      "Import"
    ]
  },
  {
    "objectID": "contents/chap19.html",
    "href": "contents/chap19.html",
    "title": "Chapter 19",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)\nlibrary(haven)\nhw_mean &lt;- read_sav(\"data/chap 19 latent means/Homework means.sav\")\nhw_mean |&gt; print()\n\n# A tibble: 1,000 x 15\n  bytxrstd  bytxmstd  bytxsstd  bytxhstd  parocc f1s36a2  ethnic  f2s25f2  eng92\n  &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;  &lt;dbl&gt; &lt;dbl+lb&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt; &lt;dbl&gt;\n1 44.2      52.6      44.5      53.9        27.4 1 [1 HO~ 1 [whi~  2 [1-3~  8   \n2 39.9      40.8      33.4      41.9        56.3 0 [NONE] 1 [whi~ NA        4.4 \n3 46.4      40.2      40.9      54          56.3 3 [4-6 ~ 1 [whi~ NA        3.25\n4 39.9      56.0      55.7      56.4        70.6 4 [7-9 ~ 1 [whi~  5 [10-~  5.5 \n5 52.7      47.9      44.8      41.8        56.3 3 [4-6 ~ 1 [whi~  5 [10-~  8.37\n6 42.3      61.9      53.5      47.0        70.6 3 [4-6 ~ 1 [whi~  1 [LES~  8.33\n# i 994 more rows\n# i 6 more variables: math92 &lt;dbl&gt;, sci92 &lt;dbl&gt;, soc92 &lt;dbl&gt;, Female &lt;dbl+lbl&gt;,\n#   byfaminc &lt;dbl&gt;, bypared &lt;dbl+lbl&gt;\nlibrary(lavaan)\nhw_model &lt;- \"\n  famback =~ parocc + byfaminc + bypared\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ f2s25f2 + f1s36a2\n  grades =~ eng92 + math92 + sci92 + soc92\n\n  bytxrstd ~~ eng92\n  bytxmstd ~~ math92\n  bytxsstd ~~ sci92\n  bytxhstd ~~ soc92\n\n  prevach ~ famback\n  grades ~ prevach + hw\n  hw ~ prevach + famback\n\"\n\nsem_fit &lt;- sem(hw_model,\n  data = hw_mean,\n  mimic = \"mplus\"\n  # meanstructure = FALSE,\n)\n\nsummary(sem_fit,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 171 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        48\n\n  Number of observations                          1000\n  Number of missing patterns                        42\n\nModel Test User Model:\n                                                      \n  Test statistic                               113.358\n  Degrees of freedom                                56\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              7792.886\n  Degrees of freedom                                78\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.993\n  Tucker-Lewis Index (TLI)                       0.990\n                                                      \n  Robust Comparative Fit Index (CFI)             0.993\n  Robust Tucker-Lewis Index (TLI)                0.990\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -32104.831\n  Loglikelihood unrestricted model (H1)             NA\n                                                      \n  Akaike (AIC)                               64305.662\n  Bayesian (BIC)                             64541.235\n  Sample-size adjusted Bayesian (SABIC)      64388.784\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.032\n  90 Percent confidence interval - lower         0.023\n  90 Percent confidence interval - upper         0.040\n  P-value H_0: RMSEA &lt;= 0.050                    1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n                                                      \n  Robust RMSEA                                   0.032\n  90 Percent confidence interval - lower         0.023\n  90 Percent confidence interval - upper         0.041\n  P-value H_0: Robust RMSEA &lt;= 0.050             1.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.023\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              15.210    0.710\n    byfaminc          0.124    0.006   19.348    0.000    1.891    0.728\n    bypared           0.069    0.003   20.632    0.000    1.049    0.834\n  prevach =~                                                            \n    bytxrstd          1.000                               8.532    0.852\n    bytxmstd          0.988    0.030   32.651    0.000    8.432    0.853\n    bytxsstd          0.960    0.031   30.702    0.000    8.189    0.819\n    bytxhstd          0.942    0.030   31.597    0.000    8.039    0.830\n  hw =~                                                                 \n    f2s25f2           1.000                               1.171    0.592\n    f1s36a2           0.994    0.103    9.671    0.000    1.164    0.688\n  grades =~                                                             \n    eng92             1.000                               2.436    0.915\n    math92            0.876    0.025   34.958    0.000    2.135    0.812\n    sci92             0.961    0.023   41.780    0.000    2.341    0.884\n    soc92             1.049    0.023   45.416    0.000    2.555    0.908\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.325    0.022   14.540    0.000    0.579    0.579\n  grades ~                                                              \n    prevach           0.140    0.011   12.973    0.000    0.490    0.490\n    hw                0.655    0.101    6.491    0.000    0.315    0.315\n  hw ~                                                                  \n    prevach           0.045    0.008    5.612    0.000    0.331    0.331\n    famback           0.018    0.005    3.986    0.000    0.235    0.235\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng92             0.486    0.256    1.900    0.057    0.486    0.086\n .bytxmstd ~~                                                           \n   .math92            1.573    0.320    4.915    0.000    1.573    0.199\n .bytxsstd ~~                                                           \n   .sci92             0.459    0.294    1.563    0.118    0.459    0.064\n .bytxhstd ~~                                                           \n   .soc92             0.266    0.278    0.956    0.339    0.266    0.042\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           51.388    0.679   75.669    0.000   51.388    2.400\n   .byfaminc          9.841    0.083  118.414    0.000    9.841    3.792\n   .bypared           3.128    0.040   78.568    0.000    3.128    2.485\n   .bytxrstd         51.257    0.320  160.386    0.000   51.257    5.120\n   .bytxmstd         51.493    0.316  163.072    0.000   51.493    5.206\n   .bytxsstd         51.179    0.320  160.149    0.000   51.179    5.117\n   .bytxhstd         51.373    0.310  165.931    0.000   51.373    5.302\n   .f2s25f2           3.280    0.066   49.942    0.000    3.280    1.659\n   .f1s36a2           2.481    0.055   45.366    0.000    2.481    1.465\n   .eng92             6.074    0.084   71.892    0.000    6.074    2.281\n   .math92            5.482    0.083   65.676    0.000    5.482    2.086\n   .sci92             5.770    0.084   68.653    0.000    5.770    2.177\n   .soc92             6.207    0.089   69.614    0.000    6.207    2.207\n    famback           0.000                               0.000    0.000\n   .prevach           0.000                               0.000    0.000\n   .hw                0.000                               0.000    0.000\n   .grades            0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          227.271   13.266   17.132    0.000  227.271    0.496\n   .byfaminc          3.161    0.198   15.973    0.000    3.161    0.469\n   .bypared           0.483    0.044   10.967    0.000    0.483    0.305\n   .bytxrstd         27.413    1.704   16.091    0.000   27.413    0.274\n   .bytxmstd         26.728    1.669   16.017    0.000   26.728    0.273\n   .bytxsstd         32.977    1.881   17.536    0.000   32.977    0.330\n   .bytxhstd         29.242    1.709   17.107    0.000   29.242    0.312\n   .f2s25f2           2.537    0.178   14.290    0.000    2.537    0.649\n   .f1s36a2           1.510    0.149   10.158    0.000    1.510    0.527\n   .eng92             1.155    0.080   14.479    0.000    1.155    0.163\n   .math92            2.349    0.122   19.249    0.000    2.349    0.340\n   .sci92             1.541    0.092   16.744    0.000    1.541    0.219\n   .soc92             1.383    0.091   15.197    0.000    1.383    0.175\n    famback         231.337   19.948   11.597    0.000    1.000    1.000\n   .prevach          48.379    3.317   14.583    0.000    0.665    0.665\n   .hw                1.021    0.154    6.644    0.000    0.745    0.745\n   .grades            3.063    0.200   15.324    0.000    0.516    0.516\nhotflash &lt;- read_sav(\"data/chap 19 latent means/hot flash simulated.sav\")\n\n# make a grouping variable as a factor\nhotflash &lt;- hotflash |&gt;\n  mutate(g = factor(Group, labels = c(\"Control\", \"Treatment\")))\n\nhotflash |&gt; print()\n\n# A tibble: 96 x 6\n  Group                       HF1   HF2  int1  int2 g        \n  &lt;dbl+lbl&gt;                 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n1 0 [Control]               33.4  32.7     64    38 Control  \n2 1 [Hypnosis Intervention] 25.5  12.7     52     5 Treatment\n3 1 [Hypnosis Intervention] 11.0   7.60     9     2 Treatment\n4 1 [Hypnosis Intervention]  8.05  4.05    13     6 Treatment\n5 0 [Control]               13.2   4.34    61    36 Control  \n6 0 [Control]                9.89  8.89    15    34 Control  \n# i 90 more rows\nhotflash_model &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre + Group\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\"\n\nsem_fit_hotflash &lt;- sem(hotflash_model,\n  data = hotflash,\n  mimic = \"mplus\"\n)\n\nsummary(sem_fit_hotflash,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 171 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        16\n\n  Number of observations                            96\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                                      \n  Test statistic                                 3.501\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.174\n\nModel Test Baseline Model:\n\n  Test statistic                               244.790\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.994\n  Tucker-Lewis Index (TLI)                       0.968\n                                                      \n  Robust Comparative Fit Index (CFI)             0.994\n  Robust Tucker-Lewis Index (TLI)                0.968\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1466.241\n  Loglikelihood unrestricted model (H1)             NA\n                                                      \n  Akaike (AIC)                                2964.482\n  Bayesian (BIC)                              3005.512\n  Sample-size adjusted Bayesian (SABIC)       2954.993\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.088\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.239\n  P-value H_0: RMSEA &lt;= 0.050                    0.245\n  P-value H_0: RMSEA &gt;= 0.080                    0.649\n                                                      \n  Robust RMSEA                                   0.088\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.239\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.245\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.649\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.066\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              18.708    0.932\n    HF1               0.260    0.116    2.247    0.025    4.855    0.439\n  hf_post =~                                                            \n    int2              1.000                              20.647    0.927\n    HF2               0.310    0.037    8.327    0.000    6.409    0.659\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre            0.487    0.196    2.482    0.013    0.441    0.441\n    Group           -28.615    3.207   -8.923    0.000   -1.386   -0.693\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              60.277   10.956    5.502    0.000   60.277    0.831\n .int1 ~~                                                               \n   .int2            -24.643   57.805   -0.426    0.670  -24.643   -0.405\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1             42.656    2.048   20.826    0.000   42.656    2.126\n   .HF1              15.737    1.128   13.949    0.000   15.737    1.424\n   .int2             40.870    2.369   17.253    0.000   40.870    1.834\n   .HF2              14.713    1.050   14.014    0.000   14.713    1.514\n    hf_pre            0.000                               0.000    0.000\n   .hf_post           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1             52.754  183.493    0.287    0.774   52.754    0.131\n   .HF1              98.604   18.048    5.464    0.000   98.604    0.807\n   .int2             70.301   52.454    1.340    0.180   70.301    0.142\n   .HF2              53.415    9.290    5.750    0.000   53.415    0.565\n    hf_pre          349.971  194.090    1.803    0.071    1.000    1.000\n   .hf_post         138.752   35.498    3.909    0.000    0.325    0.325\n# free covariance between pretest and group\nhotflash_model2 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre + Group\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\n  hf_pre ~~ Group  # Group: numeric type; 0, 1\n\"\n\nsem_fit_hotflash2 &lt;- sem(hotflash_model2, data = hotflash)\n\nsummary(sem_fit_hotflash2,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 340 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        14\n\n  Number of observations                            96\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.088\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.767\n\nModel Test Baseline Model:\n\n  Test statistic                               244.790\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.039\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1534.211\n  Loglikelihood unrestricted model (H1)             NA\n                                                      \n  Akaike (AIC)                                3096.421\n  Bayesian (BIC)                              3132.322\n  Sample-size adjusted Bayesian (SABIC)       3088.118\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.182\n  P-value H_0: RMSEA &lt;= 0.050                    0.792\n  P-value H_0: RMSEA &gt;= 0.080                    0.173\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.009\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              17.499    0.872\n    HF1               0.292    0.128    2.278    0.023    5.107    0.463\n  hf_post =~                                                            \n    int2              1.000                              21.358    0.923\n    HF2               0.325    0.042    7.811    0.000    6.946    0.693\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre            0.516    0.192    2.695    0.007    0.423    0.423\n    Group           -27.460    3.413   -8.047    0.000   -1.286   -0.643\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              58.842   10.816    5.440    0.000   58.842    0.834\n .int1 ~~                                                               \n   .int2            -10.175   53.441   -0.190    0.849  -10.175   -0.116\n  hf_pre ~~                                                             \n    Group            -1.871    1.031   -1.814    0.070   -0.107   -0.214\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1             96.414  154.371    0.625    0.532   96.414    0.239\n   .HF1              95.514   18.005    5.305    0.000   95.514    0.786\n   .int2             79.741   47.039    1.695    0.090   79.741    0.149\n   .HF2              52.114    9.151    5.695    0.000   52.114    0.519\n    Group             0.250    0.036    6.928    0.000    0.250    1.000\n    hf_pre          306.198  164.330    1.863    0.062    1.000    1.000\n   .hf_post         133.014   34.072    3.904    0.000    0.292    0.292\n# no effect of group on pretest\nhotflash_model3 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre + 0*Group\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\n  hf_pre ~~ Group  # Group: numeric type; 0, 1\n\"\n\nsem_fit_hotflash3 &lt;- sem(hotflash_model3, data = hotflash)\n\nsummary(sem_fit_hotflash3,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 425 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                            96\n\nModel Test User Model:\n                                                      \n  Test statistic                                16.249\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               244.790\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.939\n  Tucker-Lewis Index (TLI)                       0.697\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1542.291\n  Loglikelihood unrestricted model (H1)             NA\n                                                      \n  Akaike (AIC)                                3110.582\n  Bayesian (BIC)                              3143.918\n  Sample-size adjusted Bayesian (SABIC)       3102.872\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.272\n  90 Percent confidence interval - lower         0.160\n  90 Percent confidence interval - upper         0.402\n  P-value H_0: RMSEA &lt;= 0.050                    0.001\n  P-value H_0: RMSEA &gt;= 0.080                    0.996\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.113\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                               4.376    0.218\n    HF1               0.459    0.368    1.247    0.212    2.010    0.182\n  hf_post =~                                                            \n    int2              1.000                              19.936    0.859\n    HF2               0.345    0.053    6.447    0.000    6.875    0.682\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre            4.286    2.141    2.002    0.045    0.941    0.941\n    Group             0.000                               0.000    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              66.943   11.395    5.875    0.000   66.943    0.836\n .int1 ~~                                                               \n   .int2            119.973   36.912    3.250    0.001  119.973    0.515\n  hf_pre ~~                                                             \n    Group            -1.828    1.016   -1.800    0.072   -0.418   -0.835\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            383.573   56.158    6.830    0.000  383.573    0.952\n   .HF1             118.138   17.185    6.874    0.000  118.138    0.967\n   .int2            141.227   44.236    3.193    0.001  141.227    0.262\n   .HF2              54.278    9.412    5.767    0.000   54.278    0.534\n    Group             0.250    0.036    6.928    0.000    0.250    1.000\n    hf_pre           19.153   20.058    0.955    0.340    1.000    1.000\n   .hf_post          45.549   99.411    0.458    0.647    0.115    0.115\n# compare the two models\nlavTestLRT(sem_fit_hotflash2, sem_fit_hotflash3) |&gt; print()\n\n\nChi-Squared Difference Test\n\n                  Df    AIC    BIC   Chisq Chisq diff   RMSEA Df diff\nsem_fit_hotflash2  1 3096.4 3132.3  0.0881                           \nsem_fit_hotflash3  2 3110.6 3143.9 16.2486      16.16 0.39739       1\n                  Pr(&gt;Chisq)    \nsem_fit_hotflash2               \nsem_fit_hotflash3   5.82e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Statistics",
      "Chapter 19"
    ]
  },
  {
    "objectID": "contents/chap19.html#multigroup-analysis",
    "href": "contents/chap19.html#multigroup-analysis",
    "title": "Chapter 19",
    "section": "Multigroup Analysis",
    "text": "Multigroup Analysis\n\n# multigroup analysis\nhotflash_model_mg &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\n  hf_pre ~ 0*1\n\"\n\nsem_fit_hotflash_mg &lt;- sem(hotflash_model_mg,\n  data = hotflash,\n  group = \"g\", # factor type\n  group.equal = c(\"loadings\", \"intercepts\")\n)\n\nsummary(sem_fit_hotflash_mg,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 326 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        31\n  Number of equality constraints                     6\n\n  Number of observations per group:                   \n    Control                                         48\n    Treatment                                       48\n\nModel Test User Model:\n                                                      \n  Test statistic                                 5.399\n  Degrees of freedom                                 3\n  P-value (Chi-square)                           0.145\n  Test statistic for each group:\n    Control                                      2.169\n    Treatment                                    3.229\n\nModel Test Baseline Model:\n\n  Test statistic                               182.941\n  Degrees of freedom                                12\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.986\n  Tucker-Lewis Index (TLI)                       0.944\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1425.733\n  Loglikelihood unrestricted model (H1)      -1423.033\n                                                      \n  Akaike (AIC)                                2901.465\n  Bayesian (BIC)                              2965.574\n  Sample-size adjusted Bayesian (SABIC)       2886.638\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.129\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.302\n  P-value H_0: RMSEA &lt;= 0.050                    0.185\n  P-value H_0: RMSEA &gt;= 0.080                    0.751\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.088\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Control]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              16.945    0.792\n    HF1     (.p2.)    0.388    0.115    3.388    0.001    6.573    0.607\n  hf_post =~                                                            \n    int2              1.000                              21.010    0.959\n    HF2     (.p4.)    0.312    0.037    8.433    0.000    6.559    0.587\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre            1.139    0.274    4.156    0.000    0.919    0.919\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              67.058   18.003    3.725    0.000   67.058    0.862\n .int1 ~~                                                               \n   .int2            -28.916   98.630   -0.293    0.769  -28.916   -0.355\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.15.)   42.049    2.027   20.742    0.000   42.049    1.966\n   .HF1     (.16.)   15.473    1.089   14.206    0.000   15.473    1.429\n   .int2    (.17.)   39.131    2.690   14.548    0.000   39.131    1.785\n   .HF2     (.18.)   14.044    1.248   11.256    0.000   14.044    1.258\n   .hf_post           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            170.287  124.805    1.364    0.172  170.287    0.372\n   .HF1              74.107   19.536    3.793    0.000   74.107    0.632\n   .int2             39.030  107.473    0.363    0.716   39.030    0.081\n   .HF2              81.674   19.666    4.153    0.000   81.674    0.655\n    hf_pre          287.136  148.917    1.928    0.054    1.000    1.000\n   .hf_post          68.950   49.956    1.380    0.168    0.156    0.156\n\n\nGroup 2 [Treatment]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              10.689    0.567\n    HF1     (.p2.)    0.388    0.115    3.388    0.001    4.146    0.384\n  hf_post =~                                                            \n    int2              1.000                               5.158    0.496\n    HF2     (.p4.)    0.312    0.037    8.433    0.000    1.610    0.308\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre           -0.456    0.459   -0.993    0.321   -0.944   -0.944\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              47.191   10.578    4.461    0.000   47.191    0.950\n .int1 ~~                                                               \n   .int2             44.760   37.540    1.192    0.233   44.760    0.319\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.15.)   42.049    2.027   20.742    0.000   42.049    2.230\n   .HF1     (.16.)   15.473    1.089   14.206    0.000   15.473    1.432\n   .int2    (.17.)   39.131    2.690   14.548    0.000   39.131    3.764\n   .HF2     (.18.)   14.044    1.248   11.256    0.000   14.044    2.684\n   .hf_post         -28.439    3.135   -9.071    0.000   -5.513   -5.513\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            241.229   74.588    3.234    0.001  241.229    0.679\n   .HF1              99.551   23.596    4.219    0.000   99.551    0.853\n   .int2             81.492   25.448    3.202    0.001   81.492    0.754\n   .HF2              24.780    5.610    4.417    0.000   24.780    0.905\n    hf_pre          114.255   70.087    1.630    0.103    1.000    1.000\n   .hf_post           2.901   52.283    0.055    0.956    0.109    0.109\n\n\n\n\nparameterTable(sem_fit_hotflash_mg) |&gt; print()\n\n   id     lhs op     rhs user block group free ustart exo label plabel   start\n1   1  hf_pre =~    int1    1     1     1    0      1   0         .p1.   1.000\n2   2  hf_pre =~     HF1    1     1     1    1     NA   0  .p2.   .p2.   0.388\n3   3 hf_post =~    int2    1     1     1    0      1   0         .p3.   1.000\n4   4 hf_post =~     HF2    1     1     1    2     NA   0  .p4.   .p4.   0.448\n5   5 hf_post  ~  hf_pre    1     1     1    3     NA   0         .p5.   0.000\n6   6     HF1 ~~     HF2    1     1     1    4     NA   0         .p6.   0.000\n7   7    int1 ~~    int2    1     1     1    5     NA   0         .p7.   0.000\n8   8  hf_pre ~1            1     1     1    0      0   0         .p8.   0.000\n9   9    int1 ~~    int1    0     1     1    6     NA   0         .p9. 224.087\n10 10     HF1 ~~     HF1    0     1     1    7     NA   0        .p10.  57.344\n11 11    int2 ~~    int2    0     1     1    8     NA   0        .p11. 233.573\n12 12     HF2 ~~     HF2    0     1     1    9     NA   0        .p12.  61.474\n13 13  hf_pre ~~  hf_pre    0     1     1   10     NA   0        .p13.   0.050\n14 14 hf_post ~~ hf_post    0     1     1   11     NA   0        .p14.   0.050\n15 15    int1 ~1            0     1     1   12     NA   0 .p15.  .p15.  46.312\n16 16     HF1 ~1            0     1     1   13     NA   0 .p16.  .p16.  17.077\n17 17    int2 ~1            0     1     1   14     NA   0 .p17.  .p17.  42.250\n18 18     HF2 ~1            0     1     1   15     NA   0 .p18.  .p18.  15.508\n19 19 hf_post ~1            0     1     1    0      0   0        .p19.   0.000\n20 20  hf_pre =~    int1    1     2     2    0      1   0        .p20.   1.000\n21 21  hf_pre =~     HF1    1     2     2   16     NA   0  .p2.  .p21.   0.275\n22 22 hf_post =~    int2    1     2     2    0      1   0        .p22.   1.000\n23 23 hf_post =~     HF2    1     2     2   17     NA   0  .p4.  .p23.   0.044\n24 24 hf_post  ~  hf_pre    1     2     2   18     NA   0        .p24.   0.000\n25 25     HF1 ~~     HF2    1     2     2   19     NA   0        .p25.   0.000\n26 26    int1 ~~    int2    1     2     2   20     NA   0        .p26.   0.000\n27 27  hf_pre ~1            1     2     2    0      0   0        .p27.   0.000\n28 28    int1 ~~    int1    0     2     2   21     NA   0        .p28. 165.271\n29 29     HF1 ~~     HF1    0     2     2   22     NA   0        .p29.  63.038\n30 30    int2 ~~    int2    0     2     2   23     NA   0        .p30.  56.430\n31 31     HF2 ~~     HF2    0     2     2   24     NA   0        .p31.  12.662\n32 32  hf_pre ~~  hf_pre    0     2     2   25     NA   0        .p32.   0.050\n33 33 hf_post ~~ hf_post    0     2     2   26     NA   0        .p33.   0.050\n34 34    int1 ~1            0     2     2   27     NA   0 .p15.  .p34.  39.000\n35 35     HF1 ~1            0     2     2   28     NA   0 .p16.  .p35.  14.396\n36 36    int2 ~1            0     2     2   29     NA   0 .p17.  .p36.  10.875\n37 37     HF2 ~1            0     2     2   30     NA   0 .p18.  .p37.   5.036\n38 38 hf_post ~1            0     2     2   31     NA   0        .p38.   0.000\n39 39    .p2. ==   .p21.    2     0     0    0     NA   0                0.000\n40 40    .p4. ==   .p23.    2     0     0    0     NA   0                0.000\n41 41   .p15. ==   .p34.    2     0     0    0     NA   0                0.000\n42 42   .p16. ==   .p35.    2     0     0    0     NA   0                0.000\n43 43   .p17. ==   .p36.    2     0     0    0     NA   0                0.000\n44 44   .p18. ==   .p37.    2     0     0    0     NA   0                0.000\n       est      se\n1    1.000   0.000\n2    0.388   0.115\n3    1.000   0.000\n4    0.312   0.037\n5    1.139   0.274\n6   67.058  18.003\n7  -28.916  98.630\n8    0.000   0.000\n9  170.287 124.805\n10  74.107  19.536\n11  39.030 107.473\n12  81.674  19.666\n13 287.136 148.917\n14  68.950  49.956\n15  42.049   2.027\n16  15.473   1.089\n17  39.131   2.690\n18  14.044   1.248\n19   0.000   0.000\n20   1.000   0.000\n21   0.388   0.115\n22   1.000   0.000\n23   0.312   0.037\n24  -0.456   0.459\n25  47.191  10.578\n26  44.760  37.540\n27   0.000   0.000\n28 241.229  74.588\n29  99.551  23.596\n30  81.492  25.448\n31  24.780   5.610\n32 114.255  70.087\n33   2.901  52.283\n34  42.049   2.027\n35  15.473   1.089\n36  39.131   2.690\n37  14.044   1.248\n38 -28.439   3.135\n39   0.000   0.000\n40   0.000   0.000\n41   0.000   0.000\n42   0.000   0.000\n43   0.000   0.000\n44   0.000   0.000\n\n\n\n# free pretest intercepts\nhotflash_model_mg2 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\"\nsem_fit_hotflash_mg2 &lt;- sem(hotflash_model_mg2,\n  data = hotflash,\n  group = \"g\", # factor type\n  group.equal = c(\"loadings\", \"intercepts\")\n)\n\nsummary(sem_fit_hotflash_mg2,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 316 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        32\n  Number of equality constraints                     6\n\n  Number of observations per group:                   \n    Control                                         48\n    Treatment                                       48\n\nModel Test User Model:\n                                                      \n  Test statistic                                 1.827\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.401\n  Test statistic for each group:\n    Control                                      0.069\n    Treatment                                    1.758\n\nModel Test Baseline Model:\n\n  Test statistic                               182.941\n  Degrees of freedom                                12\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.006\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1423.947\n  Loglikelihood unrestricted model (H1)      -1423.033\n                                                      \n  Akaike (AIC)                                2899.894\n  Bayesian (BIC)                              2966.567\n  Sample-size adjusted Bayesian (SABIC)       2884.473\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.278\n  P-value H_0: RMSEA &lt;= 0.050                    0.444\n  P-value H_0: RMSEA &gt;= 0.080                    0.495\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.041\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Control]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              15.824    0.753\n    HF1     (.p2.)    0.422    0.133    3.178    0.001    6.677    0.623\n  hf_post =~                                                            \n    int2              1.000                              19.820    0.914\n    HF2     (.p4.)    0.339    0.044    7.780    0.000    6.715    0.606\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre            1.148    0.272    4.227    0.000    0.917    0.917\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              63.289   17.987    3.519    0.000   63.289    0.858\n .int1 ~~                                                               \n   .int2             -1.590   92.867   -0.017    0.986   -1.590   -0.013\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1    (.14.)   46.160    2.931   15.749    0.000   46.160    2.197\n   .HF1     (.15.)   17.176    1.460   11.763    0.000   17.176    1.603\n   .int2    (.16.)   42.265    3.128   13.513    0.000   42.265    1.950\n   .HF2     (.17.)   15.587    1.547   10.077    0.000   15.587    1.407\n    hf_pre            0.000                               0.000    0.000\n   .hf_post           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            191.182  115.106    1.661    0.097  191.182    0.433\n   .HF1              70.168   19.782    3.547    0.000   70.168    0.611\n   .int2             77.023  100.443    0.767    0.443   77.023    0.164\n   .HF2              77.590   19.612    3.956    0.000   77.590    0.632\n    hf_pre          250.405  133.264    1.879    0.060    1.000    1.000\n   .hf_post          62.775   45.436    1.382    0.167    0.160    0.160\n\n\nGroup 2 [Treatment]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                               9.962    0.536\n    HF1     (.p2.)    0.422    0.133    3.178    0.001    4.203    0.390\n  hf_post =~                                                            \n    int2              1.000                               4.956    0.477\n    HF2     (.p4.)    0.339    0.044    7.780    0.000    1.679    0.321\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre           -0.480    0.493   -0.974    0.330   -0.964   -0.964\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              47.660   10.693    4.457    0.000   47.660    0.971\n .int1 ~~                                                               \n   .int2             40.639   35.830    1.134    0.257   40.639    0.283\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1    (.14.)   46.160    2.931   15.749    0.000   46.160    2.483\n   .HF1     (.15.)   17.176    1.460   11.763    0.000   17.176    1.595\n   .int2    (.16.)   42.265    3.128   13.513    0.000   42.265    4.065\n   .HF2     (.17.)   15.587    1.547   10.077    0.000   15.587    2.981\n    hf_pre           -6.970    3.653   -1.908    0.056   -0.700   -0.700\n   .hf_post         -34.716    5.690   -6.101    0.000   -7.004   -7.004\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            246.389   71.237    3.459    0.001  246.389    0.713\n   .HF1              98.344   23.936    4.109    0.000   98.344    0.848\n   .int2             83.545   24.502    3.410    0.001   83.545    0.773\n   .HF2              24.522    5.663    4.330    0.000   24.522    0.897\n    hf_pre           99.234   63.648    1.559    0.119    1.000    1.000\n   .hf_post           1.724   49.966    0.035    0.972    0.070    0.070\n\n\n\n\nlavTestLRT(sem_fit_hotflash_mg, sem_fit_hotflash_mg2) |&gt; print()\n\n\nChi-Squared Difference Test\n\n                     Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff\nsem_fit_hotflash_mg2  2 2899.9 2966.6 1.8271                           \nsem_fit_hotflash_mg   3 2901.5 2965.6 5.3986     3.5715 0.23146       1\n                     Pr(&gt;Chisq)  \nsem_fit_hotflash_mg2             \nsem_fit_hotflash_mg     0.05878 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# Test assumptions\nhotflash_model_mg3 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ c(a, a)*hf_pre\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\n  hf_pre ~ 0*1\n\"\n\nsem_fit_hotflash_mg3 &lt;- sem(hotflash_model_mg3,\n  data = hotflash,\n  group = \"g\", # factor type\n  group.equal = c(\n    \"loadings\", \"intercepts\",\n    \"residuals\", \"lv.variances\",\n    \"residual.covariances\"\n  )\n)\n\nsummary(sem_fit_hotflash_mg3,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 179 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        31\n  Number of equality constraints                    15\n\n  Number of observations per group:                   \n    Control                                         48\n    Treatment                                       48\n\nModel Test User Model:\n                                                      \n  Test statistic                                86.416\n  Degrees of freedom                                12\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    Control                                     28.363\n    Treatment                                   58.053\n\nModel Test Baseline Model:\n\n  Test statistic                               182.941\n  Degrees of freedom                                12\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.565\n  Tucker-Lewis Index (TLI)                       0.565\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1466.241\n  Loglikelihood unrestricted model (H1)      -1423.033\n                                                      \n  Akaike (AIC)                                2964.482\n  Bayesian (BIC)                              3005.512\n  Sample-size adjusted Bayesian (SABIC)       2954.993\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.359\n  90 Percent confidence interval - lower         0.290\n  90 Percent confidence interval - upper         0.433\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.534\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Control]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              18.708    0.932\n    HF1     (.p2.)    0.260    0.112    2.323    0.020    4.855    0.439\n  hf_post =~                                                            \n    int2              1.000                              14.886    0.871\n    HF2     (.p4.)    0.310    0.037    8.480    0.000    4.621    0.534\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre     (a)    0.487    0.195    2.500    0.012    0.611    0.611\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2     (.p6.)   60.277   10.820    5.571    0.000   60.277    0.831\n .int1 ~~                                                               \n   .int2    (.p7.)  -24.643   55.046   -0.448    0.654  -24.643   -0.405\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.15.)   42.656    2.048   20.826    0.000   42.656    2.126\n   .HF1     (.16.)   15.737    1.128   13.949    0.000   15.737    1.424\n   .int2    (.17.)   40.870    2.349   17.396    0.000   40.870    2.392\n   .HF2     (.18.)   14.713    1.044   14.088    0.000   14.713    1.702\n   .hf_post           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1    (.p9.)   52.755  179.207    0.294    0.768   52.755    0.131\n   .HF1     (.10.)   98.604   17.851    5.524    0.000   98.604    0.807\n   .int2    (.11.)   70.300   51.321    1.370    0.171   70.300    0.241\n   .HF2     (.12.)   53.415    9.107    5.865    0.000   53.415    0.714\n    hf_pre  (.13.)  349.971  190.043    1.842    0.066    1.000    1.000\n   .hf_post (.14.)  138.752   35.569    3.901    0.000    0.626    0.626\n\n\nGroup 2 [Treatment]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              18.708    0.932\n    HF1     (.p2.)    0.260    0.112    2.323    0.020    4.855    0.439\n  hf_post =~                                                            \n    int2              1.000                              14.886    0.871\n    HF2     (.p4.)    0.310    0.037    8.480    0.000    4.621    0.534\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre     (a)    0.487    0.195    2.500    0.012    0.611    0.611\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2     (.p6.)   60.277   10.820    5.571    0.000   60.277    0.831\n .int1 ~~                                                               \n   .int2    (.p7.)  -24.643   55.046   -0.448    0.654  -24.643   -0.405\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.15.)   42.656    2.048   20.826    0.000   42.656    2.126\n   .HF1     (.16.)   15.737    1.128   13.949    0.000   15.737    1.424\n   .int2    (.17.)   40.870    2.349   17.396    0.000   40.870    2.392\n   .HF2     (.18.)   14.713    1.044   14.088    0.000   14.713    1.702\n   .hf_post         -28.615    3.149   -9.087    0.000   -1.922   -1.922\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1    (.p9.)   52.755  179.207    0.294    0.768   52.755    0.131\n   .HF1     (.10.)   98.604   17.851    5.524    0.000   98.604    0.807\n   .int2    (.11.)   70.300   51.321    1.370    0.171   70.300    0.241\n   .HF2     (.12.)   53.415    9.107    5.865    0.000   53.415    0.714\n    hf_pre  (.13.)  349.971  190.043    1.842    0.066    1.000    1.000\n   .hf_post (.14.)  138.752   35.569    3.901    0.000    0.626    0.626\n\n\n\n\nlavTestLRT(sem_fit_hotflash_mg, sem_fit_hotflash_mg3) |&gt; print()\n\n\nChi-Squared Difference Test\n\n                     Df    AIC    BIC   Chisq Chisq diff  RMSEA Df diff\nsem_fit_hotflash_mg   3 2901.5 2965.6  5.3986                          \nsem_fit_hotflash_mg3 12 2964.5 3005.5 86.4155     81.017 0.4083       9\n                     Pr(&gt;Chisq)    \nsem_fit_hotflash_mg                \nsem_fit_hotflash_mg3  1.015e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# Slopes Vary\nhotflash_model_mg4 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ c(a, b)*hf_pre\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\n  hf_pre ~ 0*1\n\"\n\nsem_fit_hotflash_mg4 &lt;- sem(hotflash_model_mg4,\n  data = hotflash,\n  group = \"g\", # factor type\n  group.equal = c(\n    \"loadings\", \"intercepts\",\n    \"residuals\", \"lv.variances\",\n    \"residual.covariances\"\n  )\n)\n\nsummary(sem_fit_hotflash_mg4,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 158 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        31\n  Number of equality constraints                    14\n\n  Number of observations per group:                   \n    Control                                         48\n    Treatment                                       48\n\nModel Test User Model:\n                                                      \n  Test statistic                                45.194\n  Degrees of freedom                                11\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    Control                                     12.421\n    Treatment                                   32.772\n\nModel Test Baseline Model:\n\n  Test statistic                               182.941\n  Degrees of freedom                                12\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.800\n  Tucker-Lewis Index (TLI)                       0.782\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1445.630\n  Loglikelihood unrestricted model (H1)      -1423.033\n                                                      \n  Akaike (AIC)                                2925.260\n  Bayesian (BIC)                              2968.854\n  Sample-size adjusted Bayesian (SABIC)       2915.178\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.254\n  90 Percent confidence interval - lower         0.180\n  90 Percent confidence interval - upper         0.334\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.209\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Control]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              12.737    0.628\n    HF1     (.p2.)    0.454    0.089    5.088    0.000    5.789    0.560\n  hf_post =~                                                            \n    int2              1.000                              19.515    0.899\n    HF2     (.p4.)    0.334    0.033   10.242    0.000    6.513    0.683\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre     (a)    1.506    0.281    5.361    0.000    0.983    0.983\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2     (.p6.)   47.886    8.577    5.583    0.000   47.886    0.803\n .int1 ~~                                                               \n   .int2    (.p7.)   38.457   27.860    1.380    0.167   38.457    0.256\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.15.)   42.656    2.071   20.596    0.000   42.656    2.102\n   .HF1     (.16.)   15.737    1.055   14.916    0.000   15.737    1.522\n   .int2    (.17.)   38.862    2.653   14.651    0.000   38.862    1.790\n   .HF2     (.18.)   14.377    1.074   13.381    0.000   14.377    1.507\n   .hf_post           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1    (.p9.)  249.532   50.019    4.989    0.000  249.532    0.606\n   .HF1     (.10.)   73.335   12.582    5.828    0.000   73.335    0.686\n   .int2    (.11.)   90.406   28.265    3.199    0.001   90.406    0.192\n   .HF2     (.12.)   48.542    7.729    6.280    0.000   48.542    0.534\n    hf_pre  (.13.)  162.236   58.242    2.786    0.005    1.000    1.000\n   .hf_post (.14.)   12.657   30.309    0.418    0.676    0.033    0.033\n\n\nGroup 2 [Treatment]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              12.737    0.628\n    HF1     (.p2.)    0.454    0.089    5.088    0.000    5.789    0.560\n  hf_post =~                                                            \n    int2              1.000                               4.546    0.431\n    HF2     (.p4.)    0.334    0.033   10.242    0.000    1.517    0.213\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre     (b)   -0.222    0.189   -1.176    0.240   -0.623   -0.623\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2     (.p6.)   47.886    8.577    5.583    0.000   47.886    0.803\n .int1 ~~                                                               \n   .int2    (.p7.)   38.457   27.860    1.380    0.167   38.457    0.256\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.15.)   42.656    2.071   20.596    0.000   42.656    2.102\n   .HF1     (.16.)   15.737    1.055   14.916    0.000   15.737    1.522\n   .int2    (.17.)   38.862    2.653   14.651    0.000   38.862    3.687\n   .HF2     (.18.)   14.377    1.074   13.381    0.000   14.377    2.016\n   .hf_post         -27.918    3.030   -9.214    0.000   -6.141   -6.141\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1    (.p9.)  249.532   50.019    4.989    0.000  249.532    0.606\n   .HF1     (.10.)   73.335   12.582    5.828    0.000   73.335    0.686\n   .int2    (.11.)   90.406   28.265    3.199    0.001   90.406    0.814\n   .HF2     (.12.)   48.542    7.729    6.280    0.000   48.542    0.955\n    hf_pre  (.13.)  162.236   58.242    2.786    0.005    1.000    1.000\n   .hf_post (.14.)   12.657   30.309    0.418    0.676    0.612    0.612\n\n\n\n\nlavTestLRT(sem_fit_hotflash_mg3, sem_fit_hotflash_mg4) |&gt; print()\n\n\nChi-Squared Difference Test\n\n                     Df    AIC    BIC  Chisq Chisq diff  RMSEA Df diff\nsem_fit_hotflash_mg4 11 2925.3 2968.8 45.194                          \nsem_fit_hotflash_mg3 12 2964.5 3005.5 86.415     41.222 0.9154       1\n                     Pr(&gt;Chisq)    \nsem_fit_hotflash_mg4               \nsem_fit_hotflash_mg3  1.359e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# initial2: remove covariances between int1, int2\nhotflash_model_mg5 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre\n\n  HF1 ~~ HF2\n\n  hf_pre ~ 0*1\n\"\n\nsem_fit_hotflash_mg5 &lt;- sem(hotflash_model_mg5,\n  data = hotflash,\n  group = \"g\", # factor type\n  group.equal = c(\"loadings\", \"intercepts\")\n)\n\nsummary(sem_fit_hotflash_mg5,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 287 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        29\n  Number of equality constraints                     6\n\n  Number of observations per group:                   \n    Control                                         48\n    Treatment                                       48\n\nModel Test User Model:\n                                                      \n  Test statistic                                 7.074\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.215\n  Test statistic for each group:\n    Control                                      2.241\n    Treatment                                    4.834\n\nModel Test Baseline Model:\n\n  Test statistic                               182.941\n  Degrees of freedom                                12\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.988\n  Tucker-Lewis Index (TLI)                       0.971\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1426.571\n  Loglikelihood unrestricted model (H1)      -1423.033\n                                                      \n  Akaike (AIC)                                2899.141\n  Bayesian (BIC)                              2958.121\n  Sample-size adjusted Bayesian (SABIC)       2885.500\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.093\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.236\n  P-value H_0: RMSEA &lt;= 0.050                    0.277\n  P-value H_0: RMSEA &gt;= 0.080                    0.629\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.095\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Control]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              15.989    0.748\n    HF1     (.p2.)    0.416    0.097    4.289    0.000    6.650    0.620\n  hf_post =~                                                            \n    int2              1.000                              20.482    0.933\n    HF2     (.p4.)    0.320    0.035    9.104    0.000    6.552    0.593\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre            1.169    0.271    4.315    0.000    0.912    0.912\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              64.439   15.850    4.066    0.000   64.439    0.860\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.14.)   42.076    2.027   20.758    0.000   42.076    1.969\n   .HF1     (.15.)   15.623    1.097   14.241    0.000   15.623    1.457\n   .int2    (.16.)   38.970    2.680   14.541    0.000   38.970    1.774\n   .HF2     (.17.)   14.276    1.232   11.590    0.000   14.276    1.292\n   .hf_post           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            200.813   65.273    3.076    0.002  200.813    0.440\n   .HF1              70.829   16.758    4.226    0.000   70.829    0.616\n   .int2             62.914   75.243    0.836    0.403   62.914    0.130\n   .HF2              79.187   17.862    4.433    0.000   79.187    0.648\n    hf_pre          255.650   97.491    2.622    0.009    1.000    1.000\n   .hf_post          70.303   49.163    1.430    0.153    0.168    0.168\n\n\nGroup 2 [Treatment]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              11.989    0.641\n    HF1     (.p2.)    0.416    0.097    4.289    0.000    4.986    0.452\n  hf_post =~                                                            \n    int2              1.000                               6.661    0.627\n    HF2     (.p4.)    0.320    0.035    9.104    0.000    2.131    0.401\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre           -0.191    0.197   -0.972    0.331   -0.345   -0.345\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              46.821   10.332    4.532    0.000   46.821    0.977\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.14.)   42.076    2.027   20.758    0.000   42.076    2.249\n   .HF1     (.15.)   15.623    1.097   14.241    0.000   15.623    1.415\n   .int2    (.16.)   38.970    2.680   14.541    0.000   38.970    3.671\n   .HF2     (.17.)   14.276    1.232   11.590    0.000   14.276    2.687\n   .hf_post         -28.308    3.143   -9.006    0.000   -4.250   -4.250\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            206.182   65.952    3.126    0.002  206.182    0.589\n   .HF1              96.979   23.606    4.108    0.000   96.979    0.796\n   .int2             68.308   20.877    3.272    0.001   68.308    0.606\n   .HF2              23.690    5.445    4.351    0.000   23.690    0.839\n    hf_pre          143.736   60.058    2.393    0.017    1.000    1.000\n   .hf_post          39.097   22.874    1.709    0.087    0.881    0.881\n\n\n\n\n# No main effect\nhotflash_model_mg6 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre\n\n  HF1 ~~ HF2\n\n  hf_pre ~ 0*1\n\n  # 또는 hf_post ~ 0*1\n\"\n\nsem_fit_hotflash_mg6 &lt;- sem(hotflash_model_mg6,\n  data = hotflash,\n  group = \"g\", # factor type\n  group.equal = c(\"loadings\", \"intercepts\", \"means\")\n)\n\nsummary(sem_fit_hotflash_mg6,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 255 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        28\n  Number of equality constraints                     6\n\n  Number of observations per group:                   \n    Control                                         48\n    Treatment                                       48\n\nModel Test User Model:\n                                                      \n  Test statistic                                68.745\n  Degrees of freedom                                 6\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    Control                                     54.194\n    Treatment                                   14.551\n\nModel Test Baseline Model:\n\n  Test statistic                               182.941\n  Degrees of freedom                                12\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.633\n  Tucker-Lewis Index (TLI)                       0.266\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1457.406\n  Loglikelihood unrestricted model (H1)      -1423.033\n                                                      \n  Akaike (AIC)                                2958.811\n  Bayesian (BIC)                              3015.227\n  Sample-size adjusted Bayesian (SABIC)       2945.763\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.467\n  90 Percent confidence interval - lower         0.371\n  90 Percent confidence interval - upper         0.569\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.524\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Control]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              19.218    0.800\n    HF1     (.p2.)    0.501    0.089    5.637    0.000    9.635    0.763\n  hf_post =~                                                            \n    int2              1.000                              33.875    0.970\n    HF2     (.p4.)    0.375    0.053    7.027    0.000   12.696    0.820\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre            1.637    0.274    5.967    0.000    0.929    0.929\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              63.008   17.384    3.625    0.000   63.008    0.871\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.14.)   34.183    1.786   19.142    0.000   34.183    1.423\n   .HF1     (.15.)   10.714    0.901   11.889    0.000   10.714    0.849\n   .int2    (.16.)   14.842    1.410   10.527    0.000   14.842    0.425\n   .HF2     (.17.)    4.945    0.627    7.891    0.000    4.945    0.319\n   .hf_post           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            208.016   61.528    3.381    0.001  208.016    0.360\n   .HF1              66.548   17.601    3.781    0.000   66.548    0.418\n   .int2             72.980  115.129    0.634    0.526   72.980    0.060\n   .HF2              78.723   22.701    3.468    0.001   78.723    0.328\n    hf_pre          369.314  117.973    3.131    0.002    1.000    1.000\n   .hf_post         157.820   80.531    1.960    0.050    0.138    0.138\n\n\nGroup 2 [Treatment]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              11.075    0.571\n    HF1     (.p2.)    0.501    0.089    5.637    0.000    5.553    0.494\n  hf_post =~                                                            \n    int2              1.000                               6.310    0.562\n    HF2     (.p4.)    0.375    0.053    7.027    0.000    2.365    0.441\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre           -0.378    0.254   -1.488    0.137   -0.664   -0.664\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              50.408   10.900    4.625    0.000   50.408    1.072\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.14.)   34.183    1.786   19.142    0.000   34.183    1.764\n   .HF1     (.15.)   10.714    0.901   11.889    0.000   10.714    0.954\n   .int2    (.16.)   14.842    1.410   10.527    0.000   14.842    1.322\n   .HF2     (.17.)    4.945    0.627    7.891    0.000    4.945    0.921\n   .hf_post           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            252.923   64.159    3.942    0.000  252.923    0.673\n   .HF1              95.269   24.474    3.893    0.000   95.269    0.755\n   .int2             86.205   22.612    3.812    0.000   86.205    0.684\n   .HF2              23.222    5.620    4.132    0.000   23.222    0.806\n    hf_pre          122.652   50.232    2.442    0.015    1.000    1.000\n   .hf_post          22.267   28.996    0.768    0.443    0.559    0.559\n\n\n\n\n# No slope difference: interaction\nhotflash_model_mg7 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ c(a, a)*hf_pre\n\n  HF1 ~~ HF2\n\n  hf_pre ~ 0*1\n\"\n\nsem_fit_hotflash_mg7 &lt;- sem(hotflash_model_mg7,\n  data = hotflash,\n  group = \"g\", # factor type\n  group.equal = c(\"loadings\", \"intercepts\")\n)\n\nsummary(sem_fit_hotflash_mg7,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 226 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        29\n  Number of equality constraints                     7\n\n  Number of observations per group:                   \n    Control                                         48\n    Treatment                                       48\n\nModel Test User Model:\n                                                      \n  Test statistic                                22.975\n  Degrees of freedom                                 6\n  P-value (Chi-square)                           0.001\n  Test statistic for each group:\n    Control                                      2.791\n    Treatment                                   20.184\n\nModel Test Baseline Model:\n\n  Test statistic                               182.941\n  Degrees of freedom                                12\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.901\n  Tucker-Lewis Index (TLI)                       0.801\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1434.521\n  Loglikelihood unrestricted model (H1)      -1423.033\n                                                      \n  Akaike (AIC)                                2913.041\n  Bayesian (BIC)                              2969.457\n  Sample-size adjusted Bayesian (SABIC)       2899.993\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.243\n  90 Percent confidence interval - lower         0.143\n  90 Percent confidence interval - upper         0.352\n  P-value H_0: RMSEA &lt;= 0.050                    0.002\n  P-value H_0: RMSEA &gt;= 0.080                    0.994\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.113\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Control]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              16.360    0.758\n    HF1     (.p2.)    0.395    0.093    4.244    0.000    6.469    0.607\n  hf_post =~                                                            \n    int2              1.000                              20.603    0.929\n    HF2     (.p4.)    0.319    0.038    8.466    0.000    6.573    0.595\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre     (a)    1.161    0.269    4.318    0.000    0.922    0.922\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              64.363   15.818    4.069    0.000   64.363    0.855\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.14.)   41.568    1.970   21.100    0.000   41.568    1.927\n   .HF1     (.15.)   15.430    1.068   14.449    0.000   15.430    1.448\n   .int2    (.16.)   38.358    2.623   14.625    0.000   38.358    1.729\n   .HF2     (.17.)   14.122    1.209   11.680    0.000   14.122    1.278\n   .hf_post           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            197.742   66.985    2.952    0.003  197.742    0.425\n   .HF1              71.775   16.726    4.291    0.000   71.775    0.632\n   .int2             67.392   75.715    0.890    0.373   67.392    0.137\n   .HF2              78.925   17.807    4.432    0.000   78.925    0.646\n    hf_pre          267.659  101.065    2.648    0.008    1.000    1.000\n   .hf_post          63.824   48.810    1.308    0.191    0.150    0.150\n\n\nGroup 2 [Treatment]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                                  NA       NA\n    HF1     (.p2.)    0.395    0.093    4.244    0.000       NA       NA\n  hf_post =~                                                            \n    int2              1.000                               5.747    0.543\n    HF2     (.p4.)    0.319    0.038    8.466    0.000    1.833    0.353\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre     (a)    1.161    0.269    4.318    0.000       NA       NA\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              47.574   10.825    4.395    0.000   47.574    0.859\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                                  NA       NA\n   .int1    (.14.)   41.568    1.970   21.100    0.000   41.568    2.232\n   .HF1     (.15.)   15.430    1.068   14.449    0.000   15.430    1.369\n   .int2    (.16.)   38.358    2.623   14.625    0.000   38.358    3.627\n   .HF2     (.17.)   14.122    1.209   11.680    0.000   14.122    2.722\n   .hf_post         -27.603    3.080   -8.961    0.000   -4.803   -4.803\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            367.497   75.977    4.837    0.000  367.497    1.059\n   .HF1             130.206   26.200    4.970    0.000  130.206    1.025\n   .int2             78.807   23.605    3.339    0.001   78.807    0.705\n   .HF2              23.551    5.373    4.383    0.000   23.551    0.875\n    hf_pre          -20.543   19.456   -1.056    0.291       NA       NA\n   .hf_post          60.715   25.745    2.358    0.018    1.838    1.838",
    "crumbs": [
      "Statistics",
      "Chapter 19"
    ]
  },
  {
    "objectID": "contents/chap16.html",
    "href": "contents/chap16.html",
    "title": "Chapter 16",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)\n# MODEL:\n# Gc by namevoc worddef verbsim;\n# Gf BY pictsim matrix seqquant;\n# Gv BY pattcon redesign repict;\n# Gsm BY Digits digback seqord;\n\nlibrary(haven)\ndas2 &lt;- read_sav(\"data/chap 16 CFA 1/das 2 cov.sav\")\n\nlibrary(lavaan)\ndas2cov &lt;- das2[1:12, 3:14] |&gt;\n  as.matrix() |&gt;\n  lav_matrix_vechr(diagonal = TRUE) |&gt;\n  getCov(names = c(\"wdss\", \"vsss\", \"sqss\", \"soss\", \"rpss\", \"rdss\", \"psss\", \"pcss\", \"nvss\", \"mass\", \"dfss\", \"dbss\"))\n\ndas2cov |&gt; print()\n\n     wdss vsss sqss soss rpss rdss psss pcss nvss mass dfss dbss\nwdss   92   58   42   50   28   31   37   37   54   42   44   42\nvsss   58  104   53   55   36   44   42   49   60   48   52   51\nsqss   42   53   94   54   44   50   39   54   44   60   47   53\nsoss   50   55   54  113   40   48   37   49   52   55   62   63\nrpss   28   36   44   40  102   48   34   41   34   40   33   38\nrdss   31   44   50   48   48  100   41   56   44   41   46   47\npsss   37   42   39   37   34   41  106   39   39   39   37   37\npcss   37   49   54   49   41   56   39   85   47   47   44   48\nnvss   54   60   44   52   34   44   39   47  102   40   50   43\nmass   42   48   60   55   40   41   39   47   40  105   40   51\ndfss   44   52   47   62   33   46   37   44   50   40  122   56\ndbss   42   51   53   63   38   47   37   48   43   51   56  103",
    "crumbs": [
      "Statistics",
      "Chapter 16"
    ]
  },
  {
    "objectID": "contents/chap16.html#higher-order-model",
    "href": "contents/chap16.html#higher-order-model",
    "title": "Chapter 16",
    "section": "Higher-order model",
    "text": "Higher-order model\n\n## Higher-order model\ndas2_model_higher &lt;- \"\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss\n  Spatial =~ pcss + rdss + rpss\n  Memory =~ dfss + dbss + soss\n\n  G =~ Verbal + Nonverbal + Spatial + Memory\n\"\n\nfit_higher &lt;- sem(das2_model_higher,\n  sample.cov = das2cov,\n  sample.nobs = 800,\n  sample.cov.rescale = FALSE\n)\n\nsummary(fit_higher, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 107 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        28\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               141.972\n  Degrees of freedom                                50\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              4335.087\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.978\n  Tucker-Lewis Index (TLI)                       0.972\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33721.564\n  Loglikelihood unrestricted model (H1)     -33650.578\n                                                      \n  Akaike (AIC)                               67499.128\n  Bayesian (BIC)                             67630.297\n  Sample-size adjusted Bayesian (SABIC)      67541.381\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.048\n  90 Percent confidence interval - lower         0.039\n  90 Percent confidence interval - upper         0.057\n  P-value H_0: RMSEA &lt;= 0.050                    0.626\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              1.000                               7.478    0.740\n    wdss              0.937    0.049   19.183    0.000    7.004    0.730\n    vsss              1.100    0.053   20.914    0.000    8.229    0.807\n  Nonverbal =~                                                          \n    psss              1.000                               5.562    0.540\n    mass              1.315    0.093   14.090    0.000    7.312    0.714\n    sqss              1.405    0.094   14.951    0.000    7.815    0.806\n  Spatial =~                                                            \n    pcss              1.000                               7.511    0.815\n    rdss              0.983    0.047   20.993    0.000    7.382    0.738\n    rpss              0.790    0.049   16.285    0.000    5.937    0.588\n  Memory =~                                                             \n    dfss              1.000                               7.349    0.665\n    dbss              1.047    0.058   17.904    0.000    7.695    0.758\n    soss              1.123    0.062   18.215    0.000    8.253    0.776\n  G =~                                                                  \n    Verbal            1.000                               0.858    0.858\n    Nonverbal         0.828    0.063   13.044    0.000    0.955    0.955\n    Spatial           1.057    0.060   17.475    0.000    0.903    0.903\n    Memory            1.046    0.069   15.141    0.000    0.913    0.913\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .nvss             46.075    2.929   15.728    0.000   46.075    0.452\n   .wdss             42.944    2.686   15.990    0.000   42.944    0.467\n   .vsss             36.291    2.718   13.352    0.000   36.291    0.349\n   .psss             75.069    4.014   18.702    0.000   75.069    0.708\n   .mass             51.529    3.115   16.543    0.000   51.529    0.491\n   .sqss             32.921    2.492   13.213    0.000   32.921    0.350\n   .pcss             28.590    2.250   12.706    0.000   28.590    0.336\n   .rdss             45.504    2.891   15.738    0.000   45.504    0.455\n   .rpss             66.754    3.671   18.187    0.000   66.754    0.654\n   .dfss             67.997    3.932   17.293    0.000   67.997    0.557\n   .dbss             43.783    2.871   15.248    0.000   43.783    0.425\n   .soss             44.890    3.070   14.620    0.000   44.890    0.397\n   .Verbal           14.760    2.029    7.274    0.000    0.264    0.264\n   .Nonverbal         2.701    1.031    2.619    0.009    0.087    0.087\n   .Spatial          10.425    1.925    5.415    0.000    0.185    0.185\n   .Memory            8.952    1.828    4.897    0.000    0.166    0.166\n    G                41.165    4.202    9.797    0.000    1.000    1.000\n\n\n\n\nlavResiduals(fit3, type = \"normalized\", zstat = F, summary = F) |&gt; print()\n\n$type\n[1] \"normalized\"\n\n$cov\n       nvss   wdss   vsss   psss   mass   sqss   pcss   rdss   rpss   dfss\nnvss  0.000                                                               \nwdss  0.363  0.000                                                        \nvsss -0.351  0.108  0.000                                                 \npsss  1.346  1.456  1.271  0.000                                          \nmass -0.615  0.605  0.385 -0.052  0.000                                   \nsqss -0.547 -0.318  0.643 -0.948  1.565  0.000                            \npcss  0.997 -1.052  0.418 -0.289 -0.905 -0.197  0.000                     \nrdss  0.486 -2.365 -0.535  0.530 -2.040 -0.827  1.570  0.000              \nrpss -0.317 -1.393 -0.661  0.376 -0.239 -0.125 -0.227  1.861  0.000       \ndfss  0.992  0.294  0.444  0.321 -1.139 -0.413 -0.458  0.352 -1.026  0.000\ndbss -1.069 -0.586 -0.166  0.050  1.150  0.685  0.192  0.263 -0.111 -0.130\nsoss  0.226  0.548 -0.207 -0.686  1.144 -0.095 -0.553 -0.413 -0.385  0.173\n       dbss   soss\nnvss              \nwdss              \nvsss              \npsss              \nmass              \nsqss              \npcss              \nrdss              \nrpss              \ndfss              \ndbss  0.000       \nsoss -0.040  0.000\n\n\n\n\nsemPlot::semPaths(fit_higher, what = \"est\", edge.label.cex = .8, fade = FALSE)\n\n\n\n\n\n\n\n\n\nBifactor model\n\n## Bifactor model\ndas2_model_bifactor &lt;- \"\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss\n  Spatial =~ pcss + rdss + rpss\n  Memory =~ dfss + dbss + soss\n\n  G =~ NA*nvss + 1*wdss + vsss + psss + mass + sqss + pcss + rdss + rpss + dfss + dbss + soss\n\n  mass ~~ 0*mass\n\n  # orthogonal factors (uncorrelated factors)\n  Verbal ~~ 0*Nonverbal\n  Verbal ~~ 0*Spatial\n  Verbal ~~ 0*Memory\n  Nonverbal ~~ 0*Spatial\n  Nonverbal ~~ 0*Memory\n  Spatial ~~ 0*Memory\n  G ~~ 0*Verbal\n  G ~~ 0*Nonverbal\n  G ~~ 0*Spatial\n  G ~~ 0*Memory\n\"\n\nfit_bif &lt;- sem(das2_model_bifactor,\n  sample.cov = das2cov,\n  sample.nobs = 800,\n  sample.cov.rescale = FALSE\n)\n\n# 또는 아래와 같이 uncorrelated factors로 설정\nfit_bif &lt;- sem(das2_model_bifactor,\n  sample.cov = das2cov,\n  sample.nobs = 800,\n  sample.cov.rescale = FALSE,\n  auto.cov.lv.x = FALSE\n) # uncorrelated factors\n\nsummary(fit_bif, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 594 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        35\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               108.487\n  Degrees of freedom                                43\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              4335.087\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.985\n  Tucker-Lewis Index (TLI)                       0.976\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33704.821\n  Loglikelihood unrestricted model (H1)     -33650.578\n                                                      \n  Akaike (AIC)                               67479.642\n  Bayesian (BIC)                             67643.604\n  Sample-size adjusted Bayesian (SABIC)      67532.459\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.044\n  90 Percent confidence interval - lower         0.033\n  90 Percent confidence interval - upper         0.054\n  P-value H_0: RMSEA &lt;= 0.050                    0.839\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.025\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              1.000                               3.675    0.364\n    wdss              1.189    0.210    5.661    0.000    4.371    0.456\n    vsss              0.999    0.162    6.159    0.000    3.670    0.360\n  Nonverbal =~                                                          \n    psss              1.000                               0.109    0.011\n    mass             70.326  219.176    0.321    0.748    7.646    0.746\n    sqss             11.347   35.165    0.323    0.747    1.234    0.127\n  Spatial =~                                                            \n    pcss              1.000                               1.687    0.183\n    rdss              3.889    2.449    1.588    0.112    6.562    0.656\n    rpss              1.110    0.273    4.070    0.000    1.873    0.185\n  Memory =~                                                             \n    dfss              1.000                               3.291    0.298\n    dbss              0.837    0.227    3.686    0.000    2.754    0.271\n    soss              1.136    0.350    3.247    0.001    3.738    0.352\n  G =~                                                                  \n    nvss              1.109    0.064   17.389    0.000    6.485    0.642\n    wdss              1.000                               5.850    0.610\n    vsss              1.226    0.065   18.772    0.000    7.172    0.703\n    psss              0.956    0.074   12.902    0.000    5.595    0.543\n    mass              1.166    0.078   14.989    0.000    6.822    0.666\n    sqss              1.267    0.076   16.629    0.000    7.412    0.765\n    pcss              1.180    0.072   16.387    0.000    6.900    0.748\n    rdss              1.113    0.075   14.795    0.000    6.511    0.651\n    rpss              0.937    0.073   12.815    0.000    5.484    0.543\n    dfss              1.137    0.082   13.930    0.000    6.653    0.602\n    dbss              1.206    0.078   15.540    0.000    7.055    0.695\n    soss              1.277    0.082   15.664    0.000    7.471    0.703\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal ~~                                                             \n    Nonverbal         0.000                               0.000    0.000\n    Spatial           0.000                               0.000    0.000\n    Memory            0.000                               0.000    0.000\n  Nonverbal ~~                                                          \n    Spatial           0.000                               0.000    0.000\n    Memory            0.000                               0.000    0.000\n  Spatial ~~                                                            \n    Memory            0.000                               0.000    0.000\n  Verbal ~~                                                             \n    G                 0.000                               0.000    0.000\n  Nonverbal ~~                                                          \n    G                 0.000                               0.000    0.000\n  Spatial ~~                                                            \n    G                 0.000                               0.000    0.000\n  Memory ~~                                                             \n    G                 0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .mass              0.000                               0.000    0.000\n   .nvss             46.440    3.259   14.248    0.000   46.440    0.455\n   .wdss             38.673    3.802   10.172    0.000   38.673    0.420\n   .vsss             39.087    3.019   12.945    0.000   39.087    0.376\n   .psss             74.685    3.977   18.778    0.000   74.685    0.705\n   .sqss             37.534    2.243   16.737    0.000   37.534    0.399\n   .pcss             34.543    2.497   13.834    0.000   34.543    0.406\n   .rdss             14.542   26.089    0.557    0.577   14.542    0.145\n   .rpss             68.417    3.959   17.282    0.000   68.417    0.671\n   .dfss             66.910    4.717   14.186    0.000   66.910    0.548\n   .dbss             45.638    3.274   13.938    0.000   45.638    0.443\n   .soss             43.221    4.804    8.998    0.000   43.221    0.382\n    Verbal           13.508    3.302    4.091    0.000    1.000    1.000\n    Nonverbal         0.012    0.074    0.160    0.873    1.000    1.000\n    Spatial           2.847    1.993    1.428    0.153    1.000    1.000\n    Memory           10.833    4.351    2.490    0.013    1.000    1.000\n    G                34.221    3.819    8.961    0.000    1.000    1.000\n\n\n\n\ninspect(fit_bif, what = \"est\") |&gt; print() # minus variance for mass\n\n$lambda\n     Verbal Nnvrbl Spatil Memory     G\nnvss  1.000  0.000  0.000  0.000 1.109\nwdss  1.189  0.000  0.000  0.000 1.000\nvsss  0.999  0.000  0.000  0.000 1.226\npsss  0.000  1.000  0.000  0.000 0.956\nmass  0.000 70.326  0.000  0.000 1.166\nsqss  0.000 11.347  0.000  0.000 1.267\npcss  0.000  0.000  1.000  0.000 1.180\nrdss  0.000  0.000  3.889  0.000 1.113\nrpss  0.000  0.000  1.110  0.000 0.937\ndfss  0.000  0.000  0.000  1.000 1.137\ndbss  0.000  0.000  0.000  0.837 1.206\nsoss  0.000  0.000  0.000  1.136 1.277\n\n$theta\n       nvss   wdss   vsss   psss   mass   sqss   pcss   rdss   rpss   dfss\nnvss 46.440                                                               \nwdss  0.000 38.673                                                        \nvsss  0.000  0.000 39.087                                                 \npsss  0.000  0.000  0.000 74.685                                          \nmass  0.000  0.000  0.000  0.000  0.000                                   \nsqss  0.000  0.000  0.000  0.000  0.000 37.534                            \npcss  0.000  0.000  0.000  0.000  0.000  0.000 34.543                     \nrdss  0.000  0.000  0.000  0.000  0.000  0.000  0.000 14.542              \nrpss  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 68.417       \ndfss  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 66.910\ndbss  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nsoss  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n       dbss   soss\nnvss              \nwdss              \nvsss              \npsss              \nmass              \nsqss              \npcss              \nrdss              \nrpss              \ndfss              \ndbss 45.638       \nsoss  0.000 43.221\n\n$psi\n          Verbal Nnvrbl Spatil Memory      G\nVerbal    13.508                            \nNonverbal  0.000  0.012                     \nSpatial    0.000  0.000  2.847              \nMemory     0.000  0.000  0.000 10.833       \nG          0.000  0.000  0.000  0.000 34.221\n\n\n\n\nparameterTable(fit_bif) |&gt; print()\n\n   id       lhs op       rhs user block group free ustart exo label plabel\n1   1    Verbal =~      nvss    1     1     1    0      1   0         .p1.\n2   2    Verbal =~      wdss    1     1     1    1     NA   0         .p2.\n3   3    Verbal =~      vsss    1     1     1    2     NA   0         .p3.\n4   4 Nonverbal =~      psss    1     1     1    0      1   0         .p4.\n5   5 Nonverbal =~      mass    1     1     1    3     NA   0         .p5.\n6   6 Nonverbal =~      sqss    1     1     1    4     NA   0         .p6.\n7   7   Spatial =~      pcss    1     1     1    0      1   0         .p7.\n8   8   Spatial =~      rdss    1     1     1    5     NA   0         .p8.\n9   9   Spatial =~      rpss    1     1     1    6     NA   0         .p9.\n10 10    Memory =~      dfss    1     1     1    0      1   0        .p10.\n11 11    Memory =~      dbss    1     1     1    7     NA   0        .p11.\n12 12    Memory =~      soss    1     1     1    8     NA   0        .p12.\n13 13         G =~      nvss    1     1     1    9     NA   0        .p13.\n14 14         G =~      wdss    1     1     1    0      1   0        .p14.\n15 15         G =~      vsss    1     1     1   10     NA   0        .p15.\n16 16         G =~      psss    1     1     1   11     NA   0        .p16.\n17 17         G =~      mass    1     1     1   12     NA   0        .p17.\n18 18         G =~      sqss    1     1     1   13     NA   0        .p18.\n19 19         G =~      pcss    1     1     1   14     NA   0        .p19.\n20 20         G =~      rdss    1     1     1   15     NA   0        .p20.\n21 21         G =~      rpss    1     1     1   16     NA   0        .p21.\n22 22         G =~      dfss    1     1     1   17     NA   0        .p22.\n23 23         G =~      dbss    1     1     1   18     NA   0        .p23.\n24 24         G =~      soss    1     1     1   19     NA   0        .p24.\n25 25      mass ~~      mass    1     1     1    0      0   0        .p25.\n26 26    Verbal ~~ Nonverbal    1     1     1    0      0   0        .p26.\n27 27    Verbal ~~   Spatial    1     1     1    0      0   0        .p27.\n28 28    Verbal ~~    Memory    1     1     1    0      0   0        .p28.\n29 29 Nonverbal ~~   Spatial    1     1     1    0      0   0        .p29.\n30 30 Nonverbal ~~    Memory    1     1     1    0      0   0        .p30.\n31 31   Spatial ~~    Memory    1     1     1    0      0   0        .p31.\n32 32    Verbal ~~         G    1     1     1    0      0   0        .p32.\n33 33 Nonverbal ~~         G    1     1     1    0      0   0        .p33.\n34 34   Spatial ~~         G    1     1     1    0      0   0        .p34.\n35 35    Memory ~~         G    1     1     1    0      0   0        .p35.\n36 36      nvss ~~      nvss    0     1     1   20     NA   0        .p36.\n37 37      wdss ~~      wdss    0     1     1   21     NA   0        .p37.\n38 38      vsss ~~      vsss    0     1     1   22     NA   0        .p38.\n39 39      psss ~~      psss    0     1     1   23     NA   0        .p39.\n40 40      sqss ~~      sqss    0     1     1   24     NA   0        .p40.\n41 41      pcss ~~      pcss    0     1     1   25     NA   0        .p41.\n42 42      rdss ~~      rdss    0     1     1   26     NA   0        .p42.\n43 43      rpss ~~      rpss    0     1     1   27     NA   0        .p43.\n44 44      dfss ~~      dfss    0     1     1   28     NA   0        .p44.\n45 45      dbss ~~      dbss    0     1     1   29     NA   0        .p45.\n46 46      soss ~~      soss    0     1     1   30     NA   0        .p46.\n47 47    Verbal ~~    Verbal    0     1     1   31     NA   0        .p47.\n48 48 Nonverbal ~~ Nonverbal    0     1     1   32     NA   0        .p48.\n49 49   Spatial ~~   Spatial    0     1     1   33     NA   0        .p49.\n50 50    Memory ~~    Memory    0     1     1   34     NA   0        .p50.\n51 51         G ~~         G    0     1     1   35     NA   0        .p51.\n    start    est      se\n1   1.000  1.000   0.000\n2   0.967  1.189   0.210\n3   1.074  0.999   0.162\n4   1.000  1.000   0.000\n5   1.538 70.326 219.176\n6   1.538 11.347  35.165\n7   1.000  1.000   0.000\n8   1.171  3.889   2.449\n9   0.857  1.110   0.273\n10  1.000  1.000   0.000\n11  1.016  0.837   0.227\n12  1.125  1.136   0.350\n13  1.000  1.109   0.064\n14  1.000  1.000   0.000\n15  1.056  1.226   0.065\n16  0.737  0.956   0.074\n17  0.852  1.166   0.078\n18  0.923  1.267   0.076\n19  0.868  1.180   0.072\n20  0.828  1.113   0.075\n21  0.676  0.937   0.073\n22  0.914  1.137   0.082\n23  0.922  1.206   0.078\n24  0.989  1.277   0.082\n25  0.000  0.000   0.000\n26  0.000  0.000   0.000\n27  0.000  0.000   0.000\n28  0.000  0.000   0.000\n29  0.000  0.000   0.000\n30  0.000  0.000   0.000\n31  0.000  0.000   0.000\n32  0.000  0.000   0.000\n33  0.000  0.000   0.000\n34  0.000  0.000   0.000\n35  0.000  0.000   0.000\n36 51.000 46.440   3.259\n37 46.000 38.673   3.802\n38 52.000 39.087   3.019\n39 53.000 74.685   3.977\n40 47.000 37.534   2.243\n41 42.500 34.543   2.497\n42 50.000 14.542  26.089\n43 51.000 68.417   3.959\n44 61.000 66.910   4.717\n45 51.500 45.638   3.274\n46 56.500 43.221   4.804\n47  0.050 13.508   3.302\n48  0.050  0.012   0.074\n49  0.050  2.847   1.993\n50  0.050 10.833   4.351\n51  0.050 34.221   3.819\n\n\n\n\nSingle-indicator model\n\n## Single-indicator model\ndas2_model_single &lt;- \"\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss\n  Spatial =~ pcss + rdss + rpss\n  Memory =~ 1*dfss\n\n  G =~ Verbal + Nonverbal + Spatial + Memory\n\n  dfss  ~~ 10.94*dfss\n\"\n\nfit_single &lt;- sem(das2_model_single,\n  sample.cov = das2cov,\n  sample.nobs = 800,\n  sample.cov.rescale = FALSE\n)\n\nsummary(fit_single, fit.measures = TRUE, standardized = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 114 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        23\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               115.280\n  Degrees of freedom                                32\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              3295.596\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.974\n  Tucker-Lewis Index (TLI)                       0.964\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -28212.815\n  Loglikelihood unrestricted model (H1)     -28155.175\n                                                      \n  Akaike (AIC)                               56471.630\n  Bayesian (BIC)                             56579.376\n  Sample-size adjusted Bayesian (SABIC)      56506.339\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.057\n  90 Percent confidence interval - lower         0.046\n  90 Percent confidence interval - upper         0.068\n  P-value H_0: RMSEA &lt;= 0.050                    0.142\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.032\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              1.000                               7.506    0.743\n    wdss              0.928    0.049   19.010    0.000    6.965    0.726\n    vsss              1.097    0.053   20.784    0.000    8.237    0.808\n  Nonverbal =~                                                          \n    psss              1.000                               5.635    0.547\n    mass              1.277    0.091   14.001    0.000    7.198    0.702\n    sqss              1.394    0.093   15.001    0.000    7.857    0.810\n  Spatial =~                                                            \n    pcss              1.000                               7.528    0.817\n    rdss              0.979    0.047   20.818    0.000    7.369    0.737\n    rpss              0.787    0.049   16.184    0.000    5.923    0.586\n  Memory =~                                                             \n    dfss              1.000                              10.539    0.954\n  G =~                                                                  \n    Verbal            1.000                               0.850    0.850\n    Nonverbal         0.843    0.066   12.848    0.000    0.954    0.954\n    Spatial           1.077    0.063   17.075    0.000    0.913    0.913\n    Memory            1.041    0.072   14.514    0.000    0.630    0.630\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .dfss             10.940                              10.940    0.090\n   .nvss             45.661    2.950   15.477    0.000   45.661    0.448\n   .wdss             43.487    2.730   15.929    0.000   43.487    0.473\n   .vsss             36.158    2.766   13.071    0.000   36.158    0.348\n   .psss             74.247    4.008   18.523    0.000   74.247    0.700\n   .mass             53.191    3.222   16.507    0.000   53.191    0.507\n   .sqss             32.264    2.570   12.553    0.000   32.264    0.343\n   .pcss             28.326    2.275   12.450    0.000   28.326    0.333\n   .rdss             45.694    2.918   15.660    0.000   45.694    0.457\n   .rpss             66.915    3.687   18.150    0.000   66.915    0.656\n   .Verbal           15.629    2.182    7.163    0.000    0.277    0.277\n   .Nonverbal         2.835    1.170    2.423    0.015    0.089    0.089\n   .Spatial           9.423    2.052    4.593    0.000    0.166    0.166\n   .Memory           66.929    4.294   15.586    0.000    0.603    0.603\n    G                40.710    4.237    9.609    0.000    1.000    1.000\n\n\n\n\ndas2_model_single2 &lt;- \"\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss\n  Spatial =~ pcss + rdss + rpss\n\n  G =~ Verbal + Nonverbal + Spatial + dfss\n\"\n\nfit_single2 &lt;- sem(das2_model_single2,\n  sample.cov = das2cov,\n  sample.nobs = 800,\n  sample.cov.rescale = FALSE\n)\n\nsummary(fit_single2, fit.measures = TRUE, standardized = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 97 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        23\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               115.280\n  Degrees of freedom                                32\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              3295.596\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.974\n  Tucker-Lewis Index (TLI)                       0.964\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -28212.815\n  Loglikelihood unrestricted model (H1)     -28155.175\n                                                      \n  Akaike (AIC)                               56471.630\n  Bayesian (BIC)                             56579.376\n  Sample-size adjusted Bayesian (SABIC)      56506.339\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.057\n  90 Percent confidence interval - lower         0.046\n  90 Percent confidence interval - upper         0.068\n  P-value H_0: RMSEA &lt;= 0.050                    0.142\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.032\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              1.000                               7.506    0.743\n    wdss              0.928    0.049   19.010    0.000    6.965    0.726\n    vsss              1.097    0.053   20.784    0.000    8.237    0.808\n  Nonverbal =~                                                          \n    psss              1.000                               5.635    0.547\n    mass              1.277    0.091   14.001    0.000    7.198    0.702\n    sqss              1.394    0.093   15.001    0.000    7.857    0.810\n  Spatial =~                                                            \n    pcss              1.000                               7.528    0.817\n    rdss              0.979    0.047   20.818    0.000    7.369    0.737\n    rpss              0.787    0.049   16.184    0.000    5.923    0.586\n  G =~                                                                  \n    Verbal            1.000                               0.850    0.850\n    Nonverbal         0.843    0.066   12.848    0.000    0.954    0.954\n    Spatial           1.077    0.063   17.075    0.000    0.913    0.913\n    dfss              1.041    0.072   14.514    0.000    6.643    0.601\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .nvss             45.661    2.950   15.477    0.000   45.661    0.448\n   .wdss             43.487    2.730   15.929    0.000   43.487    0.473\n   .vsss             36.158    2.766   13.071    0.000   36.158    0.348\n   .psss             74.247    4.008   18.523    0.000   74.247    0.700\n   .mass             53.191    3.222   16.507    0.000   53.191    0.507\n   .sqss             32.264    2.570   12.553    0.000   32.264    0.343\n   .pcss             28.326    2.275   12.450    0.000   28.326    0.333\n   .rdss             45.694    2.918   15.660    0.000   45.694    0.457\n   .rpss             66.915    3.687   18.150    0.000   66.915    0.656\n   .dfss             77.869    4.294   18.133    0.000   77.869    0.638\n   .Verbal           15.629    2.182    7.163    0.000    0.277    0.277\n   .Nonverbal         2.835    1.170    2.423    0.015    0.089    0.089\n   .Spatial           9.423    2.052    4.593    0.000    0.166    0.166\n    G                40.710    4.237    9.609    0.000    1.000    1.000",
    "crumbs": [
      "Statistics",
      "Chapter 16"
    ]
  },
  {
    "objectID": "contents/chap13.html",
    "href": "contents/chap13.html",
    "title": "Chapter 13",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)\n\n\n\nlibrary(tidyverse)\n\n# Load the data\nnels &lt;- read_csv(\"data/n=1000,stud & par shorter all miss blank.csv\")\nnels |&gt; print()\n\n# A tibble: 1,000 x 93\n  stu_id sch_id sstratid   sex  race ethnic bys42a bys42b bys44a bys44b bys44c\n   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 124966   1249        1     2     4      1      3      4      2      4      4\n2 124972   1249        1     1     4      1      4      5      1      3      3\n3 175551   1755        1     2     3      0     NA      3      2      3      3\n4 180660   1806        1     1     4      1      2     NA      1      4      4\n5 180672   1806        1     2     4      1      2      3      1      4      3\n6 298885   2988        2     1     3      0      5      4      2      3      3\n# i 994 more rows\n# i 82 more variables: bys44d &lt;dbl&gt;, bys44e &lt;dbl&gt;, bys44f &lt;dbl&gt;, bys44g &lt;dbl&gt;,\n#   bys44h &lt;dbl&gt;, bys44i &lt;dbl&gt;, bys44j &lt;dbl&gt;, bys44k &lt;dbl&gt;, bys44l &lt;dbl&gt;,\n#   bys44m &lt;dbl&gt;, bys48a &lt;dbl&gt;, bys48b &lt;dbl&gt;, bys79a &lt;dbl&gt;, byfamsiz &lt;dbl&gt;,\n#   famcomp &lt;dbl&gt;, bygrads &lt;dbl&gt;, byses &lt;dbl&gt;, byfaminc &lt;dbl&gt;, parocc &lt;dbl&gt;,\n#   bytxrstd &lt;dbl&gt;, bytxmstd &lt;dbl&gt;, bytxsstd &lt;dbl&gt;, bytxhstd &lt;dbl&gt;,\n#   bypared &lt;dbl&gt;, bytests &lt;dbl&gt;, par_inv &lt;dbl&gt;, f1s36a1 &lt;dbl&gt;, ...\n\n\n\n# SPSS data: labelled data\nlibrary(haven) # install.packages(\"haven\")\nnels_sav &lt;- read_sav(\"data/n=1000,stud & par shorter.sav\")\nnels_sav |&gt; print()\n\n# A tibble: 1,000 x 93\n  stu_id    sch_id    sstratid sex     race    ethnic  bys42a   bys42b   bys44a \n  &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lb&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt; &lt;dbl+lb&gt; &lt;dbl+l&gt;\n1 124966    1249      1        2 [Fem~ 4 [Whi~ 1 [whi~  3 [2-3~  4 [3-4~ 2 [Agr~\n2 124972    1249      1        1 [Mal~ 4 [Whi~ 1 [whi~  4 [3-4~  5 [4-5~ 1 [Str~\n3 175551    1755      1        2 [Fem~ 3 [Bla~ 0 [blk~ NA        3 [2-3~ 2 [Agr~\n4 180660    1806      1        1 [Mal~ 4 [Whi~ 1 [whi~  2 [1-2~ NA       1 [Str~\n5 180672    1806      1        2 [Fem~ 4 [Whi~ 1 [whi~  2 [1-2~  3 [2-3~ 1 [Str~\n6 298885    2988      2        1 [Mal~ 3 [Bla~ 0 [blk~  5 [4-5~  4 [3-4~ 2 [Agr~\n# i 994 more rows\n# i 84 more variables: bys44b &lt;dbl+lbl&gt;, bys44c &lt;dbl+lbl&gt;, bys44d &lt;dbl+lbl&gt;,\n#   bys44e &lt;dbl+lbl&gt;, bys44f &lt;dbl+lbl&gt;, bys44g &lt;dbl+lbl&gt;, bys44h &lt;dbl+lbl&gt;,\n#   bys44i &lt;dbl+lbl&gt;, bys44j &lt;dbl+lbl&gt;, bys44k &lt;dbl+lbl&gt;, bys44l &lt;dbl+lbl&gt;,\n#   bys44m &lt;dbl+lbl&gt;, bys48a &lt;dbl+lbl&gt;, bys48b &lt;dbl+lbl&gt;, bys79a &lt;dbl+lbl&gt;,\n#   byfamsiz &lt;dbl+lbl&gt;, famcomp &lt;dbl+lbl&gt;, bygrads &lt;dbl+lbl&gt;, byses &lt;dbl+lbl&gt;,\n#   byfaminc &lt;dbl+lbl&gt;, parocc &lt;dbl&gt;, bytxrstd &lt;dbl+lbl&gt;, ...\n\n\n\nnels_sav$ethnic |&gt; labelled::val_labels() |&gt; print()\n\nblk,namer,hisp    white-asian        missing \n             0              1              8 \n\n\nvariables: byses, bytests, par_inv, ffugrad, ethnic\nUnderrepresented ethnic minority, or URM, is coded so that students from African American, Hispanic, and Native backgrounds are coded 1 and students of Asian and Caucasian descent are coded 0.\n\nnels_gpa &lt;-\n  nels |&gt;\n  select(ethnic, ses = byses, prev = bytests, par = par_inv, gpa = ffugrad) |&gt;\n  na.omit()\n\nnels_gpa |&gt; print()\n\n# A tibble: 811 x 5\n  ethnic    ses  prev     par   gpa\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1      1 -0.563  64.4  1.04    5.25\n2      1  0.123  48.6 -0.0881  3   \n3      0  0.229  49.7 -0.390   2.5 \n4      1  0.687  46.6  0.199   6.5 \n5      1  0.633  54.9  0.975   4.25\n6      0  0.992  38.5 -0.157   6   \n# i 805 more rows\n\n\n\n상관계수\n\nlibrary(psych)\nnels_gpa |&gt; lowerCor(digits = 3)\n\n       ethnc ses   prev  par   gpa  \nethnic 1.000                        \nses    0.333 1.000                  \nprev   0.330 0.461 1.000            \npar    0.075 0.432 0.445 1.000      \ngpa    0.131 0.299 0.499 0.364 1.000\n\n\n\n\n구조모형\n\n\nlibrary(lavaan)\nlibrary(semTools)\n\nmod &lt;- \"\n\n  gpa ~ ethnic + ses + prev + par\n  par ~ prev + ses + ethnic\n  prev ~ ses + ethnic\n\n\"\n\nsem_fit &lt;- sem(model = mod, data = nels_gpa)\nsummary(sem_fit, standardized = TRUE, rsquare = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Number of observations                           811\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  gpa ~                                                                 \n    ethnic           -0.124    0.117   -1.058    0.290   -0.124   -0.035\n    ses               0.093    0.069    1.355    0.175    0.093    0.049\n    prev              0.070    0.006   11.414    0.000    0.070    0.417\n    par               0.292    0.064    4.531    0.000    0.292    0.160\n  par ~                                                                 \n    prev              0.032    0.003   10.040    0.000    0.032    0.345\n    ses               0.333    0.036    9.351    0.000    0.333    0.321\n    ethnic           -0.286    0.063   -4.528    0.000   -0.286   -0.146\n  prev ~                                                                \n    ses               4.431    0.362   12.236    0.000    4.431    0.395\n    ethnic            4.195    0.684    6.135    0.000    4.195    0.198\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .gpa               1.521    0.076   20.137    0.000    1.521    0.724\n   .par               0.452    0.022   20.137    0.000    0.452    0.719\n   .prev             55.370    2.750   20.137    0.000   55.370    0.752\n\nR-Square:\n                   Estimate\n    gpa               0.276\n    par               0.281\n    prev              0.248\n\n\n\n옵션들\n\nci: confidence interval\nheader: 헤더 표시 여부\nnd: the number of digits\n\n\nsummary(sem_fit, standardized = TRUE, ci = TRUE, header = FALSE, nd = 2) |&gt; print()\n\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n  gpa ~                                                                 \n    ethnic            -0.12     0.12    -1.06     0.29    -0.35     0.11\n    ses                0.09     0.07     1.36     0.18    -0.04     0.23\n    prev               0.07     0.01    11.41     0.00     0.06     0.08\n    par                0.29     0.06     4.53     0.00     0.17     0.42\n  par ~                                                                 \n    prev               0.03     0.00    10.04     0.00     0.03     0.04\n    ses                0.33     0.04     9.35     0.00     0.26     0.40\n    ethnic            -0.29     0.06    -4.53     0.00    -0.41    -0.16\n  prev ~                                                                \n    ses                4.43     0.36    12.24     0.00     3.72     5.14\n    ethnic             4.20     0.68     6.13     0.00     2.85     5.54\n   Std.lv  Std.all\n                  \n    -0.12    -0.03\n     0.09     0.05\n     0.07     0.42\n     0.29     0.16\n                  \n     0.03     0.34\n     0.33     0.32\n    -0.29    -0.15\n                  \n     4.43     0.40\n     4.20     0.20\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n   .gpa                1.52     0.08    20.14     0.00     1.37     1.67\n   .par                0.45     0.02    20.14     0.00     0.41     0.50\n   .prev              55.37     2.75    20.14     0.00    49.98    60.76\n   Std.lv  Std.all\n     1.52     0.72\n     0.45     0.72\n    55.37     0.75\n\n\n\n표준화된 파라미터 추정치만 표시\n\nstandardizedSolution(sem_fit, type = \"std.all\") |&gt; print()\n\n      lhs op    rhs est.std    se      z pvalue ci.lower ci.upper\n1     gpa  ~ ethnic  -0.035 0.033 -1.058  0.290   -0.099    0.030\n2     gpa  ~    ses   0.049 0.036  1.357  0.175   -0.022    0.120\n3     gpa  ~   prev   0.417 0.034 12.127  0.000    0.349    0.484\n4     gpa  ~    par   0.160 0.035  4.565  0.000    0.091    0.228\n5     par  ~   prev   0.345 0.033 10.473  0.000    0.280    0.409\n6     par  ~    ses   0.321 0.033  9.873  0.000    0.258    0.385\n7     par  ~ ethnic  -0.146 0.032 -4.583  0.000   -0.209   -0.084\n8    prev  ~    ses   0.395 0.029 13.605  0.000    0.338    0.452\n9    prev  ~ ethnic   0.198 0.032  6.288  0.000    0.136    0.260\n10    gpa ~~    gpa   0.724 0.027 27.296  0.000    0.672    0.776\n11    par ~~    par   0.719 0.026 27.781  0.000    0.668    0.770\n12   prev ~~   prev   0.752 0.025 30.565  0.000    0.704    0.801\n13 ethnic ~~ ethnic   1.000 0.000     NA     NA    1.000    1.000\n14 ethnic ~~    ses   0.333 0.000     NA     NA    0.333    0.333\n15    ses ~~    ses   1.000 0.000     NA     NA    1.000    1.000\n\n\n파라미터 추정 방식: lavaan website\n기본적으로 ML (Maximum Likelihood) 방법을 사용\n\n여러 robust 버전들이 있음: estimator 옵션을 사용하여 변경\n“MLM”: maximum likelihood estimation with robust standard errors and a Satorra-Bentler scaled test statistic. For complete data only.\n“MLMVS”: maximum likelihood estimation with robust standard errors and a mean- and variance-adjusted test statistic (aka the Satterthwaite approach). For complete data only.\n“MLMV”: maximum likelihood estimation with robust standard errors and a mean- and variance-adjusted test statistic (using a scale-shifted approach). For complete data only.\n“MLF”: for maximum likelihood estimation with standard errors based on the first-order derivatives, and a conventional test statistic. For both complete and incomplete data.\n“MLR”: maximum likelihood estimation with robust (Huber-White) standard errors and a scaled test statistic that is (asymptotically) equal to the Yuan-Bentler test statistic. For both complete and incomplete data.\n\n부트스트랩(bootstrap) 방법을 사용한 표준오차 추정치\n\n# MLR estimator\nsem_fit &lt;- sem(\n    model = mod,\n    data = nels_gpa,\n    estimator = \"MLR\"\n)\nsummary(sem_fit, standardized = TRUE, header = F) |&gt; print()\n\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  gpa ~                                                                 \n    ethnic           -0.124    0.119   -1.044    0.297   -0.124   -0.035\n    ses               0.093    0.068    1.367    0.172    0.093    0.049\n    prev              0.070    0.006   11.554    0.000    0.070    0.417\n    par               0.292    0.068    4.282    0.000    0.292    0.160\n  par ~                                                                 \n    prev              0.032    0.003   10.240    0.000    0.032    0.345\n    ses               0.333    0.035    9.491    0.000    0.333    0.321\n    ethnic           -0.286    0.066   -4.333    0.000   -0.286   -0.146\n  prev ~                                                                \n    ses               4.431    0.363   12.211    0.000    4.431    0.395\n    ethnic            4.195    0.692    6.062    0.000    4.195    0.198\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .gpa               1.521    0.073   20.851    0.000    1.521    0.724\n   .par               0.452    0.025   18.293    0.000    0.452    0.719\n   .prev             55.370    2.475   22.368    0.000   55.370    0.752\n\n\n\n\n# Bootstrap\nsem_fit &lt;- sem(\n    model = mod, \n    data = nels_gpa, \n    estimator = \"ML\",  # default\n    se = \"bootstrap\",  # standard errors\n    bootstrap = 1000,  # number of bootstrap samples\n)\n\n\nsummary(sem_fit, standardized = TRUE, header = F, ci = T) |&gt; print()\n\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws             1000\n  Number of successful bootstrap draws            1000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n  gpa ~                                                                 \n    ethnic           -0.124    0.120   -1.031    0.303   -0.354    0.113\n    ses               0.093    0.070    1.328    0.184   -0.040    0.234\n    prev              0.070    0.006   11.472    0.000    0.059    0.083\n    par               0.292    0.066    4.407    0.000    0.165    0.422\n  par ~                                                                 \n    prev              0.032    0.003   10.211    0.000    0.026    0.038\n    ses               0.333    0.034    9.746    0.000    0.267    0.401\n    ethnic           -0.286    0.067   -4.279    0.000   -0.424   -0.154\n  prev ~                                                                \n    ses               4.431    0.377   11.745    0.000    3.682    5.161\n    ethnic            4.195    0.707    5.932    0.000    2.706    5.573\n   Std.lv  Std.all\n                  \n   -0.124   -0.035\n    0.093    0.049\n    0.070    0.417\n    0.292    0.160\n                  \n    0.032    0.345\n    0.333    0.321\n   -0.286   -0.146\n                  \n    4.431    0.395\n    4.195    0.198\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n   .gpa               1.521    0.074   20.508    0.000    1.365    1.653\n   .par               0.452    0.024   18.823    0.000    0.400    0.494\n   .prev             55.370    2.477   22.351    0.000   50.331   60.256\n   Std.lv  Std.all\n    1.521    0.724\n    0.452    0.719\n   55.370    0.752\n\n\n\n\n\n플롯 그리기\n\ntidySEM 참조\n\nlavaanExtra 참조\n\nsemPlot 참조\n\n\ntidySEM::graph_sem(sem_fit)\nlavaanExtra::nice_tidySEM(sem_fit)\n\nsemPlot::semPaths(sem_fit, what = 'est', edge.label.cex = 1, fade = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n간접효과 ses -&gt; par -&gt; gpa\n\nmod2 &lt;- \"\n  \n  gpa ~ b1*ethnic + b2*ses + b3*prev + b4*par\n  par ~ b5*prev + b6*ses + b7*ethnic\n  prev ~ b8*ses + b9*ethnic\n  \n  ses_par_gpa := b6*b4\n\"\n\nsem_fit2 &lt;- sem(model = mod2, data = nels_gpa)\nsummary(sem_fit2, standardized = TRUE,  rsquare = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Number of observations                           811\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  gpa ~                                                                 \n    ethnic    (b1)   -0.124    0.117   -1.058    0.290   -0.124   -0.035\n    ses       (b2)    0.093    0.069    1.355    0.175    0.093    0.049\n    prev      (b3)    0.070    0.006   11.414    0.000    0.070    0.417\n    par       (b4)    0.292    0.064    4.531    0.000    0.292    0.160\n  par ~                                                                 \n    prev      (b5)    0.032    0.003   10.040    0.000    0.032    0.345\n    ses       (b6)    0.333    0.036    9.351    0.000    0.333    0.321\n    ethnic    (b7)   -0.286    0.063   -4.528    0.000   -0.286   -0.146\n  prev ~                                                                \n    ses       (b8)    4.431    0.362   12.236    0.000    4.431    0.395\n    ethnic    (b9)    4.195    0.684    6.135    0.000    4.195    0.198\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .gpa               1.521    0.076   20.137    0.000    1.521    0.724\n   .par               0.452    0.022   20.137    0.000    0.452    0.719\n   .prev             55.370    2.750   20.137    0.000   55.370    0.752\n\nR-Square:\n                   Estimate\n    gpa               0.276\n    par               0.281\n    prev              0.248\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    ses_par_gpa       0.097    0.024    4.077    0.000    0.097    0.051\n\n\n\n\n\n모든 간접효과: ses -&gt; gpa\nmanymome 참조\n\nlibrary(manymome)\n\n# All indirect paths from x to y\npaths &lt;- all_indirect_paths(sem_fit,\n  x = \"ses\",\n  y = \"gpa\"\n)\npaths |&gt; print()\n\nCall: \nall_indirect_paths(fit = sem_fit, x = \"ses\", y = \"gpa\")\nPath(s): \n  path                     \n1 ses -&gt; par -&gt; gpa        \n2 ses -&gt; prev -&gt; gpa       \n3 ses -&gt; prev -&gt; par -&gt; gpa\n\n\n\n# Indirect effect estimates\nind_est &lt;- many_indirect_effects(paths,\n  fit = sem_fit, R = 1000,\n  boot_ci = TRUE, boot_type = \"bc\"\n)\n\nind_est |&gt; print()\n\n\n==  Indirect Effect(s)   ==\n                            ind CI.lo CI.hi Sig\nses -&gt; par -&gt; gpa         0.097 0.058 0.154 Sig\nses -&gt; prev -&gt; gpa        0.312 0.243 0.386 Sig\nses -&gt; prev -&gt; par -&gt; gpa 0.041 0.023 0.065 Sig\n\n - [CI.lo to CI.hi] are 95.0% bias-corrected confidence intervals by\n   nonparametric bootstrapping with 1000 samples.\n - The 'ind' column shows the indirect effects.\n \n\n\n\n# Standarized estimates\nind_est_std &lt;- many_indirect_effects(paths,\n  fit = sem_fit, R = 1000,\n  boot_ci = TRUE, boot_type = \"bc\",\n  standardized_x = TRUE,\n  standardized_y = TRUE\n)\n\nind_est_std |&gt; print()\n\n\n==  Indirect Effect(s) (Both x-variable(s) and y-variable(s) Standardized)  ==\n                            std CI.lo CI.hi Sig\nses -&gt; par -&gt; gpa         0.051 0.031 0.081 Sig\nses -&gt; prev -&gt; gpa        0.165 0.131 0.204 Sig\nses -&gt; prev -&gt; par -&gt; gpa 0.022 0.012 0.034 Sig\n\n - [CI.lo to CI.hi] are 95.0% bias-corrected confidence intervals by\n   nonparametric bootstrapping with 1000 samples.\n - std: The standardized indirect effects.\n \n\n\n\n\n단순한 모형\n\nmod_revised &lt;- \"\n  gpa ~ prev + par\n  par ~ prev + ses + ethnic\n  prev ~ ses + ethnic\n\"\n\nsem_fit2 &lt;- sem(model = mod_revised, data = nels_gpa)\nsummary(sem_fit2, standardized = TRUE,  rsquare = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                           811\n\nModel Test User Model:\n                                                      \n  Test statistic                                 2.382\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.304\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  gpa ~                                                                 \n    prev              0.071    0.006   12.589    0.000    0.071    0.421\n    par               0.323    0.061    5.288    0.000    0.323    0.177\n  par ~                                                                 \n    prev              0.032    0.003   10.040    0.000    0.032    0.345\n    ses               0.333    0.036    9.351    0.000    0.333    0.321\n    ethnic           -0.286    0.063   -4.528    0.000   -0.286   -0.146\n  prev ~                                                                \n    ses               4.431    0.362   12.236    0.000    4.431    0.395\n    ethnic            4.195    0.684    6.135    0.000    4.195    0.198\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .gpa               1.525    0.076   20.137    0.000    1.525    0.726\n   .par               0.452    0.022   20.137    0.000    0.452    0.719\n   .prev             55.370    2.750   20.137    0.000   55.370    0.752\n\nR-Square:\n                   Estimate\n    gpa               0.274\n    par               0.281\n    prev              0.248\n\n\n\n\nnels_full &lt;- read_csv(\"data/n=1000,stud & par_3.csv\")\nnels_hw &lt;-\n  nels_full |&gt;\n  select(ethnic, ses = byses, prev = bytests, gpa = ffugrad, hw = homewk)\n\nnels_hw |&gt; lowerCor(digits = 3)\n\n       ethnc ses   prev  gpa   hw   \nethnic 1.000                        \nses    0.344 1.000                  \nprev   0.334 0.484 1.000            \ngpa    0.143 0.294 0.486 1.000      \nhw     0.143 0.316 0.350 0.325 1.000",
    "crumbs": [
      "Statistics",
      "Chapter 13"
    ]
  },
  {
    "objectID": "contents/chap18.html",
    "href": "contents/chap18.html",
    "title": "Chapter 18",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)\n\n\n\nlibrary(tidyverse)\nlibrary(haven)\nhw &lt;- read_sav(\"data/chap 18 latent var SEM 2/HW latent matrix.sav\")\nhw |&gt; print()\n\n# A tibble: 17 x 16\n   rowtype_ varname_     Minority  bypared  byfaminc   parocc bytxrstd bytxmstd\n   &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 N        \"\"         1000       1000     1000      1000     1000     1000    \n 2 corr     \"Minority\"    1         -0.169   -0.278    -0.242   -0.204   -0.161\n 3 corr     \"bypared\"    -0.169      1        0.526     0.629    0.386    0.430\n 4 corr     \"byfaminc\"   -0.278      0.526    1         0.524    0.288    0.335\n 5 corr     \"parocc\"     -0.242      0.629    0.524     1        0.339    0.362\n 6 corr     \"bytxrstd\"   -0.204      0.386    0.288     0.339    1        0.714\n 7 corr     \"bytxmstd\"   -0.161      0.430    0.335     0.362    0.714    1    \n 8 corr     \"bytxsstd\"   -0.231      0.384    0.293     0.322    0.717    0.719\n 9 corr     \"bytxhstd\"   -0.210      0.396    0.308     0.346    0.731    0.675\n10 corr     \"hw_8\"       -0.00317    0.168    0.0750    0.105    0.226    0.271\n11 corr     \"hw10\"       -0.0559     0.208    0.155     0.173    0.219    0.286\n12 corr     \"eng_12\"     -0.0978     0.334    0.243     0.260    0.524    0.565\n13 corr     \"math_12\"    -0.0710     0.285    0.220     0.218    0.418    0.587\n14 corr     \"sci_12\"     -0.0832     0.294    0.209     0.231    0.484    0.576\n15 corr     \"ss_12\"      -0.111      0.328    0.253     0.265    0.519    0.567\n16 stddev   \"\"            0.445      1.28     2.52     21.6     10.3     10.4  \n17 mean     \"\"            0.272      3.20     9.92     51.7     52.0     52.5  \n# i 8 more variables: bytxsstd &lt;dbl&gt;, bytxhstd &lt;dbl&gt;, hw_8 &lt;dbl&gt;, hw10 &lt;dbl&gt;,\n#   eng_12 &lt;dbl&gt;, math_12 &lt;dbl&gt;, sci_12 &lt;dbl&gt;, ss_12 &lt;dbl&gt;\n\n\n\nnels_sav_full &lt;- read_sav(\"data/n=1000,stud & par_3.sav\")\n\nvars &lt;- c(\"byfaminc\", \"parocc\", \"bypared\", \"ethnic\", \"f1txhstd\", \"f1txsstd\", \"f1txrstd\", \"f1txmstd\", \"bytxhstd\", \"bytxsstd\", \"bytxrstd\", \"bytxmstd\", \"f1s36a1\",  \"f1s36a2\", \"byhomewk\")\n\nhw_raw &lt;- nels_sav_full[vars]\n\nhw_raw |&gt; psych::lowerCor() |&gt; round(2) |&gt; print()\n\n\nlibrary(lavaan)\nhwcov &lt;- hw[c(2:15), c(3:16)] |&gt; \n  as.matrix() |&gt; \n  lav_matrix_vechr(diagonal = TRUE) |&gt; \n  getCov(names = hw$varname_[2:15]) |&gt; \n  cor2cov(sds = hw[16, 3:16] |&gt; as.double())\n\nhwcov |&gt; round(2) |&gt; print()\n\n         Minority bypared byfaminc parocc bytxrstd bytxmstd bytxsstd bytxhstd\nMinority     0.20   -0.10    -0.31  -2.32    -0.93    -0.74    -1.06    -0.95\nbypared     -0.10    1.65     1.70  17.43     5.09     5.73     5.08     5.18\nbyfaminc    -0.31    1.70     6.37  28.56     7.49     8.76     7.62     7.91\nparocc      -2.32   17.43    28.56 466.51    75.24    81.22    71.66    76.05\nbytxrstd    -0.93    5.09     7.49  75.24   105.89    76.31    76.18    76.56\nbytxmstd    -0.74    5.73     8.76  81.22    76.31   107.75    77.04    71.37\nbytxsstd    -1.06    5.08     7.62  71.66    76.18    77.04   106.45    76.48\nbytxhstd    -0.95    5.18     7.91  76.05    76.56    71.37    76.48   103.68\nhw_8         0.00    0.24     0.21   2.58     2.63     3.18     2.57     1.93\nhw10        -0.05    0.51     0.75   7.13     4.29     5.64     4.04     4.01\neng_12      -0.12    1.15     1.64  14.99    14.42    15.69    12.42    13.36\nmath_12     -0.09    1.01     1.53  12.94    11.82    16.72    11.78    11.45\nsci_12      -0.10    1.01     1.42  13.40    13.35    16.04    13.64    13.00\nss_12       -0.14    1.21     1.83  16.42    15.36    16.89    14.37    15.17\n         hw_8  hw10 eng_12 math_12 sci_12 ss_12\nMinority 0.00 -0.05  -0.12   -0.09  -0.10 -0.14\nbypared  0.24  0.51   1.15    1.01   1.01  1.21\nbyfaminc 0.21  0.75   1.64    1.53   1.42  1.83\nparocc   2.58  7.13  14.99   12.94  13.40 16.42\nbytxrstd 2.63  4.29  14.42   11.82  13.35 15.36\nbytxmstd 3.18  5.64  15.69   16.72  16.04 16.89\nbytxsstd 2.57  4.04  12.42   11.78  13.64 14.37\nbytxhstd 1.93  4.01  13.36   11.45  13.00 15.17\nhw_8     1.28  0.58   0.62    0.54   0.58  0.59\nhw10     0.58  3.62   1.59    1.51   1.44  1.55\neng_12   0.62  1.59   7.15    5.59   5.76  6.54\nmath_12  0.54  1.51   5.59    7.55   5.59  5.88\nsci_12   0.58  1.44   5.76    5.59   7.19  6.12\nss_12    0.59  1.55   6.54    5.88   6.12  8.25\n\n\n\n# generate a dataset with the same covariance matrix\nhw_sim &lt;- semTools::kd(hwcov, n = 1000, type = \"exact\")\nsum(as.numeric(hwcov - cov(hw_sim)))\n\n-3.39684010659664\n\n\n\nhw_model &lt;- \"\n  eminor =~ Minority\n  famback =~ bypared + byfaminc + parocc\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw_8 + hw10\n  grades =~ eng_12 + math_12 + sci_12 + ss_12\n\n  Minority ~~ 0.0099*Minority\n  bytxrstd ~~ eng_12\n  bytxmstd ~~ math_12\n  bytxsstd ~~ sci_12\n  bytxhstd ~~ ss_12\n\n  prevach ~ b1*famback + b2*eminor\n  grades ~ b3*prevach + b4*hw\n  hw ~ b5*prevach + b6*famback + b7*eminor\n\n  ind := b4*b5*b1\n\"\n\nsem_fit &lt;- sem(hw_model,\n  sample.cov = hwcov,\n  sample.nobs = 1000,\n  sample.cov.rescale = FALSE\n)\n\nsummary(sem_fit, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 133 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        39\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                               204.654\n  Degrees of freedom                                66\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              8392.044\n  Degrees of freedom                                91\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.983\n  Tucker-Lewis Index (TLI)                       0.977\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33338.255\n  Loglikelihood unrestricted model (H1)     -33235.928\n                                                      \n  Akaike (AIC)                               66754.509\n  Bayesian (BIC)                             66945.912\n  Sample-size adjusted Bayesian (SABIC)      66822.046\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.046\n  90 Percent confidence interval - lower         0.039\n  90 Percent confidence interval - upper         0.053\n  P-value H_0: RMSEA &lt;= 0.050                    0.825\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  eminor =~                                                             \n    Minority          1.000                               0.434    0.975\n  famback =~                                                            \n    bypared           1.000                               1.033    0.805\n    byfaminc          1.630    0.084   19.434    0.000    1.684    0.667\n    parocc           16.235    0.751   21.618    0.000   16.769    0.776\n  prevach =~                                                            \n    bytxrstd          1.000                               8.811    0.855\n    bytxmstd          0.997    0.030   33.754    0.000    8.787    0.844\n    bytxsstd          0.990    0.030   33.537    0.000    8.722    0.846\n    bytxhstd          0.967    0.029   32.925    0.000    8.517    0.837\n  hw =~                                                                 \n    hw_8              1.000                               0.510    0.451\n    hw10              2.208    0.292    7.553    0.000    1.126    0.592\n  grades =~                                                             \n    eng_12            1.000                               2.472    0.924\n    math_12           0.896    0.024   37.832    0.000    2.214    0.820\n    sci_12            0.957    0.022   43.705    0.000    2.366    0.878\n    ss_12             1.062    0.022   48.218    0.000    2.626    0.914\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback   (b1)    4.511    0.328   13.733    0.000    0.529    0.529\n    eminor    (b2)   -1.774    0.645   -2.749    0.006   -0.087   -0.087\n  grades ~                                                              \n    prevach   (b3)    0.145    0.012   12.581    0.000    0.518    0.518\n    hw        (b4)    1.328    0.278    4.776    0.000    0.274    0.274\n  hw ~                                                                  \n    prevach   (b5)    0.024    0.004    5.848    0.000    0.413    0.413\n    famback   (b6)    0.098    0.032    3.051    0.002    0.198    0.198\n    eminor    (b7)    0.127    0.056    2.264    0.024    0.108    0.108\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.705    0.248    2.844    0.004    0.705    0.128\n .bytxmstd ~~                                                           \n   .math_12           2.859    0.342    8.350    0.000    2.859    0.331\n .bytxsstd ~~                                                           \n   .sci_12            0.921    0.285    3.227    0.001    0.921    0.130\n .bytxhstd ~~                                                           \n   .ss_12             0.534    0.277    1.927    0.054    0.534    0.082\n  eminor ~~                                                             \n    famback          -0.132    0.017   -7.750    0.000   -0.294   -0.294\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Minority          0.010                               0.010    0.050\n   .bypared           0.581    0.046   12.697    0.000    0.581    0.353\n   .byfaminc          3.531    0.193   18.276    0.000    3.531    0.555\n   .parocc          185.313   13.030   14.222    0.000  185.313    0.397\n   .bytxrstd         28.583    1.719   16.625    0.000   28.583    0.269\n   .bytxmstd         31.264    1.823   17.153    0.000   31.264    0.288\n   .bytxsstd         30.196    1.769   17.068    0.000   30.196    0.284\n   .bytxhstd         30.894    1.776   17.399    0.000   30.894    0.299\n   .hw_8              1.019    0.058   17.562    0.000    1.019    0.797\n   .hw10              2.354    0.202   11.639    0.000    2.354    0.650\n   .eng_12            1.053    0.075   14.115    0.000    1.053    0.147\n   .math_12           2.380    0.122   19.545    0.000    2.380    0.327\n   .sci_12            1.655    0.094   17.625    0.000    1.655    0.228\n   .ss_12             1.366    0.090   15.133    0.000    1.366    0.165\n    eminor            0.188    0.009   21.243    0.000    1.000    1.000\n    famback           1.067    0.079   13.577    0.000    1.000    1.000\n   .prevach          53.216    3.517   15.130    0.000    0.686    0.686\n   .hw                0.188    0.039    4.856    0.000    0.722    0.722\n   .grades            3.153    0.202   15.604    0.000    0.516    0.516\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    ind               0.143    0.037    3.841    0.000    0.060    0.060\n\n\n\n\n# For simulate data\nsem_fit_raw &lt;- sem(hw_model, data = hw_sim)\nsummary(sem_fit_raw, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 133 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        39\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                               204.654\n  Degrees of freedom                                66\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              8392.044\n  Degrees of freedom                                91\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.983\n  Tucker-Lewis Index (TLI)                       0.977\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33338.255\n  Loglikelihood unrestricted model (H1)     -33235.928\n                                                      \n  Akaike (AIC)                               66754.509\n  Bayesian (BIC)                             66945.912\n  Sample-size adjusted Bayesian (SABIC)      66822.046\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.046\n  90 Percent confidence interval - lower         0.039\n  90 Percent confidence interval - upper         0.053\n  P-value H_0: RMSEA &lt;= 0.050                    0.825\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  eminor =~                                                             \n    Minority          1.000                               0.434    0.975\n  famback =~                                                            \n    bypared           1.000                               1.033    0.805\n    byfaminc          1.630    0.084   19.434    0.000    1.684    0.667\n    parocc           16.235    0.751   21.618    0.000   16.769    0.776\n  prevach =~                                                            \n    bytxrstd          1.000                               8.811    0.855\n    bytxmstd          0.997    0.030   33.754    0.000    8.787    0.844\n    bytxsstd          0.990    0.030   33.537    0.000    8.722    0.846\n    bytxhstd          0.967    0.029   32.925    0.000    8.517    0.837\n  hw =~                                                                 \n    hw_8              1.000                               0.510    0.451\n    hw10              2.208    0.292    7.553    0.000    1.126    0.592\n  grades =~                                                             \n    eng_12            1.000                               2.472    0.924\n    math_12           0.896    0.024   37.832    0.000    2.214    0.820\n    sci_12            0.957    0.022   43.705    0.000    2.366    0.878\n    ss_12             1.062    0.022   48.218    0.000    2.626    0.914\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback   (b1)    4.511    0.328   13.733    0.000    0.529    0.529\n    eminor    (b2)   -1.774    0.645   -2.749    0.006   -0.087   -0.087\n  grades ~                                                              \n    prevach   (b3)    0.145    0.012   12.581    0.000    0.518    0.518\n    hw        (b4)    1.328    0.278    4.776    0.000    0.274    0.274\n  hw ~                                                                  \n    prevach   (b5)    0.024    0.004    5.848    0.000    0.413    0.413\n    famback   (b6)    0.098    0.032    3.051    0.002    0.198    0.198\n    eminor    (b7)    0.127    0.056    2.264    0.024    0.108    0.108\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.705    0.248    2.844    0.004    0.705    0.128\n .bytxmstd ~~                                                           \n   .math_12           2.859    0.342    8.350    0.000    2.859    0.331\n .bytxsstd ~~                                                           \n   .sci_12            0.921    0.285    3.227    0.001    0.921    0.130\n .bytxhstd ~~                                                           \n   .ss_12             0.534    0.277    1.927    0.054    0.534    0.082\n  eminor ~~                                                             \n    famback          -0.132    0.017   -7.750    0.000   -0.294   -0.294\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Minority          0.010                               0.010    0.050\n   .bypared           0.581    0.046   12.697    0.000    0.581    0.353\n   .byfaminc          3.531    0.193   18.276    0.000    3.531    0.555\n   .parocc          185.313   13.030   14.222    0.000  185.313    0.397\n   .bytxrstd         28.583    1.719   16.625    0.000   28.583    0.269\n   .bytxmstd         31.264    1.823   17.153    0.000   31.264    0.288\n   .bytxsstd         30.196    1.769   17.068    0.000   30.196    0.284\n   .bytxhstd         30.894    1.776   17.399    0.000   30.894    0.299\n   .hw_8              1.019    0.058   17.562    0.000    1.019    0.797\n   .hw10              2.354    0.202   11.639    0.000    2.354    0.650\n   .eng_12            1.053    0.075   14.115    0.000    1.053    0.147\n   .math_12           2.380    0.122   19.545    0.000    2.380    0.327\n   .sci_12            1.655    0.094   17.625    0.000    1.655    0.228\n   .ss_12             1.366    0.090   15.133    0.000    1.366    0.165\n    eminor            0.188    0.009   21.243    0.000    1.000    1.000\n    famback           1.067    0.079   13.577    0.000    1.000    1.000\n   .prevach          53.216    3.517   15.130    0.000    0.686    0.686\n   .hw                0.188    0.039    4.856    0.000    0.722    0.722\n   .grades            3.153    0.202   15.604    0.000    0.516    0.516\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    ind               0.143    0.037    3.841    0.000    0.060    0.060\n\n\n\n\nAll indirect paths from family background to grandes: total effect\n\nlibrary(manymome)\n\n# All indirect paths from family background to grandes: total effect\npaths &lt;- all_indirect_paths(sem_fit_raw,\n  x = \"famback\",\n  y = \"grades\"\n)\npaths |&gt; print()\n\nCall: \nall_indirect_paths(fit = sem_fit_raw, x = \"famback\", y = \"grades\")\nPath(s): \n  path                              \n1 famback -&gt; prevach -&gt; hw -&gt; grades\n2 famback -&gt; prevach -&gt; grades      \n3 famback -&gt; hw -&gt; grades           \n\n\n\nind_est_std &lt;- many_indirect_effects(paths,\n  fit = sem_fit_raw, R = 1000,\n  boot_ci = TRUE, boot_type = \"bc\",\n  standardized_x = TRUE,\n  standardized_y = TRUE\n)\n\nind_est_std\n\n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100% elapsed=12s  \n\n\n\n==  Indirect Effect(s) (Both x-variable(s) and y-variable(s) Standardized)  ==\n                                     std CI.lo CI.hi Sig\nfamback -&gt; prevach -&gt; hw -&gt; grades 0.060 0.036 0.109 Sig\nfamback -&gt; prevach -&gt; grades       0.274 0.221 0.329 Sig\nfamback -&gt; hw -&gt; grades            0.054 0.017 0.105 Sig\n\n - [CI.lo to CI.hi] are 95.0% bias-corrected confidence intervals by\n   nonparametric bootstrapping with 1000 samples.\n - std: The standardized indirect effects.\n \n\n\n\nind_est_std[[1]] + ind_est_std[[2]] + ind_est_std[[3]] # total effect\n\n\n== Indirect Effect (Both 'famback' and 'grades' Standardized) ==\n                                                        \n Path:                famback -&gt; prevach -&gt; hw -&gt; grades\n Path:                famback -&gt; prevach -&gt; grades      \n Path:                famback -&gt; hw -&gt; grades           \n Function of Effects: 0.388                             \n 95.0% Bootstrap CI:  [0.335 to 0.439]                  \n\nComputation of the Function of Effects:\n ((famback-&gt;prevach-&gt;hw-&gt;grades)\n+(famback-&gt;prevach-&gt;grades))\n+(famback-&gt;hw-&gt;grades) \n\n\nBias-corrected confidence interval formed by nonparametric\nbootstrapping with 1000 bootstrap samples.\n\n\n\nindirect_effect(\n  fit = sem_fit_raw,\n  x = \"famback\",\n  y = \"grades\",\n  m = c(\"hw\"),\n  boot_out = out_med,\n  standardized_x = TRUE,\n  standardized_y = TRUE\n)\n\n# ==  Indirect Effect(s) (Both x-variable(s) and y-variable(s) Standardized)  ==\n#   std CI.lo CI.hi Sig\n# famback -&gt; prevach -&gt; hw -&gt; grades 0.060 0.034 0.105 Sig\n# famback -&gt; prevach -&gt; grades       0.274 0.222 0.325 Sig\n# famback -&gt; hw -&gt; grades            0.054 0.018 0.102 Sig\n\n\n== Indirect Effect (Both 'famback' and 'grades' Standardized) ==\n                                         \n Path:            famback -&gt; hw -&gt; grades\n Indirect Effect: 0.054                  \n\nComputation Formula:\n  (b.hw~famback)*(b.grades~hw)*sd_famback/sd_grades\nComputation:\n  (0.09797)*(1.32771)*(1.03290)/(2.47198)\nCoefficients of Component Paths:\n       Path Coefficient\n hw~famback       0.098\n  grades~hw       1.328\n\nNOTE:\n- The effects of the component paths are from the model, not\n  standardized.\n\n\n\n\nAll indirect paths from minority to grades: total effect\n\n# All indirect paths from minority to grades: total effect\npaths2 &lt;- all_indirect_paths(sem_fit_raw,\n  x = \"eminor\",\n  y = \"grades\"\n)\npaths2 |&gt; print()\n\nCall: \nall_indirect_paths(fit = sem_fit_raw, x = \"eminor\", y = \"grades\")\nPath(s): \n  path                             \n1 eminor -&gt; prevach -&gt; hw -&gt; grades\n2 eminor -&gt; prevach -&gt; grades      \n3 eminor -&gt; hw -&gt; grades           \n\n\n\nind_est_std2 &lt;- many_indirect_effects(paths2,\n  fit = sem_fit_raw, R = 1000,\n  boot_ci = TRUE, boot_type = \"bc\",\n  standardized_x = TRUE,\n  standardized_y = TRUE\n)\nind_est_std2\n\n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100% elapsed=11s  \n\n\n\n==  Indirect Effect(s) (Both x-variable(s) and y-variable(s) Standardized)  ==\n                                     std  CI.lo  CI.hi Sig\neminor -&gt; prevach -&gt; hw -&gt; grades -0.010 -0.023 -0.003 Sig\neminor -&gt; prevach -&gt; grades       -0.045 -0.077 -0.013 Sig\neminor -&gt; hw -&gt; grades             0.030  0.003  0.070 Sig\n\n - [CI.lo to CI.hi] are 95.0% bias-corrected confidence intervals by\n   nonparametric bootstrapping with 1000 samples.\n - std: The standardized indirect effects.\n \n\n\n\nind_est_std2[[1]] + ind_est_std2[[2]] + ind_est_std2[[3]] # total effect\n\n\n== Indirect Effect (Both 'eminor' and 'grades' Standardized) ==\n                                                       \n Path:                eminor -&gt; prevach -&gt; hw -&gt; grades\n Path:                eminor -&gt; prevach -&gt; grades      \n Path:                eminor -&gt; hw -&gt; grades           \n Function of Effects: -0.025                           \n 95.0% Bootstrap CI:  [-0.074 to 0.030]                \n\nComputation of the Function of Effects:\n ((eminor-&gt;prevach-&gt;hw-&gt;grades)\n+(eminor-&gt;prevach-&gt;grades))\n+(eminor-&gt;hw-&gt;grades) \n\n\nBias-corrected confidence interval formed by nonparametric\nbootstrapping with 1000 bootstrap samples.\n\n\n\n\nMulti-group\n\n# generate a dataset with the same covariance matrix\nlibrary(readxl)\nhw_mg_minor &lt;- read_xls(\"data/chap 18 latent var SEM 2/minority matrix.xls\")\nhw_mg_white &lt;- read_xls(\"data/chap 18 latent var SEM 2/white matrix.xls\")\n\nhw_mg_minor_cov &lt;- hw_mg_minor[c(2:14), c(3:15)] |&gt;\n  as.matrix() |&gt;\n  lav_matrix_vechr(diagonal = TRUE) |&gt;\n  getCov(names = hw_mg_minor$varname_[2:14]) |&gt;\n  cor2cov(sds = hw_mg_minor[16, 3:15] |&gt; as.double())\n\nmean_minor &lt;- hw_mg_minor[15, 3:15] |&gt; as.double()\n\nhw_mg_white_cov &lt;- hw_mg_white[c(2:14), c(3:15)] |&gt;\n  as.matrix() |&gt;\n  lav_matrix_vechr(diagonal = TRUE) |&gt;\n  getCov(names = hw_mg_white$varname_[2:14]) |&gt;\n  cor2cov(sds = hw_mg_white[16, 3:15] |&gt; as.double())\n\nmean_white &lt;- hw_mg_white[15, 3:15] |&gt; as.double()\n\n# generate a dataset with the same covariance matrix\nhw_sim_minor &lt;- semTools::kd(hw_mg_minor_cov, n = 274, type = \"exact\") |&gt;\n  sweep(2, mean_minor, FUN = \"+\")\nhw_sim_white &lt;- semTools::kd(hw_mg_white_cov, n = 751, type = \"exact\") |&gt;\n  sweep(2, mean_white, FUN = \"+\")\n\nhw_multigroup &lt;- bind_rows(\n  hw_sim_minor |&gt; mutate(group = \"minority\"),\n  hw_sim_white |&gt; mutate(group = \"white\")\n)\n\ncolnames(hw_multigroup) &lt;- tolower(colnames(hw_multigroup))\nhw_multigroup |&gt; head() |&gt; print()\n\n   bypared  byfaminc   parocc bytxrstd bytxmstd bytxsstd bytxhstd     hw_8\n1 4.241136  9.539088 51.36233 54.90054 55.30287 46.20344 47.90755 1.901280\n2 2.533918 12.476243 82.40424 60.04942 61.99253 55.24925 57.93221 2.266771\n3 2.997464 14.396761 78.22307 43.26104 39.86515 39.53191 36.02678 2.157474\n4 3.123412  9.146832 51.37951 45.61876 46.59908 48.84640 39.44524 1.688876\n5 4.795318  9.295362 64.89943 67.31889 67.58730 74.66985 69.42429 2.161667\n6 1.631700 12.129856 56.70377 46.52074 60.52231 52.27228 49.22310 1.999805\n      hw10    eng_12  math_12   sci_12    ss_12    group\n1 4.794832  4.019241 4.763743 1.842176 3.127701 minority\n2 6.684935 11.075153 7.497462 7.040717 7.753743 minority\n3 4.375377  6.334300 4.500281 5.777013 4.958751 minority\n4 4.050661  4.488926 4.448010 3.792466 4.058798 minority\n5 4.377667 10.053731 9.654807 8.600518 9.704114 minority\n6 5.264791  8.137198 9.838173 6.502873 9.399577 minority\n\n\n\n# Fit the model\nhw_model_mg &lt;- \"\n  famback =~ parocc + bypared + byfaminc\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw10 + hw_8\n  grades =~ eng_12 + math_12 + sci_12 + ss_12\n\n  bytxrstd ~~ eng_12\n  bytxmstd ~~ math_12\n  bytxsstd ~~ sci_12\n  bytxhstd ~~ ss_12\n\n  prevach ~ famback\n  grades ~ prevach + hw\n  hw ~ prevach + famback\n\"\n\nsem_fit_mg &lt;- sem(hw_model_mg,\n  data = hw_multigroup,\n  estimator = \"ML\",\n  group = \"group\"\n)\n\nsummary(sem_fit_mg, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 414 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        96\n\n  Number of observations per group:                   \n    minority                                       274\n    white                                          751\n\nModel Test User Model:\n                                                      \n  Test statistic                               223.175\n  Degrees of freedom                               112\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    minority                                    95.604\n    white                                      127.571\n\nModel Test Baseline Model:\n\n  Test statistic                              8290.390\n  Degrees of freedom                               156\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.986\n  Tucker-Lewis Index (TLI)                       0.981\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33497.853\n  Loglikelihood unrestricted model (H1)     -33386.266\n                                                      \n  Akaike (AIC)                               67187.706\n  Bayesian (BIC)                             67661.221\n  Sample-size adjusted Bayesian (SABIC)      67356.315\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.044\n  90 Percent confidence interval - lower         0.036\n  90 Percent confidence interval - upper         0.052\n  P-value H_0: RMSEA &lt;= 0.050                    0.876\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [minority]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              19.007    0.793\n    bypared           0.059    0.005   11.770    0.000    1.112    0.801\n    byfaminc          0.103    0.010   10.666    0.000    1.961    0.692\n  prevach =~                                                            \n    bytxrstd          1.000                               8.514    0.859\n    bytxmstd          1.058    0.057   18.530    0.000    9.010    0.869\n    bytxsstd          0.951    0.057   16.576    0.000    8.100    0.815\n    bytxhstd          0.942    0.057   16.552    0.000    8.016    0.813\n  hw =~                                                                 \n    hw10              1.000                               0.544    0.297\n    hw_8              0.791    0.195    4.062    0.000    0.430    0.378\n  grades =~                                                             \n    eng_12            1.000                               2.559    0.929\n    math_12           0.895    0.044   20.520    0.000    2.292    0.833\n    sci_12            0.923    0.040   22.949    0.000    2.362    0.877\n    ss_12             1.049    0.042   24.764    0.000    2.686    0.902\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.263    0.032    8.278    0.000    0.586    0.586\n  grades ~                                                              \n    prevach           3.200   66.665    0.048    0.962   10.647   10.647\n    hw              -46.744 1035.826   -0.045    0.964   -9.936   -9.936\n  hw ~                                                                  \n    prevach           0.065    0.014    4.595    0.000    1.011    1.011\n    famback          -0.000    0.004   -0.045    0.964   -0.006   -0.006\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.191    0.456    0.418    0.676    0.191    0.037\n .bytxmstd ~~                                                           \n   .math_12           2.386    0.613    3.895    0.000    2.386    0.305\n .bytxsstd ~~                                                           \n   .sci_12            0.289    0.551    0.524    0.600    0.289    0.039\n .bytxhstd ~~                                                           \n   .ss_12             1.140    0.573    1.989    0.047    1.140    0.155\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           43.085    1.448   29.755    0.000   43.085    1.798\n   .bypared           2.849    0.084   33.953    0.000    2.849    2.051\n   .byfaminc          8.757    0.171   51.131    0.000    8.757    3.089\n   .bytxrstd         48.537    0.599   81.056    0.000   48.537    4.897\n   .bytxmstd         49.808    0.626   79.526    0.000   49.808    4.804\n   .bytxsstd         47.959    0.600   79.907    0.000   47.959    4.827\n   .bytxhstd         48.160    0.596   80.823    0.000   48.160    4.883\n   .hw10              3.214    0.111   29.011    0.000    3.214    1.753\n   .hw_8              1.718    0.069   24.980    0.000    1.718    1.509\n   .eng_12            5.820    0.166   34.973    0.000    5.820    2.113\n   .math_12           5.385    0.166   32.389    0.000    5.385    1.957\n   .sci_12            5.590    0.163   34.372    0.000    5.590    2.076\n   .ss_12             5.895    0.180   32.785    0.000    5.895    1.981\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          213.212   29.517    7.223    0.000  213.212    0.371\n   .bypared           0.691    0.099    6.993    0.000    0.691    0.358\n   .byfaminc          4.193    0.447    9.378    0.000    4.193    0.522\n   .bytxrstd         25.756    3.009    8.560    0.000   25.756    0.262\n   .bytxmstd         26.302    3.181    8.268    0.000   26.302    0.245\n   .bytxsstd         33.096    3.475    9.525    0.000   33.096    0.335\n   .bytxhstd         33.025    3.448    9.577    0.000   33.025    0.339\n   .hw10              3.068    0.281   10.932    0.000    3.068    0.912\n   .hw_8              1.112    0.114    9.772    0.000    1.112    0.857\n   .eng_12            1.039    0.148    7.029    0.000    1.039    0.137\n   .math_12           2.321    0.230   10.076    0.000    2.321    0.306\n   .sci_12            1.670    0.181    9.241    0.000    1.670    0.230\n   .ss_12             1.647    0.195    8.433    0.000    1.647    0.186\n    famback         361.269   51.153    7.063    0.000    1.000    1.000\n   .prevach          47.575    6.080    7.825    0.000    0.656    0.656\n   .hw               -0.005    0.103   -0.045    0.964   -0.016   -0.016\n   .grades           14.069  226.082    0.062    0.950    2.148    2.148\n\n\nGroup 2 [white]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              14.207    0.718\n    bypared           0.072    0.004   16.238    0.000    1.025    0.842\n    byfaminc          0.094    0.007   14.113    0.000    1.336    0.593\n  prevach =~                                                            \n    bytxrstd          1.000                               8.583    0.844\n    bytxmstd          0.997    0.036   28.022    0.000    8.560    0.835\n    bytxsstd          0.987    0.035   28.103    0.000    8.475    0.842\n    bytxhstd          0.970    0.035   27.524    0.000    8.325    0.833\n  hw =~                                                                 \n    hw10              1.000                               1.280    0.664\n    hw_8              0.389    0.063    6.181    0.000    0.498    0.441\n  grades =~                                                             \n    eng_12            1.000                               2.419    0.920\n    math_12           0.901    0.028   32.154    0.000    2.179    0.815\n    sci_12            0.972    0.026   37.566    0.000    2.352    0.878\n    ss_12             1.065    0.026   41.677    0.000    2.577    0.917\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.299    0.027   11.063    0.000    0.495    0.495\n  grades ~                                                              \n    prevach           0.156    0.012   13.222    0.000    0.555    0.555\n    hw                0.468    0.114    4.118    0.000    0.248    0.248\n  hw ~                                                                  \n    prevach           0.046    0.009    4.917    0.000    0.307    0.307\n    famback           0.022    0.006    3.658    0.000    0.239    0.239\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.860    0.289    2.971    0.003    0.860    0.153\n .bytxmstd ~~                                                           \n   .math_12           2.953    0.400    7.387    0.000    2.953    0.338\n .bytxsstd ~~                                                           \n   .sci_12            1.139    0.327    3.483    0.000    1.139    0.164\n .bytxhstd ~~                                                           \n   .ss_12             0.349    0.310    1.127    0.260    0.349    0.056\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           54.883    0.722   76.051    0.000   54.883    2.775\n   .bypared           3.335    0.044   75.101    0.000    3.335    2.740\n   .byfaminc         10.348    0.082  125.757    0.000   10.348    4.589\n   .bytxrstd         53.260    0.371  143.506    0.000   53.260    5.237\n   .bytxmstd         53.559    0.374  143.173    0.000   53.559    5.224\n   .bytxsstd         53.335    0.367  145.260    0.000   53.335    5.301\n   .bytxhstd         52.956    0.365  145.243    0.000   52.956    5.300\n   .hw10              3.443    0.070   48.980    0.000    3.443    1.787\n   .hw_8              1.733    0.041   42.046    0.000    1.733    1.534\n   .eng_12            6.409    0.096   66.808    0.000    6.409    2.438\n   .math_12           5.823    0.098   59.670    0.000    5.823    2.177\n   .sci_12            6.088    0.098   62.269    0.000    6.088    2.272\n   .ss_12             6.612    0.103   64.449    0.000    6.612    2.352\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          189.274   14.140   13.386    0.000  189.274    0.484\n   .bypared           0.431    0.056    7.687    0.000    0.431    0.291\n   .byfaminc          3.299    0.198   16.701    0.000    3.299    0.649\n   .bytxrstd         29.775    2.045   14.556    0.000   29.775    0.288\n   .bytxmstd         31.816    2.137   14.890    0.000   31.816    0.303\n   .bytxsstd         29.426    2.010   14.640    0.000   29.426    0.291\n   .bytxhstd         30.535    2.047   14.917    0.000   30.535    0.306\n   .hw10              2.073    0.282    7.355    0.000    2.073    0.559\n   .hw_8              1.028    0.067   15.451    0.000    1.028    0.806\n   .eng_12            1.060    0.085   12.496    0.000    1.060    0.153\n   .math_12           2.401    0.141   17.014    0.000    2.401    0.336\n   .sci_12            1.645    0.108   15.261    0.000    1.645    0.229\n   .ss_12             1.262    0.099   12.770    0.000    1.262    0.160\n    famback         201.843   20.408    9.890    0.000    1.000    1.000\n   .prevach          55.591    4.259   13.054    0.000    0.755    0.755\n   .hw                1.271    0.275    4.623    0.000    0.776    0.776\n   .grades            3.005    0.215   13.978    0.000    0.514    0.514\n\n\n\n\nlavInspect(sem_fit_mg, \"cov.lv\") |&gt; print()\ninspect(sem_fit_mg, what = \"est\") |&gt; print()\n\n$minority\n         fambck  prevch      hw  grades\nfamback 361.269                        \nprevach  94.884  72.495                \nhw        6.071   4.668   0.296        \ngrades   19.870  13.811   1.106   6.550\n\n$white\n         fambck  prevch      hw  grades\nfamback 201.843                        \nprevach  60.406  73.668                \nhw        7.113   4.677   1.638        \ngrades   12.780  13.715   1.498   5.852\n\n$minority\n$minority$lambda\n         fambck prevch    hw grades\nparocc    1.000  0.000 0.000  0.000\nbypared   0.059  0.000 0.000  0.000\nbyfaminc  0.103  0.000 0.000  0.000\nbytxrstd  0.000  1.000 0.000  0.000\nbytxmstd  0.000  1.058 0.000  0.000\nbytxsstd  0.000  0.951 0.000  0.000\nbytxhstd  0.000  0.942 0.000  0.000\nhw10      0.000  0.000 1.000  0.000\nhw_8      0.000  0.000 0.791  0.000\neng_12    0.000  0.000 0.000  1.000\nmath_12   0.000  0.000 0.000  0.895\nsci_12    0.000  0.000 0.000  0.923\nss_12     0.000  0.000 0.000  1.049\n\n$minority$theta\n          parocc  bypard  byfmnc  bytxrs  bytxms  bytxss  bytxhs    hw10\nparocc   213.212                                                        \nbypared    0.000   0.691                                                \nbyfaminc   0.000   0.000   4.193                                        \nbytxrstd   0.000   0.000   0.000  25.756                                \nbytxmstd   0.000   0.000   0.000   0.000  26.302                        \nbytxsstd   0.000   0.000   0.000   0.000   0.000  33.096                \nbytxhstd   0.000   0.000   0.000   0.000   0.000   0.000  33.025        \nhw10       0.000   0.000   0.000   0.000   0.000   0.000   0.000   3.068\nhw_8       0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.000\neng_12     0.000   0.000   0.000   0.191   0.000   0.000   0.000   0.000\nmath_12    0.000   0.000   0.000   0.000   2.386   0.000   0.000   0.000\nsci_12     0.000   0.000   0.000   0.000   0.000   0.289   0.000   0.000\nss_12      0.000   0.000   0.000   0.000   0.000   0.000   1.140   0.000\n            hw_8  eng_12  mth_12  sci_12   ss_12\nparocc                                          \nbypared                                         \nbyfaminc                                        \nbytxrstd                                        \nbytxmstd                                        \nbytxsstd                                        \nbytxhstd                                        \nhw10                                            \nhw_8       1.112                                \neng_12     0.000   1.039                        \nmath_12    0.000   0.000   2.321                \nsci_12     0.000   0.000   0.000   1.670        \nss_12      0.000   0.000   0.000   0.000   1.647\n\n$minority$psi\n         fambck  prevch      hw  grades\nfamback 361.269                        \nprevach   0.000  47.575                \nhw        0.000   0.000  -0.005        \ngrades    0.000   0.000   0.000  14.069\n\n$minority$beta\n        fambck prevch      hw grades\nfamback  0.000  0.000   0.000      0\nprevach  0.263  0.000   0.000      0\nhw       0.000  0.065   0.000      0\ngrades   0.000  3.200 -46.744      0\n\n$minority$nu\n         intrcp\nparocc   43.085\nbypared   2.849\nbyfaminc  8.757\nbytxrstd 48.537\nbytxmstd 49.808\nbytxsstd 47.959\nbytxhstd 48.160\nhw10      3.214\nhw_8      1.718\neng_12    5.820\nmath_12   5.385\nsci_12    5.590\nss_12     5.895\n\n$minority$alpha\n        intrcp\nfamback      0\nprevach      0\nhw           0\ngrades       0\n\n\n$white\n$white$lambda\n         fambck prevch    hw grades\nparocc    1.000  0.000 0.000  0.000\nbypared   0.072  0.000 0.000  0.000\nbyfaminc  0.094  0.000 0.000  0.000\nbytxrstd  0.000  1.000 0.000  0.000\nbytxmstd  0.000  0.997 0.000  0.000\nbytxsstd  0.000  0.987 0.000  0.000\nbytxhstd  0.000  0.970 0.000  0.000\nhw10      0.000  0.000 1.000  0.000\nhw_8      0.000  0.000 0.389  0.000\neng_12    0.000  0.000 0.000  1.000\nmath_12   0.000  0.000 0.000  0.901\nsci_12    0.000  0.000 0.000  0.972\nss_12     0.000  0.000 0.000  1.065\n\n$white$theta\n          parocc  bypard  byfmnc  bytxrs  bytxms  bytxss  bytxhs    hw10\nparocc   189.274                                                        \nbypared    0.000   0.431                                                \nbyfaminc   0.000   0.000   3.299                                        \nbytxrstd   0.000   0.000   0.000  29.775                                \nbytxmstd   0.000   0.000   0.000   0.000  31.816                        \nbytxsstd   0.000   0.000   0.000   0.000   0.000  29.426                \nbytxhstd   0.000   0.000   0.000   0.000   0.000   0.000  30.535        \nhw10       0.000   0.000   0.000   0.000   0.000   0.000   0.000   2.073\nhw_8       0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.000\neng_12     0.000   0.000   0.000   0.860   0.000   0.000   0.000   0.000\nmath_12    0.000   0.000   0.000   0.000   2.953   0.000   0.000   0.000\nsci_12     0.000   0.000   0.000   0.000   0.000   1.139   0.000   0.000\nss_12      0.000   0.000   0.000   0.000   0.000   0.000   0.349   0.000\n            hw_8  eng_12  mth_12  sci_12   ss_12\nparocc                                          \nbypared                                         \nbyfaminc                                        \nbytxrstd                                        \nbytxmstd                                        \nbytxsstd                                        \nbytxhstd                                        \nhw10                                            \nhw_8       1.028                                \neng_12     0.000   1.060                        \nmath_12    0.000   0.000   2.401                \nsci_12     0.000   0.000   0.000   1.645        \nss_12      0.000   0.000   0.000   0.000   1.262\n\n$white$psi\n         fambck  prevch      hw  grades\nfamback 201.843                        \nprevach   0.000  55.591                \nhw        0.000   0.000   1.271        \ngrades    0.000   0.000   0.000   3.005\n\n$white$beta\n        fambck prevch    hw grades\nfamback  0.000  0.000 0.000      0\nprevach  0.299  0.000 0.000      0\nhw       0.022  0.046 0.000      0\ngrades   0.000  0.156 0.468      0\n\n$white$nu\n         intrcp\nparocc   54.883\nbypared   3.335\nbyfaminc 10.348\nbytxrstd 53.260\nbytxmstd 53.559\nbytxsstd 53.335\nbytxhstd 52.956\nhw10      3.443\nhw_8      1.733\neng_12    6.409\nmath_12   5.823\nsci_12    6.088\nss_12     6.612\n\n$white$alpha\n        intrcp\nfamback      0\nprevach      0\nhw           0\ngrades       0\n\n\n\n\n\n# contrain factor loadings\nsem_fit_mg2 &lt;- sem(hw_model_mg,\n  data = hw_multigroup,\n  estimator = \"ML\",\n  group = \"group\",\n  group.equal = c(\"loadings\")\n)\n\nsummary(sem_fit_mg2, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 397 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        96\n  Number of equality constraints                     9\n\n  Number of observations per group:                   \n    minority                                       274\n    white                                          751\n\nModel Test User Model:\n                                                      \n  Test statistic                               238.778\n  Degrees of freedom                               121\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    minority                                   106.830\n    white                                      131.949\n\nModel Test Baseline Model:\n\n  Test statistic                              8290.390\n  Degrees of freedom                               156\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.986\n  Tucker-Lewis Index (TLI)                       0.981\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33505.655\n  Loglikelihood unrestricted model (H1)     -33386.266\n                                                      \n  Akaike (AIC)                               67185.310\n  Bayesian (BIC)                             67614.433\n  Sample-size adjusted Bayesian (SABIC)      67338.111\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.044\n  90 Percent confidence interval - lower         0.035\n  90 Percent confidence interval - upper         0.052\n  P-value H_0: RMSEA &lt;= 0.050                    0.902\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.033\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [minority]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              17.928    0.760\n    bypared (.p2.)    0.067    0.003   20.083    0.000    1.207    0.846\n    byfamnc (.p3.)    0.096    0.006   17.481    0.000    1.727    0.629\n  prevach =~                                                            \n    bytxrst           1.000                               8.530    0.860\n    bytxmst (.p5.)    1.014    0.030   33.540    0.000    8.647    0.854\n    bytxsst (.p6.)    0.978    0.030   32.678    0.000    8.340    0.826\n    bytxhst (.p7.)    0.962    0.030   32.151    0.000    8.209    0.823\n  hw =~                                                                 \n    hw10              1.000                               0.679    0.361\n    hw_8    (.p9.)    0.468    0.061    7.685    0.000    0.317    0.286\n  grades =~                                                             \n    eng_12            1.000                               2.527    0.926\n    math_12 (.11.)    0.900    0.024   38.162    0.000    2.274    0.831\n    sci_12  (.12.)    0.958    0.022   44.037    0.000    2.421    0.884\n    ss_12   (.13.)    1.061    0.022   48.511    0.000    2.681    0.902\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.282    0.031    9.079    0.000    0.592    0.592\n  grades ~                                                              \n    prevach          -2.916    0.478   -6.102    0.000   -9.843   -9.843\n    hw               39.428    2.357   16.726    0.000   10.587   10.587\n  hw ~                                                                  \n    prevach           0.078    0.011    6.990    0.000    0.986    0.986\n    famback           0.000    0.000    0.806    0.420    0.006    0.006\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.222    0.456    0.487    0.626    0.222    0.042\n .bytxmstd ~~                                                           \n   .math_12           2.426    0.616    3.937    0.000    2.426    0.302\n .bytxsstd ~~                                                           \n   .sci_12            0.347    0.552    0.628    0.530    0.347    0.047\n .bytxhstd ~~                                                           \n   .ss_12             1.099    0.571    1.925    0.054    1.099    0.151\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           43.085    1.425   30.232    0.000   43.085    1.826\n   .bypared           2.849    0.086   33.058    0.000    2.849    1.997\n   .byfaminc          8.757    0.166   52.839    0.000    8.757    3.192\n   .bytxrstd         48.537    0.599   80.980    0.000   48.537    4.892\n   .bytxmstd         49.808    0.612   81.393    0.000   49.808    4.917\n   .bytxsstd         47.959    0.610   78.615    0.000   47.959    4.749\n   .bytxhstd         48.160    0.603   79.915    0.000   48.160    4.828\n   .hw10              3.214    0.114   28.309    0.000    3.214    1.710\n   .hw_8              1.718    0.067   25.611    0.000    1.718    1.547\n   .eng_12            5.820    0.165   35.298    0.000    5.820    2.132\n   .math_12           5.385    0.165   32.569    0.000    5.385    1.968\n   .sci_12            5.590    0.165   33.777    0.000    5.590    2.041\n   .ss_12             5.895    0.180   32.823    0.000    5.895    1.983\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          235.082   27.675    8.494    0.000  235.082    0.422\n   .bypared           0.578    0.096    6.031    0.000    0.578    0.284\n   .byfaminc          4.543    0.442   10.284    0.000    4.543    0.604\n   .bytxrstd         25.675    2.946    8.715    0.000   25.675    0.261\n   .bytxmstd         27.839    3.136    8.878    0.000   27.839    0.271\n   .bytxsstd         32.420    3.423    9.471    0.000   32.420    0.318\n   .bytxhstd         32.120    3.371    9.528    0.000   32.120    0.323\n   .hw10              3.072    0.265   11.610    0.000    3.072    0.870\n   .hw_8              1.133    0.097   11.671    0.000    1.133    0.918\n   .eng_12            1.064    0.143    7.437    0.000    1.064    0.143\n   .math_12           2.320    0.228   10.161    0.000    2.320    0.310\n   .sci_12            1.643    0.179    9.178    0.000    1.643    0.219\n   .ss_12             1.652    0.192    8.612    0.000    1.652    0.187\n    famback         321.430   39.202    8.199    0.000    1.000    1.000\n   .prevach          47.246    5.437    8.690    0.000    0.649    0.649\n   .hw                0.010    0.005    1.917    0.055    0.021    0.021\n   .grades          -11.533    6.103   -1.890    0.059   -1.806   -1.806\n\n\nGroup 2 [white]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              14.625    0.735\n    bypared (.p2.)    0.067    0.003   20.083    0.000    0.984    0.818\n    byfamnc (.p3.)    0.096    0.006   17.481    0.000    1.409    0.617\n  prevach =~                                                            \n    bytxrst           1.000                               8.580    0.844\n    bytxmst (.p5.)    1.014    0.030   33.540    0.000    8.698    0.841\n    bytxsst (.p6.)    0.978    0.030   32.678    0.000    8.389    0.839\n    bytxhst (.p7.)    0.962    0.030   32.151    0.000    8.258    0.830\n  hw =~                                                                 \n    hw10              1.000                               1.177    0.615\n    hw_8    (.p9.)    0.468    0.061    7.685    0.000    0.550    0.484\n  grades =~                                                             \n    eng_12            1.000                               2.431    0.921\n    math_12 (.11.)    0.900    0.024   38.162    0.000    2.187    0.816\n    sci_12  (.12.)    0.958    0.022   44.037    0.000    2.329    0.875\n    ss_12   (.13.)    1.061    0.022   48.511    0.000    2.579    0.917\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.292    0.025   11.434    0.000    0.497    0.497\n  grades ~                                                              \n    prevach           0.156    0.012   13.103    0.000    0.551    0.551\n    hw                0.513    0.121    4.234    0.000    0.249    0.249\n  hw ~                                                                  \n    prevach           0.045    0.009    5.024    0.000    0.326    0.326\n    famback           0.019    0.005    3.546    0.000    0.239    0.239\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.850    0.289    2.938    0.003    0.850    0.152\n .bytxmstd ~~                                                           \n   .math_12           2.951    0.400    7.377    0.000    2.951    0.340\n .bytxsstd ~~                                                           \n   .sci_12            1.141    0.327    3.491    0.000    1.141    0.163\n .bytxhstd ~~                                                           \n   .ss_12             0.361    0.310    1.163    0.245    0.361    0.058\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           54.883    0.726   75.580    0.000   54.883    2.758\n   .bypared           3.335    0.044   75.936    0.000    3.335    2.771\n   .byfaminc         10.348    0.083  124.155    0.000   10.348    4.530\n   .bytxrstd         53.260    0.371  143.520    0.000   53.260    5.237\n   .bytxmstd         53.559    0.378  141.840    0.000   53.559    5.176\n   .bytxsstd         53.335    0.365  146.123    0.000   53.335    5.332\n   .bytxhstd         52.956    0.363  145.852    0.000   52.956    5.322\n   .hw10              3.443    0.070   49.289    0.000    3.443    1.799\n   .hw_8              1.733    0.042   41.718    0.000    1.733    1.522\n   .eng_12            6.409    0.096   66.567    0.000    6.409    2.429\n   .math_12           5.823    0.098   59.527    0.000    5.823    2.172\n   .sci_12            6.088    0.097   62.700    0.000    6.088    2.288\n   .ss_12             6.612    0.103   64.415    0.000    6.612    2.351\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          182.116   13.639   13.352    0.000  182.116    0.460\n   .bypared           0.480    0.050    9.615    0.000    0.480    0.331\n   .byfaminc          3.233    0.196   16.479    0.000    3.233    0.620\n   .bytxrstd         29.799    2.030   14.681    0.000   29.799    0.288\n   .bytxmstd         31.427    2.125   14.792    0.000   31.427    0.293\n   .bytxsstd         29.674    1.994   14.880    0.000   29.674    0.297\n   .bytxhstd         30.813    2.036   15.133    0.000   30.813    0.311\n   .hw10              2.279    0.229    9.959    0.000    2.279    0.622\n   .hw_8              0.993    0.067   14.787    0.000    0.993    0.766\n   .eng_12            1.053    0.084   12.506    0.000    1.053    0.151\n   .math_12           2.401    0.141   17.038    0.000    2.401    0.334\n   .sci_12            1.655    0.107   15.444    0.000    1.655    0.234\n   .ss_12             1.261    0.098   12.866    0.000    1.261    0.159\n    famback         213.887   19.110   11.192    0.000    1.000    1.000\n   .prevach          55.447    4.061   13.654    0.000    0.753    0.753\n   .hw                1.051    0.209    5.032    0.000    0.759    0.759\n   .grades            3.031    0.213   14.223    0.000    0.513    0.513\n\n\n\n\n# compare the two models\nlavTestLRT(sem_fit_mg, sem_fit_mg2) |&gt; print()\n\n\nChi-Squared Difference Test\n\n             Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)  \nsem_fit_mg  112 67188 67661 223.17                                         \nsem_fit_mg2 121 67185 67614 238.78     15.604 0.037837       9    0.07564 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# constrains the effect of homework\nhw_model_mg3 &lt;- \"\n  famback =~ parocc + bypared + byfaminc\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw10 + hw_8\n  grades =~ eng_12 + math_12 + sci_12 + ss_12\n\n  bytxrstd ~~ eng_12\n  bytxmstd ~~ math_12\n  bytxsstd ~~ sci_12\n  bytxhstd ~~ ss_12\n\n  prevach ~ famback\n  grades ~ prevach + c(h, h)*hw\n  hw ~ prevach + famback\n\"\n\nsem_fit_mg3 &lt;- sem(hw_model_mg3,\n  data = hw_multigroup,\n  estimator = \"ML\",\n  group = \"group\",\n  group.equal = c(\"loadings\")\n)\n\nsummary(sem_fit_mg3, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 302 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        96\n  Number of equality constraints                    10\n\n  Number of observations per group:                   \n    minority                                       274\n    white                                          751\n\nModel Test User Model:\n                                                      \n  Test statistic                               236.984\n  Degrees of freedom                               122\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    minority                                   104.664\n    white                                      132.321\n\nModel Test Baseline Model:\n\n  Test statistic                              8290.390\n  Degrees of freedom                               156\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.986\n  Tucker-Lewis Index (TLI)                       0.982\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33504.758\n  Loglikelihood unrestricted model (H1)     -33386.266\n                                                      \n  Akaike (AIC)                               67181.515\n  Bayesian (BIC)                             67605.706\n  Sample-size adjusted Bayesian (SABIC)      67332.561\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.043\n  90 Percent confidence interval - lower         0.035\n  90 Percent confidence interval - upper         0.051\n  P-value H_0: RMSEA &lt;= 0.050                    0.924\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.033\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [minority]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              17.948    0.761\n    bypared (.p2.)    0.067    0.003   20.086    0.000    1.207    0.845\n    byfamnc (.p3.)    0.096    0.006   17.489    0.000    1.728    0.630\n  prevach =~                                                            \n    bytxrst           1.000                               8.528    0.859\n    bytxmst (.p5.)    1.014    0.030   33.531    0.000    8.647    0.854\n    bytxsst (.p6.)    0.978    0.030   32.663    0.000    8.339    0.825\n    bytxhst (.p7.)    0.963    0.030   32.151    0.000    8.210    0.823\n  hw =~                                                                 \n    hw10              1.000                               0.991    0.528\n    hw_8    (.p9.)    0.475    0.061    7.816    0.000    0.471    0.423\n  grades =~                                                             \n    eng_12            1.000                               2.522    0.926\n    math_12 (.11.)    0.900    0.024   38.175    0.000    2.269    0.830\n    sci_12  (.12.)    0.958    0.022   44.034    0.000    2.416    0.883\n    ss_12   (.13.)    1.061    0.022   48.518    0.000    2.675    0.901\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.283    0.031    9.132    0.000    0.595    0.595\n  grades ~                                                              \n    prevach           0.146    0.019    7.856    0.000    0.493    0.493\n    hw         (h)    0.548    0.121    4.519    0.000    0.216    0.216\n  hw ~                                                                  \n    prevach           0.076    0.015    5.030    0.000    0.655    0.655\n    famback           0.001    0.007    0.094    0.925    0.012    0.012\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.202    0.456    0.443    0.658    0.202    0.039\n .bytxmstd ~~                                                           \n   .math_12           2.427    0.616    3.941    0.000    2.427    0.302\n .bytxsstd ~~                                                           \n   .sci_12            0.342    0.553    0.619    0.536    0.342    0.047\n .bytxhstd ~~                                                           \n   .ss_12             1.079    0.570    1.892    0.059    1.079    0.148\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           43.085    1.425   30.228    0.000   43.085    1.826\n   .bypared           2.849    0.086   33.036    0.000    2.849    1.996\n   .byfaminc          8.757    0.166   52.837    0.000    8.757    3.192\n   .bytxrstd         48.537    0.600   80.939    0.000   48.537    4.890\n   .bytxmstd         49.808    0.612   81.404    0.000   49.808    4.918\n   .bytxsstd         47.959    0.610   78.584    0.000   47.959    4.747\n   .bytxhstd         48.160    0.602   79.937    0.000   48.160    4.829\n   .hw10              3.214    0.114   28.318    0.000    3.214    1.711\n   .hw_8              1.718    0.067   25.554    0.000    1.718    1.544\n   .eng_12            5.820    0.165   35.367    0.000    5.820    2.137\n   .math_12           5.385    0.165   32.611    0.000    5.385    1.970\n   .sci_12            5.590    0.165   33.832    0.000    5.590    2.044\n   .ss_12             5.895    0.179   32.880    0.000    5.895    1.986\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          234.497   27.663    8.477    0.000  234.497    0.421\n   .bypared           0.581    0.096    6.047    0.000    0.581    0.285\n   .byfaminc          4.539    0.442   10.279    0.000    4.539    0.603\n   .bytxrstd         25.812    2.954    8.738    0.000   25.812    0.262\n   .bytxmstd         27.801    3.130    8.882    0.000   27.801    0.271\n   .bytxsstd         32.520    3.429    9.484    0.000   32.520    0.319\n   .bytxhstd         32.045    3.363    9.527    0.000   32.045    0.322\n   .hw10              2.548    0.310    8.213    0.000    2.548    0.722\n   .hw_8              1.017    0.101   10.048    0.000    1.017    0.821\n   .eng_12            1.062    0.143    7.424    0.000    1.062    0.143\n   .math_12           2.320    0.228   10.158    0.000    2.320    0.311\n   .sci_12            1.644    0.179    9.177    0.000    1.644    0.220\n   .ss_12             1.651    0.192    8.606    0.000    1.651    0.187\n    famback         322.146   39.268    8.204    0.000    1.000    1.000\n   .prevach          46.966    5.417    8.671    0.000    0.646    0.646\n   .hw                0.552    0.226    2.444    0.015    0.562    0.562\n   .grades            3.626    0.380    9.533    0.000    0.570    0.570\n\n\nGroup 2 [white]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              14.631    0.735\n    bypared (.p2.)    0.067    0.003   20.086    0.000    0.984    0.818\n    byfamnc (.p3.)    0.096    0.006   17.489    0.000    1.409    0.617\n  prevach =~                                                            \n    bytxrst           1.000                               8.579    0.844\n    bytxmst (.p5.)    1.014    0.030   33.531    0.000    8.699    0.841\n    bytxsst (.p6.)    0.978    0.030   32.663    0.000    8.389    0.839\n    bytxhst (.p7.)    0.963    0.030   32.151    0.000    8.260    0.830\n  hw =~                                                                 \n    hw10              1.000                               1.159    0.606\n    hw_8    (.p9.)    0.475    0.061    7.816    0.000    0.551    0.484\n  grades =~                                                             \n    eng_12            1.000                               2.433    0.921\n    math_12 (.11.)    0.900    0.024   38.175    0.000    2.190    0.816\n    sci_12  (.12.)    0.958    0.022   44.034    0.000    2.331    0.875\n    ss_12   (.13.)    1.061    0.022   48.518    0.000    2.581    0.917\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.291    0.025   11.429    0.000    0.497    0.497\n  grades ~                                                              \n    prevach           0.154    0.012   12.970    0.000    0.543    0.543\n    hw         (h)    0.548    0.121    4.519    0.000    0.261    0.261\n  hw ~                                                                  \n    prevach           0.045    0.009    5.044    0.000    0.330    0.330\n    famback           0.019    0.005    3.546    0.000    0.240    0.240\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.851    0.289    2.944    0.003    0.851    0.152\n .bytxmstd ~~                                                           \n   .math_12           2.950    0.400    7.375    0.000    2.950    0.340\n .bytxsstd ~~                                                           \n   .sci_12            1.141    0.327    3.492    0.000    1.141    0.163\n .bytxhstd ~~                                                           \n   .ss_12             0.362    0.310    1.168    0.243    0.362    0.058\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           54.883    0.726   75.571    0.000   54.883    2.758\n   .bypared           3.335    0.044   75.949    0.000    3.335    2.771\n   .byfaminc         10.348    0.083  124.151    0.000   10.348    4.530\n   .bytxrstd         53.260    0.371  143.540    0.000   53.260    5.238\n   .bytxmstd         53.559    0.378  141.817    0.000   53.559    5.175\n   .bytxsstd         53.335    0.365  146.139    0.000   53.335    5.333\n   .bytxhstd         52.956    0.363  145.841    0.000   52.956    5.322\n   .hw10              3.443    0.070   49.350    0.000    3.443    1.801\n   .hw_8              1.733    0.042   41.712    0.000    1.733    1.522\n   .eng_12            6.409    0.096   66.521    0.000    6.409    2.427\n   .math_12           5.823    0.098   59.483    0.000    5.823    2.171\n   .sci_12            6.088    0.097   62.661    0.000    6.088    2.287\n   .ss_12             6.612    0.103   64.363    0.000    6.612    2.349\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          182.019   13.641   13.343    0.000  182.019    0.460\n   .bypared           0.480    0.050    9.631    0.000    0.480    0.332\n   .byfaminc          3.232    0.196   16.477    0.000    3.232    0.620\n   .bytxrstd         29.797    2.030   14.681    0.000   29.797    0.288\n   .bytxmstd         31.440    2.126   14.791    0.000   31.440    0.294\n   .bytxsstd         29.662    1.994   14.876    0.000   29.662    0.297\n   .bytxhstd         30.798    2.036   15.128    0.000   30.798    0.311\n   .hw10              2.312    0.223   10.360    0.000    2.312    0.632\n   .hw_8              0.993    0.067   14.829    0.000    0.993    0.766\n   .eng_12            1.052    0.084   12.505    0.000    1.052    0.151\n   .math_12           2.401    0.141   17.037    0.000    2.401    0.334\n   .sci_12            1.656    0.107   15.446    0.000    1.656    0.234\n   .ss_12             1.261    0.098   12.869    0.000    1.261    0.159\n    famback         214.077   19.121   11.196    0.000    1.000    1.000\n   .prevach          55.441    4.061   13.652    0.000    0.753    0.753\n   .hw                1.014    0.201    5.045    0.000    0.755    0.755\n   .grades            3.015    0.214   14.100    0.000    0.509    0.509\n\n\n\n\n# compare the two models\nlavTestLRT(sem_fit_mg2, sem_fit_mg3) |&gt; print()\n\n\nChi-Squared Difference Test\n\n             Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nsem_fit_mg2 121 67185 67614 238.78                                    \nsem_fit_mg3 122 67182 67606 236.98    -1.7941     0       1          1\n\n\n\n# constrain all the effects\nsem_fit_mg4 &lt;- sem(hw_model_mg,\n  data = hw_multigroup,\n  estimator = \"ML\",\n  group = \"group\",\n  group.equal = c(\"loadings\", \"regressions\")\n)\nsummary(sem_fit_mg4, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 283 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        96\n  Number of equality constraints                    14\n\n  Number of observations per group:                   \n    minority                                       274\n    white                                          751\n\nModel Test User Model:\n                                                      \n  Test statistic                               241.795\n  Degrees of freedom                               126\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    minority                                   108.582\n    white                                      133.213\n\nModel Test Baseline Model:\n\n  Test statistic                              8290.390\n  Degrees of freedom                               156\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.986\n  Tucker-Lewis Index (TLI)                       0.982\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33507.163\n  Loglikelihood unrestricted model (H1)     -33386.266\n                                                      \n  Akaike (AIC)                               67178.326\n  Bayesian (BIC)                             67582.787\n  Sample-size adjusted Bayesian (SABIC)      67322.346\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.042\n  90 Percent confidence interval - lower         0.034\n  90 Percent confidence interval - upper         0.050\n  P-value H_0: RMSEA &lt;= 0.050                    0.942\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.033\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [minority]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              17.852    0.758\n    bypared (.p2.)    0.067    0.003   20.053    0.000    1.201    0.844\n    byfamnc (.p3.)    0.096    0.006   17.465    0.000    1.717    0.627\n  prevach =~                                                            \n    bytxrst           1.000                               8.569    0.861\n    bytxmst (.p5.)    1.014    0.030   33.525    0.000    8.687    0.854\n    bytxsst (.p6.)    0.978    0.030   32.671    0.000    8.380    0.828\n    bytxhst (.p7.)    0.962    0.030   32.138    0.000    8.248    0.825\n  hw =~                                                                 \n    hw10              1.000                               1.008    0.540\n    hw_8    (.p9.)    0.460    0.060    7.647    0.000    0.463    0.414\n  grades =~                                                             \n    eng_12            1.000                               2.540    0.927\n    math_12 (.11.)    0.900    0.024   38.191    0.000    2.286    0.832\n    sci_12  (.12.)    0.958    0.022   44.053    0.000    2.433    0.884\n    ss_12   (.13.)    1.061    0.022   48.543    0.000    2.695    0.903\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback (.18.)    0.288    0.021   13.859    0.000    0.600    0.600\n  grades ~                                                              \n    prevach (.19.)    0.152    0.011   13.290    0.000    0.512    0.512\n    hw      (.20.)    0.548    0.122    4.485    0.000    0.217    0.217\n  hw ~                                                                  \n    prevach (.21.)    0.053    0.008    6.710    0.000    0.450    0.450\n    famback (.22.)    0.013    0.004    3.088    0.002    0.236    0.236\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.205    0.456    0.450    0.652    0.205    0.039\n .bytxmstd ~~                                                           \n   .math_12           2.437    0.618    3.942    0.000    2.437    0.302\n .bytxsstd ~~                                                           \n   .sci_12            0.358    0.552    0.648    0.517    0.358    0.049\n .bytxhstd ~~                                                           \n   .ss_12             1.061    0.570    1.862    0.063    1.061    0.146\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           43.085    1.423   30.270    0.000   43.085    1.829\n   .bypared           2.849    0.086   33.131    0.000    2.849    2.002\n   .byfaminc          8.757    0.166   52.887    0.000    8.757    3.195\n   .bytxrstd         48.537    0.601   80.719    0.000   48.537    4.876\n   .bytxmstd         49.808    0.615   81.054    0.000   49.808    4.897\n   .bytxsstd         47.959    0.612   78.401    0.000   47.959    4.736\n   .bytxhstd         48.160    0.604   79.727    0.000   48.160    4.816\n   .hw10              3.214    0.113   28.488    0.000    3.214    1.721\n   .hw_8              1.718    0.068   25.404    0.000    1.718    1.535\n   .eng_12            5.820    0.166   35.148    0.000    5.820    2.123\n   .math_12           5.385    0.166   32.448    0.000    5.385    1.960\n   .sci_12            5.590    0.166   33.635    0.000    5.590    2.032\n   .ss_12             5.895    0.180   32.692    0.000    5.895    1.975\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          236.422   27.644    8.553    0.000  236.422    0.426\n   .bypared           0.582    0.095    6.150    0.000    0.582    0.287\n   .byfaminc          4.563    0.443   10.306    0.000    4.563    0.607\n   .bytxrstd         25.638    2.949    8.694    0.000   25.638    0.259\n   .bytxmstd         28.001    3.152    8.883    0.000   28.001    0.271\n   .bytxsstd         32.304    3.418    9.451    0.000   32.304    0.315\n   .bytxhstd         31.959    3.362    9.505    0.000   31.959    0.320\n   .hw10              2.473    0.314    7.883    0.000    2.473    0.709\n   .hw_8              1.039    0.103   10.123    0.000    1.039    0.829\n   .eng_12            1.061    0.143    7.419    0.000    1.061    0.141\n   .math_12           2.322    0.228   10.161    0.000    2.322    0.308\n   .sci_12            1.648    0.179    9.184    0.000    1.648    0.218\n   .ss_12             1.648    0.192    8.601    0.000    1.648    0.185\n    famback         318.690   38.517    8.274    0.000    1.000    1.000\n   .prevach          46.956    5.362    8.757    0.000    0.639    0.639\n   .hw                0.624    0.237    2.631    0.009    0.615    0.615\n   .grades            3.608    0.379    9.517    0.000    0.559    0.559\n\n\nGroup 2 [white]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              14.660    0.736\n    bypared (.p2.)    0.067    0.003   20.053    0.000    0.987    0.819\n    byfamnc (.p3.)    0.096    0.006   17.465    0.000    1.410    0.617\n  prevach =~                                                            \n    bytxrst           1.000                               8.563    0.843\n    bytxmst (.p5.)    1.014    0.030   33.525    0.000    8.680    0.840\n    bytxsst (.p6.)    0.978    0.030   32.671    0.000    8.373    0.838\n    bytxhst (.p7.)    0.962    0.030   32.138    0.000    8.241    0.829\n  hw =~                                                                 \n    hw10              1.000                               1.174    0.613\n    hw_8    (.p9.)    0.460    0.060    7.647    0.000    0.540    0.475\n  grades =~                                                             \n    eng_12            1.000                               2.428    0.921\n    math_12 (.11.)    0.900    0.024   38.191    0.000    2.185    0.816\n    sci_12  (.12.)    0.958    0.022   44.053    0.000    2.326    0.875\n    ss_12   (.13.)    1.061    0.022   48.543    0.000    2.576    0.917\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback (.18.)    0.288    0.021   13.859    0.000    0.494    0.494\n  grades ~                                                              \n    prevach (.19.)    0.152    0.011   13.290    0.000    0.535    0.535\n    hw      (.20.)    0.548    0.122    4.485    0.000    0.265    0.265\n  hw ~                                                                  \n    prevach (.21.)    0.053    0.008    6.710    0.000    0.386    0.386\n    famback (.22.)    0.013    0.004    3.088    0.002    0.166    0.166\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.848    0.289    2.933    0.003    0.848    0.151\n .bytxmstd ~~                                                           \n   .math_12           2.946    0.400    7.373    0.000    2.946    0.339\n .bytxsstd ~~                                                           \n   .sci_12            1.144    0.327    3.502    0.000    1.144    0.163\n .bytxhstd ~~                                                           \n   .ss_12             0.369    0.310    1.187    0.235    0.369    0.059\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           54.883    0.727   75.536    0.000   54.883    2.756\n   .bypared           3.335    0.044   75.852    0.000    3.335    2.768\n   .byfaminc         10.348    0.083  124.116    0.000   10.348    4.529\n   .bytxrstd         53.260    0.371  143.714    0.000   53.260    5.244\n   .bytxmstd         53.559    0.377  142.084    0.000   53.559    5.185\n   .bytxsstd         53.335    0.365  146.302    0.000   53.335    5.339\n   .bytxhstd         52.956    0.363  146.011    0.000   52.956    5.328\n   .hw10              3.443    0.070   49.266    0.000    3.443    1.798\n   .hw_8              1.733    0.041   41.802    0.000    1.733    1.525\n   .eng_12            6.409    0.096   66.645    0.000    6.409    2.432\n   .math_12           5.823    0.098   59.578    0.000    5.823    2.174\n   .sci_12            6.088    0.097   62.768    0.000    6.088    2.290\n   .ss_12             6.612    0.103   64.482    0.000    6.612    2.353\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          181.557   13.673   13.279    0.000  181.557    0.458\n   .bypared           0.478    0.050    9.557    0.000    0.478    0.329\n   .byfaminc          3.232    0.196   16.467    0.000    3.232    0.619\n   .bytxrstd         29.827    2.030   14.693    0.000   29.827    0.289\n   .bytxmstd         31.370    2.121   14.790    0.000   31.370    0.294\n   .bytxsstd         29.695    1.995   14.887    0.000   29.695    0.298\n   .bytxhstd         30.875    2.038   15.149    0.000   30.875    0.313\n   .hw10              2.290    0.229   10.003    0.000    2.290    0.624\n   .hw_8              0.999    0.066   15.026    0.000    0.999    0.774\n   .eng_12            1.052    0.084   12.502    0.000    1.052    0.151\n   .math_12           2.401    0.141   17.038    0.000    2.401    0.335\n   .sci_12            1.656    0.107   15.448    0.000    1.656    0.234\n   .ss_12             1.262    0.098   12.872    0.000    1.262    0.160\n    famback         214.915   19.108   11.247    0.000    1.000    1.000\n   .prevach          55.460    4.038   13.734    0.000    0.756    0.756\n   .hw                1.048    0.209    5.005    0.000    0.760    0.760\n   .grades            3.011    0.214   14.059    0.000    0.511    0.511\n\n\n\n\n# compare the two models\nlavTestLRT(sem_fit_mg3, sem_fit_mg4) |&gt; print()\nlavTestLRT(sem_fit_mg2, sem_fit_mg4) |&gt; print()\n\n\nChi-Squared Difference Test\n\n             Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)\nsem_fit_mg3 122 67182 67606 236.98                                       \nsem_fit_mg4 126 67178 67583 241.79     4.8107 0.019887       4     0.3073\n\nChi-Squared Difference Test\n\n             Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nsem_fit_mg2 121 67185 67614 238.78                                    \nsem_fit_mg4 126 67178 67583 241.79     3.0166     0       5     0.6974\n\n\n\n# constrain all the effects & errors\nhw_model_mg5 &lt;- \"\n  famback =~ parocc + bypared + byfaminc\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw10 + hw_8\n  grades =~ eng_12 + math_12 + sci_12 + ss_12\n\n  bytxrstd ~~ c(en, en)*eng_12\n  bytxmstd ~~ c(ma, ma)*math_12\n  bytxsstd ~~ c(sc, sc)*sci_12\n  bytxhstd ~~ c(ss, ss)*ss_12\n\n  prevach ~ famback\n  grades ~ prevach + hw\n  hw ~ prevach + famback\n\"\n\nsem_fit_mg5 &lt;- sem(hw_model_mg5,\n  data = hw_multigroup,\n  estimator = \"ML\",\n  group = \"group\",\n  group.equal = c(\n    \"loadings\", \"regressions\",\n    \"residuals\", \"lv.variances\"\n  )\n)\n\nsummary(sem_fit_mg5, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 184 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        96\n  Number of equality constraints                    35\n\n  Number of observations per group:                   \n    minority                                       274\n    white                                          751\n\nModel Test User Model:\n                                                      \n  Test statistic                               289.323\n  Degrees of freedom                               147\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    minority                                   141.348\n    white                                      147.975\n\nModel Test Baseline Model:\n\n  Test statistic                              8290.390\n  Degrees of freedom                               156\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.983\n  Tucker-Lewis Index (TLI)                       0.981\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33530.927\n  Loglikelihood unrestricted model (H1)     -33386.266\n                                                      \n  Akaike (AIC)                               67183.854\n  Bayesian (BIC)                             67484.733\n  Sample-size adjusted Bayesian (SABIC)      67290.991\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.043\n  90 Percent confidence interval - lower         0.036\n  90 Percent confidence interval - upper         0.051\n  P-value H_0: RMSEA &lt;= 0.050                    0.927\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.051\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [minority]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              15.679    0.747\n    bypared (.p2.)    0.067    0.003   20.248    0.000    1.043    0.824\n    byfamnc (.p3.)    0.097    0.005   17.694    0.000    1.514    0.625\n  prevach =~                                                            \n    bytxrst           1.000                               8.554    0.847\n    bytxmst (.p5.)    1.013    0.030   33.355    0.000    8.665    0.843\n    bytxsst (.p6.)    0.980    0.030   32.601    0.000    8.385    0.836\n    bytxhst (.p7.)    0.963    0.030   32.072    0.000    8.239    0.828\n  hw =~                                                                 \n    hw10              1.000                               1.130    0.594\n    hw_8    (.p9.)    0.457    0.060    7.599    0.000    0.516    0.456\n  grades =~                                                             \n    eng_12            1.000                               2.459    0.923\n    math_12 (.11.)    0.899    0.024   38.183    0.000    2.211    0.820\n    sci_12  (.12.)    0.958    0.022   44.034    0.000    2.356    0.878\n    ss_12   (.13.)    1.060    0.022   48.459    0.000    2.608    0.912\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback (.18.)    0.287    0.021   13.854    0.000    0.526    0.526\n  grades ~                                                              \n    prevach (.19.)    0.150    0.012   12.896    0.000    0.523    0.523\n    hw      (.20.)    0.570    0.127    4.495    0.000    0.262    0.262\n  hw ~                                                                  \n    prevach (.21.)    0.052    0.008    6.619    0.000    0.395    0.395\n    famback (.22.)    0.014    0.004    3.169    0.002    0.190    0.190\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12    (en)    0.692    0.245    2.825    0.005    0.692    0.126\n .bytxmstd ~~                                                           \n   .math_12   (ma)    2.813    0.337    8.356    0.000    2.813    0.330\n .bytxsstd ~~                                                           \n   .sci_12    (sc)    0.941    0.281    3.344    0.001    0.941    0.133\n .bytxhstd ~~                                                           \n   .ss_12     (ss)    0.543    0.274    1.982    0.048    0.543    0.083\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           43.085    1.267   33.997    0.000   43.085    2.054\n   .bypared           2.849    0.076   37.273    0.000    2.849    2.252\n   .byfaminc          8.757    0.146   59.809    0.000    8.757    3.613\n   .bytxrstd         48.537    0.610   79.598    0.000   48.537    4.809\n   .bytxmstd         49.808    0.621   80.194    0.000   49.808    4.845\n   .bytxsstd         47.959    0.606   79.127    0.000   47.959    4.780\n   .bytxhstd         48.160    0.601   80.108    0.000   48.160    4.840\n   .hw10              3.214    0.115   27.973    0.000    3.214    1.690\n   .hw_8              1.718    0.068   25.130    0.000    1.718    1.518\n   .eng_12            5.820    0.161   36.152    0.000    5.820    2.184\n   .math_12           5.385    0.163   33.072    0.000    5.385    1.998\n   .sci_12            5.590    0.162   34.475    0.000    5.590    2.083\n   .ss_12             5.895    0.173   34.146    0.000    5.895    2.063\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc  (.23.)  194.226   13.021   14.916    0.000  194.226    0.441\n   .bypared (.24.)    0.513    0.048   10.687    0.000    0.513    0.320\n   .byfamnc (.25.)    3.580    0.188   19.087    0.000    3.580    0.609\n   .bytxrst (.26.)   28.719    1.704   16.853    0.000   28.719    0.282\n   .bytxmst (.27.)   30.622    1.793   17.074    0.000   30.622    0.290\n   .bytxsst (.28.)   30.349    1.743   17.410    0.000   30.349    0.302\n   .bytxhst (.29.)   31.142    1.761   17.689    0.000   31.142    0.314\n   .hw10    (.30.)    2.341    0.201   11.627    0.000    2.341    0.647\n   .hw_8    (.31.)    1.015    0.058   17.575    0.000    1.015    0.792\n   .eng_12  (.32.)    1.053    0.074   14.287    0.000    1.053    0.148\n   .math_12 (.33.)    2.376    0.120   19.758    0.000    2.376    0.327\n   .sci_12  (.34.)    1.653    0.093   17.821    0.000    1.653    0.229\n   .ss_12   (.35.)    1.368    0.089   15.354    0.000    1.368    0.167\n    famback (.36.)  245.843   20.005   12.289    0.000    1.000    1.000\n   .prevach (.37.)   52.921    3.490   15.164    0.000    0.723    0.723\n   .hw      (.38.)    0.931    0.182    5.109    0.000    0.729    0.729\n   .grades  (.39.)    3.161    0.198   15.985    0.000    0.523    0.523\n\n\nGroup 2 [white]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              15.679    0.747\n    bypared (.p2.)    0.067    0.003   20.248    0.000    1.043    0.824\n    byfamnc (.p3.)    0.097    0.005   17.694    0.000    1.514    0.625\n  prevach =~                                                            \n    bytxrst           1.000                               8.554    0.847\n    bytxmst (.p5.)    1.013    0.030   33.355    0.000    8.665    0.843\n    bytxsst (.p6.)    0.980    0.030   32.601    0.000    8.385    0.836\n    bytxhst (.p7.)    0.963    0.030   32.072    0.000    8.239    0.828\n  hw =~                                                                 \n    hw10              1.000                               1.130    0.594\n    hw_8    (.p9.)    0.457    0.060    7.599    0.000    0.516    0.456\n  grades =~                                                             \n    eng_12            1.000                               2.459    0.923\n    math_12 (.11.)    0.899    0.024   38.183    0.000    2.211    0.820\n    sci_12  (.12.)    0.958    0.022   44.034    0.000    2.356    0.878\n    ss_12   (.13.)    1.060    0.022   48.459    0.000    2.608    0.912\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback (.18.)    0.287    0.021   13.854    0.000    0.526    0.526\n  grades ~                                                              \n    prevach (.19.)    0.150    0.012   12.896    0.000    0.523    0.523\n    hw      (.20.)    0.570    0.127    4.495    0.000    0.262    0.262\n  hw ~                                                                  \n    prevach (.21.)    0.052    0.008    6.619    0.000    0.395    0.395\n    famback (.22.)    0.014    0.004    3.169    0.002    0.190    0.190\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12    (en)    0.692    0.245    2.825    0.005    0.692    0.126\n .bytxmstd ~~                                                           \n   .math_12   (ma)    2.813    0.337    8.356    0.000    2.813    0.330\n .bytxsstd ~~                                                           \n   .sci_12    (sc)    0.941    0.281    3.344    0.001    0.941    0.133\n .bytxhstd ~~                                                           \n   .ss_12     (ss)    0.543    0.274    1.982    0.048    0.543    0.083\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           54.883    0.765   71.697    0.000   54.883    2.616\n   .bypared           3.335    0.046   72.240    0.000    3.335    2.636\n   .byfaminc         10.348    0.088  117.008    0.000   10.348    4.270\n   .bytxrstd         53.260    0.368  144.601    0.000   53.260    5.277\n   .bytxmstd         53.559    0.375  142.766    0.000   53.559    5.210\n   .bytxsstd         53.335    0.366  145.684    0.000   53.335    5.316\n   .bytxhstd         52.956    0.363  145.831    0.000   52.956    5.321\n   .hw10              3.443    0.069   49.606    0.000    3.443    1.810\n   .hw_8              1.733    0.041   41.954    0.000    1.733    1.531\n   .eng_12            6.409    0.097   65.911    0.000    6.409    2.405\n   .math_12           5.823    0.098   59.205    0.000    5.823    2.160\n   .sci_12            6.088    0.098   62.161    0.000    6.088    2.268\n   .ss_12             6.612    0.104   63.401    0.000    6.612    2.314\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc  (.23.)  194.226   13.021   14.916    0.000  194.226    0.441\n   .bypared (.24.)    0.513    0.048   10.687    0.000    0.513    0.320\n   .byfamnc (.25.)    3.580    0.188   19.087    0.000    3.580    0.609\n   .bytxrst (.26.)   28.719    1.704   16.853    0.000   28.719    0.282\n   .bytxmst (.27.)   30.622    1.793   17.074    0.000   30.622    0.290\n   .bytxsst (.28.)   30.349    1.743   17.410    0.000   30.349    0.302\n   .bytxhst (.29.)   31.142    1.761   17.689    0.000   31.142    0.314\n   .hw10    (.30.)    2.341    0.201   11.627    0.000    2.341    0.647\n   .hw_8    (.31.)    1.015    0.058   17.575    0.000    1.015    0.792\n   .eng_12  (.32.)    1.053    0.074   14.287    0.000    1.053    0.148\n   .math_12 (.33.)    2.376    0.120   19.758    0.000    2.376    0.327\n   .sci_12  (.34.)    1.653    0.093   17.821    0.000    1.653    0.229\n   .ss_12   (.35.)    1.368    0.089   15.354    0.000    1.368    0.167\n    famback (.36.)  245.843   20.005   12.289    0.000    1.000    1.000\n   .prevach (.37.)   52.921    3.490   15.164    0.000    0.723    0.723\n   .hw      (.38.)    0.931    0.182    5.109    0.000    0.729    0.729\n   .grades  (.39.)    3.161    0.198   15.985    0.000    0.523    0.523\n\n\n\n\n# compare the two models\nlavTestLRT(sem_fit_mg4, sem_fit_mg5) |&gt; print()\n\n\nChi-Squared Difference Test\n\n             Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)    \nsem_fit_mg4 126 67178 67583 241.79                                           \nsem_fit_mg5 147 67184 67485 289.32     47.528 0.049647      21  0.0007971 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Statistics",
      "Chapter 18"
    ]
  },
  {
    "objectID": "contents/cleaning.html",
    "href": "contents/cleaning.html",
    "title": "Cleaning",
    "section": "",
    "text": "select(), mutate(), filter(), rename() : 기본 tidyverse verbs\n\nrowSums(), rowMeans() : composite 변수들의 합 또는 평균을 구함\n\nfactor() : 카테고리 변수의 변환\n\n앞서 다운받은 데이터: altruism.csv 파일 링크\n\nlibrary(tidyverse)\n\n# import data\nhelping &lt;- read_csv(\"data/altruism.csv\")\nhelping |&gt; print()\n\n# A tibble: 120 x 12\n     id pho_1 pho_2 pho_3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n# i 114 more rows\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n\n\nrename()\n\nhelping |&gt;\n    rename(ph1 = pho_1, ph2 = pho_2, ph3 = pho_3) |&gt;\n    print()\n\n# A tibble: 120 x 12\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n# i 114 more rows\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n변형 후에는 꼭 변수에 assign!\n\nhelping &lt;-     # 원래 데이터에 overwrite\n    helping |&gt;\n    rename(ph1 = pho_1, ph2 = pho_2, ph3 = pho_3)\n\nhelping |&gt; print()\n\n# A tibble: 120 x 12\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n# i 114 more rows\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n\n\n\nrowSums(row, na.rm = TRUE) 함수를 이용하는 것이 직접 덧셈보다 더 적절함\nph1, ph2, ph3 세 문항을 더하려면,\n\n# 먼저 문항을 선택/확인\nhelping |&gt;\n  select(ph1:ph3) |&gt; # position!\n  print()\n\n# A tibble: 120 x 3\n    ph1   ph2   ph3\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    95    95    95\n2    58    62    NA\n3   100    50    50\n4    77    77    64\n5    NA    NA    NA\n6   100    75   100\n# i 114 more rows\n\n\n\nhelping |&gt;\n  select(ph1:ph3) |&gt;\n  rowSums(na.rm = TRUE) |&gt;\n  print()\n\n  [1] 285 120 200 218   0 275 257 178 256 189 215 226 209 246 159 197 205 225\n [19] 150 195  44   0   0 125 225 211 270 176 241 205   0 220  98  79 143 165\n [37]  49 294 300 292 101 285 208 230 255 150 299 188 208 205 138 267 187 300\n [55] 195 300 236  59 226 193 213 250  32 228 250 300 300 190 230 281 196 268\n [73] 240 250  39 233 211 198 199 234 300 215 240   9 261 209 281 201 270 255\n [91] 177 235 161   0 242 151 182 170   3 222 172 194 300 300 293 238 243 260\n[109] 197 294 280 195 255   1 162 278 176 262 300 164\n\n\n\nhelping[\"phone\"] &lt;-    # \"phone\"이라는 새로운 변수에 assign!\n  helping |&gt;\n  select(ph1:ph3) |&gt;\n  rowSums(na.rm = TRUE)\n\nhelping |&gt; print(width = Inf)\n\n# A tibble: 120 x 13\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n  emp_q26 phone\n    &lt;dbl&gt; &lt;dbl&gt;\n1      70   285\n2      59   120\n3     100   200\n4      69   218\n5      NA     0\n6      90   275\n# i 114 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\n다음과 같이 직접 더하는 것은 부적절\nhelping |&gt;\n  mutate(phone = ph1 + ph2 + ph3) \n#      id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n# 1     1    95    95    95     1  2004      80      NA      80      80      70\n# 2     2    58    62    NA     0  2003      62      58      59      57      56\n# 3     3   100    50    50    NA  2003      90      51      51      51      52\n# 4     4    77    77    64     1  2004      66      72      88      82      67\n# 5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n# 6     6   100    75   100     0  2004     100      60      70      55      70\n#   emp_q26 phone\n#     &lt;dbl&gt; &lt;dbl&gt;\n# 1      70   285\n# 2      59    NA\n# 3     100   200\n# 4      69   218\n# 5      NA    NA\n# 6      90   275\n# # … with 114 more rows\n\n\n\n\n\nrowMeans(row, na.rm = TRUE) 함수를 이용하는 것이 적절함\n\n# 먼저, 평균을 낼 문항을 선택/확인\nhelping |&gt;\n  select(emp_q20, emp_q22:emp_q26) |&gt;  # \":\" operator와 \",\" 섞어써도 무방\n  print()\n\n# A tibble: 120 x 6\n  emp_q20 emp_q22 emp_q23 emp_q24 emp_q25 emp_q26\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1      80      NA      80      80      70      70\n2      62      58      59      57      56      59\n3      90      51      51      51      52     100\n4      66      72      88      82      67      69\n5      NA      NA      NA      NA      NA      NA\n6     100      60      70      55      70      90\n# i 114 more rows\n\n\n\nhelping[\"persp\"] &lt;- helping |&gt;    # \"persp\"라는 새로운 변수에 assign!\n  select(emp_q20, emp_q22:emp_q26) |&gt;\n  rowMeans(na.rm = TRUE)\n\nhelping |&gt; print(width = Inf)\n\n# A tibble: 120 x 14\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n  emp_q26 phone persp\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      70   285  76  \n2      59   120  58.5\n3     100   200  65.8\n4      69   218  74  \n5      NA     0 NaN  \n6      90   275  74.2\n# i 114 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nTidyverse에서 row-wise operation을 하려면,\nhelping |&gt;\n    rowwise() |&gt;\n    mutate(persp = mean(c(emp_q20, c_across(emp_q22:emp_q26)), na.rm = TRUE)) |&gt;\n    ungroup()\n참고: column-wise operation\n\n\n\n\n\n\n표준화(standardize): scale(x) 함수를 이용\n중심화(center): scale(x, scale = FALSE) 함수를 이용\n\n\nhelping |&gt; \n    mutate(\n        phone_z = scale(phone) %&gt;% as.vector,  # scale()은 matrix로 반환; vector 변환 필요\n        persp_z = scale(persp) %&gt;% as.vector\n    ) |&gt;\n    print(n = 3, width = Inf)\n\n# A tibble: 120 x 16\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n  emp_q26 phone persp  phone_z persp_z\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1      70   285  76    1.05      0.104\n2      59   120  58.5 -1.00     -0.981\n3     100   200  65.8 -0.00643  -0.527\n# i 117 more rows\n\n\nacross() 함수를 이용하면 여러 변수를 한 번에 변환할 수 있음\n\nhelping |&gt; \n    mutate(across(.cols = c(phone, persp),\n                  .fns = ~(scale(.) %&gt;% as.vector),\n                  .names = \"{.col}_z\")) |&gt;  # 변수명을 일괄 변경: 변수명 + \"_z\"\n    print(n = 3, width = Inf)\n\n# A tibble: 120 x 16\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n  emp_q26 phone persp  phone_z persp_z\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1      70   285  76    1.05      0.104\n2      59   120  58.5 -1.00     -0.981\n3     100   200  65.8 -0.00643  -0.527\n# i 117 more rows\n\n\n\n\n\n카테고리 변수는 R의 factor 타입으로 바꾸어 분석하는 것이 유리함.\n간단한 연산은 직접 계산.\n\nhelping |&gt;\n  mutate(\n    sex = factor(sex, levels = c(0, 1), labels = c(\"male\", \"female\")),  # factor 타입의 변수로 변환\n    age = 2023 - age  # 출생년도로부터 나이 계산\n  ) |&gt;\n  print(width = Inf)\n\n# A tibble: 120 x 14\n     id   ph1   ph2   ph3 sex      age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95 female    19      80      NA      80      80      70\n2     2    58    62    NA male      20      62      58      59      57      56\n3     3   100    50    50 NA        20      90      51      51      51      52\n4     4    77    77    64 female    19      66      72      88      82      67\n5     5    NA    NA    NA NA        NA      NA      NA      NA      NA      NA\n6     6   100    75   100 male      19     100      60      70      55      70\n  emp_q26 phone persp\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      70   285  76  \n2      59   120  58.5\n3     100   200  65.8\n4      69   218  74  \n5      NA     0 NaN  \n6      90   275  74.2\n# i 114 more rows\n\n\n\n\n\nfilter()를 활용\n예를 들어, 5번째 행을 지우려면\n\nhelping |&gt;\n    filter(!id == 5) |&gt; # !는 not의 의미\n    print()\n\n# 다시 helping에 assign 해야 수정됨!\n\n# A tibble: 119 x 14\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     6   100    75   100     0  2004     100      60      70      55      70\n6     7    77    94    86     1  2004      91      93      85      91      73\n# i 113 more rows\n# i 3 more variables: emp_q26 &lt;dbl&gt;, phone &lt;dbl&gt;, persp &lt;dbl&gt;\n\n\n여러 행을 지우려면?\n%in% 응용\n\nhelping |&gt;\n    filter(!id %in% c(1, 3, 5)) |&gt; # !는 not의 의미\n    print()\n\n# A tibble: 117 x 14\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     2    58    62    NA     0  2003      62      58      59      57      56\n2     4    77    77    64     1  2004      66      72      88      82      67\n3     6   100    75   100     0  2004     100      60      70      55      70\n4     7    77    94    86     1  2004      91      93      85      91      73\n5     8    90    68    20     0  2004      67      66      31      67      63\n6     9   100    79    77     0  2003      61      51      30      51      51\n# i 111 more rows\n# i 3 more variables: emp_q26 &lt;dbl&gt;, phone &lt;dbl&gt;, persp &lt;dbl&gt;\n\n\n\n\n\nselect() 활용\nemp_q23, emp_q25 두 열을 삭제\n\nhelping |&gt;\n    select(-emp_q23, -emp_q25) |&gt;\n    print()\n\n# A tibble: 120 x 12\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q24 emp_q26 phone\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      70   285\n2     2    58    62    NA     0  2003      62      58      57      59   120\n3     3   100    50    50    NA  2003      90      51      51     100   200\n4     4    77    77    64     1  2004      66      72      82      69   218\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA     0\n6     6   100    75   100     0  2004     100      60      55      90   275\n# i 114 more rows\n# i 1 more variable: persp &lt;dbl&gt;\n\n\nemp_q23부터 emp_q26 열을 삭제 (위치의 의미로)\n\nhelping |&gt;\n    select(-(emp_q23:emp_q26)) |&gt;  # () 꼭 필요\n    print()\n\n# A tibble: 120 x 10\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 phone persp\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA   285  76  \n2     2    58    62    NA     0  2003      62      58   120  58.5\n3     3   100    50    50    NA  2003      90      51   200  65.8\n4     4    77    77    64     1  2004      66      72   218  74  \n5     5    NA    NA    NA    NA    NA      NA      NA     0 NaN  \n6     6   100    75   100     0  2004     100      60   275  74.2\n# i 114 more rows",
    "crumbs": [
      "R tutorial",
      "Cleaning"
    ]
  },
  {
    "objectID": "contents/cleaning.html#유용한-함수들",
    "href": "contents/cleaning.html#유용한-함수들",
    "title": "Cleaning",
    "section": "",
    "text": "select(), mutate(), filter(), rename() : 기본 tidyverse verbs\n\nrowSums(), rowMeans() : composite 변수들의 합 또는 평균을 구함\n\nfactor() : 카테고리 변수의 변환\n\n앞서 다운받은 데이터: altruism.csv 파일 링크\n\nlibrary(tidyverse)\n\n# import data\nhelping &lt;- read_csv(\"data/altruism.csv\")\nhelping |&gt; print()\n\n# A tibble: 120 x 12\n     id pho_1 pho_2 pho_3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n# i 114 more rows\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n\n\nrename()\n\nhelping |&gt;\n    rename(ph1 = pho_1, ph2 = pho_2, ph3 = pho_3) |&gt;\n    print()\n\n# A tibble: 120 x 12\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n# i 114 more rows\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n변형 후에는 꼭 변수에 assign!\n\nhelping &lt;-     # 원래 데이터에 overwrite\n    helping |&gt;\n    rename(ph1 = pho_1, ph2 = pho_2, ph3 = pho_3)\n\nhelping |&gt; print()\n\n# A tibble: 120 x 12\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n# i 114 more rows\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n\n\n\nrowSums(row, na.rm = TRUE) 함수를 이용하는 것이 직접 덧셈보다 더 적절함\nph1, ph2, ph3 세 문항을 더하려면,\n\n# 먼저 문항을 선택/확인\nhelping |&gt;\n  select(ph1:ph3) |&gt; # position!\n  print()\n\n# A tibble: 120 x 3\n    ph1   ph2   ph3\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    95    95    95\n2    58    62    NA\n3   100    50    50\n4    77    77    64\n5    NA    NA    NA\n6   100    75   100\n# i 114 more rows\n\n\n\nhelping |&gt;\n  select(ph1:ph3) |&gt;\n  rowSums(na.rm = TRUE) |&gt;\n  print()\n\n  [1] 285 120 200 218   0 275 257 178 256 189 215 226 209 246 159 197 205 225\n [19] 150 195  44   0   0 125 225 211 270 176 241 205   0 220  98  79 143 165\n [37]  49 294 300 292 101 285 208 230 255 150 299 188 208 205 138 267 187 300\n [55] 195 300 236  59 226 193 213 250  32 228 250 300 300 190 230 281 196 268\n [73] 240 250  39 233 211 198 199 234 300 215 240   9 261 209 281 201 270 255\n [91] 177 235 161   0 242 151 182 170   3 222 172 194 300 300 293 238 243 260\n[109] 197 294 280 195 255   1 162 278 176 262 300 164\n\n\n\nhelping[\"phone\"] &lt;-    # \"phone\"이라는 새로운 변수에 assign!\n  helping |&gt;\n  select(ph1:ph3) |&gt;\n  rowSums(na.rm = TRUE)\n\nhelping |&gt; print(width = Inf)\n\n# A tibble: 120 x 13\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n  emp_q26 phone\n    &lt;dbl&gt; &lt;dbl&gt;\n1      70   285\n2      59   120\n3     100   200\n4      69   218\n5      NA     0\n6      90   275\n# i 114 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\n다음과 같이 직접 더하는 것은 부적절\nhelping |&gt;\n  mutate(phone = ph1 + ph2 + ph3) \n#      id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n# 1     1    95    95    95     1  2004      80      NA      80      80      70\n# 2     2    58    62    NA     0  2003      62      58      59      57      56\n# 3     3   100    50    50    NA  2003      90      51      51      51      52\n# 4     4    77    77    64     1  2004      66      72      88      82      67\n# 5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n# 6     6   100    75   100     0  2004     100      60      70      55      70\n#   emp_q26 phone\n#     &lt;dbl&gt; &lt;dbl&gt;\n# 1      70   285\n# 2      59    NA\n# 3     100   200\n# 4      69   218\n# 5      NA    NA\n# 6      90   275\n# # … with 114 more rows\n\n\n\n\n\nrowMeans(row, na.rm = TRUE) 함수를 이용하는 것이 적절함\n\n# 먼저, 평균을 낼 문항을 선택/확인\nhelping |&gt;\n  select(emp_q20, emp_q22:emp_q26) |&gt;  # \":\" operator와 \",\" 섞어써도 무방\n  print()\n\n# A tibble: 120 x 6\n  emp_q20 emp_q22 emp_q23 emp_q24 emp_q25 emp_q26\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1      80      NA      80      80      70      70\n2      62      58      59      57      56      59\n3      90      51      51      51      52     100\n4      66      72      88      82      67      69\n5      NA      NA      NA      NA      NA      NA\n6     100      60      70      55      70      90\n# i 114 more rows\n\n\n\nhelping[\"persp\"] &lt;- helping |&gt;    # \"persp\"라는 새로운 변수에 assign!\n  select(emp_q20, emp_q22:emp_q26) |&gt;\n  rowMeans(na.rm = TRUE)\n\nhelping |&gt; print(width = Inf)\n\n# A tibble: 120 x 14\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n  emp_q26 phone persp\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      70   285  76  \n2      59   120  58.5\n3     100   200  65.8\n4      69   218  74  \n5      NA     0 NaN  \n6      90   275  74.2\n# i 114 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nTidyverse에서 row-wise operation을 하려면,\nhelping |&gt;\n    rowwise() |&gt;\n    mutate(persp = mean(c(emp_q20, c_across(emp_q22:emp_q26)), na.rm = TRUE)) |&gt;\n    ungroup()\n참고: column-wise operation\n\n\n\n\n\n\n표준화(standardize): scale(x) 함수를 이용\n중심화(center): scale(x, scale = FALSE) 함수를 이용\n\n\nhelping |&gt; \n    mutate(\n        phone_z = scale(phone) %&gt;% as.vector,  # scale()은 matrix로 반환; vector 변환 필요\n        persp_z = scale(persp) %&gt;% as.vector\n    ) |&gt;\n    print(n = 3, width = Inf)\n\n# A tibble: 120 x 16\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n  emp_q26 phone persp  phone_z persp_z\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1      70   285  76    1.05      0.104\n2      59   120  58.5 -1.00     -0.981\n3     100   200  65.8 -0.00643  -0.527\n# i 117 more rows\n\n\nacross() 함수를 이용하면 여러 변수를 한 번에 변환할 수 있음\n\nhelping |&gt; \n    mutate(across(.cols = c(phone, persp),\n                  .fns = ~(scale(.) %&gt;% as.vector),\n                  .names = \"{.col}_z\")) |&gt;  # 변수명을 일괄 변경: 변수명 + \"_z\"\n    print(n = 3, width = Inf)\n\n# A tibble: 120 x 16\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n  emp_q26 phone persp  phone_z persp_z\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1      70   285  76    1.05      0.104\n2      59   120  58.5 -1.00     -0.981\n3     100   200  65.8 -0.00643  -0.527\n# i 117 more rows\n\n\n\n\n\n카테고리 변수는 R의 factor 타입으로 바꾸어 분석하는 것이 유리함.\n간단한 연산은 직접 계산.\n\nhelping |&gt;\n  mutate(\n    sex = factor(sex, levels = c(0, 1), labels = c(\"male\", \"female\")),  # factor 타입의 변수로 변환\n    age = 2023 - age  # 출생년도로부터 나이 계산\n  ) |&gt;\n  print(width = Inf)\n\n# A tibble: 120 x 14\n     id   ph1   ph2   ph3 sex      age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95 female    19      80      NA      80      80      70\n2     2    58    62    NA male      20      62      58      59      57      56\n3     3   100    50    50 NA        20      90      51      51      51      52\n4     4    77    77    64 female    19      66      72      88      82      67\n5     5    NA    NA    NA NA        NA      NA      NA      NA      NA      NA\n6     6   100    75   100 male      19     100      60      70      55      70\n  emp_q26 phone persp\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      70   285  76  \n2      59   120  58.5\n3     100   200  65.8\n4      69   218  74  \n5      NA     0 NaN  \n6      90   275  74.2\n# i 114 more rows\n\n\n\n\n\nfilter()를 활용\n예를 들어, 5번째 행을 지우려면\n\nhelping |&gt;\n    filter(!id == 5) |&gt; # !는 not의 의미\n    print()\n\n# 다시 helping에 assign 해야 수정됨!\n\n# A tibble: 119 x 14\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     6   100    75   100     0  2004     100      60      70      55      70\n6     7    77    94    86     1  2004      91      93      85      91      73\n# i 113 more rows\n# i 3 more variables: emp_q26 &lt;dbl&gt;, phone &lt;dbl&gt;, persp &lt;dbl&gt;\n\n\n여러 행을 지우려면?\n%in% 응용\n\nhelping |&gt;\n    filter(!id %in% c(1, 3, 5)) |&gt; # !는 not의 의미\n    print()\n\n# A tibble: 117 x 14\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     2    58    62    NA     0  2003      62      58      59      57      56\n2     4    77    77    64     1  2004      66      72      88      82      67\n3     6   100    75   100     0  2004     100      60      70      55      70\n4     7    77    94    86     1  2004      91      93      85      91      73\n5     8    90    68    20     0  2004      67      66      31      67      63\n6     9   100    79    77     0  2003      61      51      30      51      51\n# i 111 more rows\n# i 3 more variables: emp_q26 &lt;dbl&gt;, phone &lt;dbl&gt;, persp &lt;dbl&gt;\n\n\n\n\n\nselect() 활용\nemp_q23, emp_q25 두 열을 삭제\n\nhelping |&gt;\n    select(-emp_q23, -emp_q25) |&gt;\n    print()\n\n# A tibble: 120 x 12\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q24 emp_q26 phone\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      70   285\n2     2    58    62    NA     0  2003      62      58      57      59   120\n3     3   100    50    50    NA  2003      90      51      51     100   200\n4     4    77    77    64     1  2004      66      72      82      69   218\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA     0\n6     6   100    75   100     0  2004     100      60      55      90   275\n# i 114 more rows\n# i 1 more variable: persp &lt;dbl&gt;\n\n\nemp_q23부터 emp_q26 열을 삭제 (위치의 의미로)\n\nhelping |&gt;\n    select(-(emp_q23:emp_q26)) |&gt;  # () 꼭 필요\n    print()\n\n# A tibble: 120 x 10\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 phone persp\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA   285  76  \n2     2    58    62    NA     0  2003      62      58   120  58.5\n3     3   100    50    50    NA  2003      90      51   200  65.8\n4     4    77    77    64     1  2004      66      72   218  74  \n5     5    NA    NA    NA    NA    NA      NA      NA     0 NaN  \n6     6   100    75   100     0  2004     100      60   275  74.2\n# i 114 more rows",
    "crumbs": [
      "R tutorial",
      "Cleaning"
    ]
  },
  {
    "objectID": "contents/cleaning.html#이상치-발견",
    "href": "contents/cleaning.html#이상치-발견",
    "title": "Cleaning",
    "section": "이상치 발견",
    "text": "이상치 발견\nOutliers을 찾는 방법은 다양하고 복잡한 테크닉을 요하기도 하는데, 앞으로 점차 익히게 될 것임\n예를 들어, age에 잘못 기입한 경우가 있는데\n\nhelping &lt;- read_csv(\"data/altruism.csv\")\n\nhelping |&gt;\n    ggplot(aes(x = age)) +\n    geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nage는 출생년도를 물어봤으나 다른 답을 한 경우들이 있음\n값은 2002 ~ 2004 사이가 정상이므로 filter()를 써서 확인해 볼 수 있음\n\nhelping |&gt;\n    filter(age &lt; 2002 | age &gt; 2004) |&gt;\n    print()\n\n# A tibble: 7 x 12\n     id pho_1 pho_2 pho_3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1    11    65    94    56     1   203      62      86      58      47      62\n2    21    17    10    17     1 20004       0       6       1       0       4\n3    43    90    88    30     1   507     100      78      62     100      78\n4    52   100    82    85     1   723      87      83      89     100      88\n5    59    76    86    64     0   709     100      93      67      94      79\n6   108    75   100    85     1  2005     100     100     100     100      97\n7   118    92    76    94     0  1108      55      51      51      60      53\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n값을 수정\n\n# 이상치에 대한 id를 우선 추출\nids_anomaly = helping |&gt;\n    filter(age &lt; 2002 | age &gt; 2004) |&gt;\n    pull(id)  # vector로 반환\nids_anomaly |&gt; print()\n\n[1]  11  21  43  52  59 108 118\n\n\n\n# 모두 NA로 변경하는 경우\nhelping |&gt;\n    # ifelse(조건, 참일 때 값, 거짓일 때 값)\n    mutate(\n        age = ifelse(age &gt; 2004, 2004, age),\n        age = ifelse(age &lt; 2002, NA, age)\n        # 대신, age = ifelse(age &gt; 2004, 2004, ifelse(age &lt; 2002, NA, age))\n        ) |&gt; \n    filter(id %in% ids_anomaly) |&gt; \n    print()\n\n# A tibble: 7 x 12\n     id pho_1 pho_2 pho_3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1    11    65    94    56     1    NA      62      86      58      47      62\n2    21    17    10    17     1  2004       0       6       1       0       4\n3    43    90    88    30     1    NA     100      78      62     100      78\n4    52   100    82    85     1    NA      87      83      89     100      88\n5    59    76    86    64     0    NA     100      93      67      94      79\n6   108    75   100    85     1  2004     100     100     100     100      97\n7   118    92    76    94     0    NA      55      51      51      60      53\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n\n# 직접 입력하는 방식\nhelping[helping$id %in% ids_anomaly, \"age\"] |&gt; print()\n\n# A tibble: 7 x 1\n    age\n  &lt;dbl&gt;\n1   203\n2 20004\n3   507\n4   723\n5   709\n6  2005\n7  1108\n\n\n\n# 직접 입력하는 방식\nhelping[helping$id %in% ids_anomaly, \"age\"] &lt;- c(NA, 2004, NA, NA, NA, 2005, NA)\nhelping |&gt;\n    filter(id %in% ids_anomaly) |&gt;\n    print()\n\n# A tibble: 7 x 12\n     id pho_1 pho_2 pho_3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1    11    65    94    56     1    NA      62      86      58      47      62\n2    21    17    10    17     1  2004       0       6       1       0       4\n3    43    90    88    30     1    NA     100      78      62     100      78\n4    52   100    82    85     1    NA      87      83      89     100      88\n5    59    76    86    64     0    NA     100      93      67      94      79\n6   108    75   100    85     1  2005     100     100     100     100      97\n7   118    92    76    94     0    NA      55      51      51      60      53\n# i 1 more variable: emp_q26 &lt;dbl&gt;",
    "crumbs": [
      "R tutorial",
      "Cleaning"
    ]
  },
  {
    "objectID": "contents/cleaning.html#샘플-r-script",
    "href": "contents/cleaning.html#샘플-r-script",
    "title": "Cleaning",
    "section": "샘플 R script",
    "text": "샘플 R script\n\nlibrary(tidyverse)\n\n# import data\nhelping &lt;- read_csv(\"data/altruism.csv\")\n\n# rename\nhelping &lt;- helping |&gt;\n  rename(ph1 = pho_1, ph2 = pho_2, ph3 = pho_3) \n\n# delete reponses\nhelping &lt;- helping |&gt;\n  filter(!id == 5)\n\n# scoring\nhelping[\"phone\"] &lt;- helping |&gt;\n  select(ph1:ph3) |&gt;\n  rowMeans(na.rm = TRUE)\n\nhelping[\"persp\"] &lt;- helping |&gt; \n  select(emp_q20, emp_q22, emp_q24:emp_q26) |&gt;\n  rowMeans(na.rm = TRUE)\n\n# substitute anolamies\nhelping &lt;- helping |&gt;\n  mutate(age = ifelse(age &gt; 2004, 2004, ifelse(age &lt; 2002, NA, age)))\n\n# factors and etc.\nhelping &lt;- helping |&gt;\n  mutate(\n    sex = factor(sex, levels = c(0, 1), labels = c(\"male\", \"female\")),\n    age = 2023 - age\n  )\n\n# select variables\nhelping &lt;- helping |&gt;\n  select(id, sex, age, phone, persp)\n\n정리된 파일로 분석 시작!\n\nhelping |&gt; print()\n\n# A tibble: 119 x 5\n     id sex      age phone persp\n  &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1 female    19  95    75  \n2     2 male      20  60    58.4\n3     3 NA        20  66.7  68.8\n4     4 female    19  72.7  71.2\n5     6 male      19  91.7  75  \n6     7 female    19  85.7  82.6\n# i 113 more rows",
    "crumbs": [
      "R tutorial",
      "Cleaning"
    ]
  },
  {
    "objectID": "contents/setup.html",
    "href": "contents/setup.html",
    "title": "환경설정",
    "section": "",
    "text": "R 다운로드 및 설치\n\nWindows인 경우 &gt; Download R for Windows &gt; base\nMac인 경우 &gt; Download R for macOS &gt; Apple silicon 또는 Intel Macs 선택\n\nRStudio 다운로드 및 설치\n\n2: Install RStudio",
    "crumbs": [
      "R tutorial",
      "Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#r-및-rstudio-설치",
    "href": "contents/setup.html#r-및-rstudio-설치",
    "title": "환경설정",
    "section": "",
    "text": "R 다운로드 및 설치\n\nWindows인 경우 &gt; Download R for Windows &gt; base\nMac인 경우 &gt; Download R for macOS &gt; Apple silicon 또는 Intel Macs 선택\n\nRStudio 다운로드 및 설치\n\n2: Install RStudio",
    "crumbs": [
      "R tutorial",
      "Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#rstudio-소개",
    "href": "contents/setup.html#rstudio-소개",
    "title": "환경설정",
    "section": "RStudio 소개",
    "text": "RStudio 소개\n4개의 패널로 구성\nProject 단위로 분석\n\n시작시 project을 새로 만들거나 불러와서 실행: filename.Rproj 형태로 저장\nFile &gt; New Project 또는 +박스그림 버튼 &gt; New Directory &gt; New Project\n\nDirectory name, Sub directory\n\n\nWorking directory\n\nproject에서 참조하는 최상위 폴더\n하위폴더 지시: 예) data/file.sav\n\nR script 생성, 저장\nRStudio 닫기, 열기\n\nWorkspace 저장 vs. R script 저장\nWorkspace save/load: .Rdata 형태로 저장\n\nSession\n\nRestart R\n\n\n환경설정: Tools &gt; Global Options\n“Save workspace to .RData on exit” 옵션: 종료시 working space 자동 저장\nCode\n\nsoft-wrap R source files\nUse native pipe operator\n\nAppearance\n\nZoom: 전체 보기 줌\nEdiotr font: Cascadia Mono (Win), Menlo (Mac)\nEditor font size: 글자 크기\nTheme: Tomorrow Night??",
    "crumbs": [
      "R tutorial",
      "Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#패키지의-설치",
    "href": "contents/setup.html#패키지의-설치",
    "title": "환경설정",
    "section": "패키지의 설치",
    "text": "패키지의 설치\n\n# 메뉴를 통한 설치\n\n# 명령어를 통한 설치\ninstall.packages(\"name\")\n\n# 수업에서 필요한 기본 패키지\ninstall.packages(\"tidyverse\") # 패키지들의 패키지\n\n## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.7     ✔ dplyr   1.0.9\n## ✔ tidyr   1.2.0     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1\n\n# 패키지들 간의 함수의 충돌에 대해서... mask\n\n# 추가 패키지\ninstall.packages(c(\"mosaicData\", \"palmerpenguins\")) # c(): combine items\n\n# 패키지 로드: 필요한 패키지는 세션마다 시행해야 함\nlibrary(\"name\")\n    e.g. library(tidyverse)",
    "crumbs": [
      "R tutorial",
      "Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#단축키",
    "href": "contents/setup.html#단축키",
    "title": "환경설정",
    "section": "단축키",
    "text": "단축키\n\n자동완성: tab\n\n현재 라인 실행: Ctrl+Enter (Win)   |   Command+Return (Mac)\nassignment operator (&lt;-) 입력: Alt+- (Win)   |   Option+- (Mac)\npipe operator (%&gt;%) 입력: Ctrl+Shift+M (Win)   |   Shift+Command+M (Mac)\nconsol에서 화살표 키\ncopy, paste\nundo, redo: Ctrl+Z / Ctrl+Shift+Z (Win)   |   Command+Z / Command+Shift+Z (Mac)\nCopy Lines Up/Down: Shift+Alt+Up/Down (Win)   |   Option+Command+Up or Down (Mac)\n\n단축키 변경: Tools &gt;&gt; modify keyboard shortcuts: e.g. pipe operator: Alt+.",
    "crumbs": [
      "R tutorial",
      "Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#도움말",
    "href": "contents/setup.html#도움말",
    "title": "환경설정",
    "section": "도움말",
    "text": "도움말\nhelp() 또는 ?\ne.g. help(factor), ?factor",
    "crumbs": [
      "R tutorial",
      "Setup"
    ]
  },
  {
    "objectID": "contents/visualize.html",
    "href": "contents/visualize.html",
    "title": "Visualize",
    "section": "",
    "text": "데이터 시각화는 탐색적 분석에 더 초점이 맞춰져 있음.\n\n소위 data mining이라고 부르는 데이터 내의 숨겨진 패턴을 찾고 분석하는 탐색적 분석은 전통적인 통계에서 discouraging되어 왔음.\n\n확률에 근거한 통계 이론은 데이터를 수집하기 전에 가설을 세우고 그 가설을 confirm하는 방식을 취함.\n\n논란의 여지가 있지만, 원칙적으로 가설에 근거해 수집한 자료가 가설과 일치하는지를 확인하는 작업에서는 자료를 두 번 이상 들여다 보지 않아야 함.\n\n그럼에도 불구하고, 탐색적 분석은 behind doors에서 이루어지거나 새로운 가설을 세우기 위한 방편으로 이용되었음.\n\n또한, 매우 엄격한 잣대를 적용하는 상황에서도 통계 이론의 특성으로 인해 기본적인 탐색적 분석은 반드시 선행되어야 함.\n\n연구 가설의 진위를 탐구할 때, 탐색적 분석에서 쉽게 빠질 수 있는 편향성(bias)는 항상 조심할 필요가 있고, 확신을 위해서는 새로이 자료를 수집해서 가설을 재검증할 필요가 있음.\n\n탐색적 분석을 위해서는 다양한 시각화 기술이 요하나, 일반적인 통계 분석을 위해서 필요로하는 최소한으로 제한하고자 함.\n또한, 복잡한 통계치를 살펴볼 때, 직접 시각화를 하기보다는 패키지가 알아서 시각화를 해주기 때문에 자세히 알지 못해도 무방함.\n좀 더 상세한 내용에 대해서는\n\nR for Data Science/Visualize\nggplot2 book\nggplot2 extensions\n통계치 표현: ggstatsplot, ggpubr\nData Visualization with R by Rob Kabacoff : 적절한 밸런스\nggplot2 cheatsheet : pdf 다운로드\n\n\n\n\n\n\n\nNote\n\n\n\n충분히 큰 데이터의 경우, 일정량의 데이터 가령 1/4을 따로 떼어놓고, 3/4만으로 탐색적 분석을 통해 모델을 만든 후, 따로 떼어놓은 1/4로 (가설)검증을 하는 cross-validation 방법이 있는데, machine leanring분야에서는 기본적인 process.\nCross-validation 방식에는 여러 변형들이 있음; e.g. 데이터를 4등분하여 각각 4번 위의 방식을 반복하여 합치는 방식, 3가지 (training, validation, test sets)로 나누어 분석",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#exploratory-vs.-confirmatory-analysis",
    "href": "contents/visualize.html#exploratory-vs.-confirmatory-analysis",
    "title": "Visualize",
    "section": "",
    "text": "데이터 시각화는 탐색적 분석에 더 초점이 맞춰져 있음.\n\n소위 data mining이라고 부르는 데이터 내의 숨겨진 패턴을 찾고 분석하는 탐색적 분석은 전통적인 통계에서 discouraging되어 왔음.\n\n확률에 근거한 통계 이론은 데이터를 수집하기 전에 가설을 세우고 그 가설을 confirm하는 방식을 취함.\n\n논란의 여지가 있지만, 원칙적으로 가설에 근거해 수집한 자료가 가설과 일치하는지를 확인하는 작업에서는 자료를 두 번 이상 들여다 보지 않아야 함.\n\n그럼에도 불구하고, 탐색적 분석은 behind doors에서 이루어지거나 새로운 가설을 세우기 위한 방편으로 이용되었음.\n\n또한, 매우 엄격한 잣대를 적용하는 상황에서도 통계 이론의 특성으로 인해 기본적인 탐색적 분석은 반드시 선행되어야 함.\n\n연구 가설의 진위를 탐구할 때, 탐색적 분석에서 쉽게 빠질 수 있는 편향성(bias)는 항상 조심할 필요가 있고, 확신을 위해서는 새로이 자료를 수집해서 가설을 재검증할 필요가 있음.\n\n탐색적 분석을 위해서는 다양한 시각화 기술이 요하나, 일반적인 통계 분석을 위해서 필요로하는 최소한으로 제한하고자 함.\n또한, 복잡한 통계치를 살펴볼 때, 직접 시각화를 하기보다는 패키지가 알아서 시각화를 해주기 때문에 자세히 알지 못해도 무방함.\n좀 더 상세한 내용에 대해서는\n\nR for Data Science/Visualize\nggplot2 book\nggplot2 extensions\n통계치 표현: ggstatsplot, ggpubr\nData Visualization with R by Rob Kabacoff : 적절한 밸런스\nggplot2 cheatsheet : pdf 다운로드\n\n\n\n\n\n\n\nNote\n\n\n\n충분히 큰 데이터의 경우, 일정량의 데이터 가령 1/4을 따로 떼어놓고, 3/4만으로 탐색적 분석을 통해 모델을 만든 후, 따로 떼어놓은 1/4로 (가설)검증을 하는 cross-validation 방법이 있는데, machine leanring분야에서는 기본적인 process.\nCross-validation 방식에는 여러 변형들이 있음; e.g. 데이터를 4등분하여 각각 4번 위의 방식을 반복하여 합치는 방식, 3가지 (training, validation, test sets)로 나누어 분석",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#basics",
    "href": "contents/visualize.html#basics",
    "title": "Visualize",
    "section": "Basics",
    "text": "Basics\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\n\npenguins |&gt;\n    print() # 무시\n\n# A tibble: 344 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA NA     2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# … with 338 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nVariabels:\n\nspecies: a penguin’s species (Adelie, Chinstrap, or Gentoo).\n\nflipper_length_mm: length of a penguin’s flipper, in millimeters.\n\nbody_mass_g: body mass of a penguin, in grams.\n\n더 자세한 사항은 ?penguins\nggplot을 이용한 시각화는 주로 3가지 성분으로 나뉨\n\ndata: 사용할 데이터\n\nmapping: data의 변수들을 어떤 특성에 mapping할 것인지 specify\n\ngeom: 어떤 시각화 개체(graphical objects)로 데이터를 표현할 것인지 specify\n\n\n# x, y축에 변수를 mapping\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n)\n\n\n\n\n\n\n\n\n\n# point로 데이터를 표시: scatterplot\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n#&gt; Warning: Removed 2 rows containing missing values (`geom_point()`).\n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n실제로 is.na()함수를 이용해 missing을 확인해보면,\npenguins |&gt;\n  select(species, flipper_length_mm, body_mass_g) |&gt;\n  filter(is.na(body_mass_g) | is.na(flipper_length_mm))  # true, false의 boolean type\n#&gt; # A tibble: 2 × 3\n#&gt;   species flipper_length_mm body_mass_g\n#&gt;   &lt;fct&gt;               &lt;int&gt;       &lt;int&gt;\n#&gt; 1 Adelie                 NA          NA\n#&gt; 2 Gentoo                 NA          NA\n\n\n\nAdding aesthetics and layers\n\n# spcies에 color (aesthetics)를 mapping\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point()\n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n위에서 species마다 다른 색을 입혀서 다른 패턴이 나타나는지 확인해 볼 수 있음\nggplot2는 + 기호로 연결하여 계속 layer를 추가할 수 있음.\n다음은 trendline 혹은 fitted line이라고 부르는 경향성을 확인해 볼 수 있는 라인의 layer를 추가함\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_smooth()`).”\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nData에 fitted curve를 구하는 방식에는 여러 방법이 있음\n\nLinear fit: 1차 함수형태인 직선으로 fit\nSmoothing fit\n\nPolynominal fit: n차 다항함수형태로 fit\nLoess/lowess: locally estimated/weighted scatterplot smoothing\nGAM: generalized additive model\nSpine: piece-wise polynominal regression\n\n\n나중에 좀 더 자세히 알아봄\n\n\nggplot2는 플랏의 대상에 다음과 같은 속성을 부여할 수 있음\ncolor, size, shape, fill, alpha\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species, shape = island)\n) +\n  geom_point() \n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n어떤 속성을 어떤 변수에 할당하는 것이 적절한지를 선택하는 것이 기술\n\n\n\n\nCategorical vs. continuous\ncolor와 같은 속성은 카테고리 변수가 좀 더 적절하나, 연속변수에서도 적용될 수 있음\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = bill_length_mm)\n) +\n  geom_point() \n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n반대로, x, y에 카테고리 변수를 mapping하여 scatterplot을 그리면 다음과 같은 overploting의 문제가 생김\n\nggplot(\n  data = penguins,\n  mapping = aes(x = species, y = body_mass_g, color = sex)\n) +\n  geom_point() \n\nWarning message:\n“Removed 11 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\n\nOverplotting\nOverplotting의 문제를 해결하는 방식은 주로\n\nalpha(투명도)를 조정하거나 랜덤하게 흐뜨려그리는 geom_jitter()를 사용\n\n애초에 겹치지 않게 그리는 방법도 있음: e.g. beeswarm plot\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = species, y = body_mass_g, color = sex)\n) +\n  geom_jitter(width = .2) # jitter의 정도: width, height\n\nWarning message:\n“Removed 11 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = species, y = body_mass_g, color = sex)\n) +\n  geom_jitter(width = .2, alpha = .5) # alpha: 투명도 0 ~ 1\n\nWarning message:\n“Removed 11 rows containing missing values (`geom_point()`).”",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#geometric-objects",
    "href": "contents/visualize.html#geometric-objects",
    "title": "Visualize",
    "section": "Geometric objects",
    "text": "Geometric objects\nggplot2는 40가지 넘는 geom objects를 제공함.\n주로 통계를 위해 쓰일 geom들은\n\ngeom_point, geom_smooth()\ngeom_boxplot()\ngeom_histogram(), geom_freqploy(), geom_density()\n\nGlobal vs. local mapping\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species)) + # color mapping은 geom_point에만 적용\n  geom_smooth() # 맨 위의 mapping에 있는 global mapping을 inherit\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_smooth()`).”\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_smooth(mapping = aes(linetype = sex), se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_smooth()`).”\n\n\n\n\n\n\n\n\n\n\nggplot(\n    data = penguins,\n    mapping = aes(x = bill_depth_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species)) +  # color mapping은 geom_point에만 \n  geom_smooth(method = lm)  # 맨 위의 mapping에 있는 global mapping을 inherit, method: fitted line의 종류\n\n\n\n\n\n\n\n\n\nggplot(\n    data = penguins,\n    mapping = aes(x = bill_depth_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species)) +\n  geom_smooth(method = lm) +  # 맨 위의 mapping에 있는 global mapping을 inherit\n  geom_smooth(mapping = aes(color = species), method = lm) # color mapping 추가\n\n\n\n\n\n\n\n\naes() 내부, 외부에서의 mapping\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n    geom_point(mapping = aes(color = species)) # aesthetic color에 변수를 mapping\n\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n    geom_point(color = \"skyblue\") + # geom의 color 속성에 색을 지정\n    geom_smooth(color = \"orangered\")",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#statistical-transformations",
    "href": "contents/visualize.html#statistical-transformations",
    "title": "Visualize",
    "section": "Statistical transformations",
    "text": "Statistical transformations\nggplot2는 편의를 위해 통계치를 구해 표시해주는데,\n경우에 따라 직접 통계치를 계산 후 새로 얻는 데이터로 그리는 것이 유리함\n\nDistribution\ngeom_histogram(), geom_freqploy(), geom_density()\n\n# y축에 표시되는 통계치들이 계산됨\nggplot(data = penguins, mapping = aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 100) # binwidth vs. bins\n\nggplot(data = penguins, mapping = aes(x = body_mass_g, colour = sex)) +\n  geom_freqpoly(binwidth = 100)\n\nggplot(data = penguins, mapping = aes(x = body_mass_g, colour = sex)) +\n  geom_density(bw = 100) # bw: band width\n\n\n\n\n\n\n\n\n\n\n\n\nBoxplot\nBoxplot은 분포에 대한 정보은 줄어드나, 카테고리별로 간결하게 비교되는 장점\nboxplot()\n\n\n\nggplot(data = penguins, mapping = aes(x = species, y = body_mass_g)) +\n    geom_boxplot()\n\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_boxplot()`).”\n\n\n\n\n\n\n\n\n\n\nggplot(data = penguins, mapping = aes(x = species, y = body_mass_g)) +\n    geom_boxplot() +\n    geom_jitter(alpha = .6)\n\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_boxplot()`).”\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\ncps &lt;- as_tibble(mosaicData::CPS85)\ncps |&gt;\n    filter(wage &lt; 30) |&gt; \n    ggplot(aes(x = as.factor(educ), y = wage)) +  # as.factor(): numeric을 factor로 변환\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\nggplot(data = penguins, mapping = aes(x = species, y = body_mass_g, fill = sex)) + # color는 box의 테두리 색, fill은 내부색\n  geom_boxplot()\n\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_boxplot()`).”\n\n\n\n\n\n\n\n\n\n\n\nBarplot\nBarplot은 여러방식으로 쓸 수 있는데, 문법이 조금 복잡하고, 수업에서 거의 사용하지 않을 예정이므로 웹사이트를 참조\nR for Data Science/Layers/Statistical transformations\n\nggplot(data = penguins) + \n  geom_bar(mapping = aes(x = species)) # 개수\n\n\n\n\n\n\n\n\n\n\nDiscretize\n연속 변수를 임의의 구간으로 나누어 카테고리처럼 적용하기 할 수 있음\ncut_width(), cut_number(), cut_interval()\n\ncut_width(): 구간의 길이를 정함\ncut_number(): 동일한 갯수의 관측값을 갖는 n개의 그룹\ncut_interval(): 동일한 길이의 n개의 그룹\n\n\nggplot(\n  data = penguins,\n  mapping = aes(\n      x = bill_length_mm, y = bill_depth_mm,\n      color = cut_interval(body_mass_g, 3) # body_mass_g의 값을 3개의 동일한 길이의 구간으로 나눔\n  )\n) +\n  geom_point() +\n  geom_smooth(se = FALSE, span = 1) # span: smoothing 정도 조절\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_smooth()`).”\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#facets",
    "href": "contents/visualize.html#facets",
    "title": "Visualize",
    "section": "Facets",
    "text": "Facets\n카테고리 변수들이 지니는 카테고리들(레벨)로 나누어 그리기\nfacet_wrap(), facet_grid()\n\nggplot(data = penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n  geom_point() +\n  facet_wrap(~species) # species의 레벨로 나뉘어짐\n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\nfacet_wrap()은 레벨이 많아지면 다음의 facet_grid()와는 다르게 화면크기에 맞춰 다음 줄로 넘어감\n\nggplot(data = penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n  geom_point() +\n  facet_grid(sex ~ species)  # 행과 열에 각각 sex, species\n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\nggplot(\n  data = penguins, \n  aes(x = body_mass_g, y = flipper_length_mm, color = sex) # color 추가\n) +\n  geom_point(alpha = .6) +\n  facet_grid(island ~ species)  # 행과 열에 각각 sex, species\n\nWarning message:\n“Removed 11 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFacet과 color 중 어떤 방식으로 표현하는 것이 유리한가? 밸런스를 잘 선택!\n\n\nggplot(data = penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n  geom_point() +\n  facet_wrap(~species)\n\nggplot(data = penguins, aes(x = body_mass_g, y = flipper_length_mm, color = species)) +\n  geom_point()\n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#labels",
    "href": "contents/visualize.html#labels",
    "title": "Visualize",
    "section": "Labels",
    "text": "Labels\nlabs() 안에 각 요소별로 지정\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = island)) +\n  geom_smooth() +\n  labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n    color = \"Species\", shape = \"Island\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n앞으로는 pipe operator와 함께, 축약 형태로\n\ndata = 대신 첫번째 argument 위치에 data frame이 위치\nmapping = 은 두번째 argument 위치에 aes()을 위치\n\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n은 다음과 같이\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n\n\nPipe operator로 다음과 연결될 수 있음\n\npenguins |&gt;\n    filter(!is.na(sex) & island != \"Torgersen\") |&gt;  # 성별이 missing이 아니고, Torgersen섬은 제외\n    ggplot(aes(x = body_mass_g, y = flipper_length_mm, color = sex)) +\n    geom_point() +\n    geom_smooth() +\n    facet_wrap(~island)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#examples",
    "href": "contents/visualize.html#examples",
    "title": "Visualize",
    "section": "Examples",
    "text": "Examples\n이전에 다뤘던 CPS85 데이터로 보면,\n\ncps &lt;- as_tibble(mosaicData::CPS85) # mosaicData package의 CPS85 데이터셋\ncps |&gt;\n   print() # 생략!\n\n# A tibble: 534 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1   9      10 W     M     NH       NS    Married    27 Not      43 const   \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales   \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales   \n4  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n5  15      12 W     M     NH       NS    Married    40 Union    58 const   \n6   9      16 W     F     NH       NS    Married    27 Not      49 clerical\n# … with 528 more rows\n\n\n\ncps |&gt;\n    ggplot(aes(x = wage, color = married)) +\n    geom_freqpoly(binwidth=1)\n\n\n\n\n\n\n\n\n\ncps |&gt;\n    ggplot(aes(x = wage, color = married)) +\n    geom_freqpoly(binwidth = 1) +\n    facet_wrap(~sex)\n\n\n\n\n\n\n\n\n\ncps |&gt;\n  ggplot(aes(x = married, y = wage)) +\n  geom_boxplot(width = .2) +\n  geom_jitter(width = .2, alpha = .2, color = \"red\") +\n  scale_y_continuous(label = scales::label_dollar())  # y축 scale의 변경\n\n\n\n\n\n\n\n\n\ncps |&gt;\n  ggplot(aes(x = married, y = wage, fill = sex)) +\n  geom_boxplot()\n  \n\n\n\n\n\n\n\n\n\ncps |&gt;\n    filter(wage &lt; 30) |&gt; \n    ggplot(aes(x = sector, y = wage, fill = sex)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\ncps |&gt;\n    filter(wage &lt; 30) |&gt;\n    ggplot(aes(x = sector, y = wage, fill = sex)) +\n    geom_boxplot() +\n    facet_grid(married ~ .) \n\n\n\n\n\n\n\n\n\nplot &lt;- cps |&gt;\n  filter(wage &lt; 30) |&gt;\n  ggplot(aes(x = age, y = wage)) +\n  geom_point(alpha = .6) +\n  geom_smooth()\nplot\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n확대, 축소 혹은 제한된 범위에서 보려면 다음 2가지를 구분해야 함\ncoord_cartesian() vs. xlim() or ylim()\n\n\n\nplot + coord_cartesian(xlim = c(18, 40)) # zoom in\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nplot + xlim(18, 40) # data crop\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\nWarning message:\n“Removed 181 rows containing non-finite values (`stat_smooth()`).”\nWarning message:\n“Removed 181 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\ncps |&gt;\n    filter(wage &lt; 30 & sector %in% c(\"manag\", \"manuf\", \"prof\", \"sales\")) |&gt;\n    ggplot(aes(x = age, y = wage, color = sex)) +\n    geom_point() +\n    geom_smooth(se = FALSE, span = 1) +\n    facet_wrap(~sector)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/fit-index.html",
    "href": "contents/fit-index.html",
    "title": "Fit Indices",
    "section": "",
    "text": "다음에 설명하는 방법은 SEM의 보고 표준(Appelbaum et al., 2018)과 일치하며, 연구자가 과거보다 모델 적합도에 대해 더 많은 정보를 보고하도록 요구합니다:\n\n동시 추정 방법을 사용하는 경우 모델 카이제곱 을 자유도 및 p 값과 함께 보고합니다. 결과 변수가 이 분법적이고 이원 로지스틱 또는 확률 회귀 방법을 사용 하여 경로 공분산을 추정하는 경우와 같이 일부 분석에 서는 모델 카이제곱을 사용할 수 없는 경우도 있지만( 예는 Muthén과 Muthén(1998-2017, 3장) 참조), 이러한 경우는 예외적인 경우에 해당합니다.\n모델이 적합도 테스트에 실패하면 (a) 직접 그렇 게 말하고, 표본 크기에 관계없이 (b) 해당 모델을 10% 확률로 거부합니다. 다음으로, (c) 부적합의 크기와 가 능한 원인을 모두 진단합니다(국부적 적합도 검사). 그 근거는 실패를 설명하는 통계적으로 유의미하지만 약 간의 모델 데이터 불일치를 감지하는 것입니다. 이는 대규모 샘플에서 발생할 가능성이 가장 높습니다. 모델 을 거부하기로 한 초기 결정은 철회할 수 있지만, 관찰 된 모델-데이터 불일치가 실제로 중요하지 않은 이유에 대한 설명과 함께 국소 적합도 증거에 근거해서만 철회할 수 있습니다.\n모델이 정확한 적합도 테스트를 통과한 경우에도 로 컬 적합도를 검사해야 합니다. 그 이유는 통계적으로 유의 미하지는 않지만 모델에 의문을 제기할 만큼 큰 모델 데이 터 불일치를 감지하기 위해서입니다. 이는 작은 샘플에서 발생할 가능성이 높습니다. 지역 적합도에 대한 증거가 상 당한 불일치를 나타내는 경우, 카이제곱 테스트를 통과했 더라도 모델을 거부해야 합니다.\n본문에상관관계,표준화또는정규화된잔차등의 잔차 행렬을 보고합니다.원고를 작성합니다. 모델이 너무 커서 직접 설명하기 어려운 경우에는 (a) 부록 자료에 표를 제공하고 (b) 원고에 큰 잔류물의 위치 및 징후와 같은 잔류물의 패턴을 설명합니다. 모델이 어떻게 잘못 지정될 수 있는지 이해하는 데 진단적 가치가 있을 수 있는 패 턴을 찾습니다. 잔류에 대한 정보가 없는 결과 보고 는 불완전합니다. 안타깝게도 이 영역에서 불완전한 보고는 예외가 아니라 일반적입니다. 예를 들어, 조 직 관리 분야에서 발표된 144개의 SEM 연구를 검토 한 결과, 잔차가 언급된 연구는 약 17%에 불과했습 니다(Zhang et al., 2021).\n대략적인 적합도 지수 값을 보고하는 경우 이장의 앞부분에서 설명한 최소 집합에 대한 값을 포함하세요. 그러나 이러한 글로벌 적합도 통계에 대해 고정 또는 동적 임계값에만 의존하여 모델을 유지하는 것을 정당화하려고 해서는 안 됩니다. 특히 모델이 적합도 테스트에 실패하고 잔차의 패턴이 사소하지 않은 사양 오류를 시사하는 경우 특히 그렇습니다.\n초기 모델을 재특정하는 경우 그 근거를 설명 하세요. 또한 잔차와 같은 진단 통계가 재수정에서 어떤 역할을 했는지 설명해야 합니다. 즉, 모델에 대 한 수치 결과, 관련 이론 및 원래 모델에 대한 수정 사항 간의 연관성을 지적합니다(3장). 재조정된 모델 이 여전히 적합도 테스트에 실패하는 경우, 모델과 데이터의 불일치가 정말 미미하다는 것을 입증하고, 그렇지 않은 경우 모델에 대한 유의미한 공분산 증거 가 없다는 것을 입증하는 데 소홀히 한 것입니다.\n모델 적합도에 대한 통계적 증거는 중요하지만 모델 유지 여부를 결정하는 유일한 요소는 아닙니다. 예를 들어, 연구 문제를 고려할 때 매개변수 추정치 가 합리적이어야 합니다. 특정 데이터 집합에 잘 맞는 모델보다는 동일한 인과 프로세스에 의해 생성되는 향후의 데이터 집합에 잘 맞을 가능성이 합리적인 모델 을 선호해야 합니다. 이는 거의 모든 임의의 데이터에 잠재적으로 적합할 수 있는 복잡하고 과도하게 매개변수가 설정된 모델의 경우 특히 그렇습니다. 이러한 모델은 (1) 더 간결한 모델보다 위조 가능성이 적고 (2) 샘 플과 설정의 변화에 따라 생성될 가능성이 적습니다(Preacher et al., 2013).\n특정 모델을 유지하는 경우, 연구자는 해당 모델 이 동등하거나 거의 동등한 버전보다 선호되어야 하는 이유를 설명해야 합니다. 동일한 데이터를 정확히 또는 거의 비슷하게 설명합니 다. 이 단계는 통계보다 훨씬 더 논리적이며, 향후 연구 에서 심각한 경쟁 모델을 구별하기 위해 무엇을 할 수 있는지 설명하는 것도 포함됩니다. 동등하거나 거의 동 등한 모델에 대한 완전한 보고는 드물기 때문에, 양심 적인 독자는 이 문제를 해결함으로써 자신의 SEM 분 석을 실제로 구별할 수 있습니다. 동등한 버전의 구조 모델 생성 및 평가는 다음 장에서 다룹니다.\n모델이 유지되지 않는 경우 분석에서 테스트한 이론에 대한 의미를 설명하려면 학자로서의 기술이 필 요합니다. 결국, 유지 모델이 있든 없든 간에 진정한 영광은 최 선을 다해 논리적으로 끝까지 철저한 평가 프로세스를 따르는 데서 비롯됩니다. 시인 랄프 왈도 에머슨은 이 를 이렇게 표현했습니다: 잘한 일의 보상은 그것을 해 냈다는 것입니다(Mikis, 2012, 294쪽).\n\n결국, 유지 모델이 있든 없든 간에 진정한 영광은 최 선을 다해 논리적으로 끝까지 철저한 평가 프로세스를 따르는 데서 비롯됩니다. 시인 랄프 왈도 에머슨은 이 를 이렇게 표현했습니다: 잘한 일의 보상은 그것을 해 냈다는 것입니다(Mikis, 2012, 294쪽)."
  },
  {
    "objectID": "contents/fit-index.html#적합성-평가를-위한-권장-접근-방식",
    "href": "contents/fit-index.html#적합성-평가를-위한-권장-접근-방식",
    "title": "Fit Indices",
    "section": "",
    "text": "다음에 설명하는 방법은 SEM의 보고 표준(Appelbaum et al., 2018)과 일치하며, 연구자가 과거보다 모델 적합도에 대해 더 많은 정보를 보고하도록 요구합니다:\n\n동시 추정 방법을 사용하는 경우 모델 카이제곱 을 자유도 및 p 값과 함께 보고합니다. 결과 변수가 이 분법적이고 이원 로지스틱 또는 확률 회귀 방법을 사용 하여 경로 공분산을 추정하는 경우와 같이 일부 분석에 서는 모델 카이제곱을 사용할 수 없는 경우도 있지만( 예는 Muthén과 Muthén(1998-2017, 3장) 참조), 이러한 경우는 예외적인 경우에 해당합니다.\n모델이 적합도 테스트에 실패하면 (a) 직접 그렇 게 말하고, 표본 크기에 관계없이 (b) 해당 모델을 10% 확률로 거부합니다. 다음으로, (c) 부적합의 크기와 가 능한 원인을 모두 진단합니다(국부적 적합도 검사). 그 근거는 실패를 설명하는 통계적으로 유의미하지만 약 간의 모델 데이터 불일치를 감지하는 것입니다. 이는 대규모 샘플에서 발생할 가능성이 가장 높습니다. 모델 을 거부하기로 한 초기 결정은 철회할 수 있지만, 관찰 된 모델-데이터 불일치가 실제로 중요하지 않은 이유에 대한 설명과 함께 국소 적합도 증거에 근거해서만 철회할 수 있습니다.\n모델이 정확한 적합도 테스트를 통과한 경우에도 로 컬 적합도를 검사해야 합니다. 그 이유는 통계적으로 유의 미하지는 않지만 모델에 의문을 제기할 만큼 큰 모델 데이 터 불일치를 감지하기 위해서입니다. 이는 작은 샘플에서 발생할 가능성이 높습니다. 지역 적합도에 대한 증거가 상 당한 불일치를 나타내는 경우, 카이제곱 테스트를 통과했 더라도 모델을 거부해야 합니다.\n본문에상관관계,표준화또는정규화된잔차등의 잔차 행렬을 보고합니다.원고를 작성합니다. 모델이 너무 커서 직접 설명하기 어려운 경우에는 (a) 부록 자료에 표를 제공하고 (b) 원고에 큰 잔류물의 위치 및 징후와 같은 잔류물의 패턴을 설명합니다. 모델이 어떻게 잘못 지정될 수 있는지 이해하는 데 진단적 가치가 있을 수 있는 패 턴을 찾습니다. 잔류에 대한 정보가 없는 결과 보고 는 불완전합니다. 안타깝게도 이 영역에서 불완전한 보고는 예외가 아니라 일반적입니다. 예를 들어, 조 직 관리 분야에서 발표된 144개의 SEM 연구를 검토 한 결과, 잔차가 언급된 연구는 약 17%에 불과했습 니다(Zhang et al., 2021).\n대략적인 적합도 지수 값을 보고하는 경우 이장의 앞부분에서 설명한 최소 집합에 대한 값을 포함하세요. 그러나 이러한 글로벌 적합도 통계에 대해 고정 또는 동적 임계값에만 의존하여 모델을 유지하는 것을 정당화하려고 해서는 안 됩니다. 특히 모델이 적합도 테스트에 실패하고 잔차의 패턴이 사소하지 않은 사양 오류를 시사하는 경우 특히 그렇습니다.\n초기 모델을 재특정하는 경우 그 근거를 설명 하세요. 또한 잔차와 같은 진단 통계가 재수정에서 어떤 역할을 했는지 설명해야 합니다. 즉, 모델에 대 한 수치 결과, 관련 이론 및 원래 모델에 대한 수정 사항 간의 연관성을 지적합니다(3장). 재조정된 모델 이 여전히 적합도 테스트에 실패하는 경우, 모델과 데이터의 불일치가 정말 미미하다는 것을 입증하고, 그렇지 않은 경우 모델에 대한 유의미한 공분산 증거 가 없다는 것을 입증하는 데 소홀히 한 것입니다.\n모델 적합도에 대한 통계적 증거는 중요하지만 모델 유지 여부를 결정하는 유일한 요소는 아닙니다. 예를 들어, 연구 문제를 고려할 때 매개변수 추정치 가 합리적이어야 합니다. 특정 데이터 집합에 잘 맞는 모델보다는 동일한 인과 프로세스에 의해 생성되는 향후의 데이터 집합에 잘 맞을 가능성이 합리적인 모델 을 선호해야 합니다. 이는 거의 모든 임의의 데이터에 잠재적으로 적합할 수 있는 복잡하고 과도하게 매개변수가 설정된 모델의 경우 특히 그렇습니다. 이러한 모델은 (1) 더 간결한 모델보다 위조 가능성이 적고 (2) 샘 플과 설정의 변화에 따라 생성될 가능성이 적습니다(Preacher et al., 2013).\n특정 모델을 유지하는 경우, 연구자는 해당 모델 이 동등하거나 거의 동등한 버전보다 선호되어야 하는 이유를 설명해야 합니다. 동일한 데이터를 정확히 또는 거의 비슷하게 설명합니 다. 이 단계는 통계보다 훨씬 더 논리적이며, 향후 연구 에서 심각한 경쟁 모델을 구별하기 위해 무엇을 할 수 있는지 설명하는 것도 포함됩니다. 동등하거나 거의 동 등한 모델에 대한 완전한 보고는 드물기 때문에, 양심 적인 독자는 이 문제를 해결함으로써 자신의 SEM 분 석을 실제로 구별할 수 있습니다. 동등한 버전의 구조 모델 생성 및 평가는 다음 장에서 다룹니다.\n모델이 유지되지 않는 경우 분석에서 테스트한 이론에 대한 의미를 설명하려면 학자로서의 기술이 필 요합니다. 결국, 유지 모델이 있든 없든 간에 진정한 영광은 최 선을 다해 논리적으로 끝까지 철저한 평가 프로세스를 따르는 데서 비롯됩니다. 시인 랄프 왈도 에머슨은 이 를 이렇게 표현했습니다: 잘한 일의 보상은 그것을 해 냈다는 것입니다(Mikis, 2012, 294쪽).\n\n결국, 유지 모델이 있든 없든 간에 진정한 영광은 최 선을 다해 논리적으로 끝까지 철저한 평가 프로세스를 따르는 데서 비롯됩니다. 시인 랄프 왈도 에머슨은 이 를 이렇게 표현했습니다: 잘한 일의 보상은 그것을 해 냈다는 것입니다(Mikis, 2012, 294쪽)."
  },
  {
    "objectID": "contents/fit-index.html#잔여물-검사를-위한-팁",
    "href": "contents/fit-index.html#잔여물-검사를-위한-팁",
    "title": "Fit Indices",
    "section": "잔여물 검사를 위한 팁",
    "text": "잔여물 검사를 위한 팁\n잔차에 대해 보고하는 것이 중요하지만, 카이제곱 검정 결과 및 근사 적합도 지수 값과 마찬가지로 잔차의 크 기와 모델 오정의 유형 또는 양 사이에는 신뢰할 수 있 거나 신뢰할 수 있는 연관성이 없다는 것을 알아야 합 니다. 예를 들어, 상대적으로 작은 상관관계 잔차로 표 시되는 구체화 오류의 정도는 경미할 수도 있지만 심각 할 수도 있습니다. 한 가지 이유는 다음 장에서 정의하 는 수정 지수를 비롯한 잔차 및 기타 진단 통계의 값 자 체가 오특정에 의해 영향을 받기 때문입니다. 의학에 비유하자면, 특정 질병에 대한 진단 검사가 해당 질병 을 앓고 있는 환자에게는 정확도가 떨어질 수 있습니다 . 두 번째 이유는 모델 한 부분의 잘못된 지정이 모델의 다른 부분의 추정치를 왜곡하는 전체 추정에서의 오류전파입니다. 세 번째는 잔차는 동일하지만 인과 관계의 모순된 패턴을 갖는 동등 모델입니다. 그러나 우리는 일 반적으로 모형의 어느 부분이 잘못된 것인지 미리 알 수 없기 때문에 잔차가 우리에게 무엇을 알려주는지 정확 히 이해하기 어려울 수 있습니다. 잔차의 패턴을 검사하는 것이 때때로 도움이 될 수 있 습니다.rXY &gt;0인한쌍의변수X와Y가간접인과경로 로만 연결되어 있다고 가정해 보겠습니다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "강사: 조성균\nemail: sk.cho@snu.ac.kr\n수업시간: 월 7:00 ~ 9:50PM\n면담 시간: 수업 후\nWebsite: sem.modellings.art\n과제: Notice\n질문: Communicate/Ask",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#강의-정보",
    "href": "index.html#강의-정보",
    "title": "Welcome",
    "section": "",
    "text": "강사: 조성균\nemail: sk.cho@snu.ac.kr\n수업시간: 월 7:00 ~ 9:50PM\n면담 시간: 수업 후\nWebsite: sem.modellings.art\n과제: Notice\n질문: Communicate/Ask",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#강의-개요",
    "href": "index.html#강의-개요",
    "title": "Welcome",
    "section": "강의 개요",
    "text": "강의 개요\n심리통계에서 배운 회귀분석을 기반으로 하여, 잠재변수(latent variable)을 포함해 여러 변수들 간의 관계에 대한 가설에 대한 통계적 검증을 위한 구조모형을 다룹니다. 이는 확인적 요인 분석 (confirmatory factor analysis, CFA)과 경로분석 (path analysis)을 통합한 구조방정식모형 (Structural equation model, SEM)의 프레임워크를 포함하며, 인과관계의 대한 추론을 위해 SEM을 활용하는 방법과 그 한계에 대해 다룹니다. 본 수업에서는 R의 SEM 패키지들을 활용해 실습을 하며, 다양한 실제 사례를 통해 분석의 결과를 통계 이론의 이해를 바탕으로 올바로 해석할 수 있는 능력을 갖춥니다.\n\n교재 및 R 코드\n\n\n다중회귀분석과 구조방정식모형분석 - 다중회귀분석을 넘어 (3판), 2024 by Timothy Z. Keith (지은이), 노석준 (옮긴이)\n원제: Multiple Regression and Beyond: An Introduction to Multiple Regression and Structural Equation Modeling, Third Edition (2019)\n저자 웹사이트: https://tzkeith.com\n실습을 위한 참고 홈페이지\n\nR Cookbook for Structural Equation Modeling by Ge Jiang\nGithub repository\n\n\n\n\n\n\n\n참고도서\nPrinciples and Practice of Structural Equation Modeling (5e), 2023 by Rex B. Kline,\n\n\nR 참고도서\nR for Data Science (2e) by Hadley Wickham and Garrett Grolemund",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#수업-활동",
    "href": "index.html#수업-활동",
    "title": "Welcome",
    "section": "수업 활동",
    "text": "수업 활동\n출석 (10%), 일반과제 (30%), 중간고사 대체 과제 (30%), 기말고사 (30%)",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "contents/chap12.html",
    "href": "contents/chap12.html",
    "title": "Chapter 12",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)",
    "crumbs": [
      "Statistics",
      "Chapter 12"
    ]
  },
  {
    "objectID": "contents/chap12.html#연습문제",
    "href": "contents/chap12.html#연습문제",
    "title": "Chapter 12",
    "section": "연습문제",
    "text": "연습문제\n\nTable 12.1에서 familiy background에 대해서도 동일한 분석을 수행해보세요.\n예전 salary 데이터에 대해서도 동일한 분석을 수행해보세요; 링크",
    "crumbs": [
      "Statistics",
      "Chapter 12"
    ]
  },
  {
    "objectID": "contents/ml.html",
    "href": "contents/ml.html",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)\nsim1 &lt;- read_csv(\"data/sim1.csv\")\nsim1 |&gt; print(n = 5)\n\n# A tibble: 30 x 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1  4.20\n2     1  7.51\n3     1  2.13\n4     2  8.99\n5     2 10.2 \n# i 25 more rows\nScatter plot of sim1 data\nsim1 |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Scatter plot of sim1 data\")",
    "crumbs": [
      "Statistics",
      "Maximum Likelihood Estimation"
    ]
  },
  {
    "objectID": "contents/ml.html#maximum-likelihood-estimation",
    "href": "contents/ml.html#maximum-likelihood-estimation",
    "title": "Maximum Likelihood Estimation",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\n데이터가 발생된 것으로 가정하는 분포를 고려했을 때,\n어떨때 주어진 데이터가 관측될 확률/가능도(likelihood)가 최대가 되겠는가로 접근하는 방식으로,\nX, Y의 관계와 확률분포를 함께 고려함.\n\n선형관계라면, 즉 \\(E(Y|X=x_i) = \\beta_0 + \\beta_1x_i\\)   (\\(E\\): expected value, 기대값)\n분포가 Gaussian이라면, 즉 \\(Y|(X=x_i) \\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2)\\)   (\\(\\sigma\\): 표준편차)\n\n\nLikelihood \\(L = \\displaystyle\\prod_{i=1}^{n}{P_i}\\)   (관측치들 독립일 때, product rule에 의해)\n분포가 Gaussian이라면(평균: \\(\\mu\\), 표준편차: \\(\\sigma\\)), 즉 \\(f(t) = \\displaystyle\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{(t-\\mu)^2}{2\\sigma^2}\\right)\\)라면\n\\(L = \\displaystyle\\prod_{i=1}^{n}{f(y_i, x_i)} = \\displaystyle\\prod_{i=1}^{n}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}\\right)}\\)\n이 때, 이 likelihood를 최대화하는 \\(\\beta_0, \\beta_1, \\sigma\\)를 찾는 것이 목표이며,\n이처럼 분포가 Gaussian라면, OLS estimation과 동일한 값을 얻음. (단, \\(\\sigma\\)는 bias가 존재)\n다른 분포를 가지더라도 동일하게 적용할 수 있음!\n\n즉, likelihood의 관점에서 주어진 데이터에 가장 근접하도록(likelihood가 최대가 되는) “분포의 구조”를 얻는 과정임\n\n여러 편의를 위해, log likelihood를 최대화함.\n\n\n\n\n\n\nLog likelihood\n\n\n\n\n\n다음 두가지를 고려하면,\n\\(log(x*y) = log(x) + log(y)\\)\n\\(e^x * e^y = e^{x+y}\\)\n\\(log(L) = \\displaystyle\\sum_{i=1}^{n}{log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}\\right)\\right)} = \\displaystyle\\sum_{i=1}^{n}{-log(\\sqrt{2\\pi\\sigma^2}) - \\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}}\\)\n두 번째 항이 앞서 정의한 squared error와 동일함\n\n\n\n잔차에 대한 간단한 정보들\n\nlm(y ~ x, data = sim1) |&gt; summary() |&gt; print()\n\n\nCall:\nlm(formula = y ~ x, data = sim1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1469 -1.5197  0.1331  1.4670  4.6516 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.2208     0.8688   4.858 4.09e-05 ***\nx             2.0515     0.1400  14.651 1.17e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.203 on 28 degrees of freedom\nMultiple R-squared:  0.8846,    Adjusted R-squared:  0.8805 \nF-statistic: 214.7 on 1 and 28 DF,  p-value: 1.173e-14\n\n\n\n\n# 위의 모형의 잔차들에 대해 정규분포 함수의 값을 구하면,\nsigma &lt;- sd(sim1$resid)  # 잔차의 표준편차: 2.16\n# 좀 더 정확히는 샘플 수로 나누지 않고, 자유도로 나누어야 함\nsim1 &lt;- sim1 |&gt; \n  mutate(norm = dnorm(resid, mean = 0, sd = sigma))\nsim1 |&gt; print(n = 5)\n\n# A tibble: 30 x 5\n      x     y  pred  resid   norm\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1  4.20  6.27 -2.07  0.117 \n2     1  7.51  6.27  1.24  0.156 \n3     1  2.13  6.27 -4.15  0.0294\n4     2  8.99  8.32  0.665 0.176 \n5     2 10.2   8.32  1.92  0.124 \n# i 25 more rows\n\n\n\n\nGaussin distribution with mean 0 and sd 2.16\nx &lt;- seq(-10, 10, length = 100)\ny &lt;- dnorm(x, mean = 0, sd = sigma)\ntibble(x = x, y = y) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  labs(\n    title = \"Gaussian distribution with mean 0 and sd 2.16\",\n    x = \"Residuals\", y = \"Density\"\n  ) +\n  geom_bar(data = sim1, aes(x = resid, y = norm), stat = \"identity\", color = \"black\")\n\n\n\n\n\n\n\n\n\n즉, 위 그림에서 높이를 모두 곱하면, likelihood를 얻을 수 있으며,\nLog-likelihood는 log를 취해 더하면 되므로,\n\nlog(sim1$norm) |&gt; sum() |&gt; print()\n\n[1] -65.2347\n\n\n이 값은 lavaan 결과에서 보여지며, 모형 적합도 계산을 위한 기본적인 값으로 사용됨.\n\nfit &lt;- sem('y ~ x', data = sim1)\nsummary(fit, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         2\n\n  Number of observations                            30\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nModel Test Baseline Model:\n\n  Test statistic                                64.784\n  Degrees of freedom                                 1\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.000\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)                -65.226\n  Loglikelihood unrestricted model (H1)        -65.226\n                                                      \n  Akaike (AIC)                                 134.452\n  Bayesian (BIC)                               137.255\n  Sample-size adjusted Bayesian (SABIC)        131.028\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: RMSEA &lt;= 0.050                       NA\n  P-value H_0: RMSEA &gt;= 0.080                       NA\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  y ~                                                 \n    x                 2.052    0.135   15.166    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y                 4.529    1.169    3.873    0.000\n\n\n\nMaximum likelihood estimation의 결과는 분포가 Gaussian이라면 OLS와 동일하며,\n다른 분포를 가지는 데이터에 대해서도 동일한 원리로 (즉, likelihood를 최대로 하도록) 파라미터를 추정할 수 있음.\n대표적인 예가 logistic regression이며, 이 경우의 분포는 이항분포 또는 베르누이 분포를 가정하여 likelihood를 계산함.",
    "crumbs": [
      "Statistics",
      "Maximum Likelihood Estimation"
    ]
  },
  {
    "objectID": "contents/ml.html#ols-ordinary-least-squares-추정방식",
    "href": "contents/ml.html#ols-ordinary-least-squares-추정방식",
    "title": "Maximum Likelihood Estimation",
    "section": "OLS (Ordinary Least Squares) 추정방식",
    "text": "OLS (Ordinary Least Squares) 추정방식\n선형 모델 family인 \\(\\hat{y} = b_0 + b_1 x\\)을 세운 후\n잔차 \\(e_i = y_i - \\hat{y_i}\\)의 제곱의 합, 즉 \\(\\sum e_i^2\\)이 최소가 되도록하는 \\(b_0, b_1\\)을 추정하는 방법\n\nmod &lt;- lm(y ~ x, data = sim1) \nmod |&gt; coef() |&gt; print()\n\n(Intercept)           x \n   4.220822    2.051533 \n\n\n즉, OLS 방식에서 최선의 모형은 \\(\\hat{y} = 4.22 + 2.05x\\)\n이 모형의 예측값과 잔차를 보면,\n\n\nAdd predictions and residuals to sim1 data\nlibrary(modelr)\nsim1 &lt;- sim1 |&gt; \n  add_predictions(mod) |&gt; \n  add_residuals(mod)\nsim1 |&gt; print(n = 7)\n\n\n# A tibble: 30 x 4\n      x     y  pred  resid\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1  4.20  6.27 -2.07 \n2     1  7.51  6.27  1.24 \n3     1  2.13  6.27 -4.15 \n4     2  8.99  8.32  0.665\n5     2 10.2   8.32  1.92 \n6     2 11.3   8.32  2.97 \n7     3  7.36 10.4  -3.02 \n# i 23 more rows",
    "crumbs": [
      "Statistics",
      "Maximum Likelihood Estimation"
    ]
  },
  {
    "objectID": "contents/ml.html#구조방정식-모형에서의-분포에-대한-가정",
    "href": "contents/ml.html#구조방정식-모형에서의-분포에-대한-가정",
    "title": "Maximum Likelihood Estimation",
    "section": "구조방정식 모형에서의 분포에 대한 가정",
    "text": "구조방정식 모형에서의 분포에 대한 가정\n관찰된 변수들이 multivariate normal distribution을 따른다는 가정을 함.\n이 때, 각 변수들은 정규분포를 따르게 됨. 하지만, 그 반대는 아님.\n\n\nSource: p. 150, Introduction to Statistical Learning with Applications in Python by G. James, D. Witten, T. Hastie, R. Tibshirani\n\n각 잠재변수(latent variable)들은 정규분포를 따름.\n\n정규성에 대한 검증이 필요하며,\n정규성을 따르지 않는 경우, robust estimation 방법을 사용할 수 있음.",
    "crumbs": [
      "Statistics",
      "Maximum Likelihood Estimation"
    ]
  },
  {
    "objectID": "contents/ml.html#구조방정식-모형에서-분포에-대한-가정",
    "href": "contents/ml.html#구조방정식-모형에서-분포에-대한-가정",
    "title": "Maximum Likelihood Estimation",
    "section": "구조방정식 모형에서 분포에 대한 가정",
    "text": "구조방정식 모형에서 분포에 대한 가정\n관찰된 변수(measured variable)들이 multivariate normal distribution을 따른다는 가정을 함.\n이 때, 각 변수들은 정규분포를 따르게 되지만, 그 반대는 아님.\n\n\nSource: p. 150, Introduction to Statistical Learning with Applications in Python by G. James, D. Witten, T. Hastie, R. Tibshirani\n\n각 잠재변수(latent variable)들은 정규분포를 따름.\n\n정규성에 대한 검증이 필요하며,\n정규성을 따르지 않는 경우, robust estimation 방법을 사용할 수 있음.",
    "crumbs": [
      "Statistics",
      "Maximum Likelihood Estimation"
    ]
  },
  {
    "objectID": "contents/ml.html#구조방정식-모형에서-분포에-대한-가정들",
    "href": "contents/ml.html#구조방정식-모형에서-분포에-대한-가정들",
    "title": "Maximum Likelihood Estimation",
    "section": "구조방정식 모형에서 분포에 대한 가정들",
    "text": "구조방정식 모형에서 분포에 대한 가정들\n관찰된 변수(measured variable)들이 multivariate normal distribution을 따른다는 가정을 함.\n이 때, 각 변수들은 정규분포를 따르게 되지만, 그 반대는 아님.\n\n\nSource: p. 150, Introduction to Statistical Learning with Applications in Python by G. James, D. Witten, T. Hastie, R. Tibshirani\n\n각 잠재변수(latent variable)들은 정규분포를 따름.\n\n정규성에 대한 검증이 필요하며,\n정규성을 따르지 않는 경우, robust estimation 방법을 사용할 수 있음.\n\n분포 뿐만 아니라 회귀에서의 모든 가정들은 동일하게 적용됨!",
    "crumbs": [
      "Statistics",
      "Maximum Likelihood Estimation"
    ]
  }
]