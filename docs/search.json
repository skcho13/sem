[
  {
    "objectID": "contents/baser.html",
    "href": "contents/baser.html",
    "title": "Base R",
    "section": "",
    "text": "90년대에 통계 분석을 위해 개발된 R 언어와 대비하여, 좀 더 직관적이고 효율적인 데이터 분석을 위해 새로운 문법이 R내의 패키지 형태로 구현되었는데 이 새로운 생태계 안의 패키지들의 모임이 Tidyverse라는 이름하에 발전하고 있음: Tidyverse\n이 패키지들은 design philosophy, grammar, data structures를 공유하며 유기적으로 작동됨.\n기존 R의 문법과는 상당한 차이가 있어 단점도 지적되고 있고, 소위 base-R을 고수하는 사람들과 tidyverse를 기본으로 사용하는 사람들이 나뉘어 있다고 알려져 있음.\n아마도 빠르게 발전하고 있는 tidyverse/tidymodel 생태계의 언어들이 기본으로 자리잡지 않을까 함.\n본 강의에서는 주로 tidyverse의 언어로만 분석하고자 함.",
    "crumbs": [
      "R tutorial",
      "Base R"
    ]
  },
  {
    "objectID": "contents/baser.html#r의-데이터-구조와-변수-타입",
    "href": "contents/baser.html#r의-데이터-구조와-변수-타입",
    "title": "Base R",
    "section": "R의 데이터 구조와 변수 타입",
    "text": "R의 데이터 구조와 변수 타입\n주로 vector (벡터)와 data frame (데이터프레임)을 다룸\n\nSource: R in Action by Rob Kabacoff\nData frame의 예\n\n각 column이 하나의 variable (변수)를 구성하고, 한가지 타입의 데이터로 이루어짐\n\n각 Row가 하나의 observation (관측치)을 구성함.\n\n이러한 형태를 갖춘 데이터를 tidy라고도 부르며, 이를 벗어난 형태의 경우 가공이 필요함.\nex. “m23”: male이고 23세임을 나타내는 표기도 있음\n\n\nlibrary(tidyverse)\n\ncps &lt;- mosaicData::CPS85 # mosaicData package의 CPS85 데이터셋\nhead(cps) |&gt; print() # print()는 생략할 것!\n\n  wage educ race sex hispanic south married exper union age   sector\n1  9.0   10    W   M       NH    NS Married    27   Not  43    const\n2  5.5   12    W   M       NH    NS Married    20   Not  38    sales\n3  3.8   12    W   F       NH    NS  Single     4   Not  22    sales\n4 10.5   12    W   F       NH    NS Married    29   Not  47 clerical\n5 15.0   12    W   M       NH    NS Married    40 Union  58    const\n6  9.0   16    W   F       NH    NS Married    27   Not  49 clerical\n\n\n\ncps &lt;- as_tibble(cps) # tibble vs. data.frame\nhead(cps) |&gt; print()\n\n# A tibble: 6 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1   9      10 W     M     NH       NS    Married    27 Not      43 const   \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales   \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales   \n4  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n5  15      12 W     M     NH       NS    Married    40 Union    58 const   \n6   9      16 W     F     NH       NS    Married    27 Not      49 clerical\n\n\n\n# Dataset의 설명\nhelp(CPS85, package=\"mosaicData\") # 또는\n?mosaicData::CPS85",
    "crumbs": [
      "R tutorial",
      "Base R"
    ]
  },
  {
    "objectID": "contents/baser.html#vector",
    "href": "contents/baser.html#vector",
    "title": "Base R",
    "section": "Vector",
    "text": "Vector\n한 가지 타입으로만 구성: 숫자 (numeric), 문자 (character), 논리형 (logical), factor, etc\n\nvar &lt;- c(1, 2, 5, 3, 6, -2, 4) # 변수에 assign: '=' 대신 '&lt;-'\nnm &lt;- c(\"one\", \"two\", \"three\")\ntf &lt;- c(TRUE, TRUE, TRUE, FALSE, TRUE, FALSE)\n\n# 타입/클래스 확인\nclass(var)\n## [1] \"double\"\n\nclass(nm)\n## [1] \"character\"\n\nclass(tf)\n## [1] \"logical\"\n\n\n원소의 추출 및 대체\n다음은 원소를 추출, 대체하는 R의 native한 방식임\n수업에서는 뒤에서 다룰 tidyverse 문법을 주로 활용할 것임\nVector의 경우\n\nvar\n## [1]  1  2  5  3  6 -2  4\n\nvar[3]\n## [1] 5\n\nvar[c(1, 3, 5)]\n## [1] 1 5 6\n\nvar[2:6] # \":\"\" slicing: c(2, 3, 4, 5, 6)\n## [1]  2  5  3  6 -2\n\nvar[c(1, 3:5)] # 혼합\n## [1] 1 5 3 6\n\nvar[-c(1, 3)] # \"-\"는 제외라는 의미\n## [1]  2  3  6 -2  4\n\nc(10, var, 100, 101) # 추가\n##  [1]  10   1   2   5   3   6  -2   4 100 101\n\nvar[2] &lt;- 55 # 대체\n## var\n## [1]  1 55  5  3  6 -2  4\n\nvar[c(2, 5)] &lt;- c(200, 500) # 대체\n## var\n## [1]   1 200   5   3 500  -2   4\n\n# numeric 벡터의 연산: recycling rule\n1:5 * 2\n## [1]  2  4  6  8 10\n\nc(1, 3, 5) - 5\n## [1] -4 -2  0\n\nc(2, 4, 6) / 2\n## [1] 1 2 3\n\nc(1, 3) * c(2, 4)\n## [1]  2 12\n\nc(1, 3) - c(2, 4)\n## [1] -1 -1",
    "crumbs": [
      "R tutorial",
      "Base R"
    ]
  },
  {
    "objectID": "contents/baser.html#factor",
    "href": "contents/baser.html#factor",
    "title": "Base R",
    "section": "Factor",
    "text": "Factor\nVector로서 명목변수(카테고리)를 다룸\npatientID &lt;- c(1, 2, 1, 3)\ndiabetes &lt;- c(\"Type1\", \"Type2\", \"Type1\", \"Type1\")\nstatus &lt;- c(\"Poor\", \"Improved\", \"Excellent\", \"Poor\")\n\n# factor로 변환: 알파벳 순서로 levels의 순서가 정해짐\nfactor(patientID)\n## [1] 1 2 1 3\n## Levels: 1 2 3\n\nfactor(diabetes)\n## [1] Type1 Type2 Type1 Type1\n## Levels: Type1 Type2\n\nfactor(status, order = TRUE) # order를 표시\n## [1] Poor      Improved  Excellent Poor     \n## Levels: Excellent &lt; Improved &lt; Poor\n\n# 구체적으로 표시하는 것을 추천: 지정한 성분 순서대로 levels의 순서가 정해짐\nfactor(status, levels = c(\"Poor\", \"Improved\", \"Excellent\"),\n                                         order = TRUE)\n## [1] Poor      Improved  Excellent Poor     \n## Levels: Poor &lt; Improved &lt; Excellent\n\n# order가 없을시\nfactor(status, levels = c(\"Poor\", \"Improved\", \"Excellent\"))\n## [1] Poor      Improved  Excellent Poor     \n## Levels: Poor Improved Excellent\n\n# 대표적으로 성별을 코딩할 때: 숫자대신 레이블로 표시\nsex &lt;- c(1, 2, 1, 1, 1, 2, 2, 1)\nfactor(sex, levels = c(1, 2), labels = c(\"Male\", \"Female\"))\n## [1] Male   Female Male   Male   Male   Female Female Male  \n## Levels: Male Female\n\nsex_fct &lt;- factor(sex, levels = c(1, 2), labels = c(\"Male\", \"Female\"))\n\nlevels(sex) # 레벨 확인\n## NULL\nlevels(sex_fct) # 레벨 확인\n## [1] \"Male\"   \"Female\"\n\nsex\n## [1] 1 2 1 1 1 2 2 1\nsex_fct\n## [1] Male   Female Male   Male   Male   Female Female Male  \n## Levels: Male Female",
    "crumbs": [
      "R tutorial",
      "Base R"
    ]
  },
  {
    "objectID": "contents/baser.html#data-frame",
    "href": "contents/baser.html#data-frame",
    "title": "Base R",
    "section": "Data Frame",
    "text": "Data Frame\n\n데이터 프레임의 구성\n# 벡터들로부터 데이터 프레임 구성\npatientID &lt;- c(1, 2, 3, 4)\nage &lt;- c(25, 34, 28, 52)\ndiabetes &lt;- c(\"Type1\", \"Type2\", \"Type1\", \"Type1\")\nstatus &lt;- c(\"Poor\", \"Improved\", \"Excellent\", \"Poor\")\n\npatientdata &lt;- data.frame(patientID, age, diabetes, status)\n\npatientdata\n##   patientID age diabetes    status\n## 1         1  25    Type1      Poor\n## 2         2  34    Type2  Improved\n## 3         3  28    Type1 Excellent\n## 4         4  52    Type1      Poor\n\nmidterm &lt;- data.frame(english = c(90, 80, 60, 70),\n                      math = c(50, 60, 100, 20),\n                      class = c(1, 1, 2, 2))\nmidterm\n##   english math class\n## 1      90   50     1\n## 2      80   60     1\n## 3      60  100     2\n## 4      70   20     2\n\n\n원소의 추출 및 대체\n# 원소의 추출\npatientdata[1:2] # 변수의 열을 지정\n##   patientID age\n## 1         1  25\n## 2         2  34\n## 3         3  28\n## 4         4  52\n\npatientdata[c(\"diabetes\", \"status\")] # 열 이름을 지정\n##   diabetes    status\n## 1    Type1      Poor\n## 2    Type2  Improved\n## 3    Type1 Excellent\n## 4    Type1      Poor\n\npatientdata[c(1, 3), c(\"age\", \"status\")] # 행과 열을 모두 지정\n##   age    status\n## 1  25      Poor\n## 3  28 Excellent\n\npatientdata[c(1, 3), c(2, 4)]\n##   age    status\n## 1  25      Poor\n## 3  28 Excellent\n\npatientdata[, 1:2] # patientdata[1:2]과 동일, 빈칸은 모든 행을 의미\n##   patientID age\n## 1         1  25\n## 2         2  34\n## 3         3  28\n## 4         4  52\n\npatientdata[1:2, ] # 빈칸은 모든 열을 의미\n##   patientID age diabetes   status\n## 1         1  25    Type1     Poor\n## 2         2  34    Type2 Improved\n\npatientdata[-1] # 열 제외\n##   age diabetes    status\n## 1  25    Type1      Poor\n## 2  34    Type2  Improved\n## 3  28    Type1 Excellent\n## 4  52    Type1      Poor\n\npatientdata[-c(1, 3)] # 열 제외\n##   age    status\n## 1  25      Poor\n## 2  34  Improved\n## 3  28 Excellent\n## 4  52      Poor\n\npatientdata[-c(1:2), 2:4] # 행 제외 & 열 선택\n##   age diabetes    status\n## 3  28    Type1 Excellent\n## 4  52    Type1      Poor\n\n\n# 변수/열의 성분을 벡터로 추출: $ 또는 [[ ]]을 이용\npatientdata$age # $를 이용\n## [1] 25 34 28 52\n\nclass(patientdata$age) # numeric vector임을 확인\n## [1] \"numeric\"\n\npatientdata[[\"age\"]] # patientdata$age과 동일, [[ ]] doule bracket을 이용해 벡터로 추출\n## [1] 25 34 28 52\n\npatientdata[[2]] # 열의 위치를 이용해도 동일한 추출\n## [1] 25 34 28 52\n\npatientdata[\"age\"] # [ ] single bracket은 열을 선택하는 것으로 데이터 프레임으로 추출\n##   age\n## 1  25\n## 2  34\n## 3  28\n## 4  52\n\npatientdata[2] # 2번째 열을 추출; patientdata[\"age\"]과 동일\n##   age\n## 1  25\n## 2  34\n## 3  28\n## 4  52\n\n\n데이터의 추가 및 대체\n# 데이터 추가\npatientdata$gender &lt;- c(1, 1, 2, 2) \n\npatientdata\n##   patientID age diabetes    status gender\n## 1         1  25    Type1      Poor      1\n## 2         2  34    Type2  Improved      1\n## 3         3  28    Type1 Excellent      2\n## 4         4  52    Type1      Poor      2\n\n# 데이터 대체\npatientdata[c(1,3), \"age\"] # 혼동: 원칙적으로 데이터프레임으로 추출되어야하나 벡터로 추출됨\n## [1] 25 28\n\npatientdata[c(1,3), \"age\"] &lt;- c(88, 99)\npatientdata\n##   patientID age diabetes    status gender\n## 1         1  88    Type1      Poor      1\n## 2         2  34    Type2  Improved      1\n## 3         3  99    Type1 Excellent      2\n## 4         4  52    Type1      Poor      2\n\n# 참고\nrow.names(patientdata) # 데이터 프레임의 행 이름\n## [1] \"1\" \"2\" \"3\" \"4\"\n\nrow.names(patientdata) &lt;- c(\"a\", \"b\", \"c\", \"d\")\npatientdata\n##   patientID age diabetes    status gender\n## a         1  88    Type1      Poor      1\n## b         2  34    Type2  Improved      1\n## c         3  99    Type1 Excellent      2\n## d         4  52    Type1      Poor      2",
    "crumbs": [
      "R tutorial",
      "Base R"
    ]
  },
  {
    "objectID": "contents/baser.html#tibble",
    "href": "contents/baser.html#tibble",
    "title": "Base R",
    "section": "Tibble",
    "text": "Tibble\n기존 data.frame의 단점을 보안한 tidyverse에서 기본이 되는 데이터 형식\n\nData frame vs. tibble\nPrinting의 차이\ncps &lt;- mosaicData::CPS85 # data.frame\ncps\n#   wage educ race sex hispanic south married exper union age   sector\n# 1  9.0   10    W   M       NH    NS Married    27   Not  43    const\n# 2  5.5   12    W   M       NH    NS Married    20   Not  38    sales\n# 3  3.8   12    W   F       NH    NS  Single     4   Not  22    sales\n# 4 10.5   12    W   F       NH    NS Married    29   Not  47 clerical\n# 5 15.0   12    W   M       NH    NS Married    40 Union  58    const\n# 6  9.0   16    W   F       NH    NS Married    27   Not  49 clerical\n...\n\ncps_tibble &lt;- as_tibble(cps)\ncps_tibble\n# # A tibble: 534 × 11\n#    wage  educ race  sex   hispanic south married exper union   age sector  \n#   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n# 1   9      10 W     M     NH       NS    Married    27 Not      43 const   \n# 2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales   \n# 3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales   \n# 4  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n# 5  15      12 W     M     NH       NS    Married    40 Union    58 const   \n# 6   9      16 W     F     NH       NS    Married    27 Not      49 clerical\n# # … with 528 more rows\n그 외의 차이는 R for Data Science/10.3 Tibbles vs. data.frame을 참고",
    "crumbs": [
      "R tutorial",
      "Base R"
    ]
  },
  {
    "objectID": "contents/notice.html#중간시험-대체-과제",
    "href": "contents/notice.html#중간시험-대체-과제",
    "title": "Notice",
    "section": "중간시험 대체 과제",
    "text": "중간시험 대체 과제",
    "crumbs": [
      "Notice"
    ]
  },
  {
    "objectID": "contents/notice.html#기말시험",
    "href": "contents/notice.html#기말시험",
    "title": "Notice",
    "section": "기말시험",
    "text": "기말시험",
    "crumbs": [
      "Notice"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "contents/tidyverse.html",
    "href": "contents/tidyverse.html",
    "title": "Tidyverse",
    "section": "",
    "text": "함수들: print(), glimpse(), summary(), count()\n() 안에 들어가는 것을 argument라고 부름\n\nlibrary(tidyverse)\n\ncps &lt;- as_tibble(mosaicData::CPS85) # mosaicData package의 CPS85 데이터셋\nprint(cps) # print 생략!\n\n# A tibble: 534 x 11\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1   9      10 W     M     NH       NS    Married    27 Not      43 const   \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales   \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales   \n4  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n5  15      12 W     M     NH       NS    Married    40 Union    58 const   \n6   9      16 W     F     NH       NS    Married    27 Not      49 clerical\n# i 528 more rows\n\n\n\n\n\n\n\n\nprint()\n\n\n\n강의 노트에서 print()를 쓰는 것은 jupyter notebook에서 data frame을 표시하는 방식때문이므로 무시하셔도 됩니다.\n\n\n보통 print()없이 데이터 프레임을 살펴보지만, print()을 이용하면, 표시되는 방식을 조정해서 볼 수 있음.\n\nprint(cps, n = 3) # 처음 3개 행\n\n# A tibble: 534 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector\n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n1   9      10 W     M     NH       NS    Married    27 Not      43 const \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales \n# … with 531 more rows\n\n\n\n\n\n\n\n\ntip: print() 옵션\n\n\n\n\n\nprint(tibble, n = 10, width = Inf) # 10개의 rows와 모든 columns\n기본 셋팅을 변경하려면\noptions(tibble.print_min = 10, tibble.width = Inf)\nColumns/변수들이 많은 경우 화면에서 다음과 같이 축약되어 나오는데, 이를 다 보려면\nprint(nycflights13::flights) # nycflights13 패키지의 flights 데이터\n# # A tibble: 336,776 × 19\n#    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;       &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n# 1  2013     1     1      517         515       2     830     819      11 UA     \n# 2  2013     1     1      533         529       4     850     830      20 UA     \n# 3  2013     1     1      542         540       2     923     850      33 AA     \n# 4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n# 5  2013     1     1      554         600      -6     812     837     -25 DL     \n# 6  2013     1     1      554         558      -4     740     728      12 UA     \n# # … with 336,770 more rows, 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;,\n# #   origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,\n# #   minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, and abbreviated variable names\n# #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\nprint(nycflights13::flights, n = 3, width = Inf) # 가로 열의 개수: Inf (모든 열)\n# # A tibble: 336,776 × 19\n#    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n# 1  2013     1     1      517            515         2      830            819\n# 2  2013     1     1      533            529         4      850            830\n# 3  2013     1     1      542            540         2      923            850\n#   arr_delay carrier flight tailnum origin dest  air_time distance  hour minute\n#       &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n# 1        11 UA        1545 N14228  EWR    IAH        227     1400     5     15\n# 2        20 UA        1714 N24211  LGA    IAH        227     1416     5     29\n# 3        33 AA        1141 N619AA  JFK    MIA        160     1089     5     40\n#   time_hour          \n#   &lt;dttm&gt;             \n# 1 2013-01-01 05:00:00\n# 2 2013-01-01 05:00:00\n# 3 2013-01-01 05:00:00\n# # … with 336,773 more rows\n\n\n\n많은 변수들을 간략히 보는 방법으로는 glimpse()\n\nglimpse(cps)\n\nRows: 534\nColumns: 11\n$ wage     &lt;dbl&gt; 9.00, 5.50, 3.80, 10.50, 15.00, 9.00, 9.57, 15.00, 11.00, 5.0…\n$ educ     &lt;int&gt; 10, 12, 12, 12, 12, 16, 12, 14, 8, 12, 17, 17, 14, 14, 12, 14…\n$ race     &lt;fct&gt; W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, NW, NW, W,…\n$ sex      &lt;fct&gt; M, M, F, F, M, F, F, M, M, F, M, M, M, M, M, M, M, M, M, M, F…\n$ hispanic &lt;fct&gt; NH, NH, NH, NH, NH, NH, NH, NH, NH, NH, Hisp, NH, Hisp, NH, N…\n$ south    &lt;fct&gt; NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, N…\n$ married  &lt;fct&gt; Married, Married, Single, Married, Married, Married, Married,…\n$ exper    &lt;int&gt; 27, 20, 4, 29, 40, 27, 5, 22, 42, 14, 18, 3, 4, 14, 35, 0, 7,…\n$ union    &lt;fct&gt; Not, Not, Not, Not, Union, Not, Union, Not, Not, Not, Not, No…\n$ age      &lt;int&gt; 43, 38, 22, 47, 58, 49, 23, 42, 56, 32, 41, 26, 24, 34, 53, 2…\n$ sector   &lt;fct&gt; const, sales, sales, clerical, const, clerical, service, sale…\n\n\n\n\n\n\n\n\nTip\n\n\n\n엑셀 스프레드시트처럼 보는 방법은\nEnvironment 패널에 보이는 cps 데이터셋 맨 끝에 네모난 마크를 클릭하거나,\nview(cps)\n\n\n변수들에 대한 통계치 요약 summary()\n\nsummary(cps)\n\n      wage             educ       race     sex     hispanic   south   \n Min.   : 1.000   Min.   : 2.00   NW: 67   F:245   Hisp: 27   NS:378  \n 1st Qu.: 5.250   1st Qu.:12.00   W :467   M:289   NH  :507   S :156  \n Median : 7.780   Median :12.00                                       \n Mean   : 9.024   Mean   :13.02                                       \n 3rd Qu.:11.250   3rd Qu.:15.00                                       \n Max.   :44.500   Max.   :18.00                                       \n                                                                      \n    married        exper         union          age             sector   \n Married:350   Min.   : 0.00   Not  :438   Min.   :18.00   prof    :105  \n Single :184   1st Qu.: 8.00   Union: 96   1st Qu.:28.00   clerical: 97  \n               Median :15.00               Median :35.00   service : 83  \n               Mean   :17.82               Mean   :36.83   manuf   : 68  \n               3rd Qu.:26.00               3rd Qu.:44.00   other   : 68  \n               Max.   :55.00               Max.   :64.00   manag   : 55  \n                                                           (Other) : 58  \n\n\n카테고리별 개수를 세주는 count()\nNumber(수)에 대해서도 적용 가능: ex. educ 수준 2, 3, … 18 각각에 대해서\n\ncps |&gt;  # pipe operator: alt + . (option + .)\n    count(sector) |&gt;\n    print() # 생략해도 됨\n\n# A tibble: 8 × 2\n  sector       n\n  &lt;fct&gt;    &lt;int&gt;\n1 clerical    97\n2 const       20\n3 manag       55\n4 manuf       68\n5 other       68\n6 prof       105\n7 sales       38\n8 service     83\n\n\n\ncps |&gt;\n    count(sex, married) |&gt;\n    print()\n\n# A tibble: 4 × 3\n  sex   married     n\n  &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n1 F     Married   162\n2 F     Single     83\n3 M     Married   188\n4 M     Single    101\n\n\n\n\n\n\n\n\nPipe operator\n\n\n\n|&gt; 또는 %&gt;% (’then’의 의미로…)\nx |&gt; f(y) # f(x, y),\nx |&gt; f(y) |&gt; g(z) # g(f(x, y), z)\nsummary(cps) 는 다음과 같음\ncps |&gt;\n    summary()\ncount(cps, sector)는 다음과 같음\ncps |&gt; \n    count(sector)",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/tidyverse.html#inspecting-data",
    "href": "contents/tidyverse.html#inspecting-data",
    "title": "Tidyverse",
    "section": "",
    "text": "함수들: print(), glimpse(), summary(), count()\n() 안에 들어가는 것을 argument라고 부름\n\nlibrary(tidyverse)\n\ncps &lt;- as_tibble(mosaicData::CPS85) # mosaicData package의 CPS85 데이터셋\nprint(cps) # print 생략!\n\n# A tibble: 534 x 11\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1   9      10 W     M     NH       NS    Married    27 Not      43 const   \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales   \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales   \n4  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n5  15      12 W     M     NH       NS    Married    40 Union    58 const   \n6   9      16 W     F     NH       NS    Married    27 Not      49 clerical\n# i 528 more rows\n\n\n\n\n\n\n\n\nprint()\n\n\n\n강의 노트에서 print()를 쓰는 것은 jupyter notebook에서 data frame을 표시하는 방식때문이므로 무시하셔도 됩니다.\n\n\n보통 print()없이 데이터 프레임을 살펴보지만, print()을 이용하면, 표시되는 방식을 조정해서 볼 수 있음.\n\nprint(cps, n = 3) # 처음 3개 행\n\n# A tibble: 534 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector\n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n1   9      10 W     M     NH       NS    Married    27 Not      43 const \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales \n# … with 531 more rows\n\n\n\n\n\n\n\n\ntip: print() 옵션\n\n\n\n\n\nprint(tibble, n = 10, width = Inf) # 10개의 rows와 모든 columns\n기본 셋팅을 변경하려면\noptions(tibble.print_min = 10, tibble.width = Inf)\nColumns/변수들이 많은 경우 화면에서 다음과 같이 축약되어 나오는데, 이를 다 보려면\nprint(nycflights13::flights) # nycflights13 패키지의 flights 데이터\n# # A tibble: 336,776 × 19\n#    year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n#   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;       &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n# 1  2013     1     1      517         515       2     830     819      11 UA     \n# 2  2013     1     1      533         529       4     850     830      20 UA     \n# 3  2013     1     1      542         540       2     923     850      33 AA     \n# 4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n# 5  2013     1     1      554         600      -6     812     837     -25 DL     \n# 6  2013     1     1      554         558      -4     740     728      12 UA     \n# # … with 336,770 more rows, 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;,\n# #   origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,\n# #   minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, and abbreviated variable names\n# #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\nprint(nycflights13::flights, n = 3, width = Inf) # 가로 열의 개수: Inf (모든 열)\n# # A tibble: 336,776 × 19\n#    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n# 1  2013     1     1      517            515         2      830            819\n# 2  2013     1     1      533            529         4      850            830\n# 3  2013     1     1      542            540         2      923            850\n#   arr_delay carrier flight tailnum origin dest  air_time distance  hour minute\n#       &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n# 1        11 UA        1545 N14228  EWR    IAH        227     1400     5     15\n# 2        20 UA        1714 N24211  LGA    IAH        227     1416     5     29\n# 3        33 AA        1141 N619AA  JFK    MIA        160     1089     5     40\n#   time_hour          \n#   &lt;dttm&gt;             \n# 1 2013-01-01 05:00:00\n# 2 2013-01-01 05:00:00\n# 3 2013-01-01 05:00:00\n# # … with 336,773 more rows\n\n\n\n많은 변수들을 간략히 보는 방법으로는 glimpse()\n\nglimpse(cps)\n\nRows: 534\nColumns: 11\n$ wage     &lt;dbl&gt; 9.00, 5.50, 3.80, 10.50, 15.00, 9.00, 9.57, 15.00, 11.00, 5.0…\n$ educ     &lt;int&gt; 10, 12, 12, 12, 12, 16, 12, 14, 8, 12, 17, 17, 14, 14, 12, 14…\n$ race     &lt;fct&gt; W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, NW, NW, W,…\n$ sex      &lt;fct&gt; M, M, F, F, M, F, F, M, M, F, M, M, M, M, M, M, M, M, M, M, F…\n$ hispanic &lt;fct&gt; NH, NH, NH, NH, NH, NH, NH, NH, NH, NH, Hisp, NH, Hisp, NH, N…\n$ south    &lt;fct&gt; NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, NS, N…\n$ married  &lt;fct&gt; Married, Married, Single, Married, Married, Married, Married,…\n$ exper    &lt;int&gt; 27, 20, 4, 29, 40, 27, 5, 22, 42, 14, 18, 3, 4, 14, 35, 0, 7,…\n$ union    &lt;fct&gt; Not, Not, Not, Not, Union, Not, Union, Not, Not, Not, Not, No…\n$ age      &lt;int&gt; 43, 38, 22, 47, 58, 49, 23, 42, 56, 32, 41, 26, 24, 34, 53, 2…\n$ sector   &lt;fct&gt; const, sales, sales, clerical, const, clerical, service, sale…\n\n\n\n\n\n\n\n\nTip\n\n\n\n엑셀 스프레드시트처럼 보는 방법은\nEnvironment 패널에 보이는 cps 데이터셋 맨 끝에 네모난 마크를 클릭하거나,\nview(cps)\n\n\n변수들에 대한 통계치 요약 summary()\n\nsummary(cps)\n\n      wage             educ       race     sex     hispanic   south   \n Min.   : 1.000   Min.   : 2.00   NW: 67   F:245   Hisp: 27   NS:378  \n 1st Qu.: 5.250   1st Qu.:12.00   W :467   M:289   NH  :507   S :156  \n Median : 7.780   Median :12.00                                       \n Mean   : 9.024   Mean   :13.02                                       \n 3rd Qu.:11.250   3rd Qu.:15.00                                       \n Max.   :44.500   Max.   :18.00                                       \n                                                                      \n    married        exper         union          age             sector   \n Married:350   Min.   : 0.00   Not  :438   Min.   :18.00   prof    :105  \n Single :184   1st Qu.: 8.00   Union: 96   1st Qu.:28.00   clerical: 97  \n               Median :15.00               Median :35.00   service : 83  \n               Mean   :17.82               Mean   :36.83   manuf   : 68  \n               3rd Qu.:26.00               3rd Qu.:44.00   other   : 68  \n               Max.   :55.00               Max.   :64.00   manag   : 55  \n                                                           (Other) : 58  \n\n\n카테고리별 개수를 세주는 count()\nNumber(수)에 대해서도 적용 가능: ex. educ 수준 2, 3, … 18 각각에 대해서\n\ncps |&gt;  # pipe operator: alt + . (option + .)\n    count(sector) |&gt;\n    print() # 생략해도 됨\n\n# A tibble: 8 × 2\n  sector       n\n  &lt;fct&gt;    &lt;int&gt;\n1 clerical    97\n2 const       20\n3 manag       55\n4 manuf       68\n5 other       68\n6 prof       105\n7 sales       38\n8 service     83\n\n\n\ncps |&gt;\n    count(sex, married) |&gt;\n    print()\n\n# A tibble: 4 × 3\n  sex   married     n\n  &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n1 F     Married   162\n2 F     Single     83\n3 M     Married   188\n4 M     Single    101\n\n\n\n\n\n\n\n\nPipe operator\n\n\n\n|&gt; 또는 %&gt;% (’then’의 의미로…)\nx |&gt; f(y) # f(x, y),\nx |&gt; f(y) |&gt; g(z) # g(f(x, y), z)\nsummary(cps) 는 다음과 같음\ncps |&gt;\n    summary()\ncount(cps, sector)는 다음과 같음\ncps |&gt; \n    count(sector)",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/tidyverse.html#rows",
    "href": "contents/tidyverse.html#rows",
    "title": "Tidyverse",
    "section": "Rows",
    "text": "Rows\n행에 적용되는 함수들\nfilter(), arrange(), distinct()\n\nfilter()\n조건에 맞는 행을 선택\n\nConditional operators:\n&gt;, &gt;=, &lt;, &lt;=,\n== (equal to), != (not equal to)\n& (and) | (or)\n! (not)\n%in% (includes)\n\n\n# 임금(wage)가 10이상인 사람들\ncps |&gt;\n    filter(wage &gt;= 10) |&gt;\n    print()\n\n# A tibble: 184 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n2  15      12 W     M     NH       NS    Married    40 Union    58 const   \n3  15      14 W     M     NH       NS    Single     22 Not      42 sales   \n4  11       8 W     M     NH       NS    Married    42 Not      56 manuf   \n5  25.0    17 W     M     Hisp     NS    Married    18 Not      41 prof    \n6  20.4    17 W     M     NH       NS    Single      3 Not      26 prof    \n# … with 178 more rows\n\n\n\n# 임금(wage)가 10이상이고 여성(F)들\ncps |&gt;\n    filter(wage &gt;= 10 & sex == \"F\") |&gt;\n    print()\n\n# A tibble: 62 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n2  11.2    17 NW    F     NH       NS    Married    32 Not      55 clerical\n3  25.0    17 W     F     NH       NS    Single      5 Not      28 prof    \n4  12.6    17 W     F     NH       NS    Married    13 Not      36 manag   \n5  11.7    16 W     F     NH       NS    Single     42 Not      64 clerical\n6  12.5    15 W     F     NH       NS    Married     6 Not      27 clerical\n# … with 56 more rows\n\n\n\n# 간부급(management)과 전문직(professional)에 종사하는 사람들\ncps |&gt;\n    filter(sector == \"manag\" | sector == \"prof\") |&gt;\n    print()\n\n# A tibble: 160 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector\n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n1  25.0    17 W     M     Hisp     NS    Married    18 Not      41 prof  \n2  20.4    17 W     M     NH       NS    Single      3 Not      26 prof  \n3  10      16 W     M     Hisp     NS    Married     7 Union    29 manag \n4  15      16 NW    M     NH       NS    Married    26 Union    48 manag \n5  25.0    17 W     F     NH       NS    Single      5 Not      28 prof  \n6  10      14 W     M     NH       NS    Married    22 Not      42 prof  \n# … with 154 more rows\n\n\n다음과 같이 편리하게 %in%을 이용하여 여러 항목을 포함하는, 즉 |와 ==를 합친 조건문을 생성\n즉, include인지 판별\n\n# A shorter way to select sectors for management or professional\ncps |&gt;\n    filter(sector %in% c(\"manag\", \"prof\")) |&gt;\n    print()\n\n# A tibble: 160 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector\n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n1  25.0    17 W     M     Hisp     NS    Married    18 Not      41 prof  \n2  20.4    17 W     M     NH       NS    Single      3 Not      26 prof  \n3  10      16 W     M     Hisp     NS    Married     7 Union    29 manag \n4  15      16 NW    M     NH       NS    Married    26 Union    48 manag \n5  25.0    17 W     F     NH       NS    Single      5 Not      28 prof  \n6  10      14 W     M     NH       NS    Married    22 Not      42 prof  \n# … with 154 more rows\n\n\n\n\n\n\n\n\nImportant\n\n\n\nfilter()로 얻은 데이터 프레임은 원래 데이터 프레임을 수정하는 것이 아니므로 계속 사용하려면 저장해야 함\n이후 모든 함수들에 대해서도 마찬가지\nprestige &lt;- cps |&gt;\n    filter(sector %in% c(\"manag\", \"prof\"))\n\nprestige\n#    wage  educ race  sex   hispanic south married exper union   age sector\n#   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n# 1  25.0    17 W     M     Hisp     NS    Married    18 Not      41 prof  \n# 2  20.4    17 W     M     NH       NS    Single      3 Not      26 prof  \n# 3  10      16 W     M     Hisp     NS    Married     7 Union    29 manag \n# ...\n\n\n\n\n\n\n\n\nTip\n\n\n\n잦은 실수들\ncps |&gt;\n    filter(sex = \"F\") # \"==\" vs. \"=\"\ncps |&gt;\n    filter(sector == \"manage\" | \"prof\") # | 전후 모두 완결된 조건문 필요\n\n\n\n\narrange()\nColumn의 값을 기준으로 row를 정렬\n\n# 교육정도(educ)와 임금(wage)에 따라 오름차순으로 정렬\ncps |&gt;\n    arrange(educ, wage) |&gt;\n    print(n = 10)\n\n# A tibble: 534 × 11\n    wage  educ race  sex   hispanic south married exper union   age sector \n   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;  \n 1  3.75     2 W     M     Hisp     NS    Single     16 Not      24 service\n 2  7        3 W     M     Hisp     S     Married    55 Not      64 manuf  \n 3  6        4 W     M     NH       NS    Married    54 Not      64 service\n 4 14        5 W     M     NH       S     Married    44 Not      55 const  \n 5  3        6 W     F     Hisp     NS    Married    43 Union    55 manuf  \n 6  4.62     6 NW    F     NH       S     Single     33 Not      45 manuf  \n 7  5.75     6 W     M     NH       S     Married    45 Not      57 manuf  \n 8  3.35     7 W     M     NH       S     Married    43 Not      56 manuf  \n 9  4.5      7 W     M     Hisp     S     Married    14 Not      27 service\n10  6        7 W     F     NH       S     Married    15 Not      28 manuf  \n# … with 524 more rows\n\n\ndesc()을 이용하면 내림차순으로 정렬\n\n# educ을 내림차순으로 정렬\ncps |&gt;\n    arrange(desc(educ)) |&gt;\n    print(n = 10)\n\n# A tibble: 534 × 11\n    wage  educ race  sex   hispanic south married exper union   age sector\n   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n 1 15       18 W     M     NH       NS    Married    12 Not      36 prof  \n 2 14.0     18 W     F     NH       NS    Married    14 Not      38 manag \n 3 13.5     18 W     M     NH       NS    Married    14 Union    38 prof  \n 4 20       18 W     F     NH       NS    Married    19 Not      43 manag \n 5  7       18 W     M     NH       NS    Married    33 Not      57 prof  \n 6 11.2     18 W     M     NH       NS    Married    19 Not      43 prof  \n 7  5.71    18 W     M     NH       NS    Married     3 Not      27 prof  \n 8 18       18 W     M     NH       NS    Married    15 Not      39 prof  \n 9 19       18 W     M     NH       NS    Single     13 Not      37 manag \n10 22.8     18 W     F     NH       NS    Single     37 Not      61 prof  \n# … with 524 more rows\n\n\narrange()와 filter()를 함께 사용하여 좀 더 복잡한 문제를 해결할 수 있음\n\n# 높은 지위의 섹터에서 일하는 사람들 중 임금이 상위에 있는 사람들\ncps |&gt;\n    filter(sector == \"manage\" | sector == \"prof\") |&gt;\n    arrange(desc(wage)) |&gt;\n    print()\n\n# A tibble: 105 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector\n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; \n1  25.0    17 W     M     Hisp     NS    Married    18 Not      41 prof  \n2  25.0    17 W     F     NH       NS    Single      5 Not      28 prof  \n3  25.0    17 W     M     NH       NS    Married    31 Not      54 prof  \n4  25.0    16 W     F     NH       S     Single      5 Not      27 prof  \n5  23.2    17 NW    F     NH       NS    Married    25 Union    48 prof  \n6  22.8    18 W     F     NH       NS    Single     37 Not      61 prof  \n# … with 99 more rows\n\n\n\n\ndistinct()**\n유티크한 조합들을 리스트\n\ncps |&gt;\n    distinct(sector, sex) |&gt;\n    print()\n\n# A tibble: 15 × 2\n   sector   sex  \n   &lt;fct&gt;    &lt;fct&gt;\n 1 const    M    \n 2 sales    M    \n 3 sales    F    \n 4 clerical F    \n 5 service  F    \n 6 manuf    M    \n 7 prof     M    \n 8 service  M    \n 9 other    M    \n10 clerical M    \n11 manag    M    \n12 prof     F    \n13 manag    F    \n14 manuf    F    \n15 other    F",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/tidyverse.html#columns",
    "href": "contents/tidyverse.html#columns",
    "title": "Tidyverse",
    "section": "Columns",
    "text": "Columns\n열에 적용되는 함수들\nmutate(), select(), rename()\n\nmutate()\nColumns/변수들로부터 값을 계산하여 새로운 변수를 만듦\n\ntips &lt;- as_tibble(reshape::tips) # reshpae 패키지 안에 tips 데이터셋\ntips |&gt; print()\n\n# A tibble: 244 x 7\n  total_bill   tip sex    smoker day   time    size\n       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;  &lt;int&gt;\n1       17.0  1.01 Female No     Sun   Dinner     2\n2       10.3  1.66 Male   No     Sun   Dinner     3\n3       21.0  3.5  Male   No     Sun   Dinner     3\n4       23.7  3.31 Male   No     Sun   Dinner     2\n5       24.6  3.61 Female No     Sun   Dinner     4\n6       25.3  4.71 Male   No     Sun   Dinner     4\n# i 238 more rows\n\n\n\ntips |&gt;\n    mutate(\n        tip_pct = tip / total_bill * 100,\n        tip_pct_per = tip_pct / size\n    ) |&gt;\n    print()\n\n# A tibble: 244 × 9\n  total_bill   tip sex    smoker day   time    size tip_pct tip_pct_per\n       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;  &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1       17.0  1.01 Female No     Sun   Dinner     2    5.94        2.97\n2       10.3  1.66 Male   No     Sun   Dinner     3   16.1         5.35\n3       21.0  3.5  Male   No     Sun   Dinner     3   16.7         5.55\n4       23.7  3.31 Male   No     Sun   Dinner     2   14.0         6.99\n5       24.6  3.61 Female No     Sun   Dinner     4   14.7         3.67\n6       25.3  4.71 Male   No     Sun   Dinner     4   18.6         4.66\n# … with 238 more rows\n\n\n\n\nselect()\nColumns/변수를 선택\n\ntips |&gt;\n    select(total_bill, tip, day, time) |&gt;\n    print()\n\n# A tibble: 244 × 4\n  total_bill   tip day   time  \n       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; \n1       17.0  1.01 Sun   Dinner\n2       10.3  1.66 Sun   Dinner\n3       21.0  3.5  Sun   Dinner\n4       23.7  3.31 Sun   Dinner\n5       24.6  3.61 Sun   Dinner\n6       25.3  4.71 Sun   Dinner\n# … with 238 more rows\n\n\n\n# tip에서 smoker까지, 그리고 size columns 선택\ntips |&gt;\n    select(tip:smoker, size) |&gt;  # select(2:4, 7)처럼 number로 선택가능\n    print()\n\n# A tibble: 244 × 4\n    tip sex    smoker  size\n  &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;int&gt;\n1  1.01 Female No         2\n2  1.66 Male   No         3\n3  3.5  Male   No         3\n4  3.31 Male   No         2\n5  3.61 Female No         4\n6  4.71 Male   No         4\n# … with 238 more rows\n\n\n\n# sex에서 day까지 columns은 제외하고\ntips |&gt;\n    select(!sex:day) |&gt; # !: not\n    print()\n\n# A tibble: 244 × 4\n  total_bill   tip time    size\n       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;int&gt;\n1       17.0  1.01 Dinner     2\n2       10.3  1.66 Dinner     3\n3       21.0  3.5  Dinner     3\n4       23.7  3.31 Dinner     2\n5       24.6  3.61 Dinner     4\n6       25.3  4.71 Dinner     4\n# … with 238 more rows\n\n\n\n# factor 타입의 변수들만 선택: 함수를 이용\ntips |&gt;\n    select(where(is.factor)) |&gt;  # 다른 함수들: is.numeric, is.character\n    print()\n\n# A tibble: 244 × 4\n  sex    smoker day   time  \n  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt; \n1 Female No     Sun   Dinner\n2 Male   No     Sun   Dinner\n3 Male   No     Sun   Dinner\n4 Male   No     Sun   Dinner\n5 Female No     Sun   Dinner\n6 Male   No     Sun   Dinner\n# … with 238 more rows\n\n\n다양한 select()의 선택방법은 ?select로 help참고\n예를 들어, starts_with(\"abc\")는 abc로 시작하는 열의 이름을 가진 열들\n\n\n\n\n\n\nNote\n\n\n\nBase R에서 행과 열의 선택과 비교하면,\ncps[2:5, c(\"wage\", \"married\")] # 2~5행과 wage, married열\n# # A tibble: 4 × 2\n#    wage married\n#   &lt;dbl&gt; &lt;fct&gt;  \n# 1   5.5 Married\n# 2   3.8 Single \n# 3  10.5 Married\n# 4  15   Married\n\ncps |&gt; \n    select(wage, married) |&gt; \n    slice(2:5) # 행을 선택\n\n\n\n\nrelocate()\nColumns의 순서를 변경\n\ntips |&gt; print(n = 2)\n\n# A tibble: 244 x 7\n  total_bill   tip sex    smoker day   time    size\n       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;  &lt;int&gt;\n1       17.0  1.01 Female No     Sun   Dinner     2\n2       10.3  1.66 Male   No     Sun   Dinner     3\n# i 242 more rows\n\n\n\ntips |&gt; \n    relocate(day, time) |&gt;  # day, time을 맨 앞으로 이동\n    print(n = 2)\n\n# A tibble: 244 x 7\n  day   time   total_bill   tip sex    smoker  size\n  &lt;fct&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;int&gt;\n1 Sun   Dinner       17.0  1.01 Female No         2\n2 Sun   Dinner       10.3  1.66 Male   No         3\n# i 242 more rows\n\n\n\ntips |&gt; \n    relocate(sex:time, tip) |&gt;  # sex부터 time까지와 tip을 맨 앞으로 이동\n    print(n = 2)\n\n# A tibble: 244 x 7\n  sex    smoker day   time     tip total_bill  size\n  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1 Female No     Sun   Dinner  1.01       17.0     2\n2 Male   No     Sun   Dinner  1.66       10.3     3\n# i 242 more rows\n\n\n\ntips |&gt; \n    relocate(day:size, .after = tip) |&gt;   # .before: 앞에, .after: 뒤에\n    print(n = 2)\n\n# A tibble: 244 x 7\n  total_bill   tip day   time    size sex    smoker\n       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; \n1       17.0  1.01 Sun   Dinner     2 Female No    \n2       10.3  1.66 Sun   Dinner     3 Male   No    \n# i 242 more rows\n\n\n\n\nrename()\nColumns의 이름을 변경\n\ncps |&gt;\n    rename(education = educ, marital = married) |&gt; # new = old\n    print()\n\n# A tibble: 534 × 11\n   wage education race  sex   hispanic south marital exper union   age sector  \n  &lt;dbl&gt;     &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1   9          10 W     M     NH       NS    Married    27 Not      43 const   \n2   5.5        12 W     M     NH       NS    Married    20 Not      38 sales   \n3   3.8        12 W     F     NH       NS    Single      4 Not      22 sales   \n4  10.5        12 W     F     NH       NS    Married    29 Not      47 clerical\n5  15          12 W     M     NH       NS    Married    40 Union    58 const   \n6   9          16 W     F     NH       NS    Married    27 Not      49 clerical\n# … with 528 more rows\n\n\n변수를 select할 때 동시에 이름도 바꿀 수 있음\n\ncps |&gt;\n    select(education = educ, marital = married) |&gt; # new = old\n    print()\n\n# A tibble: 534 × 2\n  education marital\n      &lt;int&gt; &lt;fct&gt;  \n1        10 Married\n2        12 Married\n3        12 Single \n4        12 Married\n5        12 Married\n6        16 Married\n# … with 528 more rows",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/tidyverse.html#groups",
    "href": "contents/tidyverse.html#groups",
    "title": "Tidyverse",
    "section": "Groups",
    "text": "Groups\n분석에서는 자주 카테고리별로 데이터를 나누어 통계치를 계산하곤 하는데,\ngroup_by()와 summarise()의 두 함수를 함께 사용하여 가장 자주 사용하게 됨\n\ngroup_by()\n데이터셋을 분석을 위해 의미있는 그룹으로 나눔\n다음은 성별로 데이터셋을 나눈 것인데, 실제 데이터를 수정하는 것은 아니고, 내부적으로 grouping되어 있음.\n맨 위 줄에 보면 Groups:  sex [2]로 표시되어 grouped data frame임을 명시함\n\ncps |&gt;\n    group_by(sex) |&gt; \n    print()\n\n# A tibble: 534 × 11\n# Groups:   sex [2]\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1   9      10 W     M     NH       NS    Married    27 Not      43 const   \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales   \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales   \n4  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n5  15      12 W     M     NH       NS    Married    40 Union    58 const   \n6   9      16 W     F     NH       NS    Married    27 Not      49 clerical\n# … with 528 more rows\n\n\n\n\nsummarise()\nsummarize()와 동일\ngroup별로 통계치를 구해 하나의 행으로 산출\n\n# 남녀별로 임금의 평균을 구함\ncps |&gt;\n    group_by(sex) |&gt;\n    summarise(\n        avg_wage = mean(wage, na.rm = TRUE),  # mean(): 평균, na.rm: NA를 remove할 것인가\n        n = n()  # n(): 개수\n    ) |&gt;\n    print()\n\n# A tibble: 2 × 3\n  sex   avg_wage     n\n  &lt;fct&gt;    &lt;dbl&gt; &lt;int&gt;\n1 F         7.88   245\n2 M         9.99   289\n\n\n2개 이상의 변수들로 grouping할 수 있음\n\ncps |&gt;\n    group_by(sex, married) |&gt;\n    summarize(\n        ave_wage = mean(wage),\n        sd_wage = sd(wage)) |&gt;\n    print()\n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 4 × 4\n# Groups:   sex [2]\n  sex   married ave_wage sd_wage\n  &lt;fct&gt; &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n1 F     Married     7.68    3.73\n2 F     Single      8.26    6.23\n3 M     Married    10.9     5.35\n4 M     Single      8.35    4.78\n\n\n이때, 결과 데이터 프레임은 sex로 grouping되어 있음.\ngrouping을 해제하려면 ungroup()이 필요함.\n그렇지 않으면, 저 결과는 sex로 grouped data frame임\n\nUseful summary functions\n자세한 사항은 R for Data Science/Data transformation\n\nMeasures of location: mean(), median()\nMeasures of spread: sd(), IQR(), mad()\nMeasures of rank: min(), max(), quantile(x, 0.25)\nMeasures of position: min_rank(), first(), nth(x, 2), last()\nMeasures of count: count(), n_distinct()",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/tidyverse.html#missing",
    "href": "contents/tidyverse.html#missing",
    "title": "Tidyverse",
    "section": "Missing",
    "text": "Missing\nR에서 missing values (결측치)는 NA로 표시\nNaN (not a number)는 주로 계산 결과로 나오는데, 예들 들어 0으로 나눌 때처럼, R에서는 NA로 취급되니 크게 신경쓰지 않아도 됨. 자세한 사항은 R for Data Science/Missing values 참고\nNA는 다음과 같은 성질을 지님\nNA &gt; 5\n#&gt; [1] NA\n10 == NA\n#&gt; [1] NA\nNA + 10\n#&gt; [1] NA\nNA / 2\n#&gt; [1] NA\nNA == NA\n#&gt; [1] NA\n\nx &lt;- NA\nis.na(x)\n#&gt; [1] TRUE\nNA는 filter()안의 조건문의 참거짓에 상관없이 모두 제외함\n\n실제로 조건문의 결과는 TRUE, FALSE로 이루어지짐\n\ndf &lt;- tibble(\n        one = c(1, NA, 3, 4, 2, NA), \n        two = c(2, 5, 3, NA, 10, NA), \n        three = c(\"a\", \"a\", \"a\", \"a\", \"b\", \"b\")\n    )\ndf\n#     one   two three\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n# 1     1     2 a    \n# 2    NA     5 a    \n# 3     3     3 a    \n# 4     4    NA a    \n# 5     2    10 b    \n# 6    NA    NA b    \n\nfilter(df, one &gt; 1)\n#     one   two three\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n# 1     3     3 a    \n# 2     4    NA a    \n# 3     2    10 b\n\n# NA를 포함하고자 할 때,\nfilter(df, one &gt; 1 | is.na(one))\n#     one   two three\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n# 1    NA     5 a    \n# 2     3     3 a    \n# 3     4    NA a    \n# 4     2    10 b    \n# 5    NA    NA b\n\n# NA를 포함하지 않은 행들만\nfilter(df, !is.na(one))\nfilter(df, !is.na(one) & !is.na(two)) # one, two 열에 모두 NA가 없는 행들만\n\nna.omit(df) # NA가 하나라도 있는 행은 모두 제거, 보통 결측치를 조심스럽게 대체한 후 사용\n#     one   two three\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n# 1     1     2 a    \n# 2     3     3 a    \n# 3     2    10 b \n\n# 함수 중에 NA를 직접 처리하는 경우들이 많음\nmean(df$one)\n## [1] NA\n\nmean(df$one, na.rm = TRUE) # NA removed\n## [1] 2.5\nna.rm = TRUE로 얻은 계산값에서 몇 개의 데이터로 계산되었는지 알기 위해서는\ndf |&gt; \n    group_by(three) |&gt; \n    summarise(\n        ave = mean(two, na.rm = TRUE), \n        n = n(), \n        n_notna = sum(!is.na(two))  # TRUE는 1로, FALSE는 0으로 계산됨\n    )\n#   three   ave     n n_notna\n#   &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;   &lt;int&gt;\n# 1 a      3.33     4       3\n# 2 b     10        2       1",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/tidyverse.html#summary",
    "href": "contents/tidyverse.html#summary",
    "title": "Tidyverse",
    "section": "Summary",
    "text": "Summary\n다음 dplyr 패키지의 기본 verb 함수들로 데이터를 가공하면서 필요한 통계치를 구함\n\n조건에 맞는 행들(관측치)만 필터링: filter()\n열을 재정렬: arrange()\n변수들의 선택: select()\n변수들과 함수들을 이용하여 새로운 변수를 생성: mutate()\n원하는 요약 통계치를 간추림: summarise()",
    "crumbs": [
      "R tutorial",
      "Tidyverse"
    ]
  },
  {
    "objectID": "contents/setup.html",
    "href": "contents/setup.html",
    "title": "환경설정",
    "section": "",
    "text": "R 다운로드 및 설치\n\nWindows인 경우 &gt; Download R for Windows &gt; base\nMac인 경우 &gt; Download R for macOS &gt; Apple silicon 또는 Intel Macs 선택\n\nRStudio 다운로드 및 설치\n\n2: Install RStudio",
    "crumbs": [
      "R tutorial",
      "Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#r-및-rstudio-설치",
    "href": "contents/setup.html#r-및-rstudio-설치",
    "title": "환경설정",
    "section": "",
    "text": "R 다운로드 및 설치\n\nWindows인 경우 &gt; Download R for Windows &gt; base\nMac인 경우 &gt; Download R for macOS &gt; Apple silicon 또는 Intel Macs 선택\n\nRStudio 다운로드 및 설치\n\n2: Install RStudio",
    "crumbs": [
      "R tutorial",
      "Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#rstudio-소개",
    "href": "contents/setup.html#rstudio-소개",
    "title": "환경설정",
    "section": "RStudio 소개",
    "text": "RStudio 소개\n4개의 패널로 구성\nProject 단위로 분석\n\n시작시 project을 새로 만들거나 불러와서 실행: filename.Rproj 형태로 저장\nFile &gt; New Project 또는 +박스그림 버튼 &gt; New Directory &gt; New Project\n\nDirectory name, Sub directory\n\n\nWorking directory\n\nproject에서 참조하는 최상위 폴더\n하위폴더 지시: 예) data/file.sav\n\nR script 생성, 저장\nRStudio 닫기, 열기\n\nWorkspace 저장 vs. R script 저장\nWorkspace save/load: .Rdata 형태로 저장\n\nSession\n\nRestart R\n\n\n환경설정: Tools &gt; Global Options\n“Save workspace to .RData on exit” 옵션: 종료시 working space 자동 저장\nCode\n\nsoft-wrap R source files\nUse native pipe operator\n\nAppearance\n\nZoom: 전체 보기 줌\nEdiotr font: Cascadia Mono (Win), Menlo (Mac)\nEditor font size: 글자 크기\nTheme: Tomorrow Night??",
    "crumbs": [
      "R tutorial",
      "Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#패키지의-설치",
    "href": "contents/setup.html#패키지의-설치",
    "title": "환경설정",
    "section": "패키지의 설치",
    "text": "패키지의 설치\n\n# 메뉴를 통한 설치\n\n# 명령어를 통한 설치\ninstall.packages(\"name\")\n\n# 수업에서 필요한 기본 패키지\ninstall.packages(\"tidyverse\") # 패키지들의 패키지\n\n## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.7     ✔ dplyr   1.0.9\n## ✔ tidyr   1.2.0     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1\n\n# 패키지들 간의 함수의 충돌에 대해서... mask\n\n# 추가 패키지\ninstall.packages(c(\"mosaicData\", \"palmerpenguins\")) # c(): combine items\n\n# 패키지 로드: 필요한 패키지는 세션마다 시행해야 함\nlibrary(\"name\")\n    e.g. library(tidyverse)",
    "crumbs": [
      "R tutorial",
      "Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#단축키",
    "href": "contents/setup.html#단축키",
    "title": "환경설정",
    "section": "단축키",
    "text": "단축키\n\n자동완성: tab\n\n현재 라인 실행: Ctrl+Enter (Win)   |   Command+Return (Mac)\nassignment operator (&lt;-) 입력: Alt+- (Win)   |   Option+- (Mac)\npipe operator (%&gt;%) 입력: Ctrl+Shift+M (Win)   |   Shift+Command+M (Mac)\nconsol에서 화살표 키\ncopy, paste\nundo, redo: Ctrl+Z / Ctrl+Shift+Z (Win)   |   Command+Z / Command+Shift+Z (Mac)\nCopy Lines Up/Down: Shift+Alt+Up/Down (Win)   |   Option+Command+Up or Down (Mac)\n\n단축키 변경: Tools &gt;&gt; modify keyboard shortcuts: e.g. pipe operator: Alt+.",
    "crumbs": [
      "R tutorial",
      "Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#도움말",
    "href": "contents/setup.html#도움말",
    "title": "환경설정",
    "section": "도움말",
    "text": "도움말\nhelp() 또는 ?\ne.g. help(factor), ?factor",
    "crumbs": [
      "R tutorial",
      "Setup"
    ]
  },
  {
    "objectID": "contents/interaction.html",
    "href": "contents/interaction.html",
    "title": "Interactions",
    "section": "",
    "text": "Load libraries\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\n이전 수업 링크",
    "crumbs": [
      "Keith's",
      "Path Modeling",
      "Interactions"
    ]
  },
  {
    "objectID": "contents/interaction.html#continuous-vs.-continuous",
    "href": "contents/interaction.html#continuous-vs.-continuous",
    "title": "Interactions",
    "section": "Continuous vs. Continuous",
    "text": "Continuous vs. Continuous\n나이가 듦(age)에 따른 지구력(endurance)의 감소가 운동을 한 기간(exercise)에 따라 변화하는가? (p.275)\nendurance: the number of minutes of sustained jogging on a treadmill\nexercise: the number of years of vigorous physical exercise\n\n\n연구자의 관심변수에 따라 다르게 표현될 수 있음.\n나이가 지구력에 미치는 부정적 영향을 운동이 완화시키는지 관심; 보효요인 (protective factor) &lt;-&gt; 위험요인 (risk factor)\n이 때, 운동 기간을 moderator (조절변수)라고 말하고, 그 moderating effect (조절효과)를 가지는지 검증.\n통계적으로는 나이과 운동기간이 서로 상호작용(interact)하여 지구력에 영향을 미치는 것으로 나타남.\n\nData: c07e01dt\n\nacad2 &lt;- read_csv('data/c07e01dt.csv')\nacad2 |&gt; print()\n\n# A tibble: 245 x 3\n     age exercise endurance\n   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1    60       10        18\n 2    40        9        36\n 3    29        2        51\n 4    47       10        18\n 5    48        9        23\n 6    42        6        30\n 7    55        8         8\n 8    43       19        40\n 9    39        9        28\n10    51       14        15\n# i 235 more rows\n\n\n\nmod_interact &lt;- lm(endurance ~ age * exercise, data = acad2)  \n# 동일: endurance ~ age + exercise + age:exercise\n\ncar::S(mod_interact)\n\nCall: lm(formula = endurance ~ age * exercise, data = acad2)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  53.17896    7.52661   7.065 1.71e-11 ***\nage          -0.76596    0.15980  -4.793 2.87e-06 ***\nexercise     -1.35095    0.66626  -2.028 0.043694 *  \nage:exercise  0.04724    0.01359   3.476 0.000604 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard deviation: 9.7 on 241 degrees of freedom\nMultiple R-squared: 0.2061\nF-statistic: 20.86 on 3 and 241 DF,  p-value: 4.764e-12 \n    AIC     BIC \n1814.57 1832.07 \n\n\nlavvan에서는 상호작용 항(interaction term)이나 범주형 변수에 대해 처리해주지 않음\n\n직접 상호작용 항을 만들어 주어야 함.\n범주형 변수의 경우는 dummy variable까지 만든 후 상호작용 항까지 만들어 주어야 함.\n\nfastDummies::dummy_cols() 함수를 사용하면 쉽게 만들 수 있음.\n\n\n\nacad2 &lt;- acad2 |&gt; \n  mutate(age_c = jtools::center(age),\n         exercise_c = jtools::center(exercise),\n         age_x_exercise_c = age_c * exercise_c)\n\nacad2 |&gt; print()\n\n# A tibble: 245 x 6\n     age exercise endurance  age_c exercise_c age_x_exercise_c\n   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;            &lt;dbl&gt;\n 1    60       10        18  10.8      -0.673            -7.28\n 2    40        9        36  -9.18     -1.67             15.4 \n 3    29        2        51 -20.2      -8.67            175.  \n 4    47       10        18  -2.18     -0.673             1.47\n 5    48        9        23  -1.18     -1.67              1.98\n 6    42        6        30  -7.18     -4.67             33.6 \n 7    55        8         8   5.82     -2.67            -15.5 \n 8    43       19        40  -6.18      8.33            -51.5 \n 9    39        9        28 -10.2      -1.67             17.0 \n10    51       14        15   1.82      3.33              6.04\n# i 235 more rows\n\n\n\n# lavaan model\nmodel &lt;- '\n  endurance ~ age_c + exercise_c + age_x_exercise_c\n'\n\nfit &lt;- sem(model, data = acad2)\nsummary(fit, standardized = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         4\n\n  Number of observations                           245\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  endurance ~                                                           \n    age_c            -0.262    0.064   -4.119    0.000   -0.262   -0.244\n    exercise_c        0.973    0.135    7.183    0.000    0.973    0.429\n    age_x_exercs_c    0.047    0.013    3.504    0.000    0.047    0.201\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .endurance        92.546    8.362   11.068    0.000   92.546    0.794",
    "crumbs": [
      "Keith's",
      "Path Modeling",
      "Interactions"
    ]
  },
  {
    "objectID": "contents/interaction.html#moderated-mediation",
    "href": "contents/interaction.html#moderated-mediation",
    "title": "Interactions",
    "section": "Moderated Mediation",
    "text": "Moderated Mediation\np. 424, Introduction to Mediation, Moderation, and Conditional Process Analysis (3e) by Andrew F. Hayes\n\n\n\n\n\n\n연구 설명\n\n\n\n\n\n11.3 예시: 업무 팀에게 자신의 감정 숨기기 수년에 걸쳐 대중음악은 우리의 직관을 강화해왔고, 친한 친구들이 제공한 조언이나 친한 친구들이 제공하고 토크쇼 심리학자들은 감정을 억누르고 다른 사람의 눈에 띄지 않도록 숨겨서는 좋은 결과를 얻을 수 없다고 강조합니다. 당대의 아티스트들은 “자신을 표현하라”(마돈나)고, “침묵의 강령”(빌리 조엘)에 따라 살면 결코 과거를 잊지 못할 것이며 “절대 말하지 않을 것들”(에이브릴 라빈)의 목록이 길수록 인생에서 원하는 것을 얻을 가능성이 줄어든다는 것을 조심하라고 말합니다. 따라서 다른 사람이 “나랑 얘기 좀 하자”(아니타 베이커)는 요청을 할 때는 경계심을 풀고 마음속에 있는 이야기를 “소통”(B-52)하는 것이 중요합니다. 하지만 적어도 일부 업무 관련 상황에서는 반드시 그렇지는 않다고 M. S. Cole 외(2008)의 팀워크에 관한 연구에 따르면 말합니다. 이 연구자들에 따르면, 때로는 함께 일하는 다른 사람들이 자신을 괴롭히는 행동이나 말에 대해 자신의 감정을 숨기는 것이 더 나을 수 있으며, 그러한 감정이 팀의 관심의 초점이 되어 팀이 적시에 효율적인 방식으로 작업을 수행하는 데 방해가 되지 않도록 하는 것이 더 나을 수 있습니다. 이 연구는 조건부 프로세스 모델의 추정과 해석의 메커니즘을 설명하는 첫 번째 사례의 데이터를 TEAMS라는 데이터 파일로 제공하며, www.afhayes.com 에서 확인할 수 있습니다. 이 연구는 자동차 부품 제조 회사에 고용된 60개의 작업 팀을 대상으로 진행되었으며, 회사 직원 200여 명을 대상으로 작업 팀에 대한 일련의 질문과 팀 감독자에 대한 다양한 인식에 대한 설문조사에 대한 응답을 기반으로 합니다. 이 연구의 일부 변수는 그룹 수준에서 측정된 것으로 같은 팀원들이 말한 내용을 종합하여 도출한 것입니다. 다행히 팀원들이 팀에 대한 질문에 응답하는 방식이 매우 유사하여 이러한 종류의 집계를 정당화할 수 있었습니다. 다른 변수는 순전히 팀 상사의 보고를 기반으로 합니다.\n이 분석과 관련된 네 가지 변수를 측정했습니다. 팀원들이 다른 팀원들의 업무를 약화시키거나 변화와 혁신을 방해하는 행동을 얼마나 자주 했는지 등 팀원들의 역기능적 행동에 대한 일련의 질문(데이터 파일에서 DYSFUNC, 점수가 높을수록 팀 내 역기능적 행동이 많음을 나타냄)을 던져 팀원들의 역기능적 행동을 측정했습니다. 또한 팀원들에게 직장에서 ‘화가 났다’, ‘역겨웠다’ 등을 얼마나 자주 느끼는지 물어봄으로써 그룹의 부정적인 정서적 분위기를 측정했습니다(NEGTONE, 점수가 높을수록 업무 환경의 부정적인 정서적 분위기를 더 많이 반영함). 팀 상사에게는 팀이 얼마나 효율적이고 적시에 일을 처리하는지, 팀이 생산 목표를 달성하는지 등 전반적인 팀 성과에 대한 평가를 제공하도록 요청했습니다(데이터의 성과, 점수가 높을수록 성과가 좋음을 반영하는 척도). 또한 슈퍼바이저는 팀원들이 자신의 감정에 대해 보내는 비언어적 신호를 얼마나 쉽게 읽을 수 있는지를 측정하는 일련의 질문, 즉 비언어적 부정적 표현력(데이터 파일의 NEGEXP, 점수가 높을수록 팀원들이 부정적인 감정 상태를 비언어적으로 더 잘 표현한다는 의미)에 응답했습니다. 이 연구의 목표는 업무 팀원의 역기능적 행동이 업무 팀의 성과에 부정적인 영향을 미칠 수 있는 메커니즘을 조사하는 것이었습니다. 연구진은 역기능적 행동(X)으로 인해 상사와 다른 직원들이 직면하고 관리하려고 하는 부정적인 감정(M)으로 가득 찬 업무 환경이 조성되면 업무에 집중하지 못하고 업무 수행에 방해가 된다는 중재 모델을 제안했습니다(Y). 그러나 이 모델에 따르면 팀원들이 부정적인 감정(W)을 조절할 수 있게 되면, 즉 자신의 감정을 다른 사람에게 숨길 수 있게 되면 업무 환경의 부정적인 분위기와 다른 사람의 감정을 관리하는 데 집중할 필요 없이 당면한 업무에 집중할 수 있게 됩니다. 즉, 이 모델에서는 업무 환경의 부정적인 정서적 어조가 팀 성과에 미치는 영향은 팀원이 자신의 감정을 숨기는 능력에 따라 달라지며, 부정적 감정을 숨기지 않고 표현하는 팀에서 부정적인 정서적 어조가 성과에 미치는 부정적 영향이 더 강하다고 가정합니다.\n\n\n\n\nData: Introduction to Mediation, Moderation, and Conditional Process Analysis; “data files and code”\n\nteams &lt;- read_csv(\"data/teams.csv\")\nteams &lt;- teams |&gt; \n  mutate(\n    negtone_c = jtools::center(negtone),\n    negexp_c = jtools::center(negexp),\n    negtone_negexp_c = negtone_c*negexp_c,\n  )\nteams |&gt; print()\n\n# A tibble: 60 x 7\n   dysfunc negtone negexp perform negtone_c negexp_c negtone_negexp_c\n     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;\n 1   -0.23   -0.51  -0.49    0.12    -0.557   -0.482           0.268 \n 2   -0.13    0.22  -0.49    0.52     0.173   -0.482          -0.0832\n 3    0      -0.08   0.84   -0.08    -0.127    0.848          -0.108 \n 4   -0.33   -0.11   0.84   -0.08    -0.157    0.848          -0.133 \n 5    0.39   -0.48   0.17    0.12    -0.527    0.178          -0.0940\n 6    1.02    0.72  -0.82    1.12     0.673   -0.812          -0.546 \n 7   -0.35   -0.18  -0.66   -0.28    -0.227   -0.652           0.148 \n 8   -0.23   -0.13  -0.16    0.32    -0.177   -0.152           0.0269\n 9    0.39    0.52  -0.16   -1.08     0.473   -0.152          -0.0717\n10   -0.08   -0.26  -0.16   -0.28    -0.307   -0.152           0.0466\n# i 50 more rows\n\n\n\n# standard deviation of negative expression \nsd(teams$negexp_c) |&gt; print()\n\n[1] 0.543701\n\n\n\nmod &lt;- \"\n  # models\n    perform ~ c*dysfunc + b1*negtone_c + b2*negexp_c + b3*negtone_negexp_c\n    negtone_c ~ a*dysfunc\n  \n  # conditional effects: m-sd, m, m+sd\n    b_low := b1 + b3*(-0.5437)  # mean - sd; sd = 0.5437\n    b_mean := b1 + b3*0  # mean\n    b_high := b1 + b3*0.5437  # mean + sd\n  \n  # conditional indirect effects\n    ab_low := a * b_low\n    ab_mean := a * b_mean\n    ab_high := a * b_high\n  \n  # index of moderated mediation\n    index_Mod_Med := a*b3\n\"\n\nfit &lt;- lavaan::sem(model = mod, data = teams, se = \"bootstrap\", meanstructure = T)\nsummary(fit, standardized = T) |&gt; print()\n\nlavaan 0.6-18 ended normally after 2 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                            60\n\nModel Test User Model:\n                                                      \n  Test statistic                                 6.999\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.030\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws             1000\n  Number of successful bootstrap draws            1000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  perform ~                                                             \n    dysfunc    (c)    0.366    0.191    1.917    0.055    0.366    0.270\n    negtone_c (b1)   -0.431    0.128   -3.376    0.001   -0.431   -0.451\n    negexp_c  (b2)   -0.044    0.099   -0.442    0.659   -0.044   -0.047\n    ngtn_ngx_ (b3)   -0.517    0.244   -2.121    0.034   -0.517   -0.278\n  negtone_c ~                                                           \n    dysfunc    (a)    0.620    0.221    2.805    0.005    0.620    0.438\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .perform          -0.032    0.060   -0.531    0.596   -0.032   -0.064\n   .negtone_c        -0.021    0.059   -0.367    0.714   -0.021   -0.041\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .perform           0.185    0.033    5.663    0.000    0.185    0.742\n   .negtone_c         0.219    0.050    4.428    0.000    0.219    0.808\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    b_low            -0.150    0.223   -0.675    0.499   -0.150   -0.300\n    b_mean           -0.431    0.128   -3.375    0.001   -0.431   -0.451\n    b_high           -0.713    0.135   -5.271    0.000   -0.713   -0.601\n    ab_low           -0.093    0.153   -0.611    0.541   -0.093   -0.131\n    ab_mean          -0.267    0.118   -2.267    0.023   -0.267   -0.198\n    ab_high          -0.442    0.161   -2.739    0.006   -0.442   -0.264\n    index_Mod_Med    -0.320    0.190   -1.683    0.092   -0.320   -0.122\n\n\n\n\n# 모든 parameter에 대한 결과\nparameterEstimates(fit, level = 0.95, boot.ci.type = \"bca.simple\") |&gt; print()\n\n                lhs op              rhs         label    est    se      z\n1           perform  ~          dysfunc             c  0.366 0.191  1.917\n2           perform  ~        negtone_c            b1 -0.431 0.128 -3.376\n3           perform  ~         negexp_c            b2 -0.044 0.099 -0.442\n4           perform  ~ negtone_negexp_c            b3 -0.517 0.244 -2.121\n5         negtone_c  ~          dysfunc             a  0.620 0.221  2.805\n6           perform ~~          perform                0.185 0.033  5.663\n7         negtone_c ~~        negtone_c                0.219 0.050  4.428\n8           dysfunc ~~          dysfunc                0.136 0.000     NA\n9           dysfunc ~~         negexp_c               -0.001 0.000     NA\n10          dysfunc ~~ negtone_negexp_c               -0.003 0.000     NA\n11         negexp_c ~~         negexp_c                0.291 0.000     NA\n12         negexp_c ~~ negtone_negexp_c                0.046 0.000     NA\n13 negtone_negexp_c ~~ negtone_negexp_c                0.072 0.000     NA\n14          perform ~1                                -0.032 0.060 -0.531\n15        negtone_c ~1                                -0.021 0.059 -0.367\n16          dysfunc ~1                                 0.035 0.000     NA\n17         negexp_c ~1                                 0.000 0.000     NA\n18 negtone_negexp_c ~1                                 0.024 0.000     NA\n19            b_low :=  b1+b3*(-0.5437)         b_low -0.150 0.223 -0.675\n20           b_mean :=          b1+b3*0        b_mean -0.431 0.128 -3.375\n21           b_high :=     b1+b3*0.5437        b_high -0.713 0.135 -5.271\n22           ab_low :=          a*b_low        ab_low -0.093 0.153 -0.611\n23          ab_mean :=         a*b_mean       ab_mean -0.267 0.118 -2.267\n24          ab_high :=         a*b_high       ab_high -0.442 0.161 -2.739\n25    index_Mod_Med :=             a*b3 index_Mod_Med -0.320 0.190 -1.683\n   pvalue ci.lower ci.upper\n1   0.055   -0.035    0.716\n2   0.001   -0.675   -0.163\n3   0.659   -0.234    0.155\n4   0.034   -1.051   -0.087\n5   0.005    0.234    1.085\n6   0.000    0.137    0.262\n7   0.000    0.144    0.345\n8      NA    0.136    0.136\n9      NA   -0.001   -0.001\n10     NA   -0.003   -0.003\n11     NA    0.291    0.291\n12     NA    0.046    0.046\n13     NA    0.072    0.072\n14  0.596   -0.154    0.082\n15  0.714   -0.140    0.099\n16     NA    0.035    0.035\n17     NA    0.000    0.000\n18     NA    0.024    0.024\n19  0.499   -0.510    0.346\n20  0.001   -0.675   -0.163\n21  0.000   -1.098   -0.520\n22  0.541   -0.435    0.182\n23  0.023   -0.585   -0.098\n24  0.006   -0.817   -0.168\n25  0.092   -0.839   -0.058",
    "crumbs": [
      "Keith's",
      "Path Modeling",
      "Interactions"
    ]
  },
  {
    "objectID": "contents/equations.html",
    "href": "contents/equations.html",
    "title": "Structural Equations",
    "section": "",
    "text": "Load libraries\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)",
    "crumbs": [
      "SEM Equations"
    ]
  },
  {
    "objectID": "contents/equations.html#structural-equation-model-sem-matrix-representation",
    "href": "contents/equations.html#structural-equation-model-sem-matrix-representation",
    "title": "Structural Equations",
    "section": "Structural Equation Model (SEM) Matrix Representation",
    "text": "Structural Equation Model (SEM) Matrix Representation\n\nMeasures of political democracy and industrialization for 75 developing countries in 1960 and 1965\n\ny1. freedom of the press, 1960\ny2. freedom of political opposition, 1960\ny3. fairness of elections, 1960\nx1. natural log of GNP per capita, 1960\nx2. natural log of energy consumption per capita, 1960\n\n\n\n\n\n\n \n\n\n외생변수(Exogenous variables): \\(X1, X2\\)\n내생변수(Endogenous variables): \\(Y1, Y2, Y3\\)\n경로계수(Path coefficients)\n\n\\(\\gamma\\): \\(X\\)와 \\(Y\\)의 관계\n\\(\\beta\\): \\(Y\\)와 \\(Y\\)의 관계\n\n오차항(Error terms): \\(\\zeta\\)\n\n\n\n\n\nFigure 4.2 Union Sentiment Model for Southern Textile Workers\nSource: p. 83, Bollen, K. A. (1989). Structural equations with latent variables\n\nMatrix representation of the model:\n\\[\n\\mathbf{Y} = \\mathbf{B}\\mathbf{Y} + \\mathbf{\\Gamma}\\mathbf{X} + \\boldsymbol{\\zeta}\n\\]\n\\[\n\\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ y_3\n\\end{bmatrix} =\n\\begin{bmatrix}\n0 & 0 & 0 \\\\\n\\beta_{21} & 0 & 0 \\\\\n\\beta_{31} & \\beta_{32} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ y_3\n\\end{bmatrix} +\n\\begin{bmatrix}\n0 & \\gamma_{12} \\\\\n0 & \\gamma_{22} \\\\\n\\gamma_{31} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\ x_2\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\zeta_1 \\\\ \\zeta_2 \\\\ \\zeta_3\n\\end{bmatrix}\n\\]\n가정 1: 내생변수의 오차항과 외생변수는 서로 독립; \\(\\zeta \\perp X\\)\n가정 2: 각 변수는 평균이 0이고, 변수들은 multivariate normal을 따름\n\n\n\n\n\n\nThe covariance matrix of exogenous variables:\n\\[\n\\mathbf{\\Phi} =\n\\begin{bmatrix}\n\\phi_{11} & \\phi_{12} \\\\\n\\phi_{21} & \\phi_{22}\n\\end{bmatrix}\n\\]\n\n\nThe covariance matrix of error terms:\n\\[\n\\mathbf{\\Psi} =\n\\begin{bmatrix}\n\\psi_{11} & 0 & 0 \\\\\n0 & \\psi_{22} & 0 \\\\\n0 & 0 & \\psi_{33}\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathbf{Y}\\) is the vector of endogenous variables\n\\(\\mathbf{X}\\) is the vector of exogenous variables\n\\(\\mathbf{B}\\) is the matrix of path coefficients between endogenous variables\n\\(\\mathbf{\\Gamma}\\) is the matrix of path coefficients from exogenous to endogenous variables\n\n\n\n\n\n\n\\(\\boldsymbol{\\zeta}\\) is the vector of error terms\n\\(\\mathbf{\\Phi}\\) is the covariance matrix of exogenous variables\n\\(\\mathbf{\\Psi}\\) is the covariance matrix of error terms\n\n\n\n\n\n\n\ndata and model\n# A dataset from Bollen (1989) containing measures of political democracy and industrialization for 75 developing countries in 1960 and 1965. \nbollen1989a &lt;- MIIVsem::bollen1989a |&gt; as_tibble()\n\n# fig 4.2\nmodel &lt;- '\n  y3 ~ x1 + y1 + y2\n  y2 ~ x2 + y1\n  y1 ~ x2\n'\nfit &lt;- sem(model, data = bollen1989a, fixed.x = FALSE)\n\n\n\n# coefficients: betas & gamma\ninspect(fit, what = \"est\")$beta |&gt; round(2) |&gt; print()\n\n   y3   y2   y1   x1   x2\ny3  0 0.06 0.76 0.36 0.00\ny2  0 0.00 0.88 0.00 0.16\ny1  0 0.00 0.00 0.00 0.56\nx1  0 0.00 0.00 0.00 0.00\nx2  0 0.00 0.00 0.00 0.00\n\n\n\n# covariances: psi & phi\ninspect(fit, what = \"est\")$psi |&gt; round(2) |&gt; print()\n\n     y3   y2   y1   x1   x2\ny3 5.64                    \ny2 0.00 9.71               \ny1 0.00 0.00 6.09          \nx1 0.00 0.00 0.00 0.53     \nx2 0.00 0.00 0.00 0.98 2.25",
    "crumbs": [
      "SEM Equations"
    ]
  },
  {
    "objectID": "contents/equations.html#parameter-estimation",
    "href": "contents/equations.html#parameter-estimation",
    "title": "Structural Equations",
    "section": "Parameter Estimation",
    "text": "Parameter Estimation\n\n\nSource: p. 86, Bollen, K. A. (1989). Structural equations with latent variables\n\n\\(y_1 = \\gamma_{11}x_1 + \\zeta_1\\)\n\\(y_2 = \\beta_{21}y_1 + \\zeta_2\\)\nMatrix representation:\n\n\n\n\n\n\n\\(\\begin{bmatrix}\ny_1 \\\\ y_2\n\\end{bmatrix} =\n\\begin{bmatrix}\n0 & 0 \\\\\n\\beta_{21} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\ny_1 \\\\ y_2\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\gamma_{11} \\\\\n0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\zeta_1 \\\\ \\zeta_2\n\\end{bmatrix}\\)\n\n\n\\(\\mathbf{\\Phi} =\n\\begin{bmatrix}\n\\phi_{11}\n\\end{bmatrix}\\)\n\n\n\\(\\mathbf{\\Psi} =\n\\begin{bmatrix}\n\\psi_{11} & \\psi_{12}\\\\\n\\psi_{21} & \\psi_{22}\\\\\n\\end{bmatrix}\\)\n\n\n\n가정들:\n\n내생변수의 잔차는 외생변수와 독립: \\(cov(\\zeta_1, x_1) = 0\\), \\(cov(\\zeta_2, x_1) = 0\\)\n내생변수의 잔차들은 서로 독립: \\(cov(\\zeta_1, \\zeta_2) = 0\\) 즉, \\(\\psi_{12} = 0\\)\n\n추정은 가능하나 non-recursive한 피드백 루프가 있는 모형이 됨.\n\n\n\n\n\n\n\n\nCoveriance에 대한 성질\n\n\n\n\\(Cov(aX+bY,~ cU+dV) = ac~Cov(X, U) + ad~Cov(X, V) + bc~Cov(Y, U) + bd~Cov(Y, V)\\)\n\n\n\\(\\begin{align}\n  cov(y_1, y_2) & = cov(\\gamma_{11}x_1 + \\zeta_1, \\beta_{21}y_1 + \\zeta_2) \\\\\n                & = \\gamma_{11} \\beta_{21} cov(x_1, y_1) + \\gamma_{11} cov(x_1, \\zeta_2) + \\beta_{21} cov(y_1, \\zeta_1) + cov(\\zeta_1, \\zeta_2) \\\\\n                & = \\gamma_{11} \\beta_{21} * \\gamma_{11} \\phi_{11} + \\beta_{21} \\psi_{11} \\\\\n                & = \\beta_{21}( \\gamma_{11}^2 \\phi_{11} + \\psi_{11})\n\\end{align}\\)\n\n\\(cov(x_1, y_1) = cov(x_1, \\gamma_{11}x_1 + \\zeta_1) = \\gamma_{11} cov(x_1, x_1) + cov(x_1, \\zeta_1) = \\gamma_{11} var(x_1) = \\gamma_{11} \\phi_{11}\\)\n\\(cov(y_1, \\zeta_1) = cov(\\gamma_{11}x_1 + \\zeta_1, \\zeta_1) = \\gamma_{11} cov(x_1, \\zeta_1) + cov(\\zeta_1, \\zeta_1) = var(\\zeta_1) = \\psi_{11}\\)\n\\(cov(\\zeta_1, \\zeta_2) = \\psi_{12}\\)\n\n\\[\\begin{equation}\n\\begin{bmatrix}\n\\text{VAR}(y_1) \\\\\n\\text{COV}(y_2, y_1) & \\text{VAR}(y_2) \\\\\n\\text{COV}(x_1, y_1) & \\text{COV}(x_1, y_2) & \\text{VAR}(x_1)\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\gamma_{11}^2\\phi_{11} + \\psi_{11} \\\\\n\\beta_{21}(\\gamma_{11}^2\\phi_{11} + \\psi_{11}) & \\beta_{21}^2(\\gamma_{11}^2\\phi_{11} + \\psi_{11}) + \\psi_{22} \\\\\n\\gamma_{11}\\phi_{11} & \\beta_{21}\\gamma_{11}\\phi_{11} & \\phi_{11}\n\\end{bmatrix}\n\\end{equation}\\]\n\n\n데이터로부터 최선의 파라미터를 추정\n\n\n\n\n\n\n기본적인 구조 모형의 가설(hypothesis):\n\\(\\Sigma = \\Sigma(\\theta)\\)\n즉, 모집단의 공분산 = 모수(parameter)로 표현되는(implied) 공분산\n\n\n\n\n모집단의 공분산에 대한 추정치: 관찰된 공분산 행렬 \\(\\mathbf{S}\\)\n파라미터로 표현된 공분산 행렬의 추정치: \\(\\hat{\\Sigma}(\\theta)\\)\n\n이제 이 둘의 차 \\(\\mathbf{S} - \\hat{\\Sigma}(\\theta)\\)를 최소화하는 파라미터들(통칭 \\(\\theta\\))를 찾고자 함.\n예를 들어,\n\n# covariance matrix\nlower &lt;- '\n  225.2\n  32.1 100.3\n  111.4 26.5 105.2'\ncov &lt;- getCov(lower, names = c(\"x1\", \"y1\", \"y2\"))\n\n\nmodel &lt;- '\n  # regressopm\n  y2 ~ 0*x1 + y1\n  y1 ~ x1\n  \n  # variacne\n  # y2 ~~ y1  # assumed to be uncorrelated\n'\nfit &lt;- sem(model, sample.cov = cov, sample.nobs = 200, fixed.x = F)\n\n\nparameterEstimates(fit) |&gt; print()\n\n  lhs op rhs     est     se      z pvalue ci.lower ci.upper\n1  y2  ~  x1   0.000  0.000     NA     NA    0.000    0.000\n2  y2  ~  y1   0.264  0.070  3.776  0.000    0.127    0.401\n3  y1  ~  x1   0.143  0.046  3.092  0.002    0.052    0.233\n4  y2 ~~  y2  97.708  9.771 10.000  0.000   78.557  116.858\n5  y1 ~~  y1  95.246  9.525 10.000  0.000   76.578  113.914\n6  x1 ~~  x1 224.074 22.407 10.000  0.000  180.156  267.992\n\n\n\n# implied covariance matrix\nsigma_hat &lt;- lavInspect(fit, \"cov.all\")[3:1, 3:1] |&gt; round(1)\nsigma_hat |&gt; print()\n\n      x1   y1    y2\nx1 224.1 31.9   8.4\ny1  31.9 99.8  26.4\ny2   8.4 26.4 104.7\n\n\n\n# 공분산 행렬 차이: residual covariance matrix\n(cov - sigma_hat) |&gt; print()\n\n      x1  y1    y2\nx1   1.1 0.2 103.0\ny1   0.2 0.5   0.1\ny2 103.0 0.1   0.5\n\n\n\n즉, \\(\\mathbf{S} - \\hat{\\Sigma}(\\theta) =\n\\begin{bmatrix}\n1.1 \\\\\n0.2 & 0.5 \\\\\n103.0 & 0.1 & 0.5\n\\end{bmatrix}\\)\n\n\n이는 어떤 다른 파라미터 값에 대해서도 이 공분산 차이보다 더 작을 수 없다는 의미임.\n한편, 어떤 의미에서 차이가 작다는지를 정의해야 함.\n\n\n\nEstimation\n\n\n\n\n\n\n\\(\\mathbf{S} - \\hat{\\Sigma}(\\theta)\\)가 작다는 것을 어떻게 정의할 것인가?\n혹은, 두 행렬 \\(\\mathbf{S}\\), \\(\\hat{\\Sigma}(\\theta)\\) 사이의 거리를 어떻게 정의할 것인가?\n\nFitting function(목적 함수)를 통해 그 함수가 최소가 되는 파라미터를 찾는 방식\n\n\n\n\n\nLikelihood의 관점에서 \\(\\mathbf{S}\\)와 \\(\\hat{\\Sigma}(\\theta)\\)의 거리를 측정: maximum likelihood estimation(MLE); scale invariant, scale free\n\n\\(F_{ML} = log|\\Sigma(\\theta)| + tr(S\\Sigma^{-1}(\\theta)) - log|S| - (p + q)\\)\n\n각 성분들의 제곱의 합이 최소가 되도록 추정: unweighted least squares(ULS); scale에 따라 변함.\n\n\\(F_{ULS} = \\frac{1}{2} tr[(\\mathbf{S} - \\Sigma(\\theta))^2]\\)\n\nFully weighted least squares(WLS): 적절한 \\(\\mathbf{W}\\)를 선택해 공분산의 크기를 조정, 분포에 대한 가정 없으나 큰 표본을 요구함.\n\n\\(F_{WLS} = \\frac{1}{2} tr\\left[\\left[\\left(\\mathbf{S} - \\Sigma(\\theta)\\right)\\mathbf{W}^{-1}\\right]^2\\right]\\)\nGeneralized least squares(GLS): \\(\\mathbf{W}\\)를 \\(\\mathbf{S}\\)로 선택하는 경우; scale invariant, scale free\n\n\nMaximum Likelihood Estimation\n관측 변수들이 multivariate normal distribution을 따른다고 가정하면,\n\nLog-likelihood \\(\\displaystyle L(\\theta) = \\frac{-N(p + q)}{2}log(2\\pi) - \\frac{N}{2}\\left(log|\\Sigma(\\theta)| + tr(S\\Sigma^{-1}(\\theta))\\right)\\)\n\\(F_{ML} = log|\\Sigma(\\theta)| + tr(S\\Sigma^{-1}(\\theta)) - log|S| - (p + q)\\)\n\n즉, likelihood를 최대화하는 것은 \\(F_{ML}\\)를 최소화하는 것과 동일함.\nFitting function으로써 \\(F_{ML}\\)의 장점은 asymptotic properties를 가지고 있음.\n\nMultivariate normal을 가정하고, N이 충분히 클 때,\n주어진 모형이 참이라면, random sampling 변화에 따라 \\((N-1)*F_{ML}\\)은 \\(\\chi^2\\) 분포 따름\n\n\n\n\n\n\n\nNon-normality에 대한 대응\n\n\n\n\nRobust ML\nBootstrapping: Bollen-Stine bootstrap\n분포에 대한 가정이 없는 방법: WLS(fully weighted least squares)\n\nML 파라미터 추정치는 일반적으로 robust하나, 표준오차는 문제가 될 수 있음.\n다음과 같은 ML에 대한 robust estimation들은 표준오차에 대한 보정을 제공함.\n\nOption “MLM” is for complete data sets only and generates the mean-adjusted Satorra-Bentler scaled chi-square.\nOption “MLR” can be applied in complete or incomplete data sets, and it generates a mean adjusted chi-square based on the Yuan-Bentler T2 statistic. Because this method accommodates missing data, it is the most flexible option listed here.\nOption “MLMV” is for complete data sets only and computes a mean- and variance-adjusted scaled chi-square.\nOption “MLMVS” generates a mean- and variance adjusted chi-square with a correction for heteroscedasticity by Satterthwaite (1941). Model degrees of freedom are estimated, and the method is for complete data sets.\n\n\nSource: p. 137, 163, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n\n\n\n\n\nModel Indentification\n위의 예에서,\n\n파라미터를 추정하기 위해 주어진 정보의 갯수: 3+2+1=6개\n\n관찰 변수가 \\(p\\)개 일 때, \\(p(p+1)/2\\)\n\n추정하고자 하는 파라미터의 개수: 5개\n\n즉, 식은 6개이며 미지수는 5개이므로 추정가능함!\n이 때, 자유도(degree of freedom) df = 6 - 5 = 1\n\nover-identified (과대 식별): df &gt; 0, 그 차이(자유도)가 클수록 유리함!\njust-identified (적정 식별): df = 0, 포화 모형 (saturated model)\nunder-identified model (과소 식별): df &lt; 0, 파라미터 추정 불가능\n\n\nsummary(fit, estimate = F) |&gt; print()\n\nlavaan 0.6-19 ended normally after 10 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                           200\n\nModel Test User Model:\n                                                      \n  Test statistic                               139.381\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.000\n\n\n\n# info for parameters\nlavaan::parTable(fit) |&gt; print()\n\n  id lhs op rhs user block group free ustart exo label plabel   start     est\n1  1  y2  ~  x1    1     1     1    0      0   0         .p1.   0.000   0.000\n2  2  y2  ~  y1    1     1     1    1     NA   0         .p2.   0.111   0.264\n3  3  y1  ~  x1    1     1     1    2     NA   0         .p3.   0.143   0.143\n4  4  y2 ~~  y2    0     1     1    3     NA   0         .p4.  48.671  97.708\n5  5  y1 ~~  y1    0     1     1    4     NA   0         .p5.  95.246  95.246\n6  6  x1 ~~  x1    0     1     1    5     NA   0         .p6. 224.074 224.074\n      se\n1  0.000\n2  0.070\n3  0.046\n4  9.771\n5  9.525\n6 22.407",
    "crumbs": [
      "SEM Equations"
    ]
  },
  {
    "objectID": "contents/equations.html#overall-fit-measures",
    "href": "contents/equations.html#overall-fit-measures",
    "title": "Structural Equations",
    "section": "Overall Fit Measures",
    "text": "Overall Fit Measures\n두 행렬 \\(\\mathbf{S}\\)와 \\(\\hat{\\Sigma}(\\theta)\\)간의 거리를 측정하여 모형이 전체적으로 얼마나 적합한지를 평가함.\n부분적인(local) 적합도를 살펴보는 것으로는 드러나지 못하는 “전체적 부적절함”을 판별할 수는 지표를 얻을 수 있음.\n\n반대로, 전체적으로 적합하다고 해서 부분적으로 적합하다는 보장은 없음.\n따라서, 여러 측면에서 동시에 평가하는 것이 중요함!\n\n\nResiduals\nIdeally, all residuals should be near zero for a “good” model. But the sample residuals are affected by several factors: (1) the departure of \\(\\Sigma\\) from \\(\\Sigma(\\theta)\\), (2) the scales of the observed variables, and (3) sampling fluctuations\n\nCoveriance residuals: 두 공분산의 차이 \\(\\mathbf{S} - \\hat{\\Sigma}(\\theta)\\)를 직접 확인\nCorrelation residuals: 각 공분산을 표준화하여 (변수들의 단위와 독립적인) 상관계수로 변환 후 비교\n\n\\(\\displaystyle r_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}\\), \\(\\sigma_{ij}\\): 해당 공분산\n그 값이 0.1 이상이면 주의\n\nStandardized residuals: 표준오차로 나눈 잔차들을 비교\n\ndivided by the square root of the asymptotic variance of these residuals (by lavaan)\n표본의 크기를 고려해 분산을 조정한 값\nN이 충분히 크다면, z-score로 이해할 수 있음\n2.0 이상이면 유의(significant)하다고 보고 주의; 단, 표본크기에 대한 가설검증의 취약성을 그대로 지님.\n\n\n표본의 크기가 커질수록, 공분산/상관 잔차의 값이 줄어드는 경향이 있음.\n예제: Reisenzein(1986)의 실험\n\n\n\n\n\n\n연구 내용\n\n\n\n\n\n\n지하철에서 한 사람이 쓰러지는 가상의 시나리오에서 동정심과 분노가 도움 행동을 매개하는지를 조사\n절반의 참가자에게는 그 사람이 술에 취했다는 정보 제공, 나머지 절반에게는 병에 걸렸다는 정보 제공\n동정심과 분노는 각각 세 가지 지표로 측정됨\n\n동정심\nx1: 그 사람에 대해 얼마나 동정심(sympathy)을 느끼십니까?\nx2: 나는 이 사람에게 연민(pity)을 느낍니다.\nx3: 이 사람에 대해 얼마나 걱정(concern)이 되십니까?\n분노\nx4: 그 사람에게 얼마나 화가 나시나요?\nx5: 그 사람에게 얼마나 짜증을 느끼시겠습니까?\nx6: 그 사람 때문에 기분이 나빠질 것입니다.\n\n\n\n\nSource: p. 260, Bollen, K. A. (1989). Structural equations with latent variables\n\n\n\nFigure 7.4 Confirmatory Factor Analysis Model for Sympathy (\\(\\xi_1\\)) and Anger (\\(\\xi_2\\)).\nEach Measured with Three Indicators (\\(X_1\\) to \\(X_6\\))\n\nMatrix representation for the CFA model:\n\\(x_1 = \\lambda_{11}\\xi_1 + \\delta_1\\),   \\(x_2 = \\lambda_{21}\\xi_1 + \\delta_2\\),  \\(x_3 = \\lambda_{31}\\xi_1 + \\delta_3\\),\n\\(x_4 = \\lambda_{42}\\xi_2 + \\delta_4\\),  \\(x_5 = \\lambda_{52}\\xi_2 + \\delta_5\\),  \\(x_6 = \\lambda_{62}\\xi_2 + \\delta_6\\)\n\\(\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\lambda_{11} & 0 \\\\\n\\lambda_{21} & 0 \\\\\n\\lambda_{31} & 0 \\\\\n0 & \\lambda_{42} \\\\\n0 & \\lambda_{52} \\\\\n0 & \\lambda_{62}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\xi_1 \\\\ \\xi_2\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\delta_1 \\\\ \\delta_2 \\\\ \\delta_3 \\\\ \\delta_4 \\\\ \\delta_5 \\\\ \\delta_6\n\\end{bmatrix}\\)\n\\(\\mathbf{x} = \\mathbf{\\Lambda}\\mathbf{\\xi} + \\boldsymbol{\\delta}\\)\n가정: \\(COV(\\xi_i, \\delta_j) = 0\\) for all \\(i\\) and \\(j\\)\n\n\n\n\n\n\nThe covariance matrix of exogenous variables:\n\\[\\mathbf{\\Phi} =\n\\begin{bmatrix}\n\\phi_{11} &  \\\\\n\\phi_{21} & \\phi_{22}\n\\end{bmatrix}\n\\]\n\n\nThe covariance matrices of the errors of measurement\n\\[\\mathbf{\\Theta_{\\delta}} =\n\\begin{bmatrix}\nV(\\delta_1) \\\\\n0 & V(\\delta_2) \\\\\n0 & 0 & V(\\delta_3) \\\\\n0 & 0 & 0 & V(\\delta_4) \\\\\n0 & 0 & 0 & 0 & V(\\delta_5) \\\\\n0 & 0 & 0 & 0 & 0 & V(\\delta_6)\n\\end{bmatrix}\n\\]\n\n\n\n\n# fig 7.4\nlower &lt;- \"\n  6.982\n  4.686 6.047\n  4.335 3.307 5.037 \n  -2.294 -1.453 -1.979 5.569\n  -2.209 -1.262 -1.738 3.931 5.328\n  -1.671 -1.401 -1.564 3.915 3.601 4.977\n\"\ncov &lt;- getCov(lower, names = c(\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\"))\n\nmodel_sa &lt;- '\n  sympathy =~ x1 + x2 + x3\n  anger =~ x4 + x5 + x6\n'\nfit_sa &lt;- sem(model_sa, sample.cov = cov, sample.nobs = 138)\n\n\n# coveriance residuals\nresiduals(fit_sa) |&gt; print()\n\n$type\n[1] \"raw\"\n\n$cov\n       x1     x2     x3     x4     x5     x6\nx1  0.000                                   \nx2  0.028  0.000                            \nx3 -0.016 -0.027  0.000                     \nx4 -0.073  0.247 -0.387  0.000              \nx5 -0.170  0.297 -0.277 -0.030  0.000       \nx6  0.334  0.136 -0.126  0.013  0.020  0.000\n\n\n\n\n# correlation residuals\nresiduals(fit_sa, type = \"cor\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       x1     x2     x3     x4     x5     x6\nx1  0.000                                   \nx2  0.004  0.000                            \nx3 -0.003 -0.005  0.000                     \nx4 -0.012  0.043 -0.074  0.000              \nx5 -0.028  0.053 -0.054 -0.006  0.000       \nx6  0.057  0.025 -0.025  0.002  0.004  0.000\n\n\n\n\n# standardized residuals\nresiduals(fit_sa, type = \"standardized\") |&gt; print()\n\n$type\n[1] \"standardized\"\n\n$cov\n       x1     x2     x3     x4     x5     x6\nx1  0.000                                   \nx2  1.353  0.000                            \nx3 -1.035 -0.379  0.000                     \nx4 -0.392  0.871 -1.503  0.000              \nx5 -0.723  0.967 -1.006 -1.203  0.000       \nx6  1.585  0.483 -0.493  0.568  0.566  0.000\n\n\n\nNormalized vs standardized residuals: ratios of covariance residuals over the standard error of the sample covariance, not the standard error of the difference between sample and predicted values.\n\n# normalized residuals\nresiduals(fit_sa, type = \"normalized\") |&gt; print()\n\n$type\n[1] \"normalized\"\n\n$cov\n       x1     x2     x3     x4     x5     x6\nx1  0.000                                   \nx2  0.042  0.000                            \nx3 -0.026 -0.049  0.000                     \nx4 -0.130  0.488 -0.809  0.000              \nx5 -0.311  0.604 -0.601 -0.053  0.000       \nx6  0.644  0.284 -0.285  0.023  0.038  0.000\n\n\n\n\n\nChi-Square( \\(\\chi^2\\)) Test\n공분산의 잔차들이 모집단에서 0과 다른지에 대한 통계적 검증하고자 함.\n\n위에서 standardize한 residuals를 이용하는 것은 대략적인 테스트임.\n소위, simultaneous significance test, 즉 모든 공분산이 0인지를 동시에 검증하는 것이 필요함.\n\n\nChi-Square( \\(\\chi^2\\)) Test\n\n\\(H_0\\): \\(\\Sigma\\) = \\(\\Sigma(\\theta)\\) ; 공분산 행렬과 모수로 표현된 공분산 행렬이 같다. 즉, 연구자의 모형은 데이터와 완전히 일치함!\n\\(H_0\\) 가정하에, sampling error로부터 \\((N-1)*F_{ML}\\) 값들은 점근적으로 \\(\\chi^2(df_M)\\) 분포 따름\n이는 포화모형(완전한 모형)에 비해 연구자의 “제한이 가해진 모형” 간의 (likelihood) 차이가 특정 분포를 따른다는 것을 의미함.\n\nLog-likelihood ratio: \\(\\displaystyle -2log\\left(\\frac{L_0}{L_1}\\right) = (N-1)F_{ML} \\sim \\chi^2(df_M)\\)\n\n\\(L_0\\): the likelihood maximized for the researcher’s model under \\(H_0\\)\n\\(L_1\\): the same quantity for an unrestricted model\n\n\\(df_M\\): the difference in the number of parameters between the two models\n\n\n위의 CFA의 예에서,\n\\(\\begin{align}\n  \\chi^2_{ML} = (N)*F_{ML}(S, \\hat{\\Sigma}(\\theta)) & = N*\\left(log|\\Sigma(\\theta)| + tr(\\Sigma^{-1}(\\theta)S) - log|S| - (p + q)\\right) \\\\\n  & = 138 * 0.0698 = 9.632 \\\\\n\\end{align}\\)\n이 값은 다음과 같이 해석할 수 있음.\n\\(\\begin{align}\n  -2log\\left(\\frac{L_0}{L_1}\\right) & = -2(logL_0 - logL_1) \\\\\n  & = -2*[-1654.635 - (-1649.817)] = 9.635 \\\\\n\\end{align}\\)\nDegree of freedom = \\(\\displaystyle\\frac{6*7}{2} - 13 = 21 - 13 = 8\\)\n\n\n\n\n\n\nChi-Square 분포 특성\n\n\n\n\n자유도(df)가 커질수록 분포의 모양이 정규분포에 가까워짐\n평균 = df, 분산 = 2df\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\chi^2\\) 분포로부터 \\(p = 0.292\\), Bootstrap 샘플 중 29.8%\n\n\\(H_0\\): \\(\\Sigma\\) = \\(\\Sigma(\\theta)\\)이 참이라면, 상정한 모형의 \\(\\chi^2\\) 값 9.6보다 더 큰 \\(\\chi^2\\)값이 나올 확률이 대략 30%임.\n이는 완전한 모형(포화모형)과 (likelihood 관점에서) 크게 다르지 않다는 것을 의미함.\n기존의 관행처럼 \\(p\\) &lt; 0.05 일 때, 모형이 완전한 적합도을 보이지 못한다고 판단할 수 있음.\n즉, 공분산의 잔차 중 일부가 모집단에서 0이 아닐 수 있음을 의미함.\n\n\n# Fitting function의 최소값\nfitmeasures(fit_sa, \"fmin\") |&gt; print(nd = 4)\n\n  fmin \n0.0349 \n\n\n\n# 설정한 모형의 log-likelihood & unrestricted 모형의 log-likelihood, chi-square 값\nfitMeasures(fit_sa, c(\"chisq\", \"pvalue\", \"logl\", \"unrestricted.logl\")) |&gt; print()\n\n            chisq            pvalue              logl unrestricted.logl \n            9.635             0.292         -1657.646         -1652.828 \n\n\n\nsummary(fit_sa, fit.measures = T, estimates = F) |&gt; print()\n\nlavaan 0.6-19 ended normally after 37 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                           138\n\nModel Test User Model:\n                                                      \n  Test statistic                                 9.635\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.292\n\nModel Test Baseline Model:\n\n  Test statistic                               473.058\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.996\n  Tucker-Lewis Index (TLI)                       0.993\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1657.646\n  Loglikelihood unrestricted model (H1)      -1652.828\n                                                      \n  Akaike (AIC)                                3341.291\n  Bayesian (BIC)                              3379.345\n  Sample-size adjusted Bayesian (SABIC)       3338.218\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.038\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.111\n  P-value H_0: RMSEA &lt;= 0.050                    0.526\n  P-value H_0: RMSEA &gt;= 0.080                    0.217\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.030\n\n\n\n\n\n\n\n\n\\(\\chi^2\\) Test에서 주의할 점\n\n\n\n\n변수들이 multivariate normal에서 벗어날수록 (특히 kurtosis) bias가 발생할 수 있음\n표본의 크기가 충분히 커야 함; 최소 100개 이상\n\n파라미터의 수에 비례해서 더 많은 표본이 필요함; 가령 N &lt; 200, p &gt; 30 이면, \\(\\chi^2\\) 값이 부풀려짐.\n\n변수 간의 상관관계가 높을수록 틀린 모형에 대한 \\(\\chi^2\\) 값이 커짐\n측정에 대한 신뢰도(reliability)가 낮을수록 통계적 검증력(power)를 낮춰 가설을 기각하기 어려움.\n영가설이 “완벽한 모형”(\\(\\Sigma = \\Sigma(\\theta)\\))이라는 점에서 현실적인 가설이 아님\n기존의 가설 검증의 문제점을 그대로 지니고 있음; statistical significance vs. substantive significance\n\nN이 큰 경우, \\(\\chi^2\\)값이 커져 가설을 쉽게 기각할 수 있음; 실제로는 \\(\\Sigma\\)와 \\(\\Sigma(\\theta)\\)의 차이가 사소함에도 불구하고, 완벽한 모형은 아니라고 판단할 수 있음.\n반대로, N이 작은 경우, \\(\\chi^2\\)값이 작아져 가설을 기각하기 어려움; 실제로는 \\(\\Sigma\\)와 \\(\\Sigma(\\theta)\\)의 차이가 크더라도 완벽한 모형으로 판단할 수 있음.\n\n\n\n\n모든 적합도 지수는 \\(\\mathbf{S}\\)와 \\(\\hat{\\Sigma}(\\theta)\\)의 함수임.\n특히, fitting function \\(F_{ML}\\)은 \\(\\mathbf{S}\\)와 \\(\\hat{\\Sigma}(\\theta)\\)의 차이를 측정해주는 스칼러 함수임.\n\n\\(F_{ML} \\to 0\\) as \\(N \\to \\infty\\)\n추정할 파라미터를 늘리면 \\(F_{ML}\\)는 감소함; 즉, 항상 더 나쁜 모형이 됨.\n\n\\(F_{ML}\\) 값 자체는 해석하기 어렵기 때문에, \\(F_{ML}\\)을 이용한 \\(\\mathbf{S}\\)와 \\(\\hat{\\Sigma}(\\theta)\\)의 차이를 측정해주는 다른 지표들이 유용함.\n\n\n\n\n\n\nModel fit indices\n\n\n\n\\(\\chi^2\\) 테스트가 가설 검증의 논리로 yes/no를 판단하는 것인 반면,\n적합도 지표들은 모형이 데이터에 얼마나 잘 부합하는지를 연속적인 값으로 표현함.\n과거 많은 지표들이 제안되어 왔으나, 시뮬레이션 연구등을 통해 현재까지 살아남아 널리 사용되고 보고되는 지표들은\nRMSEA, CFI/TLI, SRMR\n\n\n\n\n\n\n\n\n일화\n\n\n\n\nSörbom(2001)이 Karl Jöreskog와 함께 한 1985년 워크숍에 대한 일화는 방금 설명한 근사적 적합도 지수에 대한 맥락을 제공합니다.\n우리는 방금 GFI와 AGFI를 프로그램에 추가했습니다. Karl은 강의에서 카이 제곱만 있으면 충분하다고 말했습니다. 그러자 한 참가자가 “그럼 왜 GFI를 추가했나요?”라고 물었습니다. 그러자 Karl은 “음, 사용자들이 LISREL이 항상 그렇게 큰 카이 제곱을 생성하면 사용을 중단하겠다고 위협합니다. 그래서 사람들을 행복하게 할 무언가를 발명해야 했습니다. GFI가 그 목적에 부합합니다. (p. 10)\nby Google Translate\n\n\nSource: p. 164, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n\n\n\n\n\nIncremental Fit Index\nBase 모형과 비교하여 (추가된 파라미터로 인해) 연구자의 모형이 얼마나 개선되었는지를 측정함.\nBase 모형은 기본적으로 가장 적합도가 낮은 모형, 즉 모든 변수들이 서로 독립이라고 가정한 모형으로 선택: baseline model, indepdendent(null) model\n이는 \\(R^2\\)와 유사한 접근임.\n\\(\\displaystyle\\Delta_1 = \\frac{\\chi^2_b - \\chi^2_m}{\\chi^2_b - df_m}\\)\n\nN의 크기에 덜 영향을 받도록 조정했음.\n연구자의 모형이 맞다면, 분자의 값들의 평균을 분모의 값으로 이해할 수 있음. (\\(E(\\chi^2_m) = df_m\\))\n\nTucker–Lewis Index (TLI): Tucker & Lewis (1973), Bentler & Bonnett (1980)\nTLI = \\(\\displaystyle\\frac{\\chi^2_b/df_b - \\chi^2_m/df_m}{\\chi^2_b/df_b - 1}\\)\n\n동일한 \\(F_{ML}\\)을 가질 때, 더 복잡한 모형(파라미터의 개수가 더 많은)이 더 낮은 \\(df_m\\)를 가지게 되어 더 높은 값을 갖도록 함.\n소위 parsimony-adjusted, 혹은 complexity에 대한 penalty를 부여함.\n동시에 N에 따른 표본분포의 평균이 변하지 않도록 조정함.\n\n\n\nRMSEA\nSteiger–Lind Root Mean Square Error of Approximation (Steiger, 1990)\n\\(\\displaystyle RMSEA = \\sqrt{\\frac{\\hat{\\delta}}{df_m(N-1)}} = \\sqrt{\\frac{\\chi^2_{ML} - df_m}{df_m(N-1)}} = \\sqrt{\\frac{F_{ML}}{df_m} - \\frac{1}{N-1}}\\)\n\n연구자의 모형이 맞다면, \\(\\chi^2_{ML}\\) 값의 표본 분포가 \\(\\chi^2(df_m)\\)를 따르는데 반해,\n연구자의 모형이 맞지 않다면, non-central \\(\\chi^2(df_m, ~\\delta)\\) 분포를 따름\n\n\\(\\delta\\): non-centrality parameter\n평균은 \\(df_m + \\delta\\)임.\n\n\\(\\delta\\): 완전한 모형에서 벗어난 정도라도 볼 수 있고, \\(\\chi^2_{ML} - df_m\\)로 추정할 수 있음.\n\n\\(\\delta = 0\\)이면, 연구자의 모형이 완벽하고 central \\(\\chi^2(df_m)\\) 분포를 따름.\n음수가 되지 않도록 \\(max(\\chi^2_{ML} - df_m, 0)\\)으로 계산함.\n\n\nRMSEA에 대한 confidence interval:\n\\(\\displaystyle RMSEA_{lower} = \\sqrt{max\\left(0, \\frac{\\hat{\\delta}_{.05}}{(N-1)df_m}\\right)}\\),    \\(\\displaystyle RMSEA_{upper} = \\sqrt{\\frac{\\hat{\\delta}_{.95}}{(N-1)df_m}}\\)\nRMSEA &lt;= 0.05에 대한 가설검증: 0.05보다 작을 때, 모형이 데이터에 잘 부합한다고 보고 가설검증의 기준으로 사용함.\n\n\n\n\n\n\nRMSEA의 해석\n\n\n\nBrowne과 Cudeck(1993)은 \\(\\hat\\epsilon\\) ≤ .05가 유리한 결과라고 제안했지만, Chen et al.(2008)의 이후 컴퓨터 시뮬레이션 연구 결과는 \\(\\hat\\epsilon\\)이 단독으로 사용되든 90% 신뢰 구간과 함께 사용되든 .05 또는 다른 값의 보편적 임계값을 거의 뒷받침하지 않는다는 것을 나타냈습니다.\nBrowne과 Cudeck(1993)은 또한 \\(\\hat\\epsilon\\) ≥ .10이 모델 적합도가 낮음을 나타낼 수 있지만 보장은 없다고 제안했습니다.\nBrowne and Cudeck (1993) suggested that \\(\\hat\\epsilon\\) ≤ .05 is a favorable result, but results of later computer simulation studies by Chen et al. (2008) indicated little support for a universal threshold of .05—or any other value—regardless of whether \\(\\hat\\epsilon\\) is used alone or jointly with its 90% confidence interval.\nBrowne and Cudeck (1993) also suggested that \\(\\hat\\epsilon\\) ≥ .10 may indicate poor model fit, but there is no guarantee.\n\nSource: p. 167, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n\n\n\n예를 들어, 위의 CFA 예에서, 표본의 크기 N=200으로 생성된 데이터로부터 얻은 결과로 보면,\n\\(\\chi^2_{ML}(8) = 13.963\\)을 얻고, \\(\\hat\\delta = 13.963 - 8 = 5.963\\)임.\n따라서, \\(\\chi^2_{ML}\\)값의 표본 분포는 \\(\\chi^2(8, ~5.963)\\)를 따름.\n\nset.seed(123)\ndf &lt;- semTools::kd(cov, n = 200, type = \"exact\") |&gt; as_tibble()\nmod_sa &lt;- \"\n  sympathy =~ x1 + x2 + x3\n  anger =~ x4 + x5 + x6\n\"\nfit_sa &lt;- sem(mod_sa, data = df)\nfitMeasures(fit_sa, c(\"chisq\", \"df\")) |&gt; print()\n\n chisq     df \n13.963  8.000 \n\n\n\n\n\n\n\n\n\n\n\n\nfitMeasures(fit_sa, c(\"rmsea\", \"rmsea.ci.lower\", \"rmsea.ci.upper\", \"rmsea.pvalue\")) |&gt; print()\n\n         rmsea rmsea.ci.lower rmsea.ci.upper   rmsea.pvalue \n         0.061          0.000          0.113          0.317 \n\n\n\n\nCFI\nComparative Fit Index\n\\(\\displaystyle CFI = \\frac{\\hat\\delta_b - \\hat\\delta_m}{\\hat\\delta_b}\\)\n\n\\(\\hat\\delta_b = max(0, \\chi^2_{b} - df_m)\\),   \\(\\hat\\delta_m = max(0, \\chi^2_{ML} - df_m)\\)\n\nBase 모형에 비해 연구자 모형의 non-centrality parameter가 얼마나 줄었는지 그 비율을 측정함.\nTLI와 상관이 매우 높아, 주로 두 지표 중 하나만 보고함.\n\n\n\n\n\n\n\nCFI의 해석\n\n\n\nHu와 Bentler(1995)는 CFI ≥ .95가 유리한 결과라고 제안했는데, 이 기준은 Hu와 Bentler(1999)와 같은 일부 후기 몬테카를로 연구의 결과와 일반적으로 일치합니다. 하지만 훨씬 더 최근의 시뮬레이션 연구의 결과는 모델의 변화와 데이터의 비정규성 정도에 대한 CFI에 대한 특정 임계값의 일반성을 뒷받침하지 못했습니다(Fan & Sivo, 2005; Yuan, 2005). Brosseau-Liard와 Savalei(2014)가 설명한 비정규성에 대한 CFI의 robust 형태는 lavaan이 robust ML 방법에 따라 인쇄합니다.\nHu and Bentler (1995) suggested that CFI ≥ .95 is a favorable result, a benchmark generally consistent with results from some later Monte Carlo studies, such as Hu and Bentler (1999). But results from even more recent simulation studies failed to support the generality of any specific cutoff for the CFI over variations in models and degrees of nonnormality in the data (Fan & Sivo, 2005; Yuan, 2005). Robust forms of the CFI for nonnormal described by Brosseau-Liard and Savalei (2014) are printed by lavaan for robust ML methods.\n\nSource: p. 169, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n\n\n\n\n\nSRMR\nStandardized Root Mean Square Residual\n\\(\\displaystyle SRMR = \\sqrt{Ave\\left[\\left(\\frac{\\sigma_{ij} - \\hat\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}\\right)^2\\right]}\\)\n\n상관계수의 잔차의 제곱의 평균에 대한 제곱근에 근사로 볼 수 있음.\n대략적으로 상관계수의 차이의 평균으로 해석할 수 있음.\n프로그램마다 조금 다르게 계산될 수 있음.\nSRMR &lt; .08 일반적으로 유리한 것으로 간주됨(Hu & Bentler, 1999)\n하지만, 잔차 상관행렬의 각 성분을 직접 살펴보는 것이 더 좋음; 절대값이 0.1 이상이면 주의\n\n\n\n\n\n\n\nThresholds for Fit Indices\n\n\n\nKeith’s 교재 14장 ADVICE: MEASURES OF FIT 참고.\n다음은 Kline’s(2023)에서 발췌\n연속 근사 적합 지수에 대한 자연스러운 질문은 “허용 가능한” 또는 “좋은” 모델 적합도를 나타내는 값 범위와 관련이 있습니다. 안타깝게도 이 질문에 대한 간단한 답은 없습니다. 근사 적합 지수 값과 사양 오류의 심각성 또는 유형 사이에 카이 제곱 검정과 마찬가지로 직접적인 대응 관계가 없기 때문입니다. 또한 SEM에 대한 문헌이나 웹 페이지에서 볼 수 있는 RMSEA, CFI, SRMR 등의 지수에 대한 많은 해석 지침 또는 경험 규칙은 신뢰할 수 없습니다. 즉, 다양한 분야에서 분석된 광범위한 모델과 데이터에서 모델이 데이터에 “좋은” 적합도와 “나쁜” 적합도를 실제로 구별한다는 증거가 거의 없습니다.\nA natural question about continuous approximate fit indexes involves the range of values indicating “acceptable” or even “good” model fit. Well, unfortunately, there is no simple answer to this question because there is no direct correspondence between values of approximate fit indexes and the seriousness or types of specification error, just as for the chi-square test. Also, many interpretive guidelines or rules of thumb for the RMSEA, CFI, and SRMR, among other indexes, seen in the literature or web pages about SEM are untrustworthy; that is, there is little evidence that they actually differentiate between model “good” versus “poor” fit to the data across the wide range of models and data analyzed in different disciplines.\n\nSource: p. 170, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n\n\n\n\n\n\n\n\n\nDynamic (flexible) thresholds\n\n\n\nFlexible Cutoffs: Package FCO\nTailored cutoff values are generated for the RMSEA, CFI, TLI, and SRMR depending on the CFA model, the degree of nonnormality, and the amount of accepted uncertainty.\nDynamic Fit Index Cutoffs For Latent Variable Models\nSimulates fit index cutoffs for latent variable models that are tailored to the user’s model statement, model type, and sample size.\n\nSource: p. 171, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n\n\n\nKeith’s 교재 14장 ADVICE: MEASURES OF FIT 참고\n흔히 적용하는 적합도 지수의 임계값들:",
    "crumbs": [
      "SEM Equations"
    ]
  },
  {
    "objectID": "contents/equations.html#적합도에-평가에-대한-가이드라인-by-klein2023",
    "href": "contents/equations.html#적합도에-평가에-대한-가이드라인-by-klein2023",
    "title": "Structural Equations",
    "section": "적합도에 평가에 대한 가이드라인 by Klein(2023)",
    "text": "적합도에 평가에 대한 가이드라인 by Klein(2023)\n\nSource: p. 172, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n\n번역 by Google Translate\n다음에 설명된 방법은 SEM(Appelbaum et al., 2018)에 대한 보고 표준과 일치하며 연구자에게 과거 경험에서 사실이었던 것보다 더 많은 정보를 보고할 것을 요구합니다.\n\n동시 추정 방법을 사용하는 경우 자유도와 p 값이 있는 모델 카이 제곱을 보고합니다. 결과 변수가 이분형이고 경로 계수가 이진 로지스틱 또는 프로빗 회귀 방법을 사용하여 추정되는 경우와 같이 모델 카이 제곱을 사용할 수 없는 일부 분석이 있습니다. 예를 들어 Muthén 및 Muthén(1998–2017, 3장)을 참조하세요. 그러나 이는 예외적인 경우입니다.\n모델이 정확한 적합성 검정(exact-fit test)에 실패하면 (a) 직접 그렇게 말하고 표본 크기에 관계없이 (b) 모델을 잠정적으로 거부합니다. 다음으로 (c) 부적합의 크기와 가능한 원인을 모두 진단합니다(로컬 적합성 검사). 그 이유는 통계적으로 유의하지만 미미한 모델-데이터 불일치를 감지하여 실패를 설명하는 것입니다. 이는 대규모 샘플에서 발생할 가능성이 가장 높습니다. 모델을 거부하기로 한 초기 결정은 철회될 수 있지만, 관찰된 모델-데이터 불일치가 실제로 중요하지 않은 이유에 대한 설명과 함께 로컬 적합성 증거에 근거해야 합니다.\n모델이 정확한 적합성 검정(exact-fit test)을 통과하더라도 여전히 지역적 적합성을 검사해야 합니다. 그 이유는 통계적으로 유의하지 않지만 모델에 의심을 품기에 충분히 큰 모델-데이터 불일치를 감지하는 것입니다. 이는 소규모 샘플에서 발생할 가능성이 가장 높습니다. 지역적 적합성에 대한 증거가 상당한 불일치를 나타내는 경우 카이제곱 검정을 통과했더라도 모델을 거부해야 합니다.\n상관 관계, 표준화 또는 정규화된 잔차와 같은 잔차 행렬을 원고 본문에 보고합니다. 모델이 너무 커서 다루기 힘들다면 (a) 보충 자료에 이러한 표를 제공하고 (b) 논문에 더 큰 잔차의 위치와 부호와 같은 잔차 패턴을 설명합니다. 모델이 어떻게 잘못 지정되었는지 이해하는 데 진단적 가치가 있을 수 있는 패턴을 찾습니다. 잔차에 대한 정보가 없는 결과 보고서는 불충분합니다. 불행히도 이 분야에서 불완전한 보고는 예외가 아니라 표준입니다. 예를 들어, 조직 관리 분야에서 발표된 144개의 SEM 연구를 검토한 결과, 이러한 작업의 약 17%에서만 잔차가 언급되었습니다(Zhang et al., 2021).\n근사적 적합 지수 값을 보고하는 경우 이 장에서 앞서 설명한 최소 집합(RMSEA, CFI, SRMR)에 대한 값을 포함합니다. 그러나 이러한 전역적 적합 통계에 대해 고정 또는 동적 임계값에만 의존하여 모델을 유지하는 것을 정당화하려고 하지 마십시오. 이는 특히 모델이 정확한 적합 테스트(exact-fit test)에 실패하고 잔차 패턴이 사소하지 않은 모형의 설정 오류(specification error)를 시사하는 경우에 해당합니다.\n초기 모델을 다시 지정하는 경우 그렇게 하는 근거를 설명하세요. 또한 잔차와 같은 진단 통계가 재지정에서 어떤 역할을 했는지 설명해야 합니다. 즉, 모델에 대한 양적 결과, 관련 이론, 원래 모델의 수정 사항 간의 연결을 지적하세요(3장). 정확한 적합도 검정(exact-fit test)에 여전히 실패한 다시 지정된 모델을 유지하는 경우 모델-데이터 불일치가 진정으로 미미하다는 것을 보여주세요. 그렇지 않으면 모델에 대한 상당한 공분산 증거가 없다는 것을 보여주지 못한 것입니다.\n모델 적합도에 대한 통계적 증거는 중요하지만 모델을 유지할지 여부를 결정하는 유일한 요소는 아닙니다. 예를 들어, 연구 문제를 감안할 때 파라미터 추정치는 타당해야 합니다. 동일한 인과 과정으로 생성된 향후 재현연구들에서 데이터 세트에 적합할 가능성이 합리적으로 높은 모델은 특정 데이터 세트에 잘 맞는 모델보다 선호되어야 합니다. 이는 잠재적으로 임의의 모든 데이터에 적합할 수 있는 복잡하고 과도하게 많은 파라미터가 포함된 모델의 경우에 특히 그렇습니다. 이러한 모델은 (a) 보다 간결한 모델보다 반증 가능성이 낮고 (b) 샘플과 설정의 변화에 ​​대해 일반화할 가능성이 낮습니다(Preacheret al., 2013).\n모델이 유지되는 경우 연구자는 각각 동일한 데이터를 정확히 또는 거의 잘 설명하는 동등하거나 거의 동등한 버전보다 해당 모델이 선호되어야 하는 이유를 설명해야 합니다. 이 단계는 통계적 단계보다 훨씬 논리적이며, 심각한 경쟁 모델을 구별하기 위해 향후 연구에서 수행할 수 있는 작업을 설명하는 것도 포함됩니다. 동등하거나 거의 동등한 모델에 대한 완전한 보고는 드뭅니다. 따라서 신중한 독자는 이런 방식으로 자신의 SEM 분석을 두드러지게 만들 수 있습니다.\n어떤 모델도 유지되지 않는 경우 분석에서 테스트한 이론에 대한 의미를 설명하는 데 학자로서의 기술이 필요합니다.\n\n결국 모델을 유지했는지 여부에 관계없이 진정한 영예는 최선을 다해 철저한 평가 프로세스를 논리적인 끝까지 따르는 데서 나옵니다. 시인 랄프 왈도 에머슨은 이렇게 말했습니다. 잘한 일에 대한 보상은 그것을 해냈다는 것입니다(Mikis, 2012, p. 294).\n\n\n\n\n\n\n원문\n\n\n\n\n\nThe method outlined next is consistent with reporting standards for SEM (Appelbaum et al., 2018) and also calls on researchers to report more information about model fit than has been true in past experience:\n\nIf you use a simultaneous estimation method, report the model chi-square with its degrees of freedom and p value. There are some analyses, such as when outcome variables are dichotomous and path coefficients are estimated using methods for binary logistic or probit regression, where a model chi-square may be unavailable—see Muthén and Muthén (1998–2017, chap. 3) for examples—but these are exceptional cases.\nIf the model fails the exact-fit test, then (a) directly say so and, regardless of sample size, (b) tentatively reject the model. Next, (c) diagnose both the magnitude and possible sources of misfit (inspect local fit). The rationale is to detect statistically significant but slight model–data discrepancies that explain the failure. This is most likely to happen in a large sample. The initial decision to reject the model could be rescinded, but only based on local fit evidence along with explanation about why observed model–data discrepancies are actually inconsequential.\nIf the model passes the exact-fit test, you still have to inspect local fit. The rationale is to detect model–data discrepancies that are not statistically significant but still great enough to cast doubt on the model. This most likely occurs in a small sample. If evidence about local fit indicates appreciable discrepancies, then the model should be rejected even though it passed the chisquare test.\nReport a matrix of residuals, such as correlation, standardized, or normalized residuals, in the body of the manuscript. If the model is so large that doing so would be unwieldy, then (a) provide such tables in the supplemental materials and (b) describe in the manuscript the pattern of residuals, such as the locations of larger residuals and their signs. Look for patterns that may be of diagnostic value in understanding how the model may be misspecified. Any report of the results without information about the residuals is deficient. Unfortunately, incomplete reporting in this area is the norm rather than the exception. For example, in our review of 144 published SEM studies in the area of organizational management, residuals were mentioned in only about 17% of these works (Zhang et al., 2021).\nIf you report values of approximate fit indexes, then include those for the minimal set described earlier in this chapter. But do not try to justify retaining the model by depending solely on thresholds, either fixed or dynamic, for such global fit statistics. This is especially true if the model failed the exact-fit test and the pattern of residuals suggests a specification error that is not trivial.\nIf you respecify the initial model, explain the rationale for doing so. You should also explain the role that diagnostic statistics, such as residuals, played in the respecification. That is, point out the connections between the numerical results for the model, relevant theory, and modifications to the original model (Chapter 3). If you retain a respecified model that still fails the exact-fit test, then demonstrate that model–data discrepancies are truly slight; otherwise, you have neglected to show that there is no appreciable covariance evidence against the model.\nStatistical evidence about model fit is important, but it is not the sole factor in deciding whether to retain a model. For example, the parameter estimates should make sense, given the research problem. Models with reasonable prospects for fitting data sets in future replications generated by the same causal processes should be preferred over models that fit a particular data set well. This is especially true for complex, overparameterized models that could potentially fit just about any arbitrary data. Such models are (a) less falsifiable than more parsimonious models and (b) less likely to generalize over variations in samples and settings (Preacheret al., 2013).\nIf a model is retained, then the researcher should explain why that model should be preferred over equivalent or near-equivalent versions that, respectively explain the same data exactly as well or nearly as well. This step is much more logical than statistical, and it also involves describing what might be done in future research to differentiate between any serious competing models. Complete reporting about equivalent or near-equivalent models is rare; thus, conscientious readers can really distinguish their own SEM analyses by addressing this issue. The generation and assessment of equivalent versions of structural models are covered in the next chapter.\nIf no model is retained, then your skills as a scholar are needed to explain the implications for the theory tested in your analysis.\n\nAt the end of the day, regardless of whether or not you have a retained a model, the real honor comes from following, to the best of your ability, a thorough evaluation process to its logical end. The poet Ralph Waldo Emerson put it this way: The reward of a thing well done is to have done it (Mikis, 2012, p. 294).",
    "crumbs": [
      "SEM Equations"
    ]
  },
  {
    "objectID": "contents/equations.html#local-fit-measures",
    "href": "contents/equations.html#local-fit-measures",
    "title": "Structural Equations",
    "section": "Local Fit Measures",
    "text": "Local Fit Measures\nParameter estimation\n\n개별 파라미터의 추정값이 기대 혹은 이론과 얼마나 부합하는지 평가\n부적절한 추정값은 모형의 부적합을 나타낼 수 있음; 모집단에서 불가능한 추정값\n\n추정된 분산이 음수일 때; Heywood case\n추정된 상관계수가 1보다 클 때\n\n실제로 경계값(boundary)일 가능성이 있다면 수용할 수 있음\n그렇지 않다면,\n\n모형이 심하게 잘못 지정되었을 가능성이 있음\n이상치(outlier)가 존재할 수 있음\n매우 이상한 표본을 사용했을 수 있음\n\n\n\\(R^2\\): 잔차의 분산(\\(1-R^2\\))을 통해 설명력을 평가할 필요가 있음",
    "crumbs": [
      "SEM Equations"
    ]
  },
  {
    "objectID": "contents/equations.html#respecification",
    "href": "contents/equations.html#respecification",
    "title": "Structural Equations",
    "section": "Respecification",
    "text": "Respecification\n모형이 데이터에 부합하지 않는다고 판단될 때, 모형의 specification을 조정하여 모형을 개선할 수 있음.\n\n탐색적 분석의 성격이 강하므로, 가이드/조언 정도로 접근할 필요가 있음.\n\n이론적 정당화가 반드시 필요함.\n이상적으로, 새로운 표본을 이용하여 재검증하는 것이 바람직함.\n\n다음과 같이 기계적인 절차를 통한 적합도 개선은 피해야 함.\n\n파라미터를 추가로 추정(free parameter)하는 절차를 통해 점차 적합도를 계속 높일 수 있음\n적합도가 높아져 데이터에 더 잘 적합(fit)되지만, 그것이 더 진실된 모형이 되는 것은 아님.\n\n모형의 복잡도가 증가하는 것에 대한 패널티가 적용되는 적합도 지수도 있음.\n\n반대로, 파라미터의 제약을 늘려 적합도가 심각히 낮아지지 않는 선까지 모형을 단순화할 수 있음.\n\n\n\n\n\n\n\nEmprical vs. Theoretical\n\n\n\n\n가령, 아래 그림에서 b를 free parameter로 추정한 후 유의하지 않다고 통계적으로 파악되었기 때문에 경로 b에 제약을 가해 오른쪽 모형을 만드는 것은 올바르지 못함; empirical/statistical-based\n\n모형에 대한 설정은 이론적인 근거를 바탕으로 해야 함; theory/substantive-based\n\n \n\n\n\n\n예제: CFA of perceived air quality: an environmental study of the relation between objective and subjective air quality\n\nx1: overall air quality\nx2: the clarity\nx3: color\nx4: odor\n\n\nSource: p. 297, Bollen, K. A. (1989). Structural Equations with Latent Variables.\n\n\n\n\n\n\nlower &lt;- \"\n  0.331\n  0.431 1.160\n  0.406 0.847 0.898\n  0.216 0.272 0.312 0.268\n\"\ncov &lt;- getCov(lower, names = c(\"overall\", \"clarity\", \"color\", \"odor\"))\n\nmodel_air &lt;- '\n  air_quality =~ overall + clarity + color + odor\n'\nfit_air &lt;- sem(model_air, sample.cov = cov, sample.nobs = 74)\nsummary(fit_air, fit.measures = T, standardized = \"std.all\") |&gt; print()\n\nlavaan 0.6-19 ended normally after 23 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         8\n\n  Number of observations                            74\n\nModel Test User Model:\n                                                      \n  Test statistic                                21.194\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               211.967\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.907\n  Tucker-Lewis Index (TLI)                       0.720\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -234.488\n  Loglikelihood unrestricted model (H1)       -223.891\n                                                      \n  Akaike (AIC)                                 484.975\n  Bayesian (BIC)                               503.408\n  Sample-size adjusted Bayesian (SABIC)        478.197\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.360\n  90 Percent confidence interval - lower         0.232\n  90 Percent confidence interval - upper         0.506\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.063\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n  air_quality =~                                               \n    overall           1.000                               0.817\n    clarity           1.977    0.223    8.865    0.000    0.863\n    color             1.894    0.195    9.717    0.000    0.939\n    odor              0.754    0.117    6.445    0.000    0.685\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n   .overall           0.109    0.022    5.023    0.000    0.332\n   .clarity           0.293    0.066    4.427    0.000    0.256\n   .color             0.104    0.043    2.396    0.017    0.118\n   .odor              0.140    0.025    5.654    0.000    0.531\n    air_quality       0.218    0.052    4.186    0.000    1.000\n\n\n\nColor의 측정오차와 clarity의 측정오차가 상관관계가 있을 수 있음.\n\n“설문지를 실시한 사람은 일부 응답자가 색상과 선명도 질문의 차이를 구별하는 데 어려움을 겪었다고 밝혔습니다. 이는 맑은 공기는 색상이 없을 것으로 예상하고, 반대로 공기의 색상이 나쁘면 선명도 평가도 나쁠 것이기 때문에 타당해 보였습니다. 한 지표에 대한 응답 오류가 부분적으로 구분할 수 없는 다른 지표의 오류와 상관관계가 있을 수 있기 때문에 선명도 측정 오류가 색상 측정 오류와 상관관계가 있었을 수 있습니다.”\n\n\nmodel_air_modi &lt;- '\n  air_quality =~ overall + clarity + color + odor\n  color ~~ clarity\n'\nfit_air_modi &lt;- sem(model_air_modi, sample.cov = cov, sample.nobs = 74)\nsummary(fit_air_modi, fit.measures = T, standardized = \"std.all\") |&gt; print()\n\nlavaan 0.6-19 ended normally after 26 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                            74\n\nModel Test User Model:\n                                                      \n  Test statistic                                 5.581\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.018\n\nModel Test Baseline Model:\n\n  Test statistic                               211.967\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.978\n  Tucker-Lewis Index (TLI)                       0.867\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -226.681\n  Loglikelihood unrestricted model (H1)       -223.891\n                                                      \n  Akaike (AIC)                                 471.363\n  Bayesian (BIC)                               492.099\n  Sample-size adjusted Bayesian (SABIC)        463.737\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.249\n  90 Percent confidence interval - lower         0.082\n  90 Percent confidence interval - upper         0.466\n  P-value H_0: RMSEA &lt;= 0.050                    0.029\n  P-value H_0: RMSEA &gt;= 0.080                    0.952\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.024\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n  air_quality =~                                               \n    overall           1.000                               0.940\n    clarity           1.438    0.208    6.914    0.000    0.722\n    color             1.396    0.175    7.959    0.000    0.797\n    odor              0.738    0.096    7.679    0.000    0.771\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n .clarity ~~                                                   \n   .color             0.256    0.076    3.364    0.001    0.608\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n   .overall           0.038    0.025    1.493    0.135    0.115\n   .clarity           0.547    0.107    5.116    0.000    0.478\n   .color             0.323    0.072    4.501    0.000    0.365\n   .odor              0.107    0.022    4.852    0.000    0.405\n    air_quality       0.289    0.059    4.923    0.000    1.000\n\n\n\n\nModification indices\n\n파라미터가 추가로 추정할 때 \\(\\chi^2\\) 값이 얼마나 감소하는지를 추정; Lagrangian multiplier test\n한번에 하나씩 변화시켜야 함; 하나를 수정하면, 다른 것들이 영향을 받을 수 있음\n\n\nmodificationIndices(fit_air, sort = T) |&gt; print()\n\n       lhs op     rhs     mi    epc sepc.lv sepc.all sepc.nox\n13 clarity ~~   color 16.282  0.342   0.342    1.959    1.959\n12 overall ~~    odor 16.282  0.069   0.069    0.558    0.558\n14 clarity ~~    odor  9.906 -0.097  -0.097   -0.479   -0.479\n11 overall ~~   color  9.906 -0.123  -0.123   -1.159   -1.159\n10 overall ~~ clarity  0.254 -0.019  -0.019   -0.108   -0.108\n15   color ~~    odor  0.254 -0.014  -0.014   -0.115   -0.115\n\n\n\nfitMeasures(fit_air, c(\"chisq\", \"df\")) |&gt; print()\nfitMeasures(fit_air_modi, c(\"chisq\", \"df\")) |&gt; print()\n\n chisq     df \n21.194  2.000 \nchisq    df \n5.581 1.000 \n\n\n\n# Chi-Squared Difference Test\nanova(fit_air, fit_air_modi) |&gt; print()\n\n# 또는 \nlavTestLRT(fit_air, fit_air_modi) |&gt; print()\n\n\nChi-Squared Difference Test\n\n             Df    AIC    BIC   Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nfit_air_modi  1 471.36 492.10  5.5814                                          \nfit_air       2 484.98 503.41 21.1940     15.613 0.44437       1  7.774e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nChi-Squared Difference Test\n\n             Df    AIC    BIC   Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nfit_air_modi  1 471.36 492.10  5.5814                                          \nfit_air       2 484.98 503.41 21.1940     15.613 0.44437       1  7.774e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n공분산 잔차을 통해 수정한다면?\n\n잔차가 높은 변수 쌍에 문제가 있다는 것을 일부 암시해주지만, 반드시 문제가 있다고 볼 수 없음.\nimplied covariance는 여러 파라미터들의 함수이므로 복잡하게 작용됨.\n\n\nresiduals(fit_air, type = \"cor\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n        overll clarty  color   odor\noverall  0.000                     \nclarity -0.009  0.000              \ncolor   -0.023  0.019  0.000       \nodor     0.166 -0.103 -0.007  0.000\n\n\n\n\nresiduals(fit_air, type = \"normalized\") |&gt; print()\n\n$type\n[1] \"normalized\"\n\n$cov\n        overll clarty  color   odor\noverall  0.000                     \nclarity -0.065  0.000              \ncolor   -0.157  0.129  0.000       \nodor     1.153 -0.797 -0.054  0.000\n\n\n\n\nresiduals(fit_air, type = \"standardized\") |&gt; print()\n\n$type\n[1] \"standardized\"\n\n$cov\n        overll clarty  color   odor\noverall  0.000                     \nclarity -0.532  0.000              \ncolor   -3.700  3.200  0.000       \nodor     3.200 -3.700 -0.532  0.000\n\n\n\n\n\n\n\n\n\nTips for Inspecting the Residuals\n\n\n\nSource: p. 173, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n잔차에 대해 보고하는 것이 중요하지만, 카이제곱 검정의 결과와 근사 적합 지수 값과 마찬가지로 잔차 크기와 모델의 잘못된 지정(specification) 유형 또는 정도 사이에는 신뢰할 수 있는 연관성이 없다는 점을 알아야 합니다.\n예를 들어 상대적으로 작은 상관 잔차로 나타나는 지정(specification) 오류의 정도는 경미할 수도 있지만 심각할 수도 있습니다. 한 가지 이유는 다음 장에서 정의되는 수정 지수(modification index)를 포함한 잔차(residuals) 및 기타 진단 통계 값 자체가 잘못된 지정의 영향을 받기 때문입니다. 두 번째 이유는 모델의 한 부분의 잘못된 지정으로 인해 모델의 다른 부분의 추정이 왜곡되어 전역 추정(global estimation)으로 오류가 전파되기 때문입니다. 세 번째는 잔차가 동일하지만 인과 효과의 모순된 패턴을 갖는 등가(equivalent) 모델입니다. 그러나 우리는 일반적으로 모델의 어느 부분이 잘못된지 미리 알 수 없으므로 잔차가 우리에게 말하는 것을 정확히 이해하기 어려울 수 있습니다.\n잔차 패턴을 검사하는 것이 때로는 도움이 될 수 있습니다. \\(r_{XY}\\) &gt; 0인 한 쌍의 변수 X와 Y가 간접적인 인과 경로로만 연결되어 있다고 가정합니다. 해당 쌍의 잔차는 양수입니다. 즉, 모델이 해당 쌍의 연관성을 과소 예측한다는 의미입니다. 이 경우 X와 Y 사이에 직접적인 인과관계가 없다는 가설은 의심스러울 수 있으며, 따라서 이들 사이에 직접적인 인과관계를 추가하는 재지정(re-specification)이 고려해 볼 수 있습니다. 또 다른 가능성은 두 변수가 모두 내생변수인 경우 교란(disturbance) 상관 관계를 지정하는 것입니다. 그러나 모델에 추가할 효과 유형(인과 대 비인과 관계) 또는 방향성(예: X가 Y를 유발하고 Y가 X를 유발함)은 잔차가 우리에게 말해 줄 수 있는 것이 아닙니다. 전역 적합(global fit) 통계에 마법이 없는 것처럼 진단 통계에도 마법이 없습니다. 적어도 연구자들이 재지정(respecificaiton)에 대해 매우 신중하게 생각해야 하는 부담을 덜어줄 수 있는 것은 없습니다.\n번역 by Google Translate",
    "crumbs": [
      "SEM Equations"
    ]
  },
  {
    "objectID": "contents/equations.html#non-nested-models",
    "href": "contents/equations.html#non-nested-models",
    "title": "Structural Equations",
    "section": "Non-nested Models",
    "text": "Non-nested Models\nSource: pp. 190-194, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n동일한 데이터, 동일한 변수로 구성된 두 개의 서로 다른 모형이 내포되어 있지 않은 경우\n\n서로 다른 이론에 기반한 두 개 이상의 모형을 비교하는 경우가 종종 발생\n모델 적합도와 모델 복잡성을 모두 반영하여 이 두 속성이 서로 균형 이루도록 함; 복잡성에 대한 페널티가 부과\n비슷한 적합도를 가진 모형 중에서는 더 간단한 모형을 선호함; 모집단에서의 일반화 가능성이 높음!\n현재까지 매우 신뢰할 만한 지표는 없음\n정보이론에 기반하거나 bootstrapping을 통한 방법이 제안됨\n\n\nAIC\nAkaike Information Criterion\n\n3가지 버전이 있음; 프로그램마다 다름\n\n두 모형에 대한 차이를 보는 것이 목적이므로, 어떤 버전도 상관 없음\n표본의 수가 커질 수록, \\(L_0\\)가 감소하는데, 따라서 복잡성에 대한 페널티의 상대적 중요성이 감소함.\n\n연구자 모형의 log-likelihood를 \\(L_0\\), free parameter의 수를 \\(q\\)라 하면,\n\\(AIC = -2\\log L_0 + 2q\\) \\(\\begin{align}\nAIC_2 & = \\chi^2_{ML} + 2q \\\\~\n  & = -2\\log L_0 + 2\\log L_1 + 2q, ~~ L_1: likelihood ~for ~the ~perfect ~model\\\\\n\\end{align}\\)\n\\(AIC_3 = \\chi^2_{ML} - 2df_M\\)\n\n\nBIC\nBayesian Information Criterion\n\n모형의 복잡성에 대한 페널티가 AIC에 비해 (표본이 커질 수록) 더 큼\n표본의 수가 커질 수록 \\(L_0\\)가 감소하는 것을 상쇄시키면서 복잡성에 대한 페널티를 더 강하게 부과함.\nAIC와 BIC는 전혀 다른 이론적 기반을 가지고 있음\n\n\\(BIC = -2\\log L_0 + q\\log N\\)\n\\(BIC_2 = \\chi^2_{ML} + q\\log N\\)\n\nAIC는 BIC보다 복잡한 모델을 선호하는 경향이 있음.\n모델이 너무 복잡해지는 과적합(overfitting)이 우려되는 경우, BIC가 더 적합할 수 있음.\n모델이 너무 단순해지는 과소적합(underfitting)이 더 큰 문제라면, AIC가 유리할 수 있음.\n탐색적 요인 분석에서 잠재 변수를 정확하게 추정하는 것이 목표일 때:\n\n요인을 너무 많이 추정하는 것이 더 큰 오류로 간주된다면, BIC가 AIC보다 나은 선택일 수 있음.\n요인을 너무 적게 추정하는 것이 더 심각한 오류라면, AIC가 더 유리할 수 있음.\n\nLin et al. (2017)의 매개경로 모델 시뮬레이션 결과에 따르면:\n\n잘못 지정된 파라미터가 적을 때, AIC가 더 정확했으며, BIC는 지나치게 간결한 모델을 선택하는 경향이 있었음.\n잘못 지정된 파라미터가 많을 때, BIC가 더 정확해졌고, AIC는 지나치게 복잡한 모델을 선택함.",
    "crumbs": [
      "SEM Equations"
    ]
  },
  {
    "objectID": "contents/chap21.html",
    "href": "contents/chap21.html",
    "title": "Chapter 21. Latent Growth Models",
    "section": "",
    "text": "Load libraries\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\n\n\n\nlibrary(haven)\nmath_growth &lt;- read_sav(\"data/chap 21 latent growth models/math growth final.sav\")\nmath_growth |&gt; print()\n\n# A tibble: 1,000 x 9\n  math1 math2 math3 math4 math5   sex   age ParEd Cognitive\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1  57.4  65.8  78.7  86.0  93.8     1  68.2    16        95\n2  81.4  87.9  94.2 104.  111.      1  70.8    15       104\n3  74.7  82.0  95.5 109.  120.      0  65.4    15       107\n4  71.1  80.4  88.2  97.8 102.      1  68.9    14       121\n5  70.1  79.1  85.2  92.1 102.      1  69.8    17       112\n6  71.7  84.2  86.2  88.2  94.9     0  67.5    19        69\n# i 994 more rows\n\n\n\nUnconditional growth model\n\n# Unconditional growth model\nlgm_math_model &lt;- '\n\n  # latents\n  Initial =~ 1*math1 + 1*math2 + 1*math3 + 1*math4 + 1*math5\n  Growth =~ 0*math1 + 1*math2 + 2*math3 + 3*math4 + 4*math5\n  \n  # intercepts\n  Initial ~ 1\n  Growth ~ 1\n  \n  # residual (co)variances\n  Growth ~~ Initial\n  \n  math1 ~~ a*math1\n  math2 ~~ a*math2\n  math3 ~~ a*math3\n  math4 ~~ a*math4\n  math5 ~~ a*math5\n\n  # not neeed when using growth() function instead sem()\n  math1 ~ 0*1\n  math2 ~ 0*1\n  math3 ~ 0*1\n  math4 ~ 0*1\n  math5 ~ 0*1\n  # same as \"math1 + math2 + math3 + math4 + math5 ~ 0*1\"\n'\n\nlgm_math &lt;- sem(lgm_math_model, data = math_growth)\nsummary(lgm_math, \n        remove.unused = FALSE, # keep the unused parameters\n        standardized = TRUE, \n        fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6.17 ended normally after 62 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n  Number of equality constraints                     4\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                80.454\n  Degrees of freedom                                14\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              9369.171\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.993\n  Tucker-Lewis Index (TLI)                       0.995\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -14661.484\n  Loglikelihood unrestricted model (H1)     -14621.257\n                                                      \n  Akaike (AIC)                               29334.968\n  Bayesian (BIC)                             29364.414\n  Sample-size adjusted Bayesian (SABIC)      29345.358\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.069\n  90 Percent confidence interval - lower         0.055\n  90 Percent confidence interval - upper         0.084\n  P-value H_0: RMSEA &lt;= 0.050                    0.015\n  P-value H_0: RMSEA &gt;= 0.080                    0.115\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.017\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial =~                                                            \n    math1             1.000                               9.010    0.969\n    math2             1.000                               9.010    0.895\n    math3             1.000                               9.010    0.797\n    math4             1.000                               9.010    0.699\n    math5             1.000                               9.010    0.613\n  Growth =~                                                             \n    math1             0.000                               0.000    0.000\n    math2             1.000                               2.414    0.240\n    math3             2.000                               4.828    0.427\n    math4             3.000                               7.241    0.562\n    math5             4.000                               9.655    0.657\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial ~~                                                            \n    Growth            4.559    0.741    6.156    0.000    0.210    0.210\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Initial          70.467    0.290  242.668    0.000    7.821    7.821\n    Growth            8.877    0.080  111.410    0.000    3.678    3.678\n   .math1             0.000                               0.000    0.000\n   .math2             0.000                               0.000    0.000\n   .math3             0.000                               0.000    0.000\n   .math4             0.000                               0.000    0.000\n   .math5             0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .math1      (a)    5.228    0.135   38.730    0.000    5.228    0.060\n   .math2      (a)    5.228    0.135   38.730    0.000    5.228    0.052\n   .math3      (a)    5.228    0.135   38.730    0.000    5.228    0.041\n   .math4      (a)    5.228    0.135   38.730    0.000    5.228    0.031\n   .math5      (a)    5.228    0.135   38.730    0.000    5.228    0.024\n    Initial          81.187    3.772   21.524    0.000    1.000    1.000\n    Growth            5.826    0.284   20.496    0.000    1.000    1.000\n\n\n\nsem() 대신 growth()를 이용하면,\n\n# Unconditional growth model\nlgm_math_model &lt;- '\n\n  # latents\n  Initial =~ 1*math1 + 1*math2 + 1*math3 + 1*math4 + 1*math5\n  Growth =~ 0*math1 + 1*math2 + 2*math3 + 3*math4 + 4*math5\n  \n  # intercepts\n  Initial ~ 1\n  Growth ~ 1\n  \n  # residual (co)variances\n  Growth ~~ Initial\n  \n  math1 ~~ a*math1\n  math2 ~~ a*math2\n  math3 ~~ a*math3\n  math4 ~~ a*math4\n  math5 ~~ a*math5\n'\n\nlgm_math &lt;- growth(lgm_math_model, data = math_growth)\nsummary(lgm_math, \n        remove.unused = FALSE, # keep the unused parameters\n        standardized = TRUE, \n        fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6.17 ended normally after 62 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n  Number of equality constraints                     4\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                80.454\n  Degrees of freedom                                14\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              9369.171\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.993\n  Tucker-Lewis Index (TLI)                       0.995\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -14661.484\n  Loglikelihood unrestricted model (H1)     -14621.257\n                                                      \n  Akaike (AIC)                               29334.968\n  Bayesian (BIC)                             29364.414\n  Sample-size adjusted Bayesian (SABIC)      29345.358\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.069\n  90 Percent confidence interval - lower         0.055\n  90 Percent confidence interval - upper         0.084\n  P-value H_0: RMSEA &lt;= 0.050                    0.015\n  P-value H_0: RMSEA &gt;= 0.080                    0.115\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.017\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial =~                                                            \n    math1             1.000                               9.010    0.969\n    math2             1.000                               9.010    0.895\n    math3             1.000                               9.010    0.797\n    math4             1.000                               9.010    0.699\n    math5             1.000                               9.010    0.613\n  Growth =~                                                             \n    math1             0.000                               0.000    0.000\n    math2             1.000                               2.414    0.240\n    math3             2.000                               4.828    0.427\n    math4             3.000                               7.241    0.562\n    math5             4.000                               9.655    0.657\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial ~~                                                            \n    Growth            4.559    0.741    6.156    0.000    0.210    0.210\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Initial          70.467    0.290  242.668    0.000    7.821    7.821\n    Growth            8.877    0.080  111.410    0.000    3.678    3.678\n   .math1             0.000                               0.000    0.000\n   .math2             0.000                               0.000    0.000\n   .math3             0.000                               0.000    0.000\n   .math4             0.000                               0.000    0.000\n   .math5             0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .math1      (a)    5.228    0.135   38.730    0.000    5.228    0.060\n   .math2      (a)    5.228    0.135   38.730    0.000    5.228    0.052\n   .math3      (a)    5.228    0.135   38.730    0.000    5.228    0.041\n   .math4      (a)    5.228    0.135   38.730    0.000    5.228    0.031\n   .math5      (a)    5.228    0.135   38.730    0.000    5.228    0.024\n    Initial          81.187    3.772   21.524    0.000    1.000    1.000\n    Growth            5.826    0.284   20.496    0.000    1.000    1.000\n\n\n\n\n\nConditional growth model\n\n# Conditional growth model\nlgm_math_cond_model &lt;- '\n\n  # latents\n  Initial =~ 1*math1 + 1*math2 + 1*math3 + 1*math4 + 1*math5\n  Growth =~ 0*math1 + 1*math2 + 2*math3 + 3*math4 + 4*math5\n  \n  # intercepts\n  Initial + Growth ~ 1\n  sex + ParEd + Cognitive + age ~ 1\n\n  # an alterative syntax for the intercepts\n  # Initial ~ 1\n  # Growth ~ 1\n  # sex ~ 1\n  # ParEd ~ 1\n  # Cognitive ~ 1\n  # age ~ 1\n  \n  # residual (co)variances\n  Growth ~~ Initial\n  ParEd ~~ Cognitive\n  \n  math1 ~~ a*math1\n  math2 ~~ a*math2\n  math3 ~~ a*math3\n  math4 ~~ a*math4\n  math5 ~~ a*math5\n  \n  # regression\n  Initial ~ sex +  ParEd + Cognitive + age\n  Growth ~ sex + ParEd + Cognitive + age\n'\n\nlgm_math_cond &lt;- growth(lgm_math_cond_model, data = math_growth)\nsummary(lgm_math_cond, \n        standardized = TRUE, \n        fit.measures = TRUE,\n        rsquare = TRUE) |&gt; print()\n\nlavaan 0.6.17 ended normally after 138 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        27\n  Number of equality constraints                     4\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                92.768\n  Degrees of freedom                                31\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              9839.753\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.994\n  Tucker-Lewis Index (TLI)                       0.993\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23160.430\n  Loglikelihood unrestricted model (H1)     -23114.046\n                                                      \n  Akaike (AIC)                               46366.859\n  Bayesian (BIC)                             46479.738\n  Sample-size adjusted Bayesian (SABIC)      46406.688\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.045\n  90 Percent confidence interval - lower         0.034\n  90 Percent confidence interval - upper         0.055\n  P-value H_0: RMSEA &lt;= 0.050                    0.788\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.012\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial =~                                                            \n    math1             1.000                               9.013    0.969\n    math2             1.000                               9.013    0.895\n    math3             1.000                               9.013    0.796\n    math4             1.000                               9.013    0.698\n    math5             1.000                               9.013    0.612\n  Growth =~                                                             \n    math1             0.000                               0.000    0.000\n    math2             1.000                               2.418    0.240\n    math3             2.000                               4.835    0.427\n    math4             3.000                               7.253    0.562\n    math5             4.000                               9.671    0.657\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial ~                                                             \n    sex               0.777    0.516    1.505    0.132    0.086    0.043\n    ParEd             0.887    0.187    4.738    0.000    0.098    0.137\n    Cognitive         0.256    0.017   14.709    0.000    0.028    0.426\n    age              -0.051    0.127   -0.400    0.689   -0.006   -0.011\n  Growth ~                                                              \n    sex               0.246    0.143    1.723    0.085    0.102    0.051\n    ParEd             0.160    0.052    3.097    0.002    0.066    0.092\n    Cognitive         0.067    0.005   13.871    0.000    0.028    0.413\n    age              -0.201    0.035   -5.736    0.000   -0.083   -0.169\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .Initial ~~                                                            \n   .Growth            0.112    0.583    0.192    0.847    0.007    0.007\n  ParEd ~~                                                              \n    Cognitive         2.940    0.667    4.411    0.000    2.940    0.141\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Initial          33.402    9.322    3.583    0.000    3.706    3.706\n   .Growth           13.249    2.573    5.149    0.000    5.480    5.480\n    sex               0.507    0.016   32.069    0.000    0.507    1.014\n    ParEd            16.133    0.044  366.346    0.000   16.133   11.585\n    Cognitive       101.001    0.474  213.113    0.000  101.001    6.739\n    age              68.518    0.064 1068.229    0.000   68.518   33.780\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .math1      (a)    5.228    0.135   38.730    0.000    5.228    0.060\n   .math2      (a)    5.228    0.135   38.730    0.000    5.228    0.052\n   .math3      (a)    5.228    0.135   38.730    0.000    5.228    0.041\n   .math4      (a)    5.228    0.135   38.730    0.000    5.228    0.031\n   .math5      (a)    5.228    0.135   38.730    0.000    5.228    0.024\n    sex               0.250    0.011   22.361    0.000    0.250    1.000\n    ParEd             1.939    0.087   22.361    0.000    1.939    1.000\n    Cognitive       224.611   10.045   22.361    0.000  224.611    1.000\n    age               4.114    0.184   22.361    0.000    4.114    1.000\n   .Initial          63.501    2.981   21.300    0.000    0.782    0.782\n   .Growth            4.554    0.227   20.023    0.000    0.779    0.779\n\nR-Square:\n                   Estimate\n    math1             0.940\n    math2             0.948\n    math3             0.959\n    math4             0.969\n    math5             0.976\n    Initial           0.218\n    Growth            0.221\n\n\n\n\n\nConditional growth model - revised\n\n# Conditional growth model - revised\nlgm_math_cond_model2 &lt;- '\n\n  # latents\n  Initial =~ 1*math1 + 1*math2 + 1*math3 + 1*math4 + 1*math5\n  Growth =~ 0*math1 + 1*math2 + 2*math3 + math4 + math5\n  \n  # intercepts\n  Initial + Growth ~ 1\n  sex + ParEd + Cognitive + age ~ 1\n    \n  # residual (co)variances\n  Growth ~~ 0*Initial # removed covariance\n  ParEd ~~ Cognitive\n  \n  math1 ~~ a*math1\n  math2 ~~ a*math2\n  math3 ~~ a*math3\n  math4 ~~ a*math4\n  math5 ~~ a*math5\n  \n  # regression\n  Initial ~ ParEd + Cognitive  # remove sex, age\n  Growth ~ ParEd + Cognitive + age  # remove sex\n'\n\nlgm_math_cond2 &lt;- growth(lgm_math_cond_model2, data = math_growth)\n\nsummary(lgm_math_cond2,\n        standardized = TRUE, \n        fit.measures = TRUE,\n        rsquare = TRUE) |&gt; print()\n\nlavaan 0.6.17 ended normally after 127 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        25\n  Number of equality constraints                     4\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                43.024\n  Degrees of freedom                                33\n  P-value (Chi-square)                           0.114\n\nModel Test Baseline Model:\n\n  Test statistic                              9839.753\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.999\n  Tucker-Lewis Index (TLI)                       0.999\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23135.558\n  Loglikelihood unrestricted model (H1)     -23114.046\n                                                      \n  Akaike (AIC)                               46313.115\n  Bayesian (BIC)                             46416.178\n  Sample-size adjusted Bayesian (SABIC)      46349.481\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.017\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.031\n  P-value H_0: RMSEA &lt;= 0.050                    1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.017\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial =~                                                            \n    math1             1.000                               9.001    0.970\n    math2             1.000                               9.001    0.894\n    math3             1.000                               9.001    0.791\n    math4             1.000                               9.001    0.700\n    math5             1.000                               9.001    0.613\n  Growth =~                                                             \n    math1             0.000                               0.000    0.000\n    math2             1.000                               2.507    0.249\n    math3             2.000                               5.015    0.441\n    math4             2.905    0.013  217.475    0.000    7.284    0.566\n    math5             3.871    0.018  220.189    0.000    9.706    0.661\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial ~                                                             \n    ParEd             0.883    0.188    4.710    0.000    0.098    0.137\n    Cognitive         0.254    0.017   14.564    0.000    0.028    0.423\n  Growth ~                                                              \n    ParEd             0.166    0.054    3.088    0.002    0.066    0.092\n    Cognitive         0.069    0.005   13.800    0.000    0.027    0.412\n    age              -0.208    0.036   -5.701    0.000   -0.083   -0.168\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .Initial ~~                                                            \n   .Growth            0.000                               0.000    0.000\n  ParEd ~~                                                              \n    Cognitive         2.940    0.667    4.411    0.000    2.940    0.141\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Initial          30.346    3.289    9.226    0.000    3.371    3.371\n   .Growth           13.795    2.667    5.172    0.000    5.502    5.502\n    sex               0.507    0.016   32.069    0.000    0.507    1.014\n    ParEd            16.133    0.044  366.346    0.000   16.133   11.585\n    Cognitive       101.001    0.474  213.113    0.000  101.001    6.739\n    age              68.518    0.064 1068.229    0.000   68.518   33.780\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .math1      (a)    5.131    0.132   38.771    0.000    5.131    0.060\n   .math2      (a)    5.131    0.132   38.771    0.000    5.131    0.051\n   .math3      (a)    5.131    0.132   38.771    0.000    5.131    0.040\n   .math4      (a)    5.131    0.132   38.771    0.000    5.131    0.031\n   .math5      (a)    5.131    0.132   38.771    0.000    5.131    0.024\n    ParEd             1.939    0.087   22.361    0.000    1.939    1.000\n    Cognitive       224.611   10.045   22.361    0.000  224.611    1.000\n    age               4.114    0.184   22.361    0.000    4.114    1.000\n    sex               0.250    0.011   22.361    0.000    0.250    1.000\n   .Initial          63.715    2.981   21.374    0.000    0.786    0.786\n   .Growth            4.921    0.249   19.797    0.000    0.783    0.783\n\nR-Square:\n                   Estimate\n    math1             0.940\n    math2             0.949\n    math3             0.960\n    math4             0.969\n    math5             0.976\n    Initial           0.214\n    Growth            0.217\n\n\n\n\n\nCompare the two models\n\n# Compare the two models\nlavTestLRT(lgm_math_cond, lgm_math_cond2) |&gt; print()\n\n\nChi-Squared Difference Test\n\n               Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nlgm_math_cond  31 46367 46480 92.768                                    \nlgm_math_cond2 33 46313 46416 43.024    -49.744     0       2          1\n\n\n\n\nConditional growth model - free slopes\n\n# Conditional growth model - free slopes\nlgm_math_cond_model3 &lt;- '\n\n  # latents\n  Initial =~ 1*math1 + 1*math2 + 1*math3 + 1*math4 + 1*math5\n  Growth =~ 0*math1 + 1*math2 + 2*math3 + NA*math4 + NA*math5\n  \n  # intercepts\n  Initial + Growth ~ 1\n  sex + ParEd + Cognitive + age ~ 1\n  \n  math1 ~ 0*1\n  math2 ~ 0*1\n  math3 ~ 0*1\n  math4 ~ 0*1\n  math5 ~ 0*1\n  # same as \"math1 + math2 + math3 + math4 + math5 ~ 0*1\"\n  \n  # residual (co)variances\n  Growth ~~ 0*Initial # removed covariance\n  ParEd ~~ Cognitive\n  \n  math1 ~~ a*math1\n  math2 ~~ a*math2\n  math3 ~~ a*math3\n  math4 ~~ a*math4\n  math5 ~~ a*math5\n  \n  # regression\n  Initial ~ ParEd + Cognitive  # remove sex, age\n  Growth ~ ParEd + Cognitive + age  # remove sex\n'\n\nlgm_math_cond3 &lt;- growth(lgm_math_cond_model3, data = math_growth)\nsummary(lgm_math_cond3, \n        standardized = TRUE, \n        fit.measures = TRUE,\n        rsquare = TRUE) |&gt; print()\n\nlavaan 0.6.17 ended normally after 127 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        25\n  Number of equality constraints                     4\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                43.024\n  Degrees of freedom                                33\n  P-value (Chi-square)                           0.114\n\nModel Test Baseline Model:\n\n  Test statistic                              9839.753\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.999\n  Tucker-Lewis Index (TLI)                       0.999\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -23135.558\n  Loglikelihood unrestricted model (H1)     -23114.046\n                                                      \n  Akaike (AIC)                               46313.115\n  Bayesian (BIC)                             46416.178\n  Sample-size adjusted Bayesian (SABIC)      46349.481\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.017\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.031\n  P-value H_0: RMSEA &lt;= 0.050                    1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.017\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial =~                                                            \n    math1             1.000                               9.001    0.970\n    math2             1.000                               9.001    0.894\n    math3             1.000                               9.001    0.791\n    math4             1.000                               9.001    0.700\n    math5             1.000                               9.001    0.613\n  Growth =~                                                             \n    math1             0.000                               0.000    0.000\n    math2             1.000                               2.507    0.249\n    math3             2.000                               5.015    0.441\n    math4             2.905    0.013  217.475    0.000    7.284    0.566\n    math5             3.871    0.018  220.189    0.000    9.706    0.661\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Initial ~                                                             \n    ParEd             0.883    0.188    4.710    0.000    0.098    0.137\n    Cognitive         0.254    0.017   14.564    0.000    0.028    0.423\n  Growth ~                                                              \n    ParEd             0.166    0.054    3.088    0.002    0.066    0.092\n    Cognitive         0.069    0.005   13.800    0.000    0.027    0.412\n    age              -0.208    0.036   -5.701    0.000   -0.083   -0.168\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .Initial ~~                                                            \n   .Growth            0.000                               0.000    0.000\n  ParEd ~~                                                              \n    Cognitive         2.940    0.667    4.411    0.000    2.940    0.141\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Initial          30.346    3.289    9.226    0.000    3.371    3.371\n   .Growth           13.795    2.667    5.172    0.000    5.502    5.502\n    sex               0.507    0.016   32.069    0.000    0.507    1.014\n    ParEd            16.133    0.044  366.346    0.000   16.133   11.585\n    Cognitive       101.001    0.474  213.113    0.000  101.001    6.739\n    age              68.518    0.064 1068.229    0.000   68.518   33.780\n   .math1             0.000                               0.000    0.000\n   .math2             0.000                               0.000    0.000\n   .math3             0.000                               0.000    0.000\n   .math4             0.000                               0.000    0.000\n   .math5             0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .math1      (a)    5.131    0.132   38.771    0.000    5.131    0.060\n   .math2      (a)    5.131    0.132   38.771    0.000    5.131    0.051\n   .math3      (a)    5.131    0.132   38.771    0.000    5.131    0.040\n   .math4      (a)    5.131    0.132   38.771    0.000    5.131    0.031\n   .math5      (a)    5.131    0.132   38.771    0.000    5.131    0.024\n    ParEd             1.939    0.087   22.361    0.000    1.939    1.000\n    Cognitive       224.611   10.045   22.361    0.000  224.611    1.000\n    age               4.114    0.184   22.361    0.000    4.114    1.000\n    sex               0.250    0.011   22.361    0.000    0.250    1.000\n   .Initial          63.715    2.981   21.374    0.000    0.786    0.786\n   .Growth            4.921    0.249   19.797    0.000    0.783    0.783\n\nR-Square:\n                   Estimate\n    math1             0.940\n    math2             0.949\n    math3             0.960\n    math4             0.969\n    math5             0.976\n    Initial           0.214\n    Growth            0.217",
    "crumbs": [
      "Keith's",
      "Latent Growth Models"
    ]
  },
  {
    "objectID": "contents/chap18.html",
    "href": "contents/chap18.html",
    "title": "Chapter 18. Multigroup Models, Panel Models, Dangers and Assumptions",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)\n\n\n\nlibrary(tidyverse)\nlibrary(haven)\nhw &lt;- read_sav(\"data/chap 18 latent var SEM 2/HW latent matrix.sav\")\nhw |&gt; print()\n\n# A tibble: 17 x 16\n   rowtype_ varname_     Minority  bypared  byfaminc   parocc bytxrstd bytxmstd\n   &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 N        \"\"         1000       1000     1000      1000     1000     1000    \n 2 corr     \"Minority\"    1         -0.169   -0.278    -0.242   -0.204   -0.161\n 3 corr     \"bypared\"    -0.169      1        0.526     0.629    0.386    0.430\n 4 corr     \"byfaminc\"   -0.278      0.526    1         0.524    0.288    0.335\n 5 corr     \"parocc\"     -0.242      0.629    0.524     1        0.339    0.362\n 6 corr     \"bytxrstd\"   -0.204      0.386    0.288     0.339    1        0.714\n 7 corr     \"bytxmstd\"   -0.161      0.430    0.335     0.362    0.714    1    \n 8 corr     \"bytxsstd\"   -0.231      0.384    0.293     0.322    0.717    0.719\n 9 corr     \"bytxhstd\"   -0.210      0.396    0.308     0.346    0.731    0.675\n10 corr     \"hw_8\"       -0.00317    0.168    0.0750    0.105    0.226    0.271\n11 corr     \"hw10\"       -0.0559     0.208    0.155     0.173    0.219    0.286\n12 corr     \"eng_12\"     -0.0978     0.334    0.243     0.260    0.524    0.565\n13 corr     \"math_12\"    -0.0710     0.285    0.220     0.218    0.418    0.587\n14 corr     \"sci_12\"     -0.0832     0.294    0.209     0.231    0.484    0.576\n15 corr     \"ss_12\"      -0.111      0.328    0.253     0.265    0.519    0.567\n16 stddev   \"\"            0.445      1.28     2.52     21.6     10.3     10.4  \n17 mean     \"\"            0.272      3.20     9.92     51.7     52.0     52.5  \n# i 8 more variables: bytxsstd &lt;dbl&gt;, bytxhstd &lt;dbl&gt;, hw_8 &lt;dbl&gt;, hw10 &lt;dbl&gt;,\n#   eng_12 &lt;dbl&gt;, math_12 &lt;dbl&gt;, sci_12 &lt;dbl&gt;, ss_12 &lt;dbl&gt;\n\n\n\nnels_sav_full &lt;- read_sav(\"data/n=1000,stud & par_3.sav\")\n\nvars &lt;- c(\"byfaminc\", \"parocc\", \"bypared\", \"ethnic\", \"f1txhstd\", \"f1txsstd\", \"f1txrstd\", \"f1txmstd\", \"bytxhstd\", \"bytxsstd\", \"bytxrstd\", \"bytxmstd\", \"f1s36a1\",  \"f1s36a2\", \"byhomewk\")\n\nhw_raw &lt;- nels_sav_full[vars]\n\nhw_raw |&gt; psych::lowerCor() |&gt; round(2) |&gt; print()\n\n\nlibrary(lavaan)\nhwcov &lt;- hw[c(2:15), c(3:16)] |&gt; \n  as.matrix() |&gt; \n  lav_matrix_vechr(diagonal = TRUE) |&gt; \n  getCov(names = hw$varname_[2:15]) |&gt; \n  cor2cov(sds = hw[16, 3:16] |&gt; as.double())\n\nhwcov |&gt; round(2) |&gt; print()\n\n         Minority bypared byfaminc parocc bytxrstd bytxmstd bytxsstd bytxhstd\nMinority     0.20   -0.10    -0.31  -2.32    -0.93    -0.74    -1.06    -0.95\nbypared     -0.10    1.65     1.70  17.43     5.09     5.73     5.08     5.18\nbyfaminc    -0.31    1.70     6.37  28.56     7.49     8.76     7.62     7.91\nparocc      -2.32   17.43    28.56 466.51    75.24    81.22    71.66    76.05\nbytxrstd    -0.93    5.09     7.49  75.24   105.89    76.31    76.18    76.56\nbytxmstd    -0.74    5.73     8.76  81.22    76.31   107.75    77.04    71.37\nbytxsstd    -1.06    5.08     7.62  71.66    76.18    77.04   106.45    76.48\nbytxhstd    -0.95    5.18     7.91  76.05    76.56    71.37    76.48   103.68\nhw_8         0.00    0.24     0.21   2.58     2.63     3.18     2.57     1.93\nhw10        -0.05    0.51     0.75   7.13     4.29     5.64     4.04     4.01\neng_12      -0.12    1.15     1.64  14.99    14.42    15.69    12.42    13.36\nmath_12     -0.09    1.01     1.53  12.94    11.82    16.72    11.78    11.45\nsci_12      -0.10    1.01     1.42  13.40    13.35    16.04    13.64    13.00\nss_12       -0.14    1.21     1.83  16.42    15.36    16.89    14.37    15.17\n         hw_8  hw10 eng_12 math_12 sci_12 ss_12\nMinority 0.00 -0.05  -0.12   -0.09  -0.10 -0.14\nbypared  0.24  0.51   1.15    1.01   1.01  1.21\nbyfaminc 0.21  0.75   1.64    1.53   1.42  1.83\nparocc   2.58  7.13  14.99   12.94  13.40 16.42\nbytxrstd 2.63  4.29  14.42   11.82  13.35 15.36\nbytxmstd 3.18  5.64  15.69   16.72  16.04 16.89\nbytxsstd 2.57  4.04  12.42   11.78  13.64 14.37\nbytxhstd 1.93  4.01  13.36   11.45  13.00 15.17\nhw_8     1.28  0.58   0.62    0.54   0.58  0.59\nhw10     0.58  3.62   1.59    1.51   1.44  1.55\neng_12   0.62  1.59   7.15    5.59   5.76  6.54\nmath_12  0.54  1.51   5.59    7.55   5.59  5.88\nsci_12   0.58  1.44   5.76    5.59   7.19  6.12\nss_12    0.59  1.55   6.54    5.88   6.12  8.25\n\n\n\n# generate a dataset with the same covariance matrix\nhw_sim &lt;- semTools::kd(hwcov, n = 1000, type = \"exact\")\nsum(as.numeric(hwcov - cov(hw_sim)))\n\n-3.39684010659574\n\n\n\nhw_model &lt;- \"\n  eminor =~ Minority\n  famback =~ bypared + byfaminc + parocc\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw_8 + hw10\n  grades =~ eng_12 + math_12 + sci_12 + ss_12\n\n  Minority ~~ 0.0099*Minority\n  bytxrstd ~~ eng_12\n  bytxmstd ~~ math_12\n  bytxsstd ~~ sci_12\n  bytxhstd ~~ ss_12\n\n  prevach ~ b1*famback + b2*eminor\n  grades ~ b3*prevach + b4*hw\n  hw ~ b5*prevach + b6*famback + b7*eminor\n\n  ind := b4*b5*b1\n\"\n\nsem_fit &lt;- sem(hw_model,\n  sample.cov = hwcov,\n  sample.nobs = 1000,\n  sample.cov.rescale = FALSE\n)\n\nsummary(sem_fit, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-17 ended normally after 133 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        39\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                               204.654\n  Degrees of freedom                                66\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              8392.044\n  Degrees of freedom                                91\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.983\n  Tucker-Lewis Index (TLI)                       0.977\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33338.255\n  Loglikelihood unrestricted model (H1)     -33235.928\n                                                      \n  Akaike (AIC)                               66754.509\n  Bayesian (BIC)                             66945.912\n  Sample-size adjusted Bayesian (SABIC)      66822.046\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.046\n  90 Percent confidence interval - lower         0.039\n  90 Percent confidence interval - upper         0.053\n  P-value H_0: RMSEA &lt;= 0.050                    0.825\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  eminor =~                                                             \n    Minority          1.000                               0.434    0.975\n  famback =~                                                            \n    bypared           1.000                               1.033    0.805\n    byfaminc          1.630    0.084   19.434    0.000    1.684    0.667\n    parocc           16.235    0.751   21.618    0.000   16.769    0.776\n  prevach =~                                                            \n    bytxrstd          1.000                               8.811    0.855\n    bytxmstd          0.997    0.030   33.754    0.000    8.787    0.844\n    bytxsstd          0.990    0.030   33.537    0.000    8.722    0.846\n    bytxhstd          0.967    0.029   32.925    0.000    8.517    0.837\n  hw =~                                                                 \n    hw_8              1.000                               0.510    0.451\n    hw10              2.208    0.292    7.553    0.000    1.126    0.592\n  grades =~                                                             \n    eng_12            1.000                               2.472    0.924\n    math_12           0.896    0.024   37.832    0.000    2.214    0.820\n    sci_12            0.957    0.022   43.705    0.000    2.366    0.878\n    ss_12             1.062    0.022   48.218    0.000    2.626    0.914\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback   (b1)    4.511    0.328   13.733    0.000    0.529    0.529\n    eminor    (b2)   -1.774    0.645   -2.749    0.006   -0.087   -0.087\n  grades ~                                                              \n    prevach   (b3)    0.145    0.012   12.581    0.000    0.518    0.518\n    hw        (b4)    1.328    0.278    4.776    0.000    0.274    0.274\n  hw ~                                                                  \n    prevach   (b5)    0.024    0.004    5.848    0.000    0.413    0.413\n    famback   (b6)    0.098    0.032    3.051    0.002    0.198    0.198\n    eminor    (b7)    0.127    0.056    2.264    0.024    0.108    0.108\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.705    0.248    2.844    0.004    0.705    0.128\n .bytxmstd ~~                                                           \n   .math_12           2.859    0.342    8.350    0.000    2.859    0.331\n .bytxsstd ~~                                                           \n   .sci_12            0.921    0.285    3.227    0.001    0.921    0.130\n .bytxhstd ~~                                                           \n   .ss_12             0.534    0.277    1.927    0.054    0.534    0.082\n  eminor ~~                                                             \n    famback          -0.132    0.017   -7.750    0.000   -0.294   -0.294\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Minority          0.010                               0.010    0.050\n   .bypared           0.581    0.046   12.697    0.000    0.581    0.353\n   .byfaminc          3.531    0.193   18.276    0.000    3.531    0.555\n   .parocc          185.313   13.030   14.222    0.000  185.313    0.397\n   .bytxrstd         28.583    1.719   16.625    0.000   28.583    0.269\n   .bytxmstd         31.264    1.823   17.153    0.000   31.264    0.288\n   .bytxsstd         30.196    1.769   17.068    0.000   30.196    0.284\n   .bytxhstd         30.894    1.776   17.399    0.000   30.894    0.299\n   .hw_8              1.019    0.058   17.562    0.000    1.019    0.797\n   .hw10              2.354    0.202   11.639    0.000    2.354    0.650\n   .eng_12            1.053    0.075   14.115    0.000    1.053    0.147\n   .math_12           2.380    0.122   19.545    0.000    2.380    0.327\n   .sci_12            1.655    0.094   17.625    0.000    1.655    0.228\n   .ss_12             1.366    0.090   15.133    0.000    1.366    0.165\n    eminor            0.188    0.009   21.243    0.000    1.000    1.000\n    famback           1.067    0.079   13.577    0.000    1.000    1.000\n   .prevach          53.216    3.517   15.130    0.000    0.686    0.686\n   .hw                0.188    0.039    4.856    0.000    0.722    0.722\n   .grades            3.153    0.202   15.604    0.000    0.516    0.516\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    ind               0.143    0.037    3.841    0.000    0.060    0.060\n\n\n\n\n# For simulate data\nsem_fit_raw &lt;- sem(hw_model, data = hw_sim)\nsummary(sem_fit_raw, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-17 ended normally after 133 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        39\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                               204.654\n  Degrees of freedom                                66\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              8392.044\n  Degrees of freedom                                91\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.983\n  Tucker-Lewis Index (TLI)                       0.977\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33338.255\n  Loglikelihood unrestricted model (H1)     -33235.928\n                                                      \n  Akaike (AIC)                               66754.509\n  Bayesian (BIC)                             66945.912\n  Sample-size adjusted Bayesian (SABIC)      66822.046\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.046\n  90 Percent confidence interval - lower         0.039\n  90 Percent confidence interval - upper         0.053\n  P-value H_0: RMSEA &lt;= 0.050                    0.825\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  eminor =~                                                             \n    Minority          1.000                               0.434    0.975\n  famback =~                                                            \n    bypared           1.000                               1.033    0.805\n    byfaminc          1.630    0.084   19.434    0.000    1.684    0.667\n    parocc           16.235    0.751   21.618    0.000   16.769    0.776\n  prevach =~                                                            \n    bytxrstd          1.000                               8.811    0.855\n    bytxmstd          0.997    0.030   33.754    0.000    8.787    0.844\n    bytxsstd          0.990    0.030   33.537    0.000    8.722    0.846\n    bytxhstd          0.967    0.029   32.925    0.000    8.517    0.837\n  hw =~                                                                 \n    hw_8              1.000                               0.510    0.451\n    hw10              2.208    0.292    7.553    0.000    1.126    0.592\n  grades =~                                                             \n    eng_12            1.000                               2.472    0.924\n    math_12           0.896    0.024   37.832    0.000    2.214    0.820\n    sci_12            0.957    0.022   43.705    0.000    2.366    0.878\n    ss_12             1.062    0.022   48.218    0.000    2.626    0.914\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback   (b1)    4.511    0.328   13.733    0.000    0.529    0.529\n    eminor    (b2)   -1.774    0.645   -2.749    0.006   -0.087   -0.087\n  grades ~                                                              \n    prevach   (b3)    0.145    0.012   12.581    0.000    0.518    0.518\n    hw        (b4)    1.328    0.278    4.776    0.000    0.274    0.274\n  hw ~                                                                  \n    prevach   (b5)    0.024    0.004    5.848    0.000    0.413    0.413\n    famback   (b6)    0.098    0.032    3.051    0.002    0.198    0.198\n    eminor    (b7)    0.127    0.056    2.264    0.024    0.108    0.108\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.705    0.248    2.844    0.004    0.705    0.128\n .bytxmstd ~~                                                           \n   .math_12           2.859    0.342    8.350    0.000    2.859    0.331\n .bytxsstd ~~                                                           \n   .sci_12            0.921    0.285    3.227    0.001    0.921    0.130\n .bytxhstd ~~                                                           \n   .ss_12             0.534    0.277    1.927    0.054    0.534    0.082\n  eminor ~~                                                             \n    famback          -0.132    0.017   -7.750    0.000   -0.294   -0.294\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Minority          0.010                               0.010    0.050\n   .bypared           0.581    0.046   12.697    0.000    0.581    0.353\n   .byfaminc          3.531    0.193   18.276    0.000    3.531    0.555\n   .parocc          185.313   13.030   14.222    0.000  185.313    0.397\n   .bytxrstd         28.583    1.719   16.625    0.000   28.583    0.269\n   .bytxmstd         31.264    1.823   17.153    0.000   31.264    0.288\n   .bytxsstd         30.196    1.769   17.068    0.000   30.196    0.284\n   .bytxhstd         30.894    1.776   17.399    0.000   30.894    0.299\n   .hw_8              1.019    0.058   17.562    0.000    1.019    0.797\n   .hw10              2.354    0.202   11.639    0.000    2.354    0.650\n   .eng_12            1.053    0.075   14.115    0.000    1.053    0.147\n   .math_12           2.380    0.122   19.545    0.000    2.380    0.327\n   .sci_12            1.655    0.094   17.625    0.000    1.655    0.228\n   .ss_12             1.366    0.090   15.133    0.000    1.366    0.165\n    eminor            0.188    0.009   21.243    0.000    1.000    1.000\n    famback           1.067    0.079   13.577    0.000    1.000    1.000\n   .prevach          53.216    3.517   15.130    0.000    0.686    0.686\n   .hw                0.188    0.039    4.856    0.000    0.722    0.722\n   .grades            3.153    0.202   15.604    0.000    0.516    0.516\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    ind               0.143    0.037    3.841    0.000    0.060    0.060\n\n\n\n\nAll indirect paths from family background to grandes: total effect\n\nlibrary(manymome)\n\n# All indirect paths from family background to grandes: total effect\npaths &lt;- all_indirect_paths(sem_fit_raw,\n  x = \"famback\",\n  y = \"grades\"\n)\npaths |&gt; print()\n\nCall: \nall_indirect_paths(fit = sem_fit_raw, x = \"famback\", y = \"grades\")\nPath(s): \n  path                              \n1 famback -&gt; prevach -&gt; hw -&gt; grades\n2 famback -&gt; prevach -&gt; grades      \n3 famback -&gt; hw -&gt; grades           \n\n\n\nind_est_std &lt;- many_indirect_effects(paths,\n  fit = sem_fit_raw, R = 1000,\n  boot_ci = TRUE, boot_type = \"bc\",\n  standardized_x = TRUE,\n  standardized_y = TRUE\n)\n\nind_est_std\n\n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100% elapsed=17s  \n\n\n\n==  Indirect Effect(s) (Both x-variable(s) and y-variable(s) Standardized)  ==\n                                     std CI.lo CI.hi Sig\nfamback -&gt; prevach -&gt; hw -&gt; grades 0.060 0.034 0.106 Sig\nfamback -&gt; prevach -&gt; grades       0.274 0.220 0.329 Sig\nfamback -&gt; hw -&gt; grades            0.054 0.017 0.104 Sig\n\n - [CI.lo to CI.hi] are 95.0% bias-corrected confidence intervals by\n   nonparametric bootstrapping with 1000 samples.\n - std: The standardized indirect effects.\n \n\n\n\nind_est_std[[1]] + ind_est_std[[2]] + ind_est_std[[3]] # total effect\n\n\n== Indirect Effect (Both 'famback' and 'grades' Standardized) ==\n                                                        \n Path:                famback -&gt; prevach -&gt; hw -&gt; grades\n Path:                famback -&gt; prevach -&gt; grades      \n Path:                famback -&gt; hw -&gt; grades           \n Function of Effects: 0.388                             \n 95.0% Bootstrap CI:  [0.334 to 0.441]                  \n\nComputation of the Function of Effects:\n ((famback-&gt;prevach-&gt;hw-&gt;grades)\n+(famback-&gt;prevach-&gt;grades))\n+(famback-&gt;hw-&gt;grades) \n\n\nBias-corrected confidence interval formed by nonparametric\nbootstrapping with 1000 bootstrap samples.\n\n\n\nindirect_effect(\n  fit = sem_fit_raw,\n  x = \"famback\",\n  y = \"grades\",\n  m = c(\"hw\"),\n  boot_out = out_med,\n  standardized_x = TRUE,\n  standardized_y = TRUE\n)\n\n# ==  Indirect Effect(s) (Both x-variable(s) and y-variable(s) Standardized)  ==\n#   std CI.lo CI.hi Sig\n# famback -&gt; prevach -&gt; hw -&gt; grades 0.060 0.034 0.105 Sig\n# famback -&gt; prevach -&gt; grades       0.274 0.222 0.325 Sig\n# famback -&gt; hw -&gt; grades            0.054 0.018 0.102 Sig\n\n\n== Indirect Effect (Both 'famback' and 'grades' Standardized) ==\n                                         \n Path:            famback -&gt; hw -&gt; grades\n Indirect Effect: 0.054                  \n\nComputation Formula:\n  (b.hw~famback)*(b.grades~hw)*sd_famback/sd_grades\nComputation:\n  (0.09797)*(1.32771)*(1.03290)/(2.47198)\nCoefficients of Component Paths:\n       Path Coefficient\n hw~famback       0.098\n  grades~hw       1.328\n\nNOTE:\n- The effects of the component paths are from the model, not\n  standardized.\n\n\n\n\nAll indirect paths from minority to grades: total effect\n\n# All indirect paths from minority to grades: total effect\npaths2 &lt;- all_indirect_paths(sem_fit_raw,\n  x = \"eminor\",\n  y = \"grades\"\n)\npaths2 |&gt; print()\n\nCall: \nall_indirect_paths(fit = sem_fit_raw, x = \"eminor\", y = \"grades\")\nPath(s): \n  path                             \n1 eminor -&gt; prevach -&gt; hw -&gt; grades\n2 eminor -&gt; prevach -&gt; grades      \n3 eminor -&gt; hw -&gt; grades           \n\n\n\nind_est_std2 &lt;- many_indirect_effects(paths2,\n  fit = sem_fit_raw, R = 1000,\n  boot_ci = TRUE, boot_type = \"bc\",\n  standardized_x = TRUE,\n  standardized_y = TRUE\n)\nind_est_std2\n\n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100% elapsed=11s  \n\n\n\n==  Indirect Effect(s) (Both x-variable(s) and y-variable(s) Standardized)  ==\n                                     std  CI.lo  CI.hi Sig\neminor -&gt; prevach -&gt; hw -&gt; grades -0.010 -0.024 -0.003 Sig\neminor -&gt; prevach -&gt; grades       -0.045 -0.084 -0.012 Sig\neminor -&gt; hw -&gt; grades             0.030  0.004  0.066 Sig\n\n - [CI.lo to CI.hi] are 95.0% bias-corrected confidence intervals by\n   nonparametric bootstrapping with 1000 samples.\n - std: The standardized indirect effects.\n \n\n\n\nind_est_std2[[1]] + ind_est_std2[[2]] + ind_est_std2[[3]] # total effect\n\n\n== Indirect Effect (Both 'eminor' and 'grades' Standardized) ==\n                                                       \n Path:                eminor -&gt; prevach -&gt; hw -&gt; grades\n Path:                eminor -&gt; prevach -&gt; grades      \n Path:                eminor -&gt; hw -&gt; grades           \n Function of Effects: -0.025                           \n 95.0% Bootstrap CI:  [-0.074 to 0.027]                \n\nComputation of the Function of Effects:\n ((eminor-&gt;prevach-&gt;hw-&gt;grades)\n+(eminor-&gt;prevach-&gt;grades))\n+(eminor-&gt;hw-&gt;grades) \n\n\nBias-corrected confidence interval formed by nonparametric\nbootstrapping with 1000 bootstrap samples.\n\n\n\n\nUsing cSEM\n\nhw_model_c &lt;- \"\n  eminor &lt;~ Minority\n  famback =~ bypared + byfaminc + parocc\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw_8 + hw10\n  grades =~ eng_12 + math_12 + sci_12 + ss_12\n\n  prevach ~ famback + eminor\n  grades ~ prevach + hw\n  hw ~ prevach + famback + eminor\n\"\n\ncsem_fit &lt;- cSEM::csem(.data = hw_sim, .model = hw_model_c, .resample_method = \"bootstrap\")\ncSEM::summarize(csem_fit) |&gt; print()\n\n________________________________________________________________________________\n----------------------------------- Overview -----------------------------------\n\n    General information:\n    ------------------------\n    Estimation status                = Ok\n    Number of observations           = 1000\n    Weight estimator                 = PLS-PM\n    Inner weighting scheme           = \"path\"\n    Type of indicator correlation    = Pearson\n    Path model estimator             = OLS\n    Second-order approach            = NA\n    Type of path model               = Linear\n    Disattenuated                    = Yes (PLSc)\n\n    Resample information:\n    ---------------------\n    Resample method                  = \"bootstrap\"\n    Number of resamples              = 499\n    Number of admissible results     = 499\n    Approach to handle inadmissibles = \"drop\"\n    Sign change option               = \"none\"\n    Random seed                      = 528364834\n\n    Construct details:\n    ------------------\n    Name     Modeled as     Order         Mode      \n\n    famback  Common factor  First order   \"modeA\"   \n    eminor   Composite      First order   \"modeB\"   \n    prevach  Common factor  First order   \"modeA\"   \n    hw       Common factor  First order   \"modeA\"   \n    grades   Common factor  First order   \"modeA\"   \n\n----------------------------------- Estimates ----------------------------------\n\nEstimated path coefficients:\n============================\n                                                                   CI_percentile   \n  Path                 Estimate  Std. error   t-stat.   p-value         95%        \n  prevach ~ famback      0.5272      0.0305   17.3107    0.0000 [ 0.4600; 0.5844 ] \n  prevach ~ eminor      -0.0807      0.0306   -2.6349    0.0084 [-0.1395;-0.0224 ] \n  hw ~ famback           0.1715      0.0647    2.6500    0.0080 [ 0.0386; 0.2963 ] \n  hw ~ eminor            0.0903      0.0464    1.9450    0.0518 [-0.0060; 0.1866 ] \n  hw ~ prevach           0.4310      0.0644    6.6888    0.0000 [ 0.2992; 0.5590 ] \n  grades ~ prevach       0.5447      0.0418   13.0338    0.0000 [ 0.4549; 0.6169 ] \n  grades ~ hw            0.2457      0.0537    4.5755    0.0000 [ 0.1470; 0.3578 ] \n\nEstimated loadings:\n===================\n                                                                     CI_percentile   \n  Loading                Estimate  Std. error   t-stat.   p-value         95%        \n  famback =~ bypared       0.8722      0.0301   29.0067    0.0000 [ 0.8117; 0.9289 ] \n  famback =~ byfaminc      0.6449      0.0359   17.9511    0.0000 [ 0.5748; 0.7154 ] \n  famback =~ parocc        0.7290      0.0325   22.4404    0.0000 [ 0.6604; 0.7857 ] \n  eminor =~ Minority       1.0000          NA        NA        NA [     NA;     NA ] \n  prevach =~ bytxrstd      0.8226      0.0174   47.3445    0.0000 [ 0.7894; 0.8569 ] \n  prevach =~ bytxmstd      0.9497      0.0154   61.6656    0.0000 [ 0.9182; 0.9780 ] \n  prevach =~ bytxsstd      0.7942      0.0192   41.4369    0.0000 [ 0.7565; 0.8299 ] \n  prevach =~ bytxhstd      0.8034      0.0185   43.5010    0.0000 [ 0.7678; 0.8398 ] \n  hw =~ hw_8               0.4575      0.0416   10.9919    0.0000 [ 0.3767; 0.5420 ] \n  hw =~ hw10               0.5931      0.0489   12.1374    0.0000 [ 0.5070; 0.6921 ] \n  grades =~ eng_12         0.9081      0.0144   62.9597    0.0000 [ 0.8775; 0.9331 ] \n  grades =~ math_12        0.8226      0.0206   40.0087    0.0000 [ 0.7829; 0.8619 ] \n  grades =~ sci_12         0.8972      0.0147   60.8582    0.0000 [ 0.8708; 0.9271 ] \n  grades =~ ss_12          0.9175      0.0143   64.3119    0.0000 [ 0.8886; 0.9454 ] \n\nEstimated weights:\n==================\n                                                                     CI_percentile   \n  Weight                 Estimate  Std. error   t-stat.   p-value         95%        \n  famback &lt;~ bypared       0.4602      0.0168   27.3354    0.0000 [ 0.4266; 0.4940 ] \n  famback &lt;~ byfaminc      0.3403      0.0175   19.4538    0.0000 [ 0.3061; 0.3730 ] \n  famback &lt;~ parocc        0.3847      0.0158   24.3104    0.0000 [ 0.3512; 0.4144 ] \n  eminor &lt;~ Minority       1.0000          NA        NA        NA [     NA;     NA ] \n  prevach &lt;~ bytxrstd      0.2754      0.0054   50.7908    0.0000 [ 0.2642; 0.2861 ] \n  prevach &lt;~ bytxmstd      0.3180      0.0062   51.6632    0.0000 [ 0.3050; 0.3298 ] \n  prevach &lt;~ bytxsstd      0.2660      0.0054   49.1259    0.0000 [ 0.2553; 0.2765 ] \n  prevach &lt;~ bytxhstd      0.2690      0.0057   46.9445    0.0000 [ 0.2586; 0.2810 ] \n  hw &lt;~ hw_8               0.5436      0.0418   13.0001    0.0000 [ 0.4505; 0.6204 ] \n  hw &lt;~ hw10               0.7047      0.0377   18.6965    0.0000 [ 0.6325; 0.7801 ] \n  grades &lt;~ eng_12         0.2794      0.0046   60.8535    0.0000 [ 0.2702; 0.2877 ] \n  grades &lt;~ math_12        0.2531      0.0056   45.0312    0.0000 [ 0.2417; 0.2639 ] \n  grades &lt;~ sci_12         0.2760      0.0048   57.4761    0.0000 [ 0.2681; 0.2865 ] \n  grades &lt;~ ss_12          0.2823      0.0044   64.1764    0.0000 [ 0.2739; 0.2920 ] \n\nEstimated construct correlations:\n=================================\n                                                                   CI_percentile   \n  Correlation          Estimate  Std. error   t-stat.   p-value         95%        \n  famback ~~ eminor     -0.2939      0.0332   -8.8480    0.0000 [-0.3581;-0.2264 ] \n\n------------------------------------ Effects -----------------------------------\n\nEstimated total effects:\n========================\n                                                                   CI_percentile   \n  Total effect         Estimate  Std. error   t-stat.   p-value         95%        \n  prevach ~ famback      0.5272      0.0305   17.3107    0.0000 [ 0.4600; 0.5844 ] \n  prevach ~ eminor      -0.0807      0.0306   -2.6349    0.0084 [-0.1395;-0.0224 ] \n  hw ~ famback           0.3987      0.0533    7.4775    0.0000 [ 0.2881; 0.4969 ] \n  hw ~ eminor            0.0555      0.0465    1.1928    0.2330 [-0.0407; 0.1469 ] \n  hw ~ prevach           0.4310      0.0644    6.6888    0.0000 [ 0.2992; 0.5590 ] \n  grades ~ famback       0.3851      0.0253   15.2013    0.0000 [ 0.3320; 0.4315 ] \n  grades ~ eminor       -0.0303      0.0227   -1.3323    0.1828 [-0.0774; 0.0142 ] \n  grades ~ prevach       0.6506      0.0239   27.1887    0.0000 [ 0.6017; 0.6942 ] \n  grades ~ hw            0.2457      0.0537    4.5755    0.0000 [ 0.1470; 0.3578 ] \n\nEstimated indirect effects:\n===========================\n                                                                  CI_percentile   \n  Indirect effect     Estimate  Std. error   t-stat.   p-value         95%        \n  hw ~ famback          0.2272      0.0380    5.9756    0.0000 [ 0.1577; 0.3072 ] \n  hw ~ eminor          -0.0348      0.0141   -2.4657    0.0137 [-0.0636;-0.0093 ] \n  grades ~ famback      0.3851      0.0253   15.2013    0.0000 [ 0.3320; 0.4315 ] \n  grades ~ eminor      -0.0303      0.0227   -1.3323    0.1828 [-0.0774; 0.0142 ] \n  grades ~ prevach      0.1059      0.0322    3.2877    0.0010 [ 0.0553; 0.1781 ] \n________________________________________________________________________________\n\n\n\n\nMulti-group\n\n# generate a dataset with the same covariance matrix\nlibrary(readxl)\nhw_mg_minor &lt;- read_xls(\"data/chap 18 latent var SEM 2/minority matrix.xls\")\nhw_mg_white &lt;- read_xls(\"data/chap 18 latent var SEM 2/white matrix.xls\")\n\nhw_mg_minor_cov &lt;- hw_mg_minor[c(2:14), c(3:15)] |&gt;\n  as.matrix() |&gt;\n  lav_matrix_vechr(diagonal = TRUE) |&gt;\n  getCov(names = hw_mg_minor$varname_[2:14]) |&gt;\n  cor2cov(sds = hw_mg_minor[16, 3:15] |&gt; as.double())\n\nmean_minor &lt;- hw_mg_minor[15, 3:15] |&gt; as.double()\n\nhw_mg_white_cov &lt;- hw_mg_white[c(2:14), c(3:15)] |&gt;\n  as.matrix() |&gt;\n  lav_matrix_vechr(diagonal = TRUE) |&gt;\n  getCov(names = hw_mg_white$varname_[2:14]) |&gt;\n  cor2cov(sds = hw_mg_white[16, 3:15] |&gt; as.double())\n\nmean_white &lt;- hw_mg_white[15, 3:15] |&gt; as.double()\n\n# generate a dataset with the same covariance matrix\nhw_sim_minor &lt;- semTools::kd(hw_mg_minor_cov, n = 274, type = \"exact\") |&gt;\n  sweep(2, mean_minor, FUN = \"+\")\nhw_sim_white &lt;- semTools::kd(hw_mg_white_cov, n = 751, type = \"exact\") |&gt;\n  sweep(2, mean_white, FUN = \"+\")\n\nhw_multigroup &lt;- bind_rows(\n  hw_sim_minor |&gt; mutate(group = \"minority\"),\n  hw_sim_white |&gt; mutate(group = \"white\")\n)\n\ncolnames(hw_multigroup) &lt;- tolower(colnames(hw_multigroup))\nhw_multigroup |&gt; head() |&gt; print()\n\n    bypared  byfaminc    parocc bytxrstd bytxmstd bytxsstd bytxhstd       hw_8\n1 0.3828372  7.139510  3.443991 50.81496 42.23264 40.78801 47.26533  1.0615476\n2 1.6257304  8.967486 13.142849 49.69350 53.78078 47.55894 40.93426  2.8156877\n3 1.1518519  5.788364 58.299023 43.94204 34.66668 30.53849 41.88674 -0.6045725\n4 4.2245456  8.207493 58.776117 50.37986 47.96402 50.28590 45.82648  1.1304863\n5 2.8003436  8.554826 31.127037 40.34252 48.09470 46.18106 47.44408 -0.3079130\n6 4.5135428 15.393487 81.967618 72.28261 76.40078 70.26384 63.79064  2.4412002\n      hw10    eng_12   math_12   sci_12       ss_12    group\n1 4.939912 2.0199859 -1.903368 1.627215 -0.01998056 minority\n2 4.861003 9.3495783  7.201756 8.267503  5.68175432 minority\n3 3.635397 1.6409249  2.495852 2.468783  1.27290397 minority\n4 3.798215 0.9150152  2.409651 2.532526  6.17228111 minority\n5 3.066908 3.9071210  5.153388 5.235629  5.75677822 minority\n6 3.287299 7.9835941  8.577843 6.882267  6.95743065 minority\n\n\n\n# Fit the model\nhw_model_mg &lt;- \"\n  famback =~ parocc + bypared + byfaminc\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw10 + hw_8\n  grades =~ eng_12 + math_12 + sci_12 + ss_12\n\n  bytxrstd ~~ eng_12\n  bytxmstd ~~ math_12\n  bytxsstd ~~ sci_12\n  bytxhstd ~~ ss_12\n\n  prevach ~ famback\n  grades ~ prevach + hw\n  hw ~ prevach + famback\n\"\n\nsem_fit_mg &lt;- sem(hw_model_mg,\n  data = hw_multigroup,\n  estimator = \"ML\",\n  group = \"group\"\n)\n\nsummary(sem_fit_mg, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 412 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        96\n\n  Number of observations per group:                   \n    minority                                       274\n    white                                          751\n\nModel Test User Model:\n                                                      \n  Test statistic                               223.177\n  Degrees of freedom                               112\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    minority                                    95.606\n    white                                      127.571\n\nModel Test Baseline Model:\n\n  Test statistic                              8290.390\n  Degrees of freedom                               156\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.986\n  Tucker-Lewis Index (TLI)                       0.981\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33497.854\n  Loglikelihood unrestricted model (H1)     -33386.266\n                                                      \n  Akaike (AIC)                               67187.708\n  Bayesian (BIC)                             67661.223\n  Sample-size adjusted Bayesian (SABIC)      67356.317\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.044\n  90 Percent confidence interval - lower         0.036\n  90 Percent confidence interval - upper         0.052\n  P-value H_0: RMSEA &lt;= 0.050                    0.876\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [minority]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              19.007    0.793\n    bypared           0.059    0.005   11.771    0.000    1.112    0.801\n    byfaminc          0.103    0.010   10.666    0.000    1.961    0.692\n  prevach =~                                                            \n    bytxrstd          1.000                               8.514    0.859\n    bytxmstd          1.058    0.057   18.530    0.000    9.010    0.869\n    bytxsstd          0.951    0.057   16.576    0.000    8.100    0.815\n    bytxhstd          0.942    0.057   16.552    0.000    8.016    0.813\n  hw =~                                                                 \n    hw10              1.000                               0.544    0.297\n    hw_8              0.791    0.195    4.062    0.000    0.430    0.378\n  grades =~                                                             \n    eng_12            1.000                               2.559    0.929\n    math_12           0.895    0.044   20.520    0.000    2.292    0.833\n    sci_12            0.923    0.040   22.949    0.000    2.362    0.877\n    ss_12             1.049    0.042   24.764    0.000    2.686    0.902\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.263    0.032    8.278    0.000    0.586    0.586\n  grades ~                                                              \n    prevach           3.125   63.376    0.049    0.961   10.396   10.396\n    hw              -45.575  984.791   -0.046    0.963   -9.685   -9.685\n  hw ~                                                                  \n    prevach           0.065    0.014    4.595    0.000    1.011    1.011\n    famback          -0.000    0.004   -0.047    0.963   -0.006   -0.006\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.191    0.456    0.418    0.676    0.191    0.037\n .bytxmstd ~~                                                           \n   .math_12           2.386    0.613    3.895    0.000    2.386    0.305\n .bytxsstd ~~                                                           \n   .sci_12            0.289    0.551    0.524    0.600    0.289    0.039\n .bytxhstd ~~                                                           \n   .ss_12             1.140    0.573    1.989    0.047    1.140    0.155\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           43.085    1.448   29.755    0.000   43.085    1.798\n   .bypared           2.849    0.084   33.953    0.000    2.849    2.051\n   .byfaminc          8.757    0.171   51.130    0.000    8.757    3.089\n   .bytxrstd         48.537    0.599   81.056    0.000   48.537    4.897\n   .bytxmstd         49.808    0.626   79.526    0.000   49.808    4.804\n   .bytxsstd         47.959    0.600   79.907    0.000   47.959    4.827\n   .bytxhstd         48.160    0.596   80.823    0.000   48.160    4.883\n   .hw10              3.214    0.111   29.011    0.000    3.214    1.753\n   .hw_8              1.718    0.069   24.980    0.000    1.718    1.509\n   .eng_12            5.820    0.166   34.973    0.000    5.820    2.113\n   .math_12           5.385    0.166   32.389    0.000    5.385    1.957\n   .sci_12            5.590    0.163   34.372    0.000    5.590    2.076\n   .ss_12             5.895    0.180   32.785    0.000    5.895    1.981\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          213.210   29.518    7.223    0.000  213.210    0.371\n   .bypared           0.691    0.099    6.993    0.000    0.691    0.358\n   .byfaminc          4.193    0.447    9.378    0.000    4.193    0.522\n   .bytxrstd         25.756    3.009    8.560    0.000   25.756    0.262\n   .bytxmstd         26.302    3.181    8.268    0.000   26.302    0.245\n   .bytxsstd         33.096    3.475    9.525    0.000   33.096    0.335\n   .bytxhstd         33.025    3.448    9.577    0.000   33.025    0.339\n   .hw10              3.068    0.281   10.933    0.000    3.068    0.912\n   .hw_8              1.112    0.114    9.774    0.000    1.112    0.857\n   .eng_12            1.039    0.148    7.029    0.000    1.039    0.137\n   .math_12           2.321    0.230   10.076    0.000    2.321    0.306\n   .sci_12            1.670    0.181    9.241    0.000    1.670    0.230\n   .ss_12             1.647    0.195    8.433    0.000    1.647    0.186\n    famback         361.281   51.154    7.063    0.000    1.000    1.000\n   .prevach          47.575    6.080    7.825    0.000    0.656    0.656\n   .hw               -0.005    0.103   -0.046    0.963   -0.016   -0.016\n   .grades           13.806  214.777    0.064    0.949    2.108    2.108\n\n\nGroup 2 [white]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              14.207    0.718\n    bypared           0.072    0.004   16.238    0.000    1.025    0.842\n    byfaminc          0.094    0.007   14.113    0.000    1.336    0.593\n  prevach =~                                                            \n    bytxrstd          1.000                               8.583    0.844\n    bytxmstd          0.997    0.036   28.022    0.000    8.560    0.835\n    bytxsstd          0.987    0.035   28.103    0.000    8.475    0.842\n    bytxhstd          0.970    0.035   27.524    0.000    8.325    0.833\n  hw =~                                                                 \n    hw10              1.000                               1.280    0.664\n    hw_8              0.389    0.063    6.181    0.000    0.498    0.441\n  grades =~                                                             \n    eng_12            1.000                               2.419    0.920\n    math_12           0.901    0.028   32.154    0.000    2.179    0.815\n    sci_12            0.972    0.026   37.566    0.000    2.352    0.878\n    ss_12             1.065    0.026   41.677    0.000    2.577    0.917\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.299    0.027   11.063    0.000    0.495    0.495\n  grades ~                                                              \n    prevach           0.156    0.012   13.222    0.000    0.555    0.555\n    hw                0.468    0.114    4.118    0.000    0.248    0.248\n  hw ~                                                                  \n    prevach           0.046    0.009    4.917    0.000    0.307    0.307\n    famback           0.022    0.006    3.658    0.000    0.239    0.239\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.860    0.289    2.971    0.003    0.860    0.153\n .bytxmstd ~~                                                           \n   .math_12           2.953    0.400    7.387    0.000    2.953    0.338\n .bytxsstd ~~                                                           \n   .sci_12            1.139    0.327    3.483    0.000    1.139    0.164\n .bytxhstd ~~                                                           \n   .ss_12             0.349    0.310    1.127    0.260    0.349    0.056\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           54.883    0.722   76.051    0.000   54.883    2.775\n   .bypared           3.335    0.044   75.101    0.000    3.335    2.740\n   .byfaminc         10.348    0.082  125.757    0.000   10.348    4.589\n   .bytxrstd         53.260    0.371  143.506    0.000   53.260    5.237\n   .bytxmstd         53.559    0.374  143.173    0.000   53.559    5.224\n   .bytxsstd         53.335    0.367  145.260    0.000   53.335    5.301\n   .bytxhstd         52.956    0.365  145.243    0.000   52.956    5.300\n   .hw10              3.443    0.070   48.980    0.000    3.443    1.787\n   .hw_8              1.733    0.041   42.046    0.000    1.733    1.534\n   .eng_12            6.409    0.096   66.808    0.000    6.409    2.438\n   .math_12           5.823    0.098   59.670    0.000    5.823    2.177\n   .sci_12            6.088    0.098   62.269    0.000    6.088    2.272\n   .ss_12             6.612    0.103   64.449    0.000    6.612    2.352\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          189.275   14.140   13.386    0.000  189.275    0.484\n   .bypared           0.431    0.056    7.687    0.000    0.431    0.291\n   .byfaminc          3.299    0.198   16.701    0.000    3.299    0.649\n   .bytxrstd         29.775    2.045   14.556    0.000   29.775    0.288\n   .bytxmstd         31.816    2.137   14.890    0.000   31.816    0.303\n   .bytxsstd         29.426    2.010   14.640    0.000   29.426    0.291\n   .bytxhstd         30.535    2.047   14.917    0.000   30.535    0.306\n   .hw10              2.073    0.282    7.355    0.000    2.073    0.559\n   .hw_8              1.028    0.067   15.451    0.000    1.028    0.806\n   .eng_12            1.060    0.085   12.496    0.000    1.060    0.153\n   .math_12           2.401    0.141   17.014    0.000    2.401    0.336\n   .sci_12            1.645    0.108   15.261    0.000    1.645    0.229\n   .ss_12             1.262    0.099   12.770    0.000    1.262    0.160\n    famback         201.842   20.408    9.890    0.000    1.000    1.000\n   .prevach          55.591    4.259   13.054    0.000    0.755    0.755\n   .hw                1.271    0.275    4.623    0.000    0.776    0.776\n   .grades            3.005    0.215   13.978    0.000    0.514    0.514\n\n\n\n\nlavInspect(sem_fit_mg, \"cov.lv\") |&gt; print()\ninspect(sem_fit_mg, what = \"est\") |&gt; print()\n\n$minority\n         fambck  prevch      hw  grades\nfamback 361.281                        \nprevach  94.886  72.495                \nhw        6.070   4.668   0.296        \ngrades   19.872  13.811   1.106   6.550\n\n$white\n         fambck  prevch      hw  grades\nfamback 201.842                        \nprevach  60.406  73.668                \nhw        7.113   4.677   1.638        \ngrades   12.780  13.715   1.498   5.852\n\n$minority\n$minority$lambda\n         fambck prevch    hw grades\nparocc    1.000  0.000 0.000  0.000\nbypared   0.059  0.000 0.000  0.000\nbyfaminc  0.103  0.000 0.000  0.000\nbytxrstd  0.000  1.000 0.000  0.000\nbytxmstd  0.000  1.058 0.000  0.000\nbytxsstd  0.000  0.951 0.000  0.000\nbytxhstd  0.000  0.942 0.000  0.000\nhw10      0.000  0.000 1.000  0.000\nhw_8      0.000  0.000 0.791  0.000\neng_12    0.000  0.000 0.000  1.000\nmath_12   0.000  0.000 0.000  0.895\nsci_12    0.000  0.000 0.000  0.923\nss_12     0.000  0.000 0.000  1.049\n\n$minority$theta\n          parocc  bypard  byfmnc  bytxrs  bytxms  bytxss  bytxhs    hw10\nparocc   213.210                                                        \nbypared    0.000   0.691                                                \nbyfaminc   0.000   0.000   4.193                                        \nbytxrstd   0.000   0.000   0.000  25.756                                \nbytxmstd   0.000   0.000   0.000   0.000  26.302                        \nbytxsstd   0.000   0.000   0.000   0.000   0.000  33.096                \nbytxhstd   0.000   0.000   0.000   0.000   0.000   0.000  33.025        \nhw10       0.000   0.000   0.000   0.000   0.000   0.000   0.000   3.068\nhw_8       0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.000\neng_12     0.000   0.000   0.000   0.191   0.000   0.000   0.000   0.000\nmath_12    0.000   0.000   0.000   0.000   2.386   0.000   0.000   0.000\nsci_12     0.000   0.000   0.000   0.000   0.000   0.289   0.000   0.000\nss_12      0.000   0.000   0.000   0.000   0.000   0.000   1.140   0.000\n            hw_8  eng_12  mth_12  sci_12   ss_12\nparocc                                          \nbypared                                         \nbyfaminc                                        \nbytxrstd                                        \nbytxmstd                                        \nbytxsstd                                        \nbytxhstd                                        \nhw10                                            \nhw_8       1.112                                \neng_12     0.000   1.039                        \nmath_12    0.000   0.000   2.321                \nsci_12     0.000   0.000   0.000   1.670        \nss_12      0.000   0.000   0.000   0.000   1.647\n\n$minority$psi\n         fambck  prevch      hw  grades\nfamback 361.281                        \nprevach   0.000  47.575                \nhw        0.000   0.000  -0.005        \ngrades    0.000   0.000   0.000  13.806\n\n$minority$beta\n        fambck prevch      hw grades\nfamback  0.000  0.000   0.000      0\nprevach  0.263  0.000   0.000      0\nhw       0.000  0.065   0.000      0\ngrades   0.000  3.125 -45.575      0\n\n$minority$nu\n         intrcp\nparocc   43.085\nbypared   2.849\nbyfaminc  8.757\nbytxrstd 48.537\nbytxmstd 49.808\nbytxsstd 47.959\nbytxhstd 48.160\nhw10      3.214\nhw_8      1.718\neng_12    5.820\nmath_12   5.385\nsci_12    5.590\nss_12     5.895\n\n$minority$alpha\n        intrcp\nfamback      0\nprevach      0\nhw           0\ngrades       0\n\n\n$white\n$white$lambda\n         fambck prevch    hw grades\nparocc    1.000  0.000 0.000  0.000\nbypared   0.072  0.000 0.000  0.000\nbyfaminc  0.094  0.000 0.000  0.000\nbytxrstd  0.000  1.000 0.000  0.000\nbytxmstd  0.000  0.997 0.000  0.000\nbytxsstd  0.000  0.987 0.000  0.000\nbytxhstd  0.000  0.970 0.000  0.000\nhw10      0.000  0.000 1.000  0.000\nhw_8      0.000  0.000 0.389  0.000\neng_12    0.000  0.000 0.000  1.000\nmath_12   0.000  0.000 0.000  0.901\nsci_12    0.000  0.000 0.000  0.972\nss_12     0.000  0.000 0.000  1.065\n\n$white$theta\n          parocc  bypard  byfmnc  bytxrs  bytxms  bytxss  bytxhs    hw10\nparocc   189.275                                                        \nbypared    0.000   0.431                                                \nbyfaminc   0.000   0.000   3.299                                        \nbytxrstd   0.000   0.000   0.000  29.775                                \nbytxmstd   0.000   0.000   0.000   0.000  31.816                        \nbytxsstd   0.000   0.000   0.000   0.000   0.000  29.426                \nbytxhstd   0.000   0.000   0.000   0.000   0.000   0.000  30.535        \nhw10       0.000   0.000   0.000   0.000   0.000   0.000   0.000   2.073\nhw_8       0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.000\neng_12     0.000   0.000   0.000   0.860   0.000   0.000   0.000   0.000\nmath_12    0.000   0.000   0.000   0.000   2.953   0.000   0.000   0.000\nsci_12     0.000   0.000   0.000   0.000   0.000   1.139   0.000   0.000\nss_12      0.000   0.000   0.000   0.000   0.000   0.000   0.349   0.000\n            hw_8  eng_12  mth_12  sci_12   ss_12\nparocc                                          \nbypared                                         \nbyfaminc                                        \nbytxrstd                                        \nbytxmstd                                        \nbytxsstd                                        \nbytxhstd                                        \nhw10                                            \nhw_8       1.028                                \neng_12     0.000   1.060                        \nmath_12    0.000   0.000   2.401                \nsci_12     0.000   0.000   0.000   1.645        \nss_12      0.000   0.000   0.000   0.000   1.262\n\n$white$psi\n         fambck  prevch      hw  grades\nfamback 201.842                        \nprevach   0.000  55.591                \nhw        0.000   0.000   1.271        \ngrades    0.000   0.000   0.000   3.005\n\n$white$beta\n        fambck prevch    hw grades\nfamback  0.000  0.000 0.000      0\nprevach  0.299  0.000 0.000      0\nhw       0.022  0.046 0.000      0\ngrades   0.000  0.156 0.468      0\n\n$white$nu\n         intrcp\nparocc   54.883\nbypared   3.335\nbyfaminc 10.348\nbytxrstd 53.260\nbytxmstd 53.559\nbytxsstd 53.335\nbytxhstd 52.956\nhw10      3.443\nhw_8      1.733\neng_12    6.409\nmath_12   5.823\nsci_12    6.088\nss_12     6.612\n\n$white$alpha\n        intrcp\nfamback      0\nprevach      0\nhw           0\ngrades       0\n\n\n\n\n\n# contrain factor loadings\nsem_fit_mg2 &lt;- sem(hw_model_mg,\n  data = hw_multigroup,\n  estimator = \"ML\",\n  group = \"group\",\n  group.equal = c(\"loadings\")\n)\n\nsummary(sem_fit_mg2, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 381 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        96\n  Number of equality constraints                     9\n\n  Number of observations per group:                   \n    minority                                       274\n    white                                          751\n\nModel Test User Model:\n                                                      \n  Test statistic                               238.780\n  Degrees of freedom                               121\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    minority                                   106.831\n    white                                      131.949\n\nModel Test Baseline Model:\n\n  Test statistic                              8290.390\n  Degrees of freedom                               156\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.986\n  Tucker-Lewis Index (TLI)                       0.981\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33505.655\n  Loglikelihood unrestricted model (H1)     -33386.266\n                                                      \n  Akaike (AIC)                               67185.311\n  Bayesian (BIC)                             67614.434\n  Sample-size adjusted Bayesian (SABIC)      67338.113\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.044\n  90 Percent confidence interval - lower         0.035\n  90 Percent confidence interval - upper         0.052\n  P-value H_0: RMSEA &lt;= 0.050                    0.902\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.033\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [minority]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              17.929    0.760\n    bypared (.p2.)    0.067    0.003   20.083    0.000    1.207    0.846\n    byfamnc (.p3.)    0.096    0.006   17.481    0.000    1.727    0.629\n  prevach =~                                                            \n    bytxrst           1.000                               8.530    0.860\n    bytxmst (.p5.)    1.014    0.030   33.540    0.000    8.647    0.854\n    bytxsst (.p6.)    0.978    0.030   32.678    0.000    8.340    0.826\n    bytxhst (.p7.)    0.962    0.030   32.151    0.000    8.209    0.823\n  hw =~                                                                 \n    hw10              1.000                               0.678    0.361\n    hw_8    (.p9.)    0.468    0.061    7.685    0.000    0.317    0.286\n  grades =~                                                             \n    eng_12            1.000                               2.527    0.926\n    math_12 (.11.)    0.900    0.024   38.162    0.000    2.274    0.831\n    sci_12  (.12.)    0.958    0.022   44.037    0.000    2.421    0.884\n    ss_12   (.13.)    1.061    0.022   48.511    0.000    2.681    0.902\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.282    0.031    9.079    0.000    0.592    0.592\n  grades ~                                                              \n    prevach          -2.947    0.483   -6.106    0.000   -9.947   -9.947\n    hw               39.821    2.381   16.727    0.000   10.691   10.691\n  hw ~                                                                  \n    prevach           0.078    0.011    6.990    0.000    0.986    0.986\n    famback           0.000    0.000    0.806    0.420    0.006    0.006\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.222    0.456    0.487    0.626    0.222    0.042\n .bytxmstd ~~                                                           \n   .math_12           2.426    0.616    3.937    0.000    2.426    0.302\n .bytxsstd ~~                                                           \n   .sci_12            0.347    0.552    0.628    0.530    0.347    0.047\n .bytxhstd ~~                                                           \n   .ss_12             1.099    0.571    1.925    0.054    1.099    0.151\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           43.085    1.425   30.231    0.000   43.085    1.826\n   .bypared           2.849    0.086   33.058    0.000    2.849    1.997\n   .byfaminc          8.757    0.166   52.839    0.000    8.757    3.192\n   .bytxrstd         48.537    0.599   80.980    0.000   48.537    4.892\n   .bytxmstd         49.808    0.612   81.393    0.000   49.808    4.917\n   .bytxsstd         47.959    0.610   78.615    0.000   47.959    4.749\n   .bytxhstd         48.160    0.603   79.915    0.000   48.160    4.828\n   .hw10              3.214    0.114   28.309    0.000    3.214    1.710\n   .hw_8              1.718    0.067   25.611    0.000    1.718    1.547\n   .eng_12            5.820    0.165   35.298    0.000    5.820    2.132\n   .math_12           5.385    0.165   32.569    0.000    5.385    1.968\n   .sci_12            5.590    0.165   33.777    0.000    5.590    2.041\n   .ss_12             5.895    0.180   32.823    0.000    5.895    1.983\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          235.082   27.675    8.494    0.000  235.082    0.422\n   .bypared           0.578    0.096    6.031    0.000    0.578    0.284\n   .byfaminc          4.543    0.442   10.284    0.000    4.543    0.604\n   .bytxrstd         25.676    2.946    8.715    0.000   25.676    0.261\n   .bytxmstd         27.839    3.136    8.878    0.000   27.839    0.271\n   .bytxsstd         32.420    3.423    9.471    0.000   32.420    0.318\n   .bytxhstd         32.120    3.371    9.528    0.000   32.120    0.323\n   .hw10              3.072    0.265   11.610    0.000    3.072    0.870\n   .hw_8              1.133    0.097   11.671    0.000    1.133    0.918\n   .eng_12            1.064    0.143    7.437    0.000    1.064    0.143\n   .math_12           2.320    0.228   10.161    0.000    2.320    0.310\n   .sci_12            1.643    0.179    9.178    0.000    1.643    0.219\n   .ss_12             1.652    0.192    8.612    0.000    1.652    0.187\n    famback         321.434   39.202    8.199    0.000    1.000    1.000\n   .prevach          47.246    5.437    8.690    0.000    0.649    0.649\n   .hw                0.010    0.005    1.917    0.055    0.021    0.021\n   .grades          -11.683    6.164   -1.895    0.058   -1.830   -1.830\n\n\nGroup 2 [white]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              14.625    0.735\n    bypared (.p2.)    0.067    0.003   20.083    0.000    0.984    0.818\n    byfamnc (.p3.)    0.096    0.006   17.481    0.000    1.409    0.617\n  prevach =~                                                            \n    bytxrst           1.000                               8.580    0.844\n    bytxmst (.p5.)    1.014    0.030   33.540    0.000    8.698    0.841\n    bytxsst (.p6.)    0.978    0.030   32.678    0.000    8.389    0.839\n    bytxhst (.p7.)    0.962    0.030   32.151    0.000    8.258    0.830\n  hw =~                                                                 \n    hw10              1.000                               1.177    0.615\n    hw_8    (.p9.)    0.468    0.061    7.685    0.000    0.550    0.484\n  grades =~                                                             \n    eng_12            1.000                               2.431    0.921\n    math_12 (.11.)    0.900    0.024   38.162    0.000    2.187    0.816\n    sci_12  (.12.)    0.958    0.022   44.037    0.000    2.329    0.875\n    ss_12   (.13.)    1.061    0.022   48.511    0.000    2.579    0.917\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.292    0.025   11.434    0.000    0.497    0.497\n  grades ~                                                              \n    prevach           0.156    0.012   13.103    0.000    0.551    0.551\n    hw                0.513    0.121    4.234    0.000    0.249    0.249\n  hw ~                                                                  \n    prevach           0.045    0.009    5.024    0.000    0.326    0.326\n    famback           0.019    0.005    3.546    0.000    0.239    0.239\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.850    0.289    2.938    0.003    0.850    0.152\n .bytxmstd ~~                                                           \n   .math_12           2.951    0.400    7.377    0.000    2.951    0.340\n .bytxsstd ~~                                                           \n   .sci_12            1.141    0.327    3.491    0.000    1.141    0.163\n .bytxhstd ~~                                                           \n   .ss_12             0.361    0.310    1.163    0.245    0.361    0.058\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           54.883    0.726   75.580    0.000   54.883    2.758\n   .bypared           3.335    0.044   75.936    0.000    3.335    2.771\n   .byfaminc         10.348    0.083  124.155    0.000   10.348    4.530\n   .bytxrstd         53.260    0.371  143.520    0.000   53.260    5.237\n   .bytxmstd         53.559    0.378  141.840    0.000   53.559    5.176\n   .bytxsstd         53.335    0.365  146.123    0.000   53.335    5.332\n   .bytxhstd         52.956    0.363  145.852    0.000   52.956    5.322\n   .hw10              3.443    0.070   49.289    0.000    3.443    1.799\n   .hw_8              1.733    0.042   41.718    0.000    1.733    1.522\n   .eng_12            6.409    0.096   66.567    0.000    6.409    2.429\n   .math_12           5.823    0.098   59.527    0.000    5.823    2.172\n   .sci_12            6.088    0.097   62.700    0.000    6.088    2.288\n   .ss_12             6.612    0.103   64.415    0.000    6.612    2.351\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          182.115   13.639   13.352    0.000  182.115    0.460\n   .bypared           0.480    0.050    9.615    0.000    0.480    0.331\n   .byfaminc          3.233    0.196   16.479    0.000    3.233    0.620\n   .bytxrstd         29.799    2.030   14.681    0.000   29.799    0.288\n   .bytxmstd         31.427    2.125   14.792    0.000   31.427    0.293\n   .bytxsstd         29.674    1.994   14.880    0.000   29.674    0.297\n   .bytxhstd         30.813    2.036   15.133    0.000   30.813    0.311\n   .hw10              2.279    0.229    9.959    0.000    2.279    0.622\n   .hw_8              0.993    0.067   14.787    0.000    0.993    0.766\n   .eng_12            1.053    0.084   12.506    0.000    1.053    0.151\n   .math_12           2.401    0.141   17.038    0.000    2.401    0.334\n   .sci_12            1.655    0.107   15.444    0.000    1.655    0.234\n   .ss_12             1.261    0.098   12.866    0.000    1.261    0.159\n    famback         213.888   19.110   11.192    0.000    1.000    1.000\n   .prevach          55.447    4.061   13.654    0.000    0.753    0.753\n   .hw                1.051    0.209    5.032    0.000    0.759    0.759\n   .grades            3.031    0.213   14.223    0.000    0.513    0.513\n\n\n\n\n# compare the two models\nlavTestLRT(sem_fit_mg, sem_fit_mg2) |&gt; print()\n\n\nChi-Squared Difference Test\n\n             Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)  \nsem_fit_mg  112 67188 67661 223.18                                         \nsem_fit_mg2 121 67185 67614 238.78     15.603 0.037835       9    0.07565 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# constrains the effect of homework\nhw_model_mg3 &lt;- \"\n  famback =~ parocc + bypared + byfaminc\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw10 + hw_8\n  grades =~ eng_12 + math_12 + sci_12 + ss_12\n\n  bytxrstd ~~ eng_12\n  bytxmstd ~~ math_12\n  bytxsstd ~~ sci_12\n  bytxhstd ~~ ss_12\n\n  prevach ~ famback\n  grades ~ prevach + c(h, h)*hw\n  hw ~ prevach + famback\n\"\n\nsem_fit_mg3 &lt;- sem(hw_model_mg3,\n  data = hw_multigroup,\n  estimator = \"ML\",\n  group = \"group\",\n  group.equal = c(\"loadings\")\n)\n\nsummary(sem_fit_mg3, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 311 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        96\n  Number of equality constraints                    10\n\n  Number of observations per group:                   \n    minority                                       274\n    white                                          751\n\nModel Test User Model:\n                                                      \n  Test statistic                               236.984\n  Degrees of freedom                               122\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    minority                                   104.664\n    white                                      132.321\n\nModel Test Baseline Model:\n\n  Test statistic                              8290.390\n  Degrees of freedom                               156\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.986\n  Tucker-Lewis Index (TLI)                       0.982\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33504.758\n  Loglikelihood unrestricted model (H1)     -33386.266\n                                                      \n  Akaike (AIC)                               67181.515\n  Bayesian (BIC)                             67605.706\n  Sample-size adjusted Bayesian (SABIC)      67332.561\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.043\n  90 Percent confidence interval - lower         0.035\n  90 Percent confidence interval - upper         0.051\n  P-value H_0: RMSEA &lt;= 0.050                    0.924\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.033\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [minority]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              17.948    0.761\n    bypared (.p2.)    0.067    0.003   20.086    0.000    1.207    0.845\n    byfamnc (.p3.)    0.096    0.006   17.489    0.000    1.728    0.630\n  prevach =~                                                            \n    bytxrst           1.000                               8.528    0.859\n    bytxmst (.p5.)    1.014    0.030   33.531    0.000    8.647    0.854\n    bytxsst (.p6.)    0.978    0.030   32.663    0.000    8.339    0.825\n    bytxhst (.p7.)    0.963    0.030   32.151    0.000    8.210    0.823\n  hw =~                                                                 \n    hw10              1.000                               0.991    0.528\n    hw_8    (.p9.)    0.475    0.061    7.816    0.000    0.471    0.423\n  grades =~                                                             \n    eng_12            1.000                               2.522    0.926\n    math_12 (.11.)    0.900    0.024   38.175    0.000    2.269    0.830\n    sci_12  (.12.)    0.958    0.022   44.034    0.000    2.416    0.883\n    ss_12   (.13.)    1.061    0.022   48.518    0.000    2.675    0.901\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.283    0.031    9.132    0.000    0.595    0.595\n  grades ~                                                              \n    prevach           0.146    0.019    7.856    0.000    0.493    0.493\n    hw         (h)    0.548    0.121    4.519    0.000    0.216    0.216\n  hw ~                                                                  \n    prevach           0.076    0.015    5.030    0.000    0.655    0.655\n    famback           0.001    0.007    0.094    0.925    0.012    0.012\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.202    0.456    0.443    0.658    0.202    0.039\n .bytxmstd ~~                                                           \n   .math_12           2.427    0.616    3.941    0.000    2.427    0.302\n .bytxsstd ~~                                                           \n   .sci_12            0.342    0.553    0.619    0.536    0.342    0.047\n .bytxhstd ~~                                                           \n   .ss_12             1.079    0.570    1.892    0.059    1.079    0.148\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           43.085    1.425   30.228    0.000   43.085    1.826\n   .bypared           2.849    0.086   33.036    0.000    2.849    1.996\n   .byfaminc          8.757    0.166   52.837    0.000    8.757    3.192\n   .bytxrstd         48.537    0.600   80.939    0.000   48.537    4.890\n   .bytxmstd         49.808    0.612   81.404    0.000   49.808    4.918\n   .bytxsstd         47.959    0.610   78.584    0.000   47.959    4.747\n   .bytxhstd         48.160    0.602   79.937    0.000   48.160    4.829\n   .hw10              3.214    0.114   28.318    0.000    3.214    1.711\n   .hw_8              1.718    0.067   25.554    0.000    1.718    1.544\n   .eng_12            5.820    0.165   35.367    0.000    5.820    2.137\n   .math_12           5.385    0.165   32.611    0.000    5.385    1.970\n   .sci_12            5.590    0.165   33.832    0.000    5.590    2.044\n   .ss_12             5.895    0.179   32.880    0.000    5.895    1.986\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          234.497   27.663    8.477    0.000  234.497    0.421\n   .bypared           0.581    0.096    6.047    0.000    0.581    0.285\n   .byfaminc          4.539    0.442   10.279    0.000    4.539    0.603\n   .bytxrstd         25.812    2.954    8.738    0.000   25.812    0.262\n   .bytxmstd         27.801    3.130    8.882    0.000   27.801    0.271\n   .bytxsstd         32.520    3.429    9.484    0.000   32.520    0.319\n   .bytxhstd         32.045    3.363    9.527    0.000   32.045    0.322\n   .hw10              2.548    0.310    8.213    0.000    2.548    0.722\n   .hw_8              1.017    0.101   10.048    0.000    1.017    0.821\n   .eng_12            1.062    0.143    7.424    0.000    1.062    0.143\n   .math_12           2.320    0.228   10.158    0.000    2.320    0.311\n   .sci_12            1.644    0.179    9.177    0.000    1.644    0.220\n   .ss_12             1.651    0.192    8.606    0.000    1.651    0.187\n    famback         322.146   39.268    8.204    0.000    1.000    1.000\n   .prevach          46.966    5.417    8.671    0.000    0.646    0.646\n   .hw                0.552    0.226    2.444    0.015    0.562    0.562\n   .grades            3.626    0.380    9.533    0.000    0.570    0.570\n\n\nGroup 2 [white]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              14.631    0.735\n    bypared (.p2.)    0.067    0.003   20.086    0.000    0.984    0.818\n    byfamnc (.p3.)    0.096    0.006   17.489    0.000    1.409    0.617\n  prevach =~                                                            \n    bytxrst           1.000                               8.579    0.844\n    bytxmst (.p5.)    1.014    0.030   33.531    0.000    8.699    0.841\n    bytxsst (.p6.)    0.978    0.030   32.663    0.000    8.389    0.839\n    bytxhst (.p7.)    0.963    0.030   32.151    0.000    8.260    0.830\n  hw =~                                                                 \n    hw10              1.000                               1.159    0.606\n    hw_8    (.p9.)    0.475    0.061    7.816    0.000    0.551    0.484\n  grades =~                                                             \n    eng_12            1.000                               2.433    0.921\n    math_12 (.11.)    0.900    0.024   38.175    0.000    2.190    0.816\n    sci_12  (.12.)    0.958    0.022   44.034    0.000    2.331    0.875\n    ss_12   (.13.)    1.061    0.022   48.518    0.000    2.581    0.917\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.291    0.025   11.429    0.000    0.497    0.497\n  grades ~                                                              \n    prevach           0.154    0.012   12.970    0.000    0.543    0.543\n    hw         (h)    0.548    0.121    4.519    0.000    0.261    0.261\n  hw ~                                                                  \n    prevach           0.045    0.009    5.044    0.000    0.330    0.330\n    famback           0.019    0.005    3.546    0.000    0.240    0.240\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.851    0.289    2.944    0.003    0.851    0.152\n .bytxmstd ~~                                                           \n   .math_12           2.950    0.400    7.375    0.000    2.950    0.340\n .bytxsstd ~~                                                           \n   .sci_12            1.141    0.327    3.492    0.000    1.141    0.163\n .bytxhstd ~~                                                           \n   .ss_12             0.362    0.310    1.168    0.243    0.362    0.058\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           54.883    0.726   75.571    0.000   54.883    2.758\n   .bypared           3.335    0.044   75.949    0.000    3.335    2.771\n   .byfaminc         10.348    0.083  124.151    0.000   10.348    4.530\n   .bytxrstd         53.260    0.371  143.540    0.000   53.260    5.238\n   .bytxmstd         53.559    0.378  141.817    0.000   53.559    5.175\n   .bytxsstd         53.335    0.365  146.139    0.000   53.335    5.333\n   .bytxhstd         52.956    0.363  145.841    0.000   52.956    5.322\n   .hw10              3.443    0.070   49.350    0.000    3.443    1.801\n   .hw_8              1.733    0.042   41.712    0.000    1.733    1.522\n   .eng_12            6.409    0.096   66.521    0.000    6.409    2.427\n   .math_12           5.823    0.098   59.483    0.000    5.823    2.171\n   .sci_12            6.088    0.097   62.661    0.000    6.088    2.287\n   .ss_12             6.612    0.103   64.363    0.000    6.612    2.349\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          182.019   13.641   13.343    0.000  182.019    0.460\n   .bypared           0.480    0.050    9.631    0.000    0.480    0.332\n   .byfaminc          3.232    0.196   16.477    0.000    3.232    0.620\n   .bytxrstd         29.797    2.030   14.681    0.000   29.797    0.288\n   .bytxmstd         31.440    2.126   14.791    0.000   31.440    0.294\n   .bytxsstd         29.662    1.994   14.876    0.000   29.662    0.297\n   .bytxhstd         30.798    2.036   15.128    0.000   30.798    0.311\n   .hw10              2.312    0.223   10.360    0.000    2.312    0.632\n   .hw_8              0.993    0.067   14.829    0.000    0.993    0.766\n   .eng_12            1.052    0.084   12.505    0.000    1.052    0.151\n   .math_12           2.401    0.141   17.037    0.000    2.401    0.334\n   .sci_12            1.656    0.107   15.446    0.000    1.656    0.234\n   .ss_12             1.261    0.098   12.869    0.000    1.261    0.159\n    famback         214.077   19.121   11.196    0.000    1.000    1.000\n   .prevach          55.441    4.061   13.652    0.000    0.753    0.753\n   .hw                1.014    0.201    5.045    0.000    0.755    0.755\n   .grades            3.015    0.214   14.100    0.000    0.509    0.509\n\n\n\n\n# compare the two models\nlavTestLRT(sem_fit_mg2, sem_fit_mg3) |&gt; print()\n\n\nChi-Squared Difference Test\n\n             Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nsem_fit_mg2 121 67185 67614 238.78                                    \nsem_fit_mg3 122 67182 67606 236.98    -1.7954     0       1          1\n\n\n\n# constrain all the effects\nsem_fit_mg4 &lt;- sem(hw_model_mg,\n  data = hw_multigroup,\n  estimator = \"ML\",\n  group = \"group\",\n  group.equal = c(\"loadings\", \"regressions\")\n)\nsummary(sem_fit_mg4, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 283 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        96\n  Number of equality constraints                    14\n\n  Number of observations per group:                   \n    minority                                       274\n    white                                          751\n\nModel Test User Model:\n                                                      \n  Test statistic                               241.795\n  Degrees of freedom                               126\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    minority                                   108.582\n    white                                      133.213\n\nModel Test Baseline Model:\n\n  Test statistic                              8290.390\n  Degrees of freedom                               156\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.986\n  Tucker-Lewis Index (TLI)                       0.982\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33507.163\n  Loglikelihood unrestricted model (H1)     -33386.266\n                                                      \n  Akaike (AIC)                               67178.326\n  Bayesian (BIC)                             67582.787\n  Sample-size adjusted Bayesian (SABIC)      67322.346\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.042\n  90 Percent confidence interval - lower         0.034\n  90 Percent confidence interval - upper         0.050\n  P-value H_0: RMSEA &lt;= 0.050                    0.942\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.033\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [minority]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              17.852    0.758\n    bypared (.p2.)    0.067    0.003   20.053    0.000    1.201    0.844\n    byfamnc (.p3.)    0.096    0.006   17.465    0.000    1.717    0.627\n  prevach =~                                                            \n    bytxrst           1.000                               8.569    0.861\n    bytxmst (.p5.)    1.014    0.030   33.525    0.000    8.687    0.854\n    bytxsst (.p6.)    0.978    0.030   32.671    0.000    8.380    0.828\n    bytxhst (.p7.)    0.962    0.030   32.138    0.000    8.248    0.825\n  hw =~                                                                 \n    hw10              1.000                               1.008    0.540\n    hw_8    (.p9.)    0.460    0.060    7.647    0.000    0.463    0.414\n  grades =~                                                             \n    eng_12            1.000                               2.540    0.927\n    math_12 (.11.)    0.900    0.024   38.191    0.000    2.286    0.832\n    sci_12  (.12.)    0.958    0.022   44.053    0.000    2.433    0.884\n    ss_12   (.13.)    1.061    0.022   48.543    0.000    2.695    0.903\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback (.18.)    0.288    0.021   13.859    0.000    0.600    0.600\n  grades ~                                                              \n    prevach (.19.)    0.152    0.011   13.289    0.000    0.512    0.512\n    hw      (.20.)    0.548    0.122    4.485    0.000    0.217    0.217\n  hw ~                                                                  \n    prevach (.21.)    0.053    0.008    6.710    0.000    0.450    0.450\n    famback (.22.)    0.013    0.004    3.088    0.002    0.236    0.236\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.205    0.456    0.450    0.652    0.205    0.039\n .bytxmstd ~~                                                           \n   .math_12           2.437    0.618    3.942    0.000    2.437    0.302\n .bytxsstd ~~                                                           \n   .sci_12            0.358    0.552    0.648    0.517    0.358    0.049\n .bytxhstd ~~                                                           \n   .ss_12             1.061    0.570    1.862    0.063    1.061    0.146\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           43.085    1.423   30.270    0.000   43.085    1.829\n   .bypared           2.849    0.086   33.131    0.000    2.849    2.002\n   .byfaminc          8.757    0.166   52.887    0.000    8.757    3.195\n   .bytxrstd         48.537    0.601   80.719    0.000   48.537    4.876\n   .bytxmstd         49.808    0.615   81.054    0.000   49.808    4.897\n   .bytxsstd         47.959    0.612   78.401    0.000   47.959    4.736\n   .bytxhstd         48.160    0.604   79.727    0.000   48.160    4.816\n   .hw10              3.214    0.113   28.488    0.000    3.214    1.721\n   .hw_8              1.718    0.068   25.404    0.000    1.718    1.535\n   .eng_12            5.820    0.166   35.148    0.000    5.820    2.123\n   .math_12           5.385    0.166   32.448    0.000    5.385    1.960\n   .sci_12            5.590    0.166   33.635    0.000    5.590    2.032\n   .ss_12             5.895    0.180   32.692    0.000    5.895    1.975\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          236.422   27.644    8.553    0.000  236.422    0.426\n   .bypared           0.582    0.095    6.150    0.000    0.582    0.287\n   .byfaminc          4.563    0.443   10.306    0.000    4.563    0.607\n   .bytxrstd         25.638    2.949    8.694    0.000   25.638    0.259\n   .bytxmstd         28.001    3.152    8.883    0.000   28.001    0.271\n   .bytxsstd         32.304    3.418    9.451    0.000   32.304    0.315\n   .bytxhstd         31.959    3.362    9.505    0.000   31.959    0.320\n   .hw10              2.473    0.314    7.883    0.000    2.473    0.709\n   .hw_8              1.039    0.103   10.123    0.000    1.039    0.829\n   .eng_12            1.061    0.143    7.419    0.000    1.061    0.141\n   .math_12           2.322    0.228   10.161    0.000    2.322    0.308\n   .sci_12            1.648    0.179    9.184    0.000    1.648    0.218\n   .ss_12             1.648    0.192    8.601    0.000    1.648    0.185\n    famback         318.690   38.517    8.274    0.000    1.000    1.000\n   .prevach          46.956    5.362    8.757    0.000    0.639    0.639\n   .hw                0.624    0.237    2.631    0.009    0.615    0.615\n   .grades            3.608    0.379    9.517    0.000    0.559    0.559\n\n\nGroup 2 [white]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              14.660    0.736\n    bypared (.p2.)    0.067    0.003   20.053    0.000    0.987    0.819\n    byfamnc (.p3.)    0.096    0.006   17.465    0.000    1.410    0.617\n  prevach =~                                                            \n    bytxrst           1.000                               8.563    0.843\n    bytxmst (.p5.)    1.014    0.030   33.525    0.000    8.680    0.840\n    bytxsst (.p6.)    0.978    0.030   32.671    0.000    8.373    0.838\n    bytxhst (.p7.)    0.962    0.030   32.138    0.000    8.241    0.829\n  hw =~                                                                 \n    hw10              1.000                               1.174    0.613\n    hw_8    (.p9.)    0.460    0.060    7.647    0.000    0.540    0.475\n  grades =~                                                             \n    eng_12            1.000                               2.428    0.921\n    math_12 (.11.)    0.900    0.024   38.191    0.000    2.185    0.816\n    sci_12  (.12.)    0.958    0.022   44.053    0.000    2.326    0.875\n    ss_12   (.13.)    1.061    0.022   48.543    0.000    2.576    0.917\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback (.18.)    0.288    0.021   13.859    0.000    0.494    0.494\n  grades ~                                                              \n    prevach (.19.)    0.152    0.011   13.289    0.000    0.535    0.535\n    hw      (.20.)    0.548    0.122    4.485    0.000    0.265    0.265\n  hw ~                                                                  \n    prevach (.21.)    0.053    0.008    6.710    0.000    0.386    0.386\n    famback (.22.)    0.013    0.004    3.088    0.002    0.166    0.166\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12            0.848    0.289    2.933    0.003    0.848    0.151\n .bytxmstd ~~                                                           \n   .math_12           2.946    0.400    7.373    0.000    2.946    0.339\n .bytxsstd ~~                                                           \n   .sci_12            1.144    0.327    3.502    0.000    1.144    0.163\n .bytxhstd ~~                                                           \n   .ss_12             0.369    0.310    1.187    0.235    0.369    0.059\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           54.883    0.727   75.536    0.000   54.883    2.756\n   .bypared           3.335    0.044   75.852    0.000    3.335    2.768\n   .byfaminc         10.348    0.083  124.116    0.000   10.348    4.529\n   .bytxrstd         53.260    0.371  143.714    0.000   53.260    5.244\n   .bytxmstd         53.559    0.377  142.084    0.000   53.559    5.185\n   .bytxsstd         53.335    0.365  146.302    0.000   53.335    5.339\n   .bytxhstd         52.956    0.363  146.011    0.000   52.956    5.328\n   .hw10              3.443    0.070   49.266    0.000    3.443    1.798\n   .hw_8              1.733    0.041   41.802    0.000    1.733    1.525\n   .eng_12            6.409    0.096   66.645    0.000    6.409    2.432\n   .math_12           5.823    0.098   59.578    0.000    5.823    2.174\n   .sci_12            6.088    0.097   62.768    0.000    6.088    2.290\n   .ss_12             6.612    0.103   64.482    0.000    6.612    2.353\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          181.557   13.673   13.279    0.000  181.557    0.458\n   .bypared           0.478    0.050    9.557    0.000    0.478    0.329\n   .byfaminc          3.232    0.196   16.467    0.000    3.232    0.619\n   .bytxrstd         29.827    2.030   14.693    0.000   29.827    0.289\n   .bytxmstd         31.370    2.121   14.790    0.000   31.370    0.294\n   .bytxsstd         29.695    1.995   14.887    0.000   29.695    0.298\n   .bytxhstd         30.875    2.038   15.149    0.000   30.875    0.313\n   .hw10              2.290    0.229   10.003    0.000    2.290    0.624\n   .hw_8              0.999    0.066   15.027    0.000    0.999    0.774\n   .eng_12            1.052    0.084   12.502    0.000    1.052    0.151\n   .math_12           2.401    0.141   17.038    0.000    2.401    0.335\n   .sci_12            1.656    0.107   15.448    0.000    1.656    0.234\n   .ss_12             1.262    0.098   12.872    0.000    1.262    0.160\n    famback         214.915   19.108   11.247    0.000    1.000    1.000\n   .prevach          55.460    4.038   13.734    0.000    0.756    0.756\n   .hw                1.048    0.209    5.005    0.000    0.760    0.760\n   .grades            3.011    0.214   14.059    0.000    0.511    0.511\n\n\n\n\n# compare the two models\nlavTestLRT(sem_fit_mg3, sem_fit_mg4) |&gt; print()\nlavTestLRT(sem_fit_mg2, sem_fit_mg4) |&gt; print()\n\n\nChi-Squared Difference Test\n\n             Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)\nsem_fit_mg3 122 67182 67606 236.98                                       \nsem_fit_mg4 126 67178 67583 241.79     4.8107 0.019887       4     0.3073\n\nChi-Squared Difference Test\n\n             Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nsem_fit_mg2 121 67185 67614 238.78                                    \nsem_fit_mg4 126 67178 67583 241.79     3.0153     0       5     0.6976\n\n\n\n# constrain all the effects & errors\nhw_model_mg5 &lt;- \"\n  famback =~ parocc + bypared + byfaminc\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ hw10 + hw_8\n  grades =~ eng_12 + math_12 + sci_12 + ss_12\n\n  bytxrstd ~~ c(en, en)*eng_12\n  bytxmstd ~~ c(ma, ma)*math_12\n  bytxsstd ~~ c(sc, sc)*sci_12\n  bytxhstd ~~ c(ss, ss)*ss_12\n\n  prevach ~ famback\n  grades ~ prevach + hw\n  hw ~ prevach + famback\n\"\n\nsem_fit_mg5 &lt;- sem(hw_model_mg5,\n  data = hw_multigroup,\n  estimator = \"ML\",\n  group = \"group\",\n  group.equal = c(\n    \"loadings\", \"regressions\",\n    \"residuals\", \"lv.variances\"\n  )\n)\n\nsummary(sem_fit_mg5, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 184 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        96\n  Number of equality constraints                    35\n\n  Number of observations per group:                   \n    minority                                       274\n    white                                          751\n\nModel Test User Model:\n                                                      \n  Test statistic                               289.323\n  Degrees of freedom                               147\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    minority                                   141.348\n    white                                      147.975\n\nModel Test Baseline Model:\n\n  Test statistic                              8290.390\n  Degrees of freedom                               156\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.983\n  Tucker-Lewis Index (TLI)                       0.981\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33530.927\n  Loglikelihood unrestricted model (H1)     -33386.266\n                                                      \n  Akaike (AIC)                               67183.854\n  Bayesian (BIC)                             67484.733\n  Sample-size adjusted Bayesian (SABIC)      67290.991\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.043\n  90 Percent confidence interval - lower         0.036\n  90 Percent confidence interval - upper         0.051\n  P-value H_0: RMSEA &lt;= 0.050                    0.927\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.051\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [minority]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              15.679    0.747\n    bypared (.p2.)    0.067    0.003   20.248    0.000    1.043    0.824\n    byfamnc (.p3.)    0.097    0.005   17.694    0.000    1.514    0.625\n  prevach =~                                                            \n    bytxrst           1.000                               8.554    0.847\n    bytxmst (.p5.)    1.013    0.030   33.355    0.000    8.665    0.843\n    bytxsst (.p6.)    0.980    0.030   32.601    0.000    8.385    0.836\n    bytxhst (.p7.)    0.963    0.030   32.072    0.000    8.239    0.828\n  hw =~                                                                 \n    hw10              1.000                               1.130    0.594\n    hw_8    (.p9.)    0.457    0.060    7.599    0.000    0.516    0.456\n  grades =~                                                             \n    eng_12            1.000                               2.459    0.923\n    math_12 (.11.)    0.899    0.024   38.183    0.000    2.211    0.820\n    sci_12  (.12.)    0.958    0.022   44.034    0.000    2.356    0.878\n    ss_12   (.13.)    1.060    0.022   48.459    0.000    2.608    0.912\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback (.18.)    0.287    0.021   13.854    0.000    0.526    0.526\n  grades ~                                                              \n    prevach (.19.)    0.150    0.012   12.896    0.000    0.523    0.523\n    hw      (.20.)    0.570    0.127    4.495    0.000    0.262    0.262\n  hw ~                                                                  \n    prevach (.21.)    0.052    0.008    6.619    0.000    0.395    0.395\n    famback (.22.)    0.014    0.004    3.169    0.002    0.190    0.190\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12    (en)    0.692    0.245    2.825    0.005    0.692    0.126\n .bytxmstd ~~                                                           \n   .math_12   (ma)    2.813    0.337    8.356    0.000    2.813    0.330\n .bytxsstd ~~                                                           \n   .sci_12    (sc)    0.941    0.281    3.344    0.001    0.941    0.133\n .bytxhstd ~~                                                           \n   .ss_12     (ss)    0.543    0.274    1.982    0.048    0.543    0.083\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           43.085    1.267   33.997    0.000   43.085    2.054\n   .bypared           2.849    0.076   37.273    0.000    2.849    2.252\n   .byfaminc          8.757    0.146   59.809    0.000    8.757    3.613\n   .bytxrstd         48.537    0.610   79.598    0.000   48.537    4.809\n   .bytxmstd         49.808    0.621   80.194    0.000   49.808    4.845\n   .bytxsstd         47.959    0.606   79.127    0.000   47.959    4.780\n   .bytxhstd         48.160    0.601   80.108    0.000   48.160    4.840\n   .hw10              3.214    0.115   27.973    0.000    3.214    1.690\n   .hw_8              1.718    0.068   25.130    0.000    1.718    1.518\n   .eng_12            5.820    0.161   36.152    0.000    5.820    2.184\n   .math_12           5.385    0.163   33.072    0.000    5.385    1.998\n   .sci_12            5.590    0.162   34.475    0.000    5.590    2.083\n   .ss_12             5.895    0.173   34.146    0.000    5.895    2.063\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc  (.23.)  194.226   13.021   14.916    0.000  194.226    0.441\n   .bypared (.24.)    0.513    0.048   10.687    0.000    0.513    0.320\n   .byfamnc (.25.)    3.580    0.188   19.087    0.000    3.580    0.609\n   .bytxrst (.26.)   28.719    1.704   16.853    0.000   28.719    0.282\n   .bytxmst (.27.)   30.622    1.793   17.074    0.000   30.622    0.290\n   .bytxsst (.28.)   30.349    1.743   17.410    0.000   30.349    0.302\n   .bytxhst (.29.)   31.142    1.761   17.689    0.000   31.142    0.314\n   .hw10    (.30.)    2.341    0.201   11.627    0.000    2.341    0.647\n   .hw_8    (.31.)    1.015    0.058   17.575    0.000    1.015    0.792\n   .eng_12  (.32.)    1.053    0.074   14.287    0.000    1.053    0.148\n   .math_12 (.33.)    2.376    0.120   19.758    0.000    2.376    0.327\n   .sci_12  (.34.)    1.653    0.093   17.821    0.000    1.653    0.229\n   .ss_12   (.35.)    1.368    0.089   15.354    0.000    1.368    0.167\n    famback (.36.)  245.842   20.005   12.289    0.000    1.000    1.000\n   .prevach (.37.)   52.921    3.490   15.164    0.000    0.723    0.723\n   .hw      (.38.)    0.931    0.182    5.109    0.000    0.729    0.729\n   .grades  (.39.)    3.161    0.198   15.985    0.000    0.523    0.523\n\n\nGroup 2 [white]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              15.679    0.747\n    bypared (.p2.)    0.067    0.003   20.248    0.000    1.043    0.824\n    byfamnc (.p3.)    0.097    0.005   17.694    0.000    1.514    0.625\n  prevach =~                                                            \n    bytxrst           1.000                               8.554    0.847\n    bytxmst (.p5.)    1.013    0.030   33.355    0.000    8.665    0.843\n    bytxsst (.p6.)    0.980    0.030   32.601    0.000    8.385    0.836\n    bytxhst (.p7.)    0.963    0.030   32.072    0.000    8.239    0.828\n  hw =~                                                                 \n    hw10              1.000                               1.130    0.594\n    hw_8    (.p9.)    0.457    0.060    7.599    0.000    0.516    0.456\n  grades =~                                                             \n    eng_12            1.000                               2.459    0.923\n    math_12 (.11.)    0.899    0.024   38.183    0.000    2.211    0.820\n    sci_12  (.12.)    0.958    0.022   44.034    0.000    2.356    0.878\n    ss_12   (.13.)    1.060    0.022   48.459    0.000    2.608    0.912\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback (.18.)    0.287    0.021   13.854    0.000    0.526    0.526\n  grades ~                                                              \n    prevach (.19.)    0.150    0.012   12.896    0.000    0.523    0.523\n    hw      (.20.)    0.570    0.127    4.495    0.000    0.262    0.262\n  hw ~                                                                  \n    prevach (.21.)    0.052    0.008    6.619    0.000    0.395    0.395\n    famback (.22.)    0.014    0.004    3.169    0.002    0.190    0.190\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng_12    (en)    0.692    0.245    2.825    0.005    0.692    0.126\n .bytxmstd ~~                                                           \n   .math_12   (ma)    2.813    0.337    8.356    0.000    2.813    0.330\n .bytxsstd ~~                                                           \n   .sci_12    (sc)    0.941    0.281    3.344    0.001    0.941    0.133\n .bytxhstd ~~                                                           \n   .ss_12     (ss)    0.543    0.274    1.982    0.048    0.543    0.083\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           54.883    0.765   71.697    0.000   54.883    2.616\n   .bypared           3.335    0.046   72.240    0.000    3.335    2.636\n   .byfaminc         10.348    0.088  117.008    0.000   10.348    4.270\n   .bytxrstd         53.260    0.368  144.601    0.000   53.260    5.277\n   .bytxmstd         53.559    0.375  142.766    0.000   53.559    5.210\n   .bytxsstd         53.335    0.366  145.684    0.000   53.335    5.316\n   .bytxhstd         52.956    0.363  145.831    0.000   52.956    5.321\n   .hw10              3.443    0.069   49.606    0.000    3.443    1.810\n   .hw_8              1.733    0.041   41.954    0.000    1.733    1.531\n   .eng_12            6.409    0.097   65.911    0.000    6.409    2.405\n   .math_12           5.823    0.098   59.205    0.000    5.823    2.160\n   .sci_12            6.088    0.098   62.161    0.000    6.088    2.268\n   .ss_12             6.612    0.104   63.401    0.000    6.612    2.314\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc  (.23.)  194.226   13.021   14.916    0.000  194.226    0.441\n   .bypared (.24.)    0.513    0.048   10.687    0.000    0.513    0.320\n   .byfamnc (.25.)    3.580    0.188   19.087    0.000    3.580    0.609\n   .bytxrst (.26.)   28.719    1.704   16.853    0.000   28.719    0.282\n   .bytxmst (.27.)   30.622    1.793   17.074    0.000   30.622    0.290\n   .bytxsst (.28.)   30.349    1.743   17.410    0.000   30.349    0.302\n   .bytxhst (.29.)   31.142    1.761   17.689    0.000   31.142    0.314\n   .hw10    (.30.)    2.341    0.201   11.627    0.000    2.341    0.647\n   .hw_8    (.31.)    1.015    0.058   17.575    0.000    1.015    0.792\n   .eng_12  (.32.)    1.053    0.074   14.287    0.000    1.053    0.148\n   .math_12 (.33.)    2.376    0.120   19.758    0.000    2.376    0.327\n   .sci_12  (.34.)    1.653    0.093   17.821    0.000    1.653    0.229\n   .ss_12   (.35.)    1.368    0.089   15.354    0.000    1.368    0.167\n    famback (.36.)  245.842   20.005   12.289    0.000    1.000    1.000\n   .prevach (.37.)   52.921    3.490   15.164    0.000    0.723    0.723\n   .hw      (.38.)    0.931    0.182    5.109    0.000    0.729    0.729\n   .grades  (.39.)    3.161    0.198   15.985    0.000    0.523    0.523\n\n\n\n\n# compare the two models\nlavTestLRT(sem_fit_mg4, sem_fit_mg5) |&gt; print()\n\n\nChi-Squared Difference Test\n\n             Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)    \nsem_fit_mg4 126 67178 67583 241.79                                           \nsem_fit_mg5 147 67184 67485 289.32     47.528 0.049647      21  0.0007971 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Keith's",
      "Multigroup Models"
    ]
  },
  {
    "objectID": "contents/chap13.html",
    "href": "contents/chap13.html",
    "title": "Chapter 13-14. Path Analysis",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)",
    "crumbs": [
      "Keith's",
      "Path Analysis"
    ]
  },
  {
    "objectID": "contents/chap13.html#데이터-준비",
    "href": "contents/chap13.html#데이터-준비",
    "title": "Chapter 13-14. Path Analysis",
    "section": "데이터 준비",
    "text": "데이터 준비\n\n# Load the data\nnels &lt;- read_csv(\"data/n=1000,stud & par shorter all miss blank.csv\")\nnels |&gt; print()\n\n# A tibble: 1,000 x 93\n  stu_id sch_id sstratid   sex  race ethnic bys42a bys42b bys44a bys44b bys44c\n   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 124966   1249        1     2     4      1      3      4      2      4      4\n2 124972   1249        1     1     4      1      4      5      1      3      3\n3 175551   1755        1     2     3      0     NA      3      2      3      3\n4 180660   1806        1     1     4      1      2     NA      1      4      4\n5 180672   1806        1     2     4      1      2      3      1      4      3\n6 298885   2988        2     1     3      0      5      4      2      3      3\n# i 994 more rows\n# i 82 more variables: bys44d &lt;dbl&gt;, bys44e &lt;dbl&gt;, bys44f &lt;dbl&gt;, bys44g &lt;dbl&gt;,\n#   bys44h &lt;dbl&gt;, bys44i &lt;dbl&gt;, bys44j &lt;dbl&gt;, bys44k &lt;dbl&gt;, bys44l &lt;dbl&gt;,\n#   bys44m &lt;dbl&gt;, bys48a &lt;dbl&gt;, bys48b &lt;dbl&gt;, bys79a &lt;dbl&gt;, byfamsiz &lt;dbl&gt;,\n#   famcomp &lt;dbl&gt;, bygrads &lt;dbl&gt;, byses &lt;dbl&gt;, byfaminc &lt;dbl&gt;, parocc &lt;dbl&gt;,\n#   bytxrstd &lt;dbl&gt;, bytxmstd &lt;dbl&gt;, bytxsstd &lt;dbl&gt;, bytxhstd &lt;dbl&gt;,\n#   bypared &lt;dbl&gt;, bytests &lt;dbl&gt;, par_inv &lt;dbl&gt;, f1s36a1 &lt;dbl&gt;, ...\n\n\n\n# SPSS data: labelled data\nlibrary(haven) # install.packages(\"haven\")\nnels_sav &lt;- read_sav(\"data/n=1000,stud & par shorter.sav\")\nnels_sav |&gt; print()\n\n# A tibble: 1,000 x 93\n  stu_id    sch_id    sstratid sex     race    ethnic  bys42a   bys42b   bys44a \n  &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lb&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt; &lt;dbl+lb&gt; &lt;dbl+l&gt;\n1 124966    1249      1        2 [Fem~ 4 [Whi~ 1 [whi~  3 [2-3~  4 [3-4~ 2 [Agr~\n2 124972    1249      1        1 [Mal~ 4 [Whi~ 1 [whi~  4 [3-4~  5 [4-5~ 1 [Str~\n3 175551    1755      1        2 [Fem~ 3 [Bla~ 0 [blk~ NA        3 [2-3~ 2 [Agr~\n4 180660    1806      1        1 [Mal~ 4 [Whi~ 1 [whi~  2 [1-2~ NA       1 [Str~\n5 180672    1806      1        2 [Fem~ 4 [Whi~ 1 [whi~  2 [1-2~  3 [2-3~ 1 [Str~\n6 298885    2988      2        1 [Mal~ 3 [Bla~ 0 [blk~  5 [4-5~  4 [3-4~ 2 [Agr~\n# i 994 more rows\n# i 84 more variables: bys44b &lt;dbl+lbl&gt;, bys44c &lt;dbl+lbl&gt;, bys44d &lt;dbl+lbl&gt;,\n#   bys44e &lt;dbl+lbl&gt;, bys44f &lt;dbl+lbl&gt;, bys44g &lt;dbl+lbl&gt;, bys44h &lt;dbl+lbl&gt;,\n#   bys44i &lt;dbl+lbl&gt;, bys44j &lt;dbl+lbl&gt;, bys44k &lt;dbl+lbl&gt;, bys44l &lt;dbl+lbl&gt;,\n#   bys44m &lt;dbl+lbl&gt;, bys48a &lt;dbl+lbl&gt;, bys48b &lt;dbl+lbl&gt;, bys79a &lt;dbl+lbl&gt;,\n#   byfamsiz &lt;dbl+lbl&gt;, famcomp &lt;dbl+lbl&gt;, bygrads &lt;dbl+lbl&gt;, byses &lt;dbl+lbl&gt;,\n#   byfaminc &lt;dbl+lbl&gt;, parocc &lt;dbl&gt;, bytxrstd &lt;dbl+lbl&gt;, ...\n\n\n\nnels_sav$ethnic |&gt; labelled::val_labels() |&gt; print()\n\nblk,namer,hisp    white-asian        missing \n             0              1              8 \n\n\nvariables: byses, bytests, par_inv, ffugrad, ethnic\nUnderrepresented ethnic minority, or URM, is coded so that students from African American, Hispanic, and Native backgrounds are coded 1 and students of Asian and Caucasian descent are coded 0.\n\nnels_gpa &lt;-\n  nels |&gt;\n  select(ethnic, ses = byses, prev = bytests, par = par_inv, gpa = ffugrad) |&gt;\n  na.omit()\n\nnels_gpa |&gt; print()\n\n# A tibble: 811 x 5\n  ethnic    ses  prev     par   gpa\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1      1 -0.563  64.4  1.04    5.25\n2      1  0.123  48.6 -0.0881  3   \n3      0  0.229  49.7 -0.390   2.5 \n4      1  0.687  46.6  0.199   6.5 \n5      1  0.633  54.9  0.975   4.25\n6      0  0.992  38.5 -0.157   6   \n# i 805 more rows\n\n\n\n상관계수와 평균, 표준편차\n\nlibrary(psych)\nnels_gpa |&gt; lowerCor(digits = 3)\n\n       ethnc ses   prev  par   gpa  \nethnic 1.000                        \nses    0.333 1.000                  \nprev   0.330 0.461 1.000            \npar    0.075 0.432 0.445 1.000      \ngpa    0.131 0.299 0.499 0.364 1.000\n\n\n\nnels_gpa |&gt; describe(skew = F) |&gt; print(digits = 3)\n\n       vars   n   mean    sd median    min    max  range    se\nethnic    1 811  0.793 0.406  1.000  0.000  1.000  1.000 0.014\nses       2 811  0.047 0.766  0.011 -2.414  1.874  4.288 0.027\nprev      3 811 52.323 8.584 52.649 30.397 70.240 39.844 0.301\npar       4 811  0.059 0.794  0.191 -3.148  1.493  4.642 0.028\ngpa       5 811  5.760 1.450  6.000  1.000  8.000  7.000 0.051",
    "crumbs": [
      "Keith's",
      "Path Analysis"
    ]
  },
  {
    "objectID": "contents/chap13.html#포화모형",
    "href": "contents/chap13.html#포화모형",
    "title": "Chapter 13-14. Path Analysis",
    "section": "포화모형",
    "text": "포화모형\n모형의 적합도에 대한 신뢰할 만한 정보를 얻기 위해서는 자유도(degree of freedom)가 높도록 모형을 만들어야 함(specification).\n여기서는 자유도가 0인 포화모형을 우선 고려한 후, 이후 더 단순한 모형을 고려할 것임. 자유도가 0이면 모형의 적합도는 의미가 없으며, 이 경우를 포화모형(saturated model)이라고 함.\n\n구조모형\n\n\nlibrary(lavaan)\nlibrary(semTools)\n\nmod &lt;- \"\n  gpa ~ ethnic + ses + prev + par\n  par ~ prev + ses + ethnic\n  prev ~ ses + ethnic\n\"\n\nsem_fit &lt;- sem(model = mod, data = nels_gpa, fixed.x = FALSE)\nsummary(sem_fit, standardized = TRUE, rsquare = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n\n  Number of observations                           811\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  gpa ~                                                                 \n    ethnic           -0.124    0.117   -1.058    0.290   -0.124   -0.035\n    ses               0.093    0.069    1.355    0.175    0.093    0.049\n    prev              0.070    0.006   11.414    0.000    0.070    0.417\n    par               0.292    0.064    4.531    0.000    0.292    0.160\n  par ~                                                                 \n    prev              0.032    0.003   10.040    0.000    0.032    0.345\n    ses               0.333    0.036    9.351    0.000    0.333    0.321\n    ethnic           -0.286    0.063   -4.528    0.000   -0.286   -0.146\n  prev ~                                                                \n    ses               4.431    0.362   12.236    0.000    4.431    0.395\n    ethnic            4.195    0.684    6.135    0.000    4.195    0.198\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  ethnic ~~                                                             \n    ses               0.103    0.011    9.002    0.000    0.103    0.333\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .gpa               1.521    0.076   20.137    0.000    1.521    0.724\n   .par               0.452    0.022   20.137    0.000    0.452    0.719\n   .prev             55.370    2.750   20.137    0.000   55.370    0.752\n    ethnic            0.164    0.008   20.137    0.000    0.164    1.000\n    ses               0.586    0.029   20.137    0.000    0.586    1.000\n\nR-Square:\n                   Estimate\n    gpa               0.276\n    par               0.281\n    prev              0.248\n\n\n\n옵션들\n\nci: confidence interval\nheader: 헤더 표시 여부\nnd: the number of digits\n\n\nsummary(sem_fit, standardized = TRUE, ci = TRUE, header = FALSE, nd = 2) |&gt; print()\n\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n  gpa ~                                                                 \n    ethnic            -0.12     0.12    -1.06     0.29    -0.35     0.11\n    ses                0.09     0.07     1.36     0.18    -0.04     0.23\n    prev               0.07     0.01    11.41     0.00     0.06     0.08\n    par                0.29     0.06     4.53     0.00     0.17     0.42\n  par ~                                                                 \n    prev               0.03     0.00    10.04     0.00     0.03     0.04\n    ses                0.33     0.04     9.35     0.00     0.26     0.40\n    ethnic            -0.29     0.06    -4.53     0.00    -0.41    -0.16\n  prev ~                                                                \n    ses                4.43     0.36    12.24     0.00     3.72     5.14\n    ethnic             4.20     0.68     6.13     0.00     2.85     5.54\n   Std.lv  Std.all\n                  \n    -0.12    -0.03\n     0.09     0.05\n     0.07     0.42\n     0.29     0.16\n                  \n     0.03     0.34\n     0.33     0.32\n    -0.29    -0.15\n                  \n     4.43     0.40\n     4.20     0.20\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n  ethnic ~~                                                             \n    ses                0.10     0.01     9.00     0.00     0.08     0.13\n   Std.lv  Std.all\n                  \n     0.10     0.33\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n   .gpa                1.52     0.08    20.14     0.00     1.37     1.67\n   .par                0.45     0.02    20.14     0.00     0.41     0.50\n   .prev              55.37     2.75    20.14     0.00    49.98    60.76\n    ethnic             0.16     0.01    20.14     0.00     0.15     0.18\n    ses                0.59     0.03    20.14     0.00     0.53     0.64\n   Std.lv  Std.all\n     1.52     0.72\n     0.45     0.72\n    55.37     0.75\n     0.16     1.00\n     0.59     1.00\n\n\n\n표준화된 파라미터 추정치만 표시\n\nstandardizedSolution(sem_fit, type = \"std.all\") |&gt; print()\n\n      lhs op    rhs est.std    se      z pvalue ci.lower ci.upper\n1     gpa  ~ ethnic  -0.035 0.033 -1.058  0.290   -0.099    0.030\n2     gpa  ~    ses   0.049 0.036  1.356  0.175   -0.022    0.120\n3     gpa  ~   prev   0.417 0.034 12.111  0.000    0.349    0.484\n4     gpa  ~    par   0.160 0.035  4.564  0.000    0.091    0.228\n5     par  ~   prev   0.345 0.033 10.462  0.000    0.280    0.409\n6     par  ~    ses   0.321 0.033  9.682  0.000    0.256    0.386\n7     par  ~ ethnic  -0.146 0.032 -4.553  0.000   -0.209   -0.083\n8    prev  ~    ses   0.395 0.030 13.137  0.000    0.336    0.454\n9    prev  ~ ethnic   0.198 0.032  6.224  0.000    0.136    0.261\n10    gpa ~~    gpa   0.724 0.027 27.093  0.000    0.671    0.776\n11    par ~~    par   0.719 0.027 26.858  0.000    0.666    0.771\n12   prev ~~   prev   0.752 0.026 28.610  0.000    0.701    0.804\n13 ethnic ~~ ethnic   1.000 0.000     NA     NA    1.000    1.000\n14 ethnic ~~    ses   0.333 0.031 10.673  0.000    0.272    0.394\n15    ses ~~    ses   1.000 0.000     NA     NA    1.000    1.000\n\n\n파라미터 추정 방식: lavaan website\n기본적으로 ML (Maximum Likelihood) 방법을 사용\n변경하려면, estimator 옵션을 사용\n\nULS: Unweighted Least Squares\nGLS: Generalized Least Squares (weight: \\(S^{-1}\\))\nWLS: Weighted Least Squares (also called ADF: Asymptotically Distribution-Free)\n여러 robust estimators\n\n정규분포 가정에 어긋나는 경우 대안들\n\n정규성을 가정하지 않는 estimator을 선택: WLS(Weighted Least Squares)\nML을 이용하지만, 표준오차만 수정하는 방법:\n\nMLM(ML with robust standard errors and a Satorra-Bentler scaled)\nMLR(ML with robust standard errors and a Yuan-Bentler scaled): missing 수용\n\nBootstrap; the Bollen-Stine bootstrap\n\n예를 들어, MLM을 estimator로 사용하려면,\n\n# MLM estimator\nsem_fit &lt;- sem(\n    model = mod,\n    data = nels_gpa,\n    fixed.x = FALSE,\n    estimator = \"MLM\"\n)\nsummary(sem_fit, standardized = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n\n  Number of observations                           811\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                 0.000       0.000\n  Degrees of freedom                                 0           0\n\nParameter Estimates:\n\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  gpa ~                                                                 \n    ethnic           -0.124    0.119   -1.044    0.297   -0.124   -0.035\n    ses               0.093    0.068    1.367    0.172    0.093    0.049\n    prev              0.070    0.006   11.554    0.000    0.070    0.417\n    par               0.292    0.068    4.282    0.000    0.292    0.160\n  par ~                                                                 \n    prev              0.032    0.003   10.240    0.000    0.032    0.345\n    ses               0.333    0.035    9.491    0.000    0.333    0.321\n    ethnic           -0.286    0.066   -4.333    0.000   -0.286   -0.146\n  prev ~                                                                \n    ses               4.431    0.363   12.211    0.000    4.431    0.395\n    ethnic            4.195    0.692    6.062    0.000    4.195    0.198\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  ethnic ~~                                                             \n    ses               0.103    0.012    8.819    0.000    0.103    0.333\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .gpa               1.521    0.073   20.851    0.000    1.521    0.724\n   .par               0.452    0.025   18.293    0.000    0.452    0.719\n   .prev             55.370    2.475   22.368    0.000   55.370    0.752\n    ethnic            0.164    0.008   19.705    0.000    0.164    1.000\n    ses               0.586    0.026   22.311    0.000    0.586    1.000\n\n\n\n부트스트랩(bootstrap) 방법을 사용한 표준오차 추정치\n\n# Bootstrap\nsem_fit &lt;- sem(\n    model = mod, \n    data = nels_gpa, \n    fixed.x = FALSE,\n    estimator = \"ML\",  # default\n    se = \"bootstrap\",  # standard errors\n    bootstrap = 1000,  # default; the number of bootstrap samples\n)\n\n\nparameterEstimates(sem_fit, standardized = \"std.all\", boot.ci.type = \"bca.simple\") |&gt; print()\n\n      lhs op    rhs    est    se      z pvalue ci.lower ci.upper std.all\n1     gpa  ~ ethnic -0.124 0.117 -1.058  0.290   -0.382    0.077  -0.035\n2     gpa  ~    ses  0.093 0.067  1.388  0.165   -0.042    0.220   0.049\n3     gpa  ~   prev  0.070 0.006 11.462  0.000    0.059    0.083   0.417\n4     gpa  ~    par  0.292 0.068  4.274  0.000    0.161    0.420   0.160\n5     par  ~   prev  0.032 0.003 10.474  0.000    0.026    0.038   0.345\n6     par  ~    ses  0.333 0.034  9.703  0.000    0.265    0.401   0.321\n7     par  ~ ethnic -0.286 0.066 -4.348  0.000   -0.412   -0.151  -0.146\n8    prev  ~    ses  4.431 0.355 12.467  0.000    3.713    5.119   0.395\n9    prev  ~ ethnic  4.195 0.686  6.115  0.000    2.834    5.568   0.198\n10    gpa ~~    gpa  1.521 0.071 21.412  0.000    1.406    1.692   0.724\n11    par ~~    par  0.452 0.025 18.174  0.000    0.407    0.508   0.719\n12   prev ~~   prev 55.370 2.543 21.776  0.000   50.803   60.889   0.752\n13 ethnic ~~ ethnic  0.164 0.008 19.666  0.000    0.149    0.183   1.000\n14 ethnic ~~    ses  0.103 0.012  8.845  0.000    0.079    0.128   0.333\n15    ses ~~    ses  0.586 0.026 22.645  0.000    0.539    0.643   1.000\n\n\n\n\n플롯 그리기\n\ntidySEM 참조\n\nlavaanExtra 참조\n\nsemPlot 참조\n\n\ntidySEM::graph_sem(sem_fit)\n\n\n\n\n\n\n\n\n\nlavaanExtra::nice_tidySEM(sem_fit)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nDefine a customized plot function using semPlot::semPaths()\nsemPaths2 &lt;- function(model, what = 'est', layout = \"tree\", rotation = 1) {\n  semPlot::semPaths(model, what = what, edge.label.cex = 1, edge.color = \"black\", layout = layout, rotation = rotation, weighted = FALSE, asize = 2, label.cex = 1, node.width = 1.2)\n}\n\n\n\n# semPaths2: a customized plot function using semPlot::semPaths()\nsemPaths2(sem_fit, layout = \"spring\", rotation = 1)\n\n\n\n\n\n\n\n\n\n\n간접효과 ses -&gt; par -&gt; gpa\n\nmod2 &lt;- \"\n  \n  gpa ~ b1*ethnic + b2*ses + b3*prev + b4*par\n  par ~ b5*prev + b6*ses + b7*ethnic\n  prev ~ b8*ses + b9*ethnic\n  \n  ses_par_gpa := b6*b4\n\"\n\nsem_fit2 &lt;- sem(model = mod2, data = nels_gpa, fixed.x = FALSE)\nsummary(sem_fit2, standardized = TRUE,  rsquare = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n\n  Number of observations                           811\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  gpa ~                                                                 \n    ethnic    (b1)   -0.124    0.117   -1.058    0.290   -0.124   -0.035\n    ses       (b2)    0.093    0.069    1.355    0.175    0.093    0.049\n    prev      (b3)    0.070    0.006   11.414    0.000    0.070    0.417\n    par       (b4)    0.292    0.064    4.531    0.000    0.292    0.160\n  par ~                                                                 \n    prev      (b5)    0.032    0.003   10.040    0.000    0.032    0.345\n    ses       (b6)    0.333    0.036    9.351    0.000    0.333    0.321\n    ethnic    (b7)   -0.286    0.063   -4.528    0.000   -0.286   -0.146\n  prev ~                                                                \n    ses       (b8)    4.431    0.362   12.236    0.000    4.431    0.395\n    ethnic    (b9)    4.195    0.684    6.135    0.000    4.195    0.198\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  ethnic ~~                                                             \n    ses               0.103    0.011    9.002    0.000    0.103    0.333\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .gpa               1.521    0.076   20.137    0.000    1.521    0.724\n   .par               0.452    0.022   20.137    0.000    0.452    0.719\n   .prev             55.370    2.750   20.137    0.000   55.370    0.752\n    ethnic            0.164    0.008   20.137    0.000    0.164    1.000\n    ses               0.586    0.029   20.137    0.000    0.586    1.000\n\nR-Square:\n                   Estimate\n    gpa               0.276\n    par               0.281\n    prev              0.248\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    ses_par_gpa       0.097    0.024    4.077    0.000    0.097    0.051\n\n\n\n\n\n모든 간접효과: ses -&gt; gpa\nmanymome 참조\n\nlibrary(manymome)\n\n# All indirect paths from x to y\npaths &lt;- all_indirect_paths(sem_fit,\n  x = \"ses\",\n  y = \"gpa\"\n)\npaths |&gt; print()\n\nCall: \nall_indirect_paths(fit = sem_fit, x = \"ses\", y = \"gpa\")\nPath(s): \n  path                     \n1 ses -&gt; par -&gt; gpa        \n2 ses -&gt; prev -&gt; gpa       \n3 ses -&gt; prev -&gt; par -&gt; gpa\n\n\n\n# Indirect effect estimates\nind_est &lt;- many_indirect_effects(paths,\n  fit = sem_fit, R = 1000,\n  boot_ci = TRUE, boot_type = \"bc\"\n)\n\nind_est |&gt; print()\n\n\n==  Indirect Effect(s)   ==\n                            ind CI.lo CI.hi Sig\nses -&gt; par -&gt; gpa         0.097 0.052 0.154 Sig\nses -&gt; prev -&gt; gpa        0.312 0.247 0.393 Sig\nses -&gt; prev -&gt; par -&gt; gpa 0.041 0.022 0.064 Sig\n\n - [CI.lo to CI.hi] are 95.0% bias-corrected confidence intervals by\n   nonparametric bootstrapping with 1000 samples.\n - The 'ind' column shows the indirect effects.\n \n\n\n\n# Standarized estimates\nind_est_std &lt;- many_indirect_effects(paths,\n  fit = sem_fit, R = 1000,\n  boot_ci = TRUE, boot_type = \"bc\",\n  standardized_x = TRUE,\n  standardized_y = TRUE\n)\n\nind_est_std |&gt; print()\n\n\n==  Indirect Effect(s) (Both x-variable(s) and y-variable(s) Standardized)  ==\n                            std CI.lo CI.hi Sig\nses -&gt; par -&gt; gpa         0.051 0.028 0.081 Sig\nses -&gt; prev -&gt; gpa        0.165 0.131 0.203 Sig\nses -&gt; prev -&gt; par -&gt; gpa 0.022 0.012 0.034 Sig\n\n - [CI.lo to CI.hi] are 95.0% bias-corrected confidence intervals by\n   nonparametric bootstrapping with 1000 samples.\n - std: The standardized indirect effects.",
    "crumbs": [
      "Keith's",
      "Path Analysis"
    ]
  },
  {
    "objectID": "contents/chap13.html#단순한-모형",
    "href": "contents/chap13.html#단순한-모형",
    "title": "Chapter 13-14. Path Analysis",
    "section": "단순한 모형",
    "text": "단순한 모형\n포화모형보다 단순한 모형인 경우 자유도 &gt; 0 이며, 이 경우 over-identified(과대 식별) 되었다고 말함.\n자유도가 높을수록 모형이 데이터와 잘 맞지 않은지에 대한 판별을 더 신뢰할 수 있음.\n\n공분산 기반 모형\n\nnels_cov &lt;- read_sav(\"data/chap 14 path via SEM/homework overid 2018.sav\")\nnels_cov |&gt; print()\n\n# A tibble: 8 x 7\n  rowtype_ varname_    Minority   FamBack   PreAch  Homework   Grades\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 n        \"\"         1000      1000      1000     1000      1000    \n2 corr     \"Minority\"    1        -0.304    -0.323   -0.0832   -0.132\n3 corr     \"FamBack\"    -0.304     1         0.479    0.263     0.275\n4 corr     \"PreAch\"     -0.323     0.479     1        0.288     0.489\n5 corr     \"Homework\"   -0.0832    0.263     0.288    1         0.281\n6 corr     \"Grades\"     -0.132     0.275     0.489    0.281     1    \n7 stddev   \"\"            0.419     0.831     8.90     0.806     1.48 \n8 mean     \"\"            0.272     0.0025   52.0      2.56      5.75 \n\n\n\nnels_cov2 &lt;- nels_cov[c(2:6), c(3:7)] |&gt; \n  as.matrix() |&gt; \n  lav_matrix_vechr(diagonal = TRUE) |&gt; \n  getCov(names = nels_cov$varname_[2:6], sds = nels_cov[7, 3:7] |&gt; as.double())\n\nnels_cov2 |&gt; print()\n\n            Minority    FamBack    PreAch    Homework      Grades\nMinority  0.17522596 -0.1057959 -1.202307 -0.02808143 -0.08141289\nFamBack  -0.10579592  0.6907272  3.544405  0.17637451  0.33815207\nPreAch   -1.20230704  3.5444051 79.170845  2.06906701  6.43516479\nHomework -0.02808143  0.1763745  2.069067  0.65011969  0.33545523\nGrades   -0.08141289  0.3381521  6.435165  0.33545523  2.18744100\n\n\n\nnels_cov2 |&gt; cov2cor() |&gt; print()\n\n         Minority FamBack  PreAch Homework  Grades\nMinority   1.0000 -0.3041 -0.3228  -0.0832 -0.1315\nFamBack   -0.3041  1.0000  0.4793   0.2632  0.2751\nPreAch    -0.3228  0.4793  1.0000   0.2884  0.4890\nHomework  -0.0832  0.2632  0.2884   1.0000  0.2813\nGrades    -0.1315  0.2751  0.4890   0.2813  1.0000\n\n\n이제 모형 적합도(model fit)에 대한 정보를 얻을 수 있음!\n\nmod_hw &lt;- \"\n  Grades ~ PreAch + Homework\n  Homework ~ PreAch + FamBack + Minority\n  PreAch ~ FamBack + Minority\n\"\n\nhw_fit &lt;- sem(\n  model = mod_hw, \n  sample.cov = nels_cov2, \n  sample.nobs = 1000, \n  fixed.x = FALSE\n)\n\nsummary(hw_fit, fit.measures = TRUE, estimates = FALSE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 2.169\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.338\n\nModel Test Baseline Model:\n\n  Test statistic                               721.651\n  Degrees of freedom                                 9\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       0.999\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7989.958\n  Loglikelihood unrestricted model (H1)      -7988.874\n                                                      \n  Akaike (AIC)                               16005.917\n  Bayesian (BIC)                             16069.718\n  Sample-size adjusted Bayesian (SABIC)      16028.429\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.009\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.064\n  P-value H_0: RMSEA &lt;= 0.050                    0.854\n  P-value H_0: RMSEA &gt;= 0.080                    0.010\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.008\n\n\nimplied/predicted correlation matrix\n\ninspect(hw_fit, \"cor.all\")[5:1, 5:1] |&gt; print(digits = 3)\n\n         Minority FamBack PreAch Homework Grades\nMinority   1.0000  -0.304 -0.323  -0.0832 -0.156\nFamBack   -0.3041   1.000  0.479   0.2632  0.253\nPreAch    -0.3228   0.479  1.000   0.2884  0.489\nHomework  -0.0832   0.263  0.288   1.0000  0.281\nGrades    -0.1563   0.253  0.489   0.2813  1.000\n\n\n\n공분산 분석\n\n# sample covariance matrix\nnels_cov2 |&gt; print(digits = 2)\n\n         Minority FamBack PreAch Homework Grades\nMinority    0.175   -0.11   -1.2   -0.028 -0.081\nFamBack    -0.106    0.69    3.5    0.176  0.338\nPreAch     -1.202    3.54   79.2    2.069  6.435\nHomework   -0.028    0.18    2.1    0.650  0.335\nGrades     -0.081    0.34    6.4    0.335  2.187\n\n\n\n# implied/predicted covariance matrix\nfitted(hw_fit)$cov[5:1, 5:1] |&gt; print(digits = 2)\n# 또는 inspect(hw_fit, \"cov.all\")\n\n         Minority FamBack PreAch Homework Grades\nMinority    0.175   -0.11   -1.2   -0.028 -0.097\nFamBack    -0.106    0.69    3.5    0.176  0.311\nPreAch     -1.201    3.54   79.1    2.067  6.429\nHomework   -0.028    0.18    2.1    0.649  0.335\nGrades     -0.097    0.31    6.4    0.335  2.185\n\n\nResiduals\n\n# Raw\nresiduals(hw_fit, type = \"raw\")$cov[5:1, 5:1] |&gt; print(digits = 2)\n\n         Minority FamBack PreAch Homework  Grades\nMinority    0.000   0.000      0  0.0e+00 1.5e-02\nFamBack     0.000   0.000      0  0.0e+00 2.7e-02\nPreAch      0.000   0.000      0  0.0e+00 0.0e+00\nHomework    0.000   0.000      0  0.0e+00 5.6e-17\nGrades      0.015   0.027      0  5.6e-17 0.0e+00\n\n\n\n# Standardized\nresiduals(hw_fit, type = \"standardized\")$cov[5:1, 5:1] |&gt; print(digits = 2)\n\n         Minority FamBack PreAch Homework  Grades\nMinority     0.00    0.00      0  0.0e+00 9.6e-01\nFamBack      0.00    0.00      0  0.0e+00 9.1e-01\nPreAch       0.00    0.00      0  0.0e+00 0.0e+00\nHomework     0.00    0.00      0  0.0e+00 5.6e-17\nGrades       0.96    0.91      0  5.6e-17 0.0e+00\n\n\n\n# Standardized like Mplus\nresiduals(hw_fit, type = \"standardized.mplus\")$cov[5:1, 5:1] |&gt; print(digits = 2)\n\n         Minority FamBack PreAch Homework  Grades\nMinority     0.00    0.00      0  0.0e+00 9.7e-01\nFamBack      0.00    0.00      0  0.0e+00 9.1e-01\nPreAch       0.00    0.00      0  0.0e+00 0.0e+00\nHomework     0.00    0.00      0  0.0e+00 5.6e-17\nGrades       0.97    0.91      0  5.6e-17 0.0e+00\n\n\n\n\n상관행렬 분석\n\n# sample correlation\ncov2cor(nels_cov2) |&gt; print(digits = 3)\n\n         Minority FamBack PreAch Homework Grades\nMinority   1.0000  -0.304 -0.323  -0.0832 -0.132\nFamBack   -0.3041   1.000  0.479   0.2632  0.275\nPreAch    -0.3228   0.479  1.000   0.2884  0.489\nHomework  -0.0832   0.263  0.288   1.0000  0.281\nGrades    -0.1315   0.275  0.489   0.2813  1.000\n\n\n\n# implied/predicted correlation\ninspect(hw_fit, \"cor.all\")[5:1, 5:1] |&gt; print(digits = 3)\n\n         Minority FamBack PreAch Homework Grades\nMinority   1.0000  -0.304 -0.323  -0.0832 -0.156\nFamBack   -0.3041   1.000  0.479   0.2632  0.253\nPreAch    -0.3228   0.479  1.000   0.2884  0.489\nHomework  -0.0832   0.263  0.288   1.0000  0.281\nGrades    -0.1563   0.253  0.489   0.2813  1.000\n\n\nResiduals\n\nresiduals(hw_fit, type = \"cor.bollen\")$cov[5:1, 5:1] |&gt; print(digits = 2)\n\n         Minority FamBack PreAch Homework  Grades\nMinority    0.000   0.000      0  0.0e+00 2.5e-02\nFamBack     0.000   0.000      0  0.0e+00 2.2e-02\nPreAch      0.000   0.000      0  0.0e+00 0.0e+00\nHomework    0.000   0.000      0  0.0e+00 5.6e-17\nGrades      0.025   0.022      0  5.6e-17 0.0e+00\n\n\n\n\n모형 적합도 지표들\n\n# select fit statistics\nfit_stats &lt;- c(\"rmr\", \"gfi\", \"nfi\", \"pnfi\", \"fmin\", \"rmsea\", \"aic\", \"ecvi\")\n\nfitMeasures(sem_fit, fit_stats) |&gt; print(nd = 3)\n\n      rmr       gfi       nfi      pnfi      fmin     rmsea       aic      ecvi \n    0.000     1.000     1.000     0.000     0.000     0.000 12495.216     0.037 \n\n\n\n# all fit statistics\nfitMeasures(sem_fit) |&gt; print(nd = 3)\n\n                 npar                  fmin                 chisq \n               15.000                 0.000                 0.000 \n                   df                pvalue        baseline.chisq \n                0.000                    NA               760.622 \n          baseline.df       baseline.pvalue                   cfi \n                9.000                 0.000                 1.000 \n                  tli                  nnfi                   rfi \n                1.000                 1.000                 1.000 \n                  nfi                  pnfi                   ifi \n                1.000                 0.000                 1.000 \n                  rni                  logl     unrestricted.logl \n                1.000             -6232.608             -6232.608 \n                  aic                   bic                ntotal \n            12495.216             12565.690               811.000 \n                 bic2                 rmsea        rmsea.ci.lower \n            12518.057                 0.000                 0.000 \n       rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue \n                0.000                 0.900                    NA \n       rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 \n                0.050                    NA                 0.080 \n                  rmr            rmr_nomean                  srmr \n                0.000                 0.000                 0.000 \n         srmr_bentler   srmr_bentler_nomean                  crmr \n                0.000                 0.000                 0.000 \n          crmr_nomean            srmr_mplus     srmr_mplus_nomean \n                0.000                 0.000                 0.000 \n                cn_05                 cn_01                   gfi \n                1.000                 1.000                 1.000 \n                 agfi                  pgfi                   mfi \n                1.000                 0.000                 1.000 \n                 ecvi \n                0.037 \n\n\n\n\nComparing Competing Models\n\n\n\n\nmod_hw_reduced &lt;- \"\n  Grades ~ PreAch + 0*Homework\n  Homework ~ 0*PreAch + FamBack + Minority\n  PreAch ~ FamBack + Minority\n\"\n\nhw_fit_reduced &lt;- sem(\n  model = mod_hw_reduced, \n  sample.cov = nels_cov2, \n  sample.nobs = 1000, \n  fixed.x = FALSE\n)\n\nsummary(hw_fit_reduced, fit.measures = TRUE, standardized = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 11 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                69.679\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               721.651\n  Degrees of freedom                                 9\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.908\n  Tucker-Lewis Index (TLI)                       0.793\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -8023.714\n  Loglikelihood unrestricted model (H1)      -7988.874\n                                                      \n  Akaike (AIC)                               16069.427\n  Bayesian (BIC)                             16123.412\n  Sample-size adjusted Bayesian (SABIC)      16088.476\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.128\n  90 Percent confidence interval - lower         0.103\n  90 Percent confidence interval - upper         0.155\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.999\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.071\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Grades ~                                                              \n    PreAch            0.081    0.005   17.728    0.000    0.081    0.489\n    Homework          0.000                               0.000    0.000\n  Homework ~                                                            \n    PreAch            0.000                               0.000    0.000\n    FamBack           0.254    0.031    8.186    0.000    0.254    0.262\n    Minority         -0.007    0.062   -0.109    0.913   -0.007   -0.003\n  PreAch ~                                                              \n    FamBack           4.496    0.305   14.750    0.000    4.496    0.420\n    Minority         -4.147    0.605   -6.852    0.000   -4.147   -0.195\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  FamBack ~~                                                            \n    Minority         -0.106    0.011   -9.200    0.000   -0.106   -0.304\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Grades            1.663    0.074   22.361    0.000    1.663    0.761\n   .Homework          0.604    0.027   22.361    0.000    0.604    0.931\n   .PreAch           58.190    2.602   22.361    0.000   58.190    0.736\n    FamBack           0.690    0.031   22.361    0.000    0.690    1.000\n    Minority          0.175    0.008   22.361    0.000    0.175    1.000\n\n\n\n모형 비교 1: initial vs. reduced model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlavTestLRT(hw_fit, hw_fit_reduced) |&gt; print()\n# anova(hw_fit, hw_fit_reduced)\n\n\nChi-Squared Difference Test\n\n               Df   AIC   BIC   Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nhw_fit          2 16006 16070  2.1687                                          \nhw_fit_reduced  4 16069 16123 69.6789      67.51 0.18098       2   2.19e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nfit_stats &lt;- c(\"rmsea\", \"srmr\", \"cfi\", \"aic\")\nfitMeasures(hw_fit, fit_stats) |&gt; print(nd = 3)\nfitMeasures(hw_fit_reduced, fit_stats) |&gt; print(nd = 3)\n\n    rmsea      srmr       cfi       aic \n    0.009     0.008     1.000 16005.917 \n    rmsea      srmr       cfi       aic \n    0.128     0.071     0.908 16069.427 \n\n\n\nsemTools::compareFit(hw_fit, hw_fit_reduced) |&gt; summary()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n               Df   AIC   BIC   Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nhw_fit          2 16006 16070  2.1687                                          \nhw_fit_reduced  4 16069 16123 69.6789      67.51 0.18098       2   2.19e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n####################### Model Fit Indices ###########################\n                       chisq df pvalue        rmsea           cfi           tli\nhw_fit         2.169&lt;U+2020&gt;  2   .338 .009&lt;U+2020&gt; 1.000&lt;U+2020&gt; 0.999&lt;U+2020&gt;\nhw_fit_reduced       69.679   4   .000        .128          .908          .793 \n                       srmr               aic               bic\nhw_fit         .008&lt;U+2020&gt; 16005.917&lt;U+2020&gt; 16069.718&lt;U+2020&gt;\nhw_fit_reduced        .071         16069.427         16123.412 \n\n################## Differences in Fit Indices #######################\n                        df rmsea    cfi    tli  srmr   aic    bic\nhw_fit_reduced - hw_fit  2 0.119 -0.092 -0.206 0.063 63.51 53.695\n\n\n\n모형 비교 2: initial vs. larger model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmod_hw_larger &lt;- \"\n  Grades ~ PreAch + Homework + FamBack\n  Homework ~ PreAch + FamBack + Minority\n  PreAch ~ FamBack + Minority\n\"\n\nhw_fit_larger &lt;- sem(\n  model = mod_hw_larger, \n  sample.cov = nels_cov2, \n  sample.nobs = 1000, \n  fixed.x = FALSE\n)\n\n\nlavTestLRT(hw_fit_larger, hw_fit) |&gt; print()\n\n\nChi-Squared Difference Test\n\n              Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nhw_fit_larger  1 16007 16076 1.3304                                    \nhw_fit         2 16006 16070 2.1687    0.83821     0       1     0.3599\n\n\n\nfitMeasures(hw_fit, fit_stats) |&gt; print(nd = 3)\nfitMeasures(hw_fit_larger, fit_stats) |&gt; print(nd = 3)\n\n    rmsea      srmr       cfi       aic \n    0.009     0.008     1.000 16005.917 \n    rmsea      srmr       cfi       aic \n    0.018     0.008     1.000 16007.079 \n\n\n\nsemTools::compareFit(hw_fit, hw_fit_larger) |&gt; summary(fit.measures = fit_stats)\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n              Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nhw_fit_larger  1 16007 16076 1.3304                                    \nhw_fit         2 16006 16070 2.1687    0.83821     0       1     0.3599\n\n####################### Model Fit Indices ###########################\n                     rmsea         srmr           cfi               aic\nhw_fit_larger        .018  .008&lt;U+2020&gt;        1.000         16007.079 \nhw_fit        .009&lt;U+2020&gt;        .008  1.000&lt;U+2020&gt; 16005.917&lt;U+2020&gt;\n\n################## Differences in Fit Indices #######################\n                        rmsea  srmr cfi    aic\nhw_fit - hw_fit_larger -0.009 0.001   0 -1.162",
    "crumbs": [
      "Keith's",
      "Path Analysis"
    ]
  },
  {
    "objectID": "contents/chap13.html#more-complex-models",
    "href": "contents/chap13.html#more-complex-models",
    "title": "Chapter 13-14. Path Analysis",
    "section": "More Complex Models",
    "text": "More Complex Models\n\nEquivalent and Nonequivalent Models\n\n모형 적합도가 동일한 모형들(동등모형)들이 다수 혹은 무수한 많이 존재할 수 있음.\n\n충분히 가능성이 있는 동등모형들에 대해 고민할 필요가 있음.\n\nNearly-equivalent models 또는 존재하기에 구조모형에 대해서 신중할 필요가 있음.\n\n\n\n\n\n\n\n동등한 모형을 제거하기 위한 제안들\n\n\n\n\n\n\nSource: p. 196, Klein, R. B. (2023). Principles and Practice of Structural Equation Modeling (5e)\n\n번역 by Google Translate\n\n실험적 또는 종단적 설계에서 시간적 선행성은 결과 이전에 조작되거나 측정된 원인 간의 직접적 효과를 역전시키는 것을 배제합니다(역인과성 없음 또는 시간적 역방향 인과성).\n횡단면 설계에서 분석된 모델의 일부가 이전 실험적 또는 종단적 설계에서 평가된 경우 해당 연구의 결과는 일부 인과적 순서를 배제하는 데 도움이 될 수 있습니다.\n인구 통계적 특성이나 안정적인 성격 특성과 같은 특정 변수는 내생적일 가능성이 낮거나 불가능할 수 있습니다. 예를 들어, 태도 변수에서 연대기적 연령으로의 직접적 효과를 지정하는 것은 비논리적입니다.\n변수의 특성을 감안할 때 일부 인과적 순서는 이론적으로 의심스러울 수 있습니다. 예를 들어, 부모의 IQ는 그 반대보다 자녀의 IQ에 영향을 미칠 가능성이 더 높을 수 있습니다.\n매개 변수로 지정된 변수는 잠재적으로 변경 가능해야 합니다. 그렇지 않으면 매개 변수가 될 가능성이 낮습니다. 예를 들어, 안정적이고 비교적 변하지 않는 특성으로 개념화된 변수는 원인으로 지정할 수 있지만 매개 변수로 지정할 수는 없습니다(주제 상자 7.1).\n일부 매개변수를 이론이나 이전 연구 결과와 호환되는 0이 아닌 값으로 고정하면 해당 매개변수를 포함하는 동등한 모델이 배제됩니다. 이는 이러한 고정된 값이 지정된 경로나 변수의 임의적 재구성에 적합하지 않기 때문입니다(Mulaik, 2009b).\n모델의 다른 몇 가지 변수와만 선택적으로 연관된 변수를 추가하면 동등한 버전의 수를 줄이는 데 도움이 될 수 있습니다. 모델에 X → Y 경로가 있다고 가정합니다. X를 직접 유발하지만 Y를 유발하지 않는 것으로 추정되는 변수를 추가하면 X가 Y와 비교하여 고유한 부모를 가지므로 X와 Y가 모두 내생적이라면 규칙 11.2가 적용되지 않습니다. 이 전략은 일반적으로 데이터를 수집하기 전에 구현해야 합니다.\n동등한 모델은 변수 수준에서 동일한 잔차를 갖지만 사례 수준의 잔차는 이러한 모델에 따라 달라질 수 있습니다. Raykov와 Penev(2001)는 더 낮은 표준화된 평균 개별 사례 잔차가 있는 모델이 더 높은 평균을 가진 동등한 버전보다 선호될 것이라고 제안했습니다. 복잡한 점은 잠재 변수가 있는 구조적 모델이 요인 점수 불확정성으로 인해 개별 사례에 대한 고유한 예측을 생성하지 않는다는 것입니다. 이 개념은 14장에서 설명합니다. Raykov-Penev 방법을 적용하는 것은 사례 잔차가 회귀 잔차와 더 직접적으로 유사한 명백한 변수 경로 모델에 더 간단합니다.\n\n(원문)\n\nTemporal precedence in experimental or longitudinal designs precludes reversing direct effects between causes manipulated or measured before outcomes (no retrocausality, or backwards causation in time).\nIf any part of a model analyzed in a cross-sectional design has been evaluated in prior experimental or longitudinal designs, results from those studies may help to rule out some causal orderings.\nCertain variables, such as demographic characteristics or stable personality characteristics, may be unlikely or impossible to be endogenous. For example, specifying a direct effect from an attitudinal variable to chronological age in years is illogical.\nSome causal orderings may be theoretically doubtful, given the nature of the variables. For example, parental IQ may be more likely to affect child IQ than the reverse.\nVariables specified as mediators must be potentially changeable; otherwise, they are unlikely mediators. For example, variables conceptualized as stable, relatively unchanging traits could be specified as causes, but not mediators (Topic Box 7.1).\nFixing some parameters to nonzero values compatible with theory or results from prior studies would rule out equivalent models involving those parameters. This is because such fixed values are not suitable for arbitrary reconfigurations of the paths or variables for which they were specified (Mulaik, 2009b).\nAdding variables that are selectively associated with just a few of the other variables in the model can help to reduce the number of equivalent versions. Suppose that a model has the path X → Y. Adding a variable presumed to directly cause X but not Y means that X has a unique parent compared to Y, so Rule 11.2 would not apply, if both X and Y were endogenous. This strategy must usually be implemented before the data are collected.\nAlthough equivalent models have identical residuals at the variable level, residuals at the case level can vary over such models. Raykov and Penev (2001) suggested that models with the lower standardized average individual case residuals would be preferred over equivalent versions with higher averages. A complication is that structural models with latent variables do not generate unique predictions for individual cases due to factor score indeterminacy, a concept explained Chapter 14. Applying the Raykov–Penev method is more straightforward for manifest-variable path models, where case residuals are more directly analogous to regression residuals.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFor a just-identified model,\n\n완전한 모델이므로 어떻게 대체해도 모형은 동일함\n\n경로 a → b 는 경로 b → a 로 대체\n또는 상관 관계로 대체; a ↔︎ b\n내생변수인 경우 disturbances가 상관 관계로 대체\n\n\nFor overidentified models,\n\njust-identified 부분에 대해서는 동일한 룰이 적용됨\n\nOriginal 모형에서 Homework에서 Previous Achievement로 경로를 뒤집어도 동일한 모형이 됨\n\n그 외의 경우, a, b가 동일한 원인들을 갖는다면, a, b의 경로 방향이 반대로 대체될 수 있음. 또는 상관관계로 대체될 수 있음\n\nHomework에서 Grades로의 경로 뒤집거나 d2와 d3의 상관 관계로 대체할 수 없음\nGrades와 Homework이 동일한 원인들을 갖지 않기 때문임\n\na, b가 동일한 원일들을 갖지 않는다면,\n\na → b의 경로는 b의 원인들이 a의 모든 원인들을 포함하면 대체(경로 또는 상관)될 수 있음\n\n\n\n\n\n\n\n예를 들어, original 모형과 모형 B(homework와 previous achievement가 뒤집힌)를 비교하면, \n\nmod_hw_reversed &lt;- \"\n  Grades ~ PreAch + Homework\n  Homework ~ FamBack + Minority\n  PreAch ~ Homework + FamBack + Minority\n\"\nhw_fit_reversed &lt;- sem(model = mod_hw_reversed, sample.cov = nels_cov2, sample.nobs = 1000, fixed.x = FALSE)\n\n\n\n\nsemTools::compareFit(hw_fit, hw_fit_reversed) |&gt; summary(fit.measures = fit_stats)\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n                Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nhw_fit           2 16006 16070 2.1687                                    \nhw_fit_reversed  2 16006 16070 2.1687          0     0       0           \n\n####################### Model Fit Indices ###########################\n                        chisq df        rmsea         srmr           cfi\nhw_fit          2.169&lt;U+2020&gt;  2 .009&lt;U+2020&gt; .008&lt;U+2020&gt; 1.000&lt;U+2020&gt;\nhw_fit_reversed 2.169&lt;U+2020&gt;  2 .009&lt;U+2020&gt;        .008  1.000&lt;U+2020&gt;\n                              aic\nhw_fit          16005.917&lt;U+2020&gt;\nhw_fit_reversed 16005.917&lt;U+2020&gt;\n\n################## Differences in Fit Indices #######################\n                         df rmsea srmr cfi aic\nhw_fit_reversed - hw_fit  0     0    0   0   0\n\n\n\n\n\nNonrecursive Models\n\n\n\nGet the covariance matrix\nlower &lt;- \"\n  1\n  -0.181 1\n  0.09 0.05 1\n  0.05 0.09 -0.181 1\n  -0.2 0.32 -0.115 0.09 1\n  -0.076 0.087 -0.34 0.2 0.598 1\n\"\nsd &lt;- \"8.7 8.1 10.4 7.3 9.7 8\"\ncov &lt;- getCov(lower, sd = sd, names = c('mper_con', 'man_self', 'wper_con', 'wom_self', 'm_trust', 'w_trust'))\n\n\n\nmod_trust &lt;- \"\n  # regression\n  m_trust ~ w_trust + mper_con + man_self\n  w_trust ~ m_trust + wper_con + wom_self\n\n  # covariance\n  wper_con ~~ 0*man_self + wom_self + mper_con\n  mper_con ~~ 0*wom_self + man_self\n  wom_self ~~ man_self\n  m_trust ~~ w_trust\n\"\ntrust_fit &lt;- sem(model = mod_trust, sample.cov = cov, sample.nobs = 300)\nsummary(trust_fit, fit.measures = T, standardized = \"std.all\") |&gt; print()\n\nlavaan 0.6-19 ended normally after 90 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        17\n\n  Number of observations                           300\n\nModel Test User Model:\n                                                      \n  Test statistic                                 1.438\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.838\n\nModel Test Baseline Model:\n\n  Test statistic                               255.291\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.040\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6305.088\n  Loglikelihood unrestricted model (H1)      -6304.369\n                                                      \n  Akaike (AIC)                               12644.176\n  Bayesian (BIC)                             12707.140\n  Sample-size adjusted Bayesian (SABIC)      12653.226\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.050\n  P-value H_0: RMSEA &lt;= 0.050                    0.950\n  P-value H_0: RMSEA &gt;= 0.080                    0.008\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.019\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n  m_trust ~                                                    \n    w_trust           0.422    0.149    2.834    0.005    0.348\n    mper_con         -0.140    0.052   -2.690    0.007   -0.125\n    man_self          0.320    0.056    5.671    0.000    0.267\n  w_trust ~                                                    \n    m_trust           0.233    0.109    2.142    0.032    0.282\n    wper_con         -0.220    0.038   -5.788    0.000   -0.285\n    wom_self          0.135    0.052    2.569    0.010    0.123\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n  man_self ~~                                                  \n    wper_con          0.000                               0.000\n  wper_con ~~                                                  \n    wom_self        -14.475    4.408   -3.284    0.001   -0.191\n  mper_con ~~                                                  \n    wper_con          9.759    5.063    1.927    0.054    0.108\n    wom_self          0.000                               0.000\n    man_self        -13.436    4.092   -3.284    0.001   -0.191\n  man_self ~~                                                  \n    wom_self          6.378    3.309    1.927    0.054    0.108\n .m_trust ~~                                                   \n   .w_trust           1.957   10.044    0.195    0.846    0.041\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n   .m_trust          56.819    6.825    8.326    0.000    0.602\n   .w_trust          40.379    5.635    7.166    0.000    0.630\n    mper_con         75.586    6.169   12.253    0.000    1.000\n    man_self         65.520    5.347   12.253    0.000    1.000\n    wper_con        108.012    8.815   12.253    0.000    1.000\n    wom_self         53.217    4.343   12.253    0.000    1.000\n\n\n\n\n\nLongitudinal Models\nDo job stress and emotional exhaustion (or burnout) have reciprocal effects?\nEstimated via longitudinal data.\n\n\n\n\n\n\nThe study assessed stress and the three components of burnout (emotional exhaustion, depersonalisation, and low personal accomplishment) in a 3-year longitudinal study of a representative sample of 331 UK doctors.\n\nMcManus, I. C., Winder, B. C., & Gordon, D. (2002). The causal links between stress and burnout in a longitudinal study of UK doctors. The Lancet, 359(9323), 2089-2090.\n\n\n\n\n\n\n\nGet the covariance matrix\nlower &lt;- \"\n  21.623                            \n  2.052 9.797                       \n  10.98 4.548 19.625                    \n  0.186 -0.027 0.76 8.18                \n  10.614 2.773 8.911 -0.789 20.43           \n  0.808 6.377 2.756 -0.131 3.46 9.302       \n  7.301 3.795 11.361 -0.024 9.939 4.889 16.892  \n  -0.374 -0.772 0.037 4.737 -2.729 -0.777 -1.059 7.673\n\"\ncov &lt;- getCov(lower, names = c(\"Stress_2\", \"Depersonal_2\", \"EExhaust_2\", \"PAcomplish_2\",  \"Stress_1\", \"Depersonal_1\", \"EExhaust_1\", \"PAcomplish_1\"))\n\n\n\nmod_stress &lt;- \"\n  # regression\n  Stress_2 ~ Depersonal_1 + Stress_1 + EExhaust_2\n  EExhaust_2 ~ Depersonal_1 + Stress_1 + PAcomplish_1\n  \n  # covariance\n\"\nstress_fit &lt;- sem(model = mod_stress, sample.cov = cov, sample.nobs = 331, fixed.x = FALSE)\nsummary(stress_fit, fit.measures = T, standardized = \"std.all\") |&gt; print()\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        14\n\n  Number of observations                           331\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.790\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.374\n\nModel Test Baseline Model:\n\n  Test statistic                               243.540\n  Degrees of freedom                                 7\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.006\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4412.408\n  Loglikelihood unrestricted model (H1)      -4412.013\n                                                      \n  Akaike (AIC)                                8852.816\n  Bayesian (BIC)                              8906.046\n  Sample-size adjusted Bayesian (SABIC)       8861.637\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.139\n  P-value H_0: RMSEA &lt;= 0.050                    0.544\n  P-value H_0: RMSEA &gt;= 0.080                    0.276\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.010\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n  Stress_2 ~                                                   \n    Depersonal_1     -0.173    0.068   -2.538    0.011   -0.114\n    Stress_1          0.367    0.050    7.287    0.000    0.357\n    EExhaust_2        0.417    0.051    8.214    0.000    0.397\n  EExhaust_2 ~                                                 \n    Depersonal_1      0.149    0.073    2.047    0.041    0.103\n    Stress_1          0.434    0.050    8.642    0.000    0.443\n    PAcomplish_1      0.174    0.080    2.188    0.029    0.109\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n  Depersonal_1 ~~                                              \n    Stress_1          3.450    0.779    4.429    0.000    0.251\n    PAcomplish_1     -0.775    0.465   -1.666    0.096   -0.092\n  Stress_1 ~~                                                  \n    PAcomplish_1     -2.721    0.702   -3.875    0.000   -0.218\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)  Std.all\n   .Stress_2         13.248    1.030   12.865    0.000    0.615\n   .EExhaust_2       15.292    1.189   12.865    0.000    0.782\n    Depersonal_1      9.274    0.721   12.865    0.000    1.000\n    Stress_1         20.368    1.583   12.865    0.000    1.000\n    PAcomplish_1      7.650    0.595   12.865    0.000    1.000",
    "crumbs": [
      "Keith's",
      "Path Analysis"
    ]
  },
  {
    "objectID": "contents/chap13.html#exercises",
    "href": "contents/chap13.html#exercises",
    "title": "Chapter 13-14. Path Analysis",
    "section": "Exercises",
    "text": "Exercises\n연습문제 14장 5번; Henry, Tolan, and Gorman-Smith (2001)\n\nFully mediated model\nPartially mediated model: peer delinquency → individual violence is set to zero\n\n학생들의 delinquency와 violence 각각에 대해 더 중요하게 미치는 변수는 무엇인가?\n각각의 변수에 미치는 효과의 크기는? (R-squared)\nFamily가 미치는 간접효과는 어떠한가?\nFully vs. partially mediated model 비교\n\n\n\nhenri &lt;- haven::read_sav(\"data/chap 14 path via SEM/henry et al.sav\")\nhenri |&gt; print()\n\n# A tibble: 246 x 5\n  i_delin i_violen p_delin p_violen family\n    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1  1.47     0.773    1.25    1.29   -0.652\n2  3.19     1.13     0.745   1.00   -2.66 \n3 -0.0792   0.0759   0.276  -0.0967  1.71 \n4  3.83     2.45     0.279  -0.276   1.69 \n5  0.969   -0.0923  -0.884  -0.502   4.61 \n6 -0.0740   0.321   -0.755  -0.978  -0.844\n# i 240 more rows",
    "crumbs": [
      "Keith's",
      "Path Analysis"
    ]
  },
  {
    "objectID": "contents/Kline/kchap21.html",
    "href": "contents/Kline/kchap21.html",
    "title": "Chapter 21. Latent Growth Curve Models",
    "section": "",
    "text": "Load libraries\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\n\n\n\n참고 문헌\n\n\nGrimm, K. J., Ram, N., & Estabrook, R. (2017). Growth modeling: Structural equation and multilevel modeling approaches. Guilford Press.\n\nLittle, T. D. (2024). Longitudinal structural equation modeling (2e). Guilford Press.\n\nNewsom, J. (2024). Longitudinal structural equation modeling: A comprehensive introduction (2e). Routledge.\n\n\n\n# read in summary statistics\nkimspoonLower.cor &lt;- '\n1.00\n .35 1.00\n .28  .35 1.00\n .29  .40  .45 1.00\n .00  .07  .14  .04 1.00\n .00 -.09 -.12 -.10  .26 1.00 '\n \n# name the variables and convert to full correlation matrix\nkimspoon.cor &lt;- lavaan::getCov(kimspoonLower.cor, names = c(\"R1\", \"R2\",\n \"R3\", \"R4\", \"abuse\", \"neglect\"))\n \n# display the correlations\nkimspoon.cor |&gt; print()\n\n          R1    R2    R3    R4 abuse neglect\nR1      1.00  0.35  0.28  0.29  0.00    0.00\nR2      0.35  1.00  0.35  0.40  0.07   -0.09\nR3      0.28  0.35  1.00  0.45  0.14   -0.12\nR4      0.29  0.40  0.45  1.00  0.04   -0.10\nabuse   0.00  0.07  0.14  0.04  1.00    0.26\nneglect 0.00 -0.09 -0.12 -0.10  0.26    1.00\n\n\n\n# add the standard deviations and convert to covariances\nkimspoon.cov &lt;- lavaan::cor2cov(kimspoon.cor, sds = c(.05,.77,.76,1.15,\n 7.75,4.09))\n\n# create mean vector\nkimspoon.mean = c(.04,.61,.57,.83,7.17,3.03)\n \n# display the covariances and means\nkimspoon.cov |&gt; print()\nkimspoon.mean |&gt; print()\n\n              R1        R2        R3        R4     abuse   neglect\nR1      0.002500  0.013475  0.010640  0.016675  0.000000  0.000000\nR2      0.013475  0.592900  0.204820  0.354200  0.417725 -0.283437\nR3      0.010640  0.204820  0.577600  0.393300  0.824600 -0.373008\nR4      0.016675  0.354200  0.393300  1.322500  0.356500 -0.470350\nabuse   0.000000  0.417725  0.824600  0.356500 60.062500  8.241350\nneglect 0.000000 -0.283437 -0.373008 -0.470350  8.241350 16.728100\n[1] 0.04 0.61 0.57 0.83 7.17 3.03\n\n\n\n# for all models, error variance for R1\n# is fixed to equal zero\n\n# lavaan function growth() automatically fixes\n# intercepts of indicators to zero but specifies \n# latent growth factor means as free parameters \n\n# the direct tracing of the constant (delta-1) on\n# the latent growth factors are means but are labeled\n# as \"intercepts\" in lavaan output\n\n# specify all models\n\n# model 1\n# no growth (intercept only)\n\nnoGrowth.model &lt;- '\n  # specify intercept\n  # fix all loadings to 1.0\n  Intercept =~ 1*R1 + 1*R2 + 1*R3 + 1*R4\n  # fix error variance for r1 to zero\n  R1 ~~ 0*R1\n'\n\n\n# model 2\n# latent basis growth model\n# (curve fitting, level and shape)\n# this model is retained\n\n# maccallum-rmsea for model 2\n# exact fit test\n# power at N = 150\nsemTools::findRMSEApower(0, .05, 4, 150, .05, 1) |&gt; print()\n\n# minimum N for power at least .90\nsemTools::findRMSEAsamplesize(0, .05, 4, .90, .05, 1) |&gt; print()\n\nbasis.model &lt;- '\n  Intercept =~ 1*R1 + 1*R2 + 1*R3 + 1*R4\n  # specify shape, first and last loadings fixed\n  Shape =~ 0*R1 + R2 + R3 + 1*R4\n  R1 ~~ 0*R1\n'\n\n[1] 0.136742\n[1] 1542\n\n\n\n# model 3\n# linear growth model\n\nlinear.model &lt;- '\n  Intercept =~ 1*R1 + 1*R2 + 1*R3 + 1*R4 \n  # all loadings fixed to constants\n  Linear =~ 0*R1 + 1*R2 + 2*R3 + 3*R4\n  R1 ~~ 0*R1\n'\n\n\n# fit model 1 to data\nnoGrowth &lt;- lavaan::growth(noGrowth.model, sample.cov = kimspoon.cov,\n sample.mean = kimspoon.mean, sample.nobs = 150)\n\n# fit model 2 to data\nbasis &lt;- lavaan::growth(basis.model, sample.cov = kimspoon.cov,\n sample.mean = kimspoon.mean, sample.nobs = 150)\n\n# fit model 3 to data\nlinear &lt;- lavaan::growth(linear.model, sample.cov = kimspoon.cov,\n sample.mean = kimspoon.mean, sample.nobs = 150)\n\n\n# model chi-squares and chi-square difference tests\nanova(noGrowth, basis) |&gt; print()\n\n\nChi-Squared Difference Test\n\n         Df    AIC    BIC    Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nbasis     4 610.60 640.71   2.8109                                          \nnoGrowth  9 865.42 880.47 267.6294     264.82 0.58858       5  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nanova(basis, linear) |&gt; print()\n\n\nChi-Squared Difference Test\n\n       Df   AIC    BIC   Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nbasis   4 610.6 640.71  2.8109                                          \nlinear  6 638.6 662.69 34.8116     32.001 0.31623       2  1.125e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n# model 1 parameter estimates, global fit statistics,\n# residuals\n# very poor fit\nlavaan::summary(noGrowth, fit.measures = TRUE, estimates = FALSE)\nlavaan::fitted(noGrowth)\nlavaan::residuals(noGrowth, type = \"standardized\")\nlavaan::residuals(noGrowth, type = \"cor.bollen\")\n\n\n\n# model 2 parameter estimates, global fit statistics,\n# residuals\n# retained model\nlavaan::summary(basis, fit.measures = TRUE, rsquare = TRUE)\nlavaan::standardizedSolution(basis)\n\n\n\n# variance and standard error for Intercept are close to zero,\n# so estimates are printed at 5-decimal accuracy, not 3 (default)\nprint(lavaan::parameterEstimates(basis), nd = 5)\n\n\n# implied covariances and means for observed variables\nlavaan::fitted(basis)\n# implied means for latent growth factors & observed variables\nlavaan::lavInspect(basis, \"mean.lv\")\nlavaan::lavInspect(basis, \"mean.ov\")\n\n\n# residuals\nlavaan::residuals(basis, type = \"raw\")\nlavaan::residuals(basis, type = \"standardized\")\nlavaan::residuals(basis, type = \"cor.bollen\")\n\n\n# model 3 parameter estimates, global fit statistics,\n# residuals\nlavaan::summary(linear, fit.measures = TRUE)\nlavaan::fitted(linear) \nlavaan::residuals(linear, type = \"raw\")\nlavaan::residuals(linear, type = \"standardized\")\nlavaan::residuals(linear, type = \"cor.bollen\")",
    "crumbs": [
      "Kline's",
      "Latent Growth Curve"
    ]
  },
  {
    "objectID": "contents/Kline/kchap16-3.html",
    "href": "contents/Kline/kchap16-3.html",
    "title": "Chapter 16. Composite Models",
    "section": "",
    "text": "Load libraries\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(cSEM)\nlibrary(psych)",
    "crumbs": [
      "Kline's",
      "Composite",
      "Composite: PLS-PM"
    ]
  },
  {
    "objectID": "contents/Kline/kchap16-3.html#partial-least-squares-path-modeling-pls-pm-algorithm",
    "href": "contents/Kline/kchap16-3.html#partial-least-squares-path-modeling-pls-pm-algorithm",
    "title": "Chapter 16. Composite Models",
    "section": "Partial least squares path modeling (PLS-PM) algorithm",
    "text": "Partial least squares path modeling (PLS-PM) algorithm\nSoftware: cSEM\n\n\nSource: Fig 16.3 (p. 295)\n\n\n# set global seed for random number generation\nset.seed(123)\n\n# variable order is acculscl, status, percent, educ, income,\n# interpers, job, scl90d\n# read correlation matrix\nshen.cor &lt;- matrix(c(1.00, .44, .69, .21, .23, .12, .09, .03,\n                      .44,1.00, .54, .08, .15, .08, .06, .02,\n                      .69, .54,1.00, .16, .19, .08, .04,-.02,\n                      .21, .08, .16,1.00, .19, .08, .01,-.07,\n                      .23, .15, .19, .19,1.00,-.03,-.02,-.11,\n                      .12, .08, .08, .08,-.03,1.00, .38, .37,\n                      .09, .06, .04, .01,-.02, .38,1.00, .46,\n                      .03, .02,-.02,-.07,-.11, .37, .46,1.00),\n                      ncol = 8, nrow = 8)\n\n# generate raw scores and save to dataframe\nshen.data &lt;- semTools::kd(shen.cor, 983, type=\"exact\")\n\n# rename columns in data frame and display correlation matrix\nnames(shen.data) &lt;- c(\"acculscl\", \"status\", \"percent\", \"educ\", \"income\",\n \"interpers\", \"job\", \"scl90d\")\n\n# display correlation matrix\ncor(shen.data) |&gt; print()\n\n          acculscl status percent  educ income interpers   job scl90d\nacculscl      1.00   0.44    0.69  0.21   0.23      0.12  0.09   0.03\nstatus        0.44   1.00    0.54  0.08   0.15      0.08  0.06   0.02\npercent       0.69   0.54    1.00  0.16   0.19      0.08  0.04  -0.02\neduc          0.21   0.08    0.16  1.00   0.19      0.08  0.01  -0.07\nincome        0.23   0.15    0.19  0.19   1.00     -0.03 -0.02  -0.11\ninterpers     0.12   0.08    0.08  0.08  -0.03      1.00  0.38   0.37\njob           0.09   0.06    0.04  0.01  -0.02      0.38  1.00   0.46\nscl90d        0.03   0.02   -0.02 -0.07  -0.11      0.37  0.46   1.00\n\n\n\n# descriptive statistics\npsych::describe(shen.data) |&gt; print()\n\n          vars   n mean sd median trimmed  mad   min  max range  skew kurtosis\nacculscl     1 983    0  1  -0.03    0.00 1.04 -3.77 3.15  6.92  0.00     0.11\nstatus       2 983    0  1  -0.01   -0.01 1.00 -3.19 3.05  6.25  0.06    -0.05\npercent      3 983    0  1  -0.04   -0.01 1.04 -3.46 3.01  6.47  0.03    -0.10\neduc         4 983    0  1   0.00    0.00 1.03 -3.67 3.37  7.04 -0.01     0.01\nincome       5 983    0  1   0.00    0.00 1.00 -3.14 3.45  6.60  0.03     0.22\ninterpers    6 983    0  1   0.01    0.01 0.99 -3.57 3.11  6.68 -0.05     0.00\njob          7 983    0  1   0.02    0.00 1.06 -3.23 3.15  6.38 -0.03    -0.23\nscl90d       8 983    0  1   0.02    0.00 1.01 -2.73 2.98  5.71  0.05    -0.10\n            se\nacculscl  0.03\nstatus    0.03\npercent   0.03\neduc      0.03\nincome    0.03\ninterpers 0.03\njob       0.03\nscl90d    0.03\n\n\n\n# specify composite model\n\nshen.model &lt;- '\n  # outer model (measurement)\n  # exogenous composites\n  Acculturation &lt;~ acculscl + status + percent\n  SES &lt;~ educ + income\n  \n  # endogenous composites\n  Stress &lt;~ interpers + job\n  Depression &lt;~ scl90d\n\n  # inner model (structural)\n  Stress ~ Acculturation\n  Depression ~ SES + Stress\n'\n\n\n# fit model to data with package cSEM for cca\n\n# the algorithm is basic PLS-PM \n# dominant indicators specified for all composites\n# with multiple indicators\n# by default, the single indicator for depression\n# is the dominant indicator\n# outer weights are PLS mode A (correlation weights)\n# inner weights are factor (factorial)\n# bootstrapped standard errors, 1000 generated samples\n# seed for bootstrapping (123) initializes random\n# number generation in cSEM functions for boostrapping\n# the global seed in R is also set to the same value (123)\n# thus, bootstrapped estimates of standard errors and\n# percentiles for distributions of global fit test\n# statistics are reproducible\n\nshen &lt;- cSEM::csem(.data = shen.data, .model = shen.model, \n .dominant_indicators = c(Acculturation = \"acculscl\", SES = \"educ\",\n  Stress = \"interpers\"), .approach_weights = \"PLS-PM\", \n .PLS_modes = \"modeA\", .PLS_weight_scheme_inner = \"factorial\", \n .resample_method = \"bootstrap\", .R = 1000, .seed = 123,\n .disattenuate = FALSE)\n\n\n# check solution for problems\ncSEM::verify(shen)\n\n\n________________________________________________________________________________\nVerify admissibility:\n\n  admissible\nDetails:\n  Code   Status    Description\n  1      ok        Convergence achieved                                   \n  2      ok        All absolute standardized loading estimates &lt;= 1       \n  3      ok        Construct VCV is positive semi-definite                \n  4      ok        All reliability estimates &lt;= 1                         \n  5      ok        Model-implied indicator VCV is positive semi-definite  \n________________________________________________________________________________\n\n\n\n\n# parameter estimates with bootstrapped standard errors\ncSEM::summarize(shen)\n\n\n________________________________________________________________________________\n----------------------------------- Overview -----------------------------------\n    General information:\n    ------------------------\n    Estimation status                = Ok\n    Number of observations           = 983\n    Weight estimator                 = PLS-PM\n    Inner weighting scheme           = \"factorial\"\n    Type of indicator correlation    = Pearson\n    Path model estimator             = OLS\n    Second-order approach            = NA\n    Type of path model               = Linear\n    Disattenuated                    = No\n    Resample information:\n    ---------------------\n    Resample method                  = \"bootstrap\"\n    Number of resamples              = 1000\n    Number of admissible results     = 1000\n    Approach to handle inadmissibles = \"drop\"\n    Sign change option               = \"none\"\n    Random seed                      = 123\n    Construct details:\n    ------------------\n    Name           Modeled as     Order         Mode      \n    Acculturation  Composite      First order   \"modeA\"   \n    SES            Composite      First order   \"modeA\"   \n    Stress         Composite      First order   \"modeA\"   \n    Depression     Composite      First order   \"modeA\"   \n----------------------------------- Estimates ----------------------------------\nEstimated path coefficients:\n============================\n                                                                        CI_percentile   \n  Path                      Estimate  Std. error   t-stat.   p-value         95%        \n  Stress ~ Acculturation      0.1166      0.0276    4.2251    0.0000 [ 0.0687; 0.1765 ] \n  Depression ~ SES           -0.1214      0.0298   -4.0779    0.0000 [-0.1798;-0.0713 ] \n  Depression ~ Stress         0.5039      0.0230   21.9210    0.0000 [ 0.4586; 0.5479 ] \nEstimated loadings:\n===================\n                                                                           CI_percentile   \n  Loading                      Estimate  Std. error   t-stat.   p-value         95%        \n  Acculturation =~ acculscl      0.8952      0.0393   22.8018    0.0000 [ 0.8105; 0.9668 ] \n  Acculturation =~ status        0.7509      0.0706   10.6426    0.0000 [ 0.5965; 0.8567 ] \n  Acculturation =~ percent       0.8582      0.0408   21.0136    0.0000 [ 0.7484; 0.8988 ] \n  SES =~ educ                    0.6440      0.1646    3.9115    0.0001 [ 0.2346; 0.8785 ] \n  SES =~ income                  0.8735      0.1293    6.7530    0.0000 [ 0.6337; 0.9974 ] \n  Stress =~ interpers            0.7942      0.0193   41.1856    0.0000 [ 0.7531; 0.8284 ] \n  Stress =~ job                  0.8639      0.0127   67.9643    0.0000 [ 0.8371; 0.8874 ] \n  Depression =~ scl90d           1.0000          NA        NA        NA [     NA;     NA ] \nEstimated weights:\n==================\n                                                                           CI_percentile   \n  Weight                       Estimate  Std. error   t-stat.   p-value         95%        \n  Acculturation &lt;~ acculscl      0.5327      0.0946    5.6283    0.0000 [ 0.3908; 0.7721 ] \n  Acculturation &lt;~ status        0.3551      0.1097    3.2366    0.0012 [ 0.1299; 0.5630 ] \n  Acculturation &lt;~ percent       0.2989      0.0886    3.3740    0.0007 [ 0.0526; 0.4114 ] \n  SES &lt;~ educ                    0.4959      0.1853    2.6766    0.0074 [ 0.0579; 0.7864 ] \n  SES &lt;~ income                  0.7793      0.1556    5.0067    0.0000 [ 0.4826; 0.9885 ] \n  Stress &lt;~ interpers            0.5446      0.0220   24.7726    0.0000 [ 0.5019; 0.5883 ] \n  Stress &lt;~ job                  0.6569      0.0224   29.2846    0.0000 [ 0.6140; 0.7004 ] \n  Depression &lt;~ scl90d           1.0000          NA        NA        NA [     NA;     NA ] \nEstimated construct correlations:\n=================================\n                                                                      CI_percentile   \n  Correlation             Estimate  Std. error   t-stat.   p-value         95%        \n  Acculturation ~~ SES      0.2745      0.0382    7.1933    0.0000 [ 0.1969; 0.3267 ] \nEstimated indicator correlations:\n=================================\n                                                                     CI_percentile   \n  Correlation            Estimate  Std. error   t-stat.   p-value         95%        \n  acculscl ~~ status       0.4400      0.0258   17.0772    0.0000 [ 0.3887; 0.4887 ] \n  acculscl ~~ percent      0.6900      0.0176   39.2730    0.0000 [ 0.6550; 0.7228 ] \n  status ~~ percent        0.5400      0.0219   24.6519    0.0000 [ 0.4945; 0.5799 ] \n  educ ~~ income           0.1900      0.0300    6.3323    0.0000 [ 0.1294; 0.2526 ] \n  interpers ~~ job         0.3800      0.0283   13.4416    0.0000 [ 0.3241; 0.4361 ] \n------------------------------------ Effects -----------------------------------\nEstimated total effects:\n========================\n                                                                            CI_percentile   \n  Total effect                  Estimate  Std. error   t-stat.   p-value         95%        \n  Stress ~ Acculturation          0.1166      0.0276    4.2251    0.0000 [ 0.0687; 0.1765 ] \n  Depression ~ Acculturation      0.0588      0.0143    4.1068    0.0000 [ 0.0348; 0.0902 ] \n  Depression ~ SES               -0.1214      0.0298   -4.0779    0.0000 [-0.1798;-0.0713 ] \n  Depression ~ Stress             0.5039      0.0230   21.9210    0.0000 [ 0.4586; 0.5479 ] \nEstimated indirect effects:\n===========================\n                                                                            CI_percentile   \n  Indirect effect               Estimate  Std. error   t-stat.   p-value         95%        \n  Depression ~ Acculturation      0.0588      0.0143    4.1068    0.0000 [ 0.0348; 0.0902 ] \n________________________________________________________________________________\n\n\n\n\n# test overall (global) model fit\n# 1000 bootstrap replications\n# seed value set\ncSEM::testOMF(shen, .R = 1000, .seed = 123)\n\n\n________________________________________________________________________________\n--------- Test for overall model fit based on Beran & Srivastava (1985) --------\nNull hypothesis:\n       +------------------------------------------------------------------+\n       |                                                                  |\n       |   H0: The model-implied indicator covariance matrix equals the   |\n       |   population indicator covariance matrix.                        |\n       |                                                                  |\n       +------------------------------------------------------------------+\nTest statistic and critical value: \n                                        Critical value\n    Distance measure    Test statistic    95%   \n    dG                      0.0062      0.0088  \n    SRMR                    0.0254      0.0408  \n    dL                      0.0232      0.0600  \n    dML                     0.0329      0.0462  \n    \nDecision: \n                            Significance level\n    Distance measure             95%        \n    dG                      Do not reject    \n    SRMR                    Do not reject    \n    dL                      Do not reject    \n    dML                     Do not reject    \n    \nAdditional information:\n    Out of 1000 bootstrap replications 1000 are admissible.\n    See ?verify() for what constitutes an inadmissible result.\n    The seed used was: 123\n________________________________________________________________________________\n\n\n\n\n# model quality criteria\ncSEM::assess(shen, .quality_criterion = c(\"df\", \"r2\", \"r2_adj\", \"f2\", \n \"chi_square\"))\n\n________________________________________________________________________________\n\n    Construct         R2          R2_adj    \n    Stress          0.0136        0.0126    \n    Depression      0.2684        0.2669    \n\n--------------------------- Distance and fit measures --------------------------\n\n\n    Chi_square     = 32.35609\n\n    Degrees of freedom    = 15\n\n-------------------------- Effect sizes (Cohen's f^2) --------------------------\n\n  Dependent construct: 'Stress'\n\n    Independent construct       f^2    \n    Acculturation             0.0138   \n\n  Dependent construct: 'Depression'\n\n    Independent construct       f^2    \n    SES                       0.0201   \n    Stress                    0.3471   \n________________________________________________________________________________\n\n\n\n# model-implied correlations among composites\ncSEM::fit(shen, .type_vcv = \"construct\") |&gt; print()\n\n              Acculturation         SES     Stress  Depression\nAcculturation    1.00000000  0.27450941 0.11664735  0.02545829\nSES              0.27450941  1.00000000 0.03202079 -0.10524925\nStress           0.11664735  0.03202079 1.00000000  0.50002142\nDepression       0.02545829 -0.10524925 0.50002142  1.00000000\n\n\n\n# model-implied correlations among indicators\npredicted &lt;- cSEM::fit(shen, .type_vcv = \"indicator\")\npredicted |&gt; round(3) |&gt; print()\n\n          acculscl status percent   educ income interpers   job scl90d\nacculscl     1.000  0.440   0.690  0.158  0.215     0.083 0.090  0.023\nstatus       0.440  1.000   0.540  0.133  0.180     0.070 0.076  0.019\npercent      0.690  0.540   1.000  0.152  0.206     0.080 0.086  0.022\neduc         0.158  0.133   0.152  1.000  0.190     0.016 0.018 -0.068\nincome       0.215  0.180   0.206  0.190  1.000     0.022 0.024 -0.092\ninterpers    0.083  0.070   0.080  0.016  0.022     1.000 0.380  0.397\njob          0.090  0.076   0.086  0.018  0.024     0.380 1.000  0.432\nscl90d       0.023  0.019   0.022 -0.068 -0.092     0.397 0.432  1.000\n\n\n\n# calculate correlation residuals\n# rounded to 3-decimal places\n\ncor_residuals = shen.cor - predicted\nround(cor_residuals, digits = 3) |&gt; print()\n\n          acculscl status percent   educ income interpers    job scl90d\nacculscl     0.000  0.000   0.000  0.052  0.015     0.037  0.000  0.007\nstatus       0.000  0.000   0.000 -0.053 -0.030     0.010 -0.016  0.001\npercent      0.000  0.000   0.000  0.008 -0.016     0.000 -0.046 -0.042\neduc         0.052 -0.053   0.008  0.000  0.000     0.064 -0.008 -0.002\nincome       0.015 -0.030  -0.016  0.000  0.000    -0.052 -0.044 -0.018\ninterpers    0.037  0.010   0.000  0.064 -0.052     0.000  0.000 -0.027\njob          0.000 -0.016  -0.046 -0.008 -0.044     0.000  0.000  0.028\nscl90d       0.007  0.001  -0.042 -0.002 -0.018    -0.027  0.028  0.000",
    "crumbs": [
      "Kline's",
      "Composite",
      "Composite: PLS-PM"
    ]
  },
  {
    "objectID": "contents/Kline/kchap16-1.html",
    "href": "contents/Kline/kchap16-1.html",
    "title": "Chapter 16. Composite Models",
    "section": "",
    "text": "Load libraries\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)",
    "crumbs": [
      "Kline's",
      "Composite",
      "Reflective"
    ]
  },
  {
    "objectID": "contents/Kline/kchap16-1.html#reflective-measurement",
    "href": "contents/Kline/kchap16-1.html#reflective-measurement",
    "title": "Chapter 16. Composite Models",
    "section": "Reflective measurement",
    "text": "Reflective measurement\nPartial SR model with reflective measurement component\n\n\nSource: Fig 16.1 (p. 289)\n\n\n# input the correlations in lower diagnonal form\nshenLower.cor &lt;- '\n1.00\n .44 1.00\n .69  .54 1.00\n .21  .08  .16 1.00\n .23  .15  .19  .19 1.00\n .12  .08  .08  .08 -.03 1.00\n .09  .06  .04  .01 -.02  .38 1.00\n .03  .02 -.02 -.07 -.11  .37  .46 1.00 '\n\n# name the variables and convert to full correlation matrix\nshen.cor &lt;- lavaan::getCov(shenLower.cor, names = c(\"acculscl\", \"status\",\n \"percent\", \"educ\", \"income\", \"interpers\", \"job\", \"scl90d\"))\n\n# add the standard deviations and convert to covariances\nshen.cov &lt;- lavaan::cor2cov(shen.cor, sds = c(3.60,3.30,2.45,3.27,3.44,2.99,\n 3.58,3.70))\n\n\n\n# display correlations and covariances\nshen.cor |&gt; print()\n\n          acculscl status percent  educ income interpers   job scl90d\nacculscl      1.00   0.44    0.69  0.21   0.23      0.12  0.09   0.03\nstatus        0.44   1.00    0.54  0.08   0.15      0.08  0.06   0.02\npercent       0.69   0.54    1.00  0.16   0.19      0.08  0.04  -0.02\neduc          0.21   0.08    0.16  1.00   0.19      0.08  0.01  -0.07\nincome        0.23   0.15    0.19  0.19   1.00     -0.03 -0.02  -0.11\ninterpers     0.12   0.08    0.08  0.08  -0.03      1.00  0.38   0.37\njob           0.09   0.06    0.04  0.01  -0.02      0.38  1.00   0.46\nscl90d        0.03   0.02   -0.02 -0.07  -0.11      0.37  0.46   1.00\n\n\n\nshen.cov |&gt; round(2) |&gt; print()\n\n          acculscl status percent  educ income interpers   job scl90d\nacculscl     12.96   5.23    6.09  2.47   2.85      1.29  1.16   0.40\nstatus        5.23  10.89    4.37  0.86   1.70      0.79  0.71   0.24\npercent       6.09   4.37    6.00  1.28   1.60      0.59  0.35  -0.18\neduc          2.47   0.86    1.28 10.69   2.14      0.78  0.12  -0.85\nincome        2.85   1.70    1.60  2.14  11.83     -0.31 -0.25  -1.40\ninterpers     1.29   0.79    0.59  0.78  -0.31      8.94  4.07   4.09\njob           1.16   0.71    0.35  0.12  -0.25      4.07 12.82   6.09\nscl90d        0.40   0.24   -0.18 -0.85  -1.40      4.09  6.09  13.69\n\n\n\n# maccallum-rmsea for whole model\n# exact fit test\n# power at N = 983\nsemTools::findRMSEApower(0, .05, 16, 983, .05, 1) |&gt; print()\n\n[1] 0.9931794\n\n\n\n# minimum N for power at least .90\nsemTools::findRMSEAsamplesize(0, .05, 16, .90, .05, 1) |&gt; print()\n\n[1] 605\n\n\n\n# scl90d score reliability (alpha) is .90 from \n# derogatis et al. (1976)\n# sample variance is 3.70**2 = 13.690\n# error variance fixed to (1 - .90) * 13.690 = 1.369\n\n# specify reflective model\n\nshenSR.model &lt;- '\n  # measurement model with error covariance\n  Acculturation =~ acculscl + status + percent\n  status ~~ percent\n  SES =~ educ + income\n  Stress =~ interpers + job\n  Depression =~ 1*scl90d\n  scl90d ~~ 1.369*scl90d\n  \n  # structural model\n  Stress ~ a*Acculturation\n  Depression ~ SES + b*Stress \n\n  # define indirect effect of acculturation\n  ab := a * b\n'\n\n\n# fit model to data\nshenSR &lt;- lavaan::sem(shenSR.model, sample.cov = shen.cov,\n sample.nobs = 983, fixed.x = FALSE)\nlavaan::summary(shenSR, fit.measures = TRUE, rsquare = TRUE) |&gt; print()\n\nlavaan 0.6.17 ended normally after 98 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        20\n\n  Number of observations                           983\n\nModel Test User Model:\n                                                      \n  Test statistic                                21.341\n  Degrees of freedom                                16\n  P-value (Chi-square)                           0.166\n\nModel Test Baseline Model:\n\n  Test statistic                              1606.002\n  Degrees of freedom                                28\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.997\n  Tucker-Lewis Index (TLI)                       0.994\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -19671.382\n  Loglikelihood unrestricted model (H1)     -19660.711\n                                                      \n  Akaike (AIC)                               39382.764\n  Bayesian (BIC)                             39480.576\n  Sample-size adjusted Bayesian (SABIC)      39417.056\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.018\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.037\n  P-value H_0: RMSEA &lt;= 0.050                    0.999\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.023\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  Acculturation =~                                    \n    acculscl          1.000                           \n    status            0.450    0.051    8.880    0.000\n    percent           0.523    0.051   10.214    0.000\n  SES =~                                              \n    educ              1.000                           \n    income            1.158    0.201    5.775    0.000\n  Stress =~                                           \n    interpers         1.000                           \n    job               1.432    0.121   11.862    0.000\n  Depression =~                                       \n    scl90d            1.000                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  Stress ~                                            \n    Acculturtn (a)    0.088    0.023    3.876    0.000\n  Depression ~                                        \n    SES              -0.578    0.138   -4.179    0.000\n    Stress     (b)    1.524    0.132   11.588    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .status ~~                                           \n   .percent           1.623    0.307    5.294    0.000\n  Acculturation ~~                                    \n    SES               2.420    0.366    6.613    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .scl90d            1.369                           \n   .acculscl          1.333    1.083    1.231    0.218\n   .status            8.523    0.451   18.918    0.000\n   .percent           2.814    0.323    8.723    0.000\n   .educ              8.739    0.541   16.142    0.000\n   .income            9.215    0.641   14.384    0.000\n   .interpers         6.114    0.355   17.203    0.000\n   .job               7.025    0.556   12.626    0.000\n    Acculturation    11.614    1.227    9.464    0.000\n    SES               1.944    0.463    4.197    0.000\n   .Stress            2.727    0.360    7.583    0.000\n   .Depression        5.438    0.647    8.402    0.000\n\nR-Square:\n                   Estimate\n    scl90d            0.899\n    acculscl          0.897\n    status            0.217\n    percent           0.531\n    educ              0.182\n    income            0.220\n    interpers         0.315\n    job               0.451\n    Stress            0.032\n    Depression        0.556\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    ab                0.134    0.035    3.825    0.000\n\n\n\n\nlavaan::standardizedSolution(shenSR) |&gt; print(nd = 2)\n\n             lhs op           rhs label est.std   se      z pvalue ci.lower\n1  Acculturation =~      acculscl          0.95 0.04  21.43   0.00     0.86\n2  Acculturation =~        status          0.47 0.03  13.65   0.00     0.40\n3  Acculturation =~       percent          0.73 0.04  19.64   0.00     0.66\n4         status ~~       percent          0.33 0.04   7.83   0.00     0.25\n5            SES =~          educ          0.43 0.05   8.82   0.00     0.33\n6            SES =~        income          0.47 0.05   9.31   0.00     0.37\n7         Stress =~     interpers          0.56 0.03  17.97   0.00     0.50\n8         Stress =~           job          0.67 0.03  21.10   0.00     0.61\n9     Depression =~        scl90d          0.95 0.00 397.85   0.00     0.94\n10        scl90d ~~        scl90d          0.10 0.00  22.23   0.00     0.09\n11        Stress  ~ Acculturation     a    0.18 0.04   4.20   0.00     0.10\n12    Depression  ~           SES         -0.23 0.05  -4.97   0.00    -0.32\n13    Depression  ~        Stress     b    0.73 0.03  21.09   0.00     0.66\n14      acculscl ~~      acculscl          0.10 0.08   1.23   0.22    -0.06\n15        status ~~        status          0.78 0.03  24.69   0.00     0.72\n16       percent ~~       percent          0.47 0.05   8.68   0.00     0.36\n17          educ ~~          educ          0.82 0.04  19.83   0.00     0.74\n18        income ~~        income          0.78 0.05  16.45   0.00     0.69\n19     interpers ~~     interpers          0.68 0.04  19.51   0.00     0.62\n20           job ~~           job          0.55 0.04  12.83   0.00     0.46\n21 Acculturation ~~ Acculturation          1.00 0.00     NA     NA     1.00\n22           SES ~~           SES          1.00 0.00     NA     NA     1.00\n23        Stress ~~        Stress          0.97 0.02  63.77   0.00     0.94\n24    Depression ~~    Depression          0.44 0.05   8.47   0.00     0.34\n25 Acculturation ~~           SES          0.51 0.06   9.00   0.00     0.40\n26            ab :=           a*b    ab    0.13 0.03   4.01   0.00     0.07\n   ci.upper\n1      1.03\n2      0.53\n3      0.80\n4      0.41\n5      0.52\n6      0.57\n7      0.62\n8      0.73\n9      0.95\n10     0.11\n11     0.26\n12    -0.14\n13     0.80\n14     0.27\n15     0.85\n16     0.58\n17     0.90\n18     0.87\n19     0.75\n20     0.63\n21     1.00\n22     1.00\n23     1.00\n24     0.55\n25     0.62\n26     0.19\n\n\n\n# predicted covariances\nlavaan::fitted(shenSR) |&gt; print()\n\n# predicted correlation matrix for indicators\nlavaan::lavInspect(shenSR, \"cor.ov\")\n\n# predicted covariance matrix for factors\nlavaan::lavInspect(shenSR, \"cov.lv\")\n\n# predicted correlation matrix for factors\nlavaan::lavInspect(shenSR, \"cor.lv\")\n\n\n# residuals\nlavaan::residuals(shenSR, type = \"raw\")\nlavaan::residuals(shenSR, type = \"standardized.mplus\")\nlavaan::residuals(shenSR, type = \"cor.bollen\")\n\n\n# calculate factor reliability coefficients (semTools)\n# note that semTools calculates AVE based on the unstandardized\n# solution, not the standardized solution, so the result for\n# AVE below will not match those in the composite models chapter,\n# which are based on the standardized solution\n\nsemTools::reliability(shenSR) |&gt; print()\n\n       Acculturation       SES    Stress\nalpha      0.7684442 0.3189834 0.5443112\nomega      0.7397905 0.3351805 0.5591078\nomega2     0.7397905 0.3351805 0.5591078\nomega3     0.7400098 0.3380762 0.5580098\navevar     0.5751605 0.2021933 0.3954391",
    "crumbs": [
      "Kline's",
      "Composite",
      "Reflective"
    ]
  },
  {
    "objectID": "contents/Kline/composite.html",
    "href": "contents/Kline/composite.html",
    "title": "Chapter 13. Multiple-Indicator Measurement",
    "section": "",
    "text": "Load libraries\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)",
    "crumbs": [
      "Kline's",
      "Composite"
    ]
  },
  {
    "objectID": "contents/Kline/composite.html#reflective-measurement-and-effect-indicators",
    "href": "contents/Kline/composite.html#reflective-measurement-and-effect-indicators",
    "title": "Chapter 13. Multiple-Indicator Measurement",
    "section": "Reflective Measurement and Effect Indicators",
    "text": "Reflective Measurement and Effect Indicators\n\n\nFIGURE 13.2. Measurement models for a set of three indicators: reflective measurement with effect indicators (a), causal–for‑ mative measurement with causal indicators (b), and composite (composite–formative) measurement with composite indicators (c). L, latent; M, manifest; C, composite.\nSource: Fig 13.2 (p. 219)",
    "crumbs": [
      "Kline's",
      "Composite"
    ]
  },
  {
    "objectID": "contents/Kline/kchap16-2.html",
    "href": "contents/Kline/kchap16-2.html",
    "title": "Chapter 16. Composite Models",
    "section": "",
    "text": "Load libraries\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)",
    "crumbs": [
      "Kline's",
      "Composite",
      "Composite: Formative"
    ]
  },
  {
    "objectID": "contents/Kline/kchap16-2.html#formative-measurement",
    "href": "contents/Kline/kchap16-2.html#formative-measurement",
    "title": "Chapter 16. Composite Models",
    "section": "Formative measurement",
    "text": "Formative measurement\n\nZero-error variance model\npartially reduced form model for SES only\n\n\n\nFIGURE 16.2. Structural regression models with causal indicators for a latent socioeconomic status composite with a disturbance (a). Zero‐error variance model with composite indicators for an SES composite with no disturbance (b). A partially reduced form model with no SES composite (c). Model (a) is not identified, models (b) and (c) are equivalent. (p. 292)\n\n\n# input the correlations in lower diagnonal form\nshenLower.cor &lt;- '\n1.00\n .44 1.00\n .69  .54 1.00\n .21  .08  .16 1.00\n .23  .15  .19  .19 1.00\n .12  .08  .08  .08 -.03 1.00\n .09  .06  .04  .01 -.02  .38 1.00\n .03  .02 -.02 -.07 -.11  .37  .46 1.00 '\n\n# name the variables and convert to full correlation matrix\nshen.cor &lt;- lavaan::getCov(shenLower.cor, names = c(\"acculscl\", \"status\",\n \"percent\", \"educ\", \"income\", \"interpers\", \"job\", \"scl90d\"))\n\n# add the standard deviations and convert to covariances\nshen.cov &lt;- cor2cov(shen.cor, sds = c(3.60,3.30,2.45,3.27,3.44,2.99,\n 3.58,3.70))\n\n\n# display correlations and covariances\nshen.cor |&gt; print()\n\n          acculscl status percent  educ income interpers   job scl90d\nacculscl      1.00   0.44    0.69  0.21   0.23      0.12  0.09   0.03\nstatus        0.44   1.00    0.54  0.08   0.15      0.08  0.06   0.02\npercent       0.69   0.54    1.00  0.16   0.19      0.08  0.04  -0.02\neduc          0.21   0.08    0.16  1.00   0.19      0.08  0.01  -0.07\nincome        0.23   0.15    0.19  0.19   1.00     -0.03 -0.02  -0.11\ninterpers     0.12   0.08    0.08  0.08  -0.03      1.00  0.38   0.37\njob           0.09   0.06    0.04  0.01  -0.02      0.38  1.00   0.46\nscl90d        0.03   0.02   -0.02 -0.07  -0.11      0.37  0.46   1.00\n\n\n\nshen.cov |&gt; round(2) |&gt; print()\n\n          acculscl status percent  educ income interpers   job scl90d\nacculscl     12.96   5.23    6.09  2.47   2.85      1.29  1.16   0.40\nstatus        5.23  10.89    4.37  0.86   1.70      0.79  0.71   0.24\npercent       6.09   4.37    6.00  1.28   1.60      0.59  0.35  -0.18\neduc          2.47   0.86    1.28 10.69   2.14      0.78  0.12  -0.85\nincome        2.85   1.70    1.60  2.14  11.83     -0.31 -0.25  -1.40\ninterpers     1.29   0.79    0.59  0.78  -0.31      8.94  4.07   4.09\njob           1.16   0.71    0.35  0.12  -0.25      4.07 12.82   6.09\nscl90d        0.40   0.24   -0.18 -0.85  -1.40      4.09  6.09  13.69\n\n\n\nFor the single indicator latent\n\nscl90d score reliability (alpha) is .90 from derogatis et al. (1976)\nsample variance is 3.70**2 = 13.690\nerror variance fixed to (1 - .90) * 13.690 = 1.369\n\n\n\nSpecify figure 16.2(b)\nzero error variance model\n\n# ses with composite indicators (educ, income)\n# educ, income each covary with acculturation factor\n\n# special lavaan syntax for composite indicators\n# \"&lt;~\" defines a composite where the disturbance\n# variance is automatically fixed to zero\n# composite is explicitly scaled with an ULI constraint\n\nshenSEScomposite.model &lt;- '\n# ses composite\n SES &lt;~ 1*educ + income\n\n# reflective factors\n Acculturation =~ acculscl + status + percent\n status ~~ percent\n Stress =~ interpers + job\n Depression =~ scl90d\n scl90d ~~ 1.369*scl90d\n\n# covariances among exogenous acculturation factor\n# and composite indicators of ses explicitly defined\n# as free parameters\n Acculturation ~~ income + educ\n income ~~ educ\n\n# covariance between ses composite and acculturation\n# factor fixed to zero\n Acculturation ~~ 0*SES\n\n# structural model\n Stress ~ Acculturation\n\n# depression regressed on ses composite and\n# reflective stress factor\n# coefficient for SES labeled for calculation of\n# indirect effects through the SES composite\n Depression ~ b*SES + Stress '\n\n\n\nSpecify figure 16.2(c)\npartially reduced form model with no SES composite\n\n# specify figure 16.2(c)\n# partially reduced form model with\n# no SES composite\n\nshenSESnoComposite.model &lt;- \"\n  # reflective factors\n  Acculturation =~ acculscl + status + percent\n  status ~~ percent\n  Stress =~ interpers + job\n  Depression =~ scl90d\n  scl90d ~~ 1.369*scl90d\n\n  # covariances among exogenous acculturation factor\n  # and the measured exogenous variables income and education\n  # explicitly declared as free parameters\n  Acculturation ~~ income + educ\n  income ~~ educ\n\n  # structural model\n  Stress ~ Acculturation\n\n  # depression regressed on income, educ, and\n  # reflective stress factor\n  Depression ~ educ + income + Stress \"\n\n\n# fit figure 16.2(b) to data\nshenSEScomposite &lt;- lavaan::sem(shenSEScomposite.model, sample.cov = shen.cov,\n sample.nobs = 983, fixed.x=FALSE)\n\n # fit figure 16.2(c) to data\nshenSESnoComposite &lt;- lavaan::sem(shenSESnoComposite.model, sample.cov = shen.cov, sample.nobs=983, fixed.x = FALSE)\n\n\n\nGlobal fit statistics for both models\nwhich are equivalent\n\n# define fit statistics for later comparison display\nfit.stats &lt;- c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"srmr\", \"rmsea\", \"rmsea.ci.lower\", \"rmsea.ci.upper\")\n\n\n# figure 16.2(b)\nlavaan::fitMeasures(shenSEScomposite, fit.stats) |&gt; print()\n\n         chisq             df         pvalue            cfi           srmr \n        22.294         15.000          0.100          0.995          0.024 \n         rmsea rmsea.ci.lower rmsea.ci.upper \n         0.022          0.000          0.040 \n\n\n\n# figure 16.2(c)\nlavaan::fitMeasures(shenSESnoComposite, fit.stats) |&gt; print()\n\n         chisq             df         pvalue            cfi           srmr \n        22.294         15.000          0.100          0.995          0.024 \n         rmsea rmsea.ci.lower rmsea.ci.upper \n         0.022          0.000          0.040 \n\n\n\n\nResiduals for both models\nwhich are also equal\n\n# figure 16.2(b)\nlavaan::residuals(shenSEScomposite, type = \"raw\") |&gt; print()\nlavaan::residuals(shenSEScomposite, type = \"standardized.mplus\") |&gt; print()\nlavaan::residuals(shenSEScomposite, type = \"cor.bollen\") |&gt; print()\n\n$type\n[1] \"raw\"\n\n$cov\n          acclsc status percnt intrpr    job scl90d   educ income\nacculscl   0.000                                                 \nstatus    -0.008  0.000                                          \npercent    0.000  0.000  0.000                                   \ninterpers  0.491  0.434  0.173  0.000                            \njob       -0.005  0.192 -0.250 -0.042  0.000                     \nscl90d    -0.264 -0.050 -0.523 -0.018  0.105  0.042              \neduc       0.001 -0.233  0.008  0.614 -0.127  0.129  0.000       \nincome    -0.026  0.427  0.119 -0.503 -0.530 -0.343  0.000  0.000\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n          acclsc status percnt intrpr    job scl90d   educ income\nacculscl   0.000                                                 \nstatus    -3.815  0.000                                          \npercent    0.144  0.000  0.000                                   \ninterpers  1.975  1.473  0.887  0.000                            \njob       -0.024  0.564 -1.198 -1.896  0.000                     \nscl90d    -1.123 -0.142 -2.354 -0.303  1.424  0.567              \neduc       0.042 -0.838  0.062  1.999 -0.349  0.687  0.000       \nincome    -1.255  1.477  1.008 -1.565 -1.391 -1.724  0.000  0.000\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n          acclsc status percnt intrpr    job scl90d   educ income\nacculscl   0.000                                                 \nstatus    -0.001  0.000                                          \npercent    0.000  0.000  0.000                                   \ninterpers  0.046  0.044  0.024  0.000                            \njob        0.000  0.016 -0.029 -0.004  0.000                     \nscl90d    -0.020 -0.004 -0.058 -0.002  0.007  0.000              \neduc       0.000 -0.022  0.001  0.063 -0.011  0.011  0.000       \nincome    -0.002  0.038  0.014 -0.049 -0.043 -0.027  0.000  0.000\n\n\n\n\n# figure 16.2(c)\nlavaan::residuals(shenSESnoComposite, type = \"raw\") |&gt; print()\nlavaan::residuals(shenSESnoComposite, type = \"standardized.mplus\") |&gt; print()\nlavaan::residuals(shenSESnoComposite, type = \"cor.bollen\") |&gt; print()\n\n$type\n[1] \"raw\"\n\n$cov\n          acclsc status percnt intrpr    job scl90d   educ income\nacculscl   0.000                                                 \nstatus    -0.008  0.000                                          \npercent    0.000  0.000  0.000                                   \ninterpers  0.491  0.434  0.173  0.000                            \njob       -0.005  0.192 -0.250 -0.042  0.000                     \nscl90d    -0.264 -0.050 -0.523 -0.018  0.105  0.042              \neduc       0.001 -0.233  0.008  0.614 -0.127  0.129  0.000       \nincome    -0.026  0.427  0.119 -0.503 -0.530 -0.343  0.000  0.000\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n          acclsc status percnt intrpr    job scl90d   educ income\nacculscl   0.000                                                 \nstatus    -3.860  0.000                                          \npercent    0.144  0.000  0.000                                   \ninterpers  1.975  1.473  0.887  0.000                            \njob       -0.024  0.564 -1.198 -1.897  0.000                     \nscl90d    -1.123 -0.142 -2.354 -0.303  1.424  0.567              \neduc       0.042 -0.838  0.062  1.999 -0.349  0.687  0.000       \nincome    -1.255  1.477  1.008 -1.565 -1.391 -1.724  0.000  0.000\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n          acclsc status percnt intrpr    job scl90d   educ income\nacculscl   0.000                                                 \nstatus    -0.001  0.000                                          \npercent    0.000  0.000  0.000                                   \ninterpers  0.046  0.044  0.024  0.000                            \njob        0.000  0.016 -0.029 -0.004  0.000                     \nscl90d    -0.020 -0.004 -0.058 -0.002  0.007  0.000              \neduc       0.000 -0.022  0.001  0.063 -0.011  0.011  0.000       \nincome    -0.002  0.038  0.014 -0.049 -0.043 -0.027  0.000  0.000\n\n\n\n\n\nParameter estimates and residuals\n\n# figure 16.2(b)\nlavaan::summary(shenSEScomposite, header = FALSE, fit.measures = FALSE,\n standardized = TRUE, rsquare = TRUE) |&gt; print()\n\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Acculturation =~                                                      \n    acculscl          1.000                               3.434    0.954\n    status            0.444    0.050    8.784    0.000    1.523    0.462\n    percent           0.516    0.051   10.070    0.000    1.771    0.723\n  Stress =~                                                             \n    interpers         1.000                               1.679    0.562\n    job               1.456    0.125   11.655    0.000    2.445    0.683\n  Depression =~                                                         \n    scl90d            1.000                               3.502    0.948\n\nComposites:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SES &lt;~                                                                \n    educ              1.000                               0.192    0.627\n    income            1.013    0.501    2.023    0.043    0.194    0.669\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Stress ~                                                              \n    Acculturtn        0.068    0.021    3.241    0.001    0.139    0.139\n  Depression ~                                                          \n    SES        (b)   -0.095    0.032   -3.014    0.003   -0.141   -0.141\n    Stress            1.469    0.126   11.704    0.000    0.704    0.704\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .status ~~                                                             \n   .percent           1.665    0.307    5.424    0.000    1.665    0.336\n  Acculturation ~~                                                      \n    income            2.871    0.404    7.101    0.000    0.836    0.243\n    educ              2.469    0.382    6.455    0.000    0.719    0.220\n  educ ~~                                                               \n    income            2.135    0.365    5.852    0.000    2.135    0.190\n  SES ~~                                                                \n    Acculturation     0.000                                 NaN      NaN\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .scl90d            1.369                               1.369    0.100\n   .acculscl          1.157    1.115    1.038    0.299    1.157    0.089\n   .status            8.559    0.451   18.997    0.000    8.559    0.787\n   .percent           2.862    0.323    8.856    0.000    2.862    0.477\n   .interpers         6.112    0.357   17.099    0.000    6.112    0.684\n   .job               6.823    0.569   11.988    0.000    6.823    0.533\n    educ             10.682    0.482   22.170    0.000   10.682    1.000\n    income           11.822    0.533   22.170    0.000   11.822    1.000\n    SES               0.000                               0.000    0.000\n    Acculturation    11.790    1.256    9.386    0.000    1.000    1.000\n   .Stress            2.765    0.365    7.574    0.000    0.981    0.981\n   .Depression        6.036    0.591   10.210    0.000    0.492    0.492\n\nR-Square:\n                   Estimate\n    scl90d            0.900\n    acculscl          0.911\n    status            0.213\n    percent           0.523\n    interpers         0.316\n    job               0.467\n    Stress            0.019\n    Depression        0.508\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nDefine a customized plot function using semPlot::semPaths()\nsemPaths2 &lt;- function(model, what = 'est', layout = \"tree2\", rotation = 2) {\n  semPlot::semPaths(model, what = what, edge.label.cex = 1, edge.color = \"black\", layout = layout, rotation = rotation, weighted = FALSE, asize = 2, label.cex = 1, node.width = 1.2)\n}\n\n\n\n# semPaths2: a customized plot function using semPlot::semPaths()\nsemPaths2(shenSEScomposite, layout = \"tree2\", rotation = 2)\n\n\n\n\n\n\n\n\n\n# figure 16.2(c)\nlavaan::summary(shenSESnoComposite, header = FALSE, fit.measures = FALSE, \n standardized = TRUE, rsquare = TRUE) |&gt; print()\n\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Acculturation =~                                                      \n    acculscl          1.000                               3.434    0.954\n    status            0.444    0.050    8.784    0.000    1.523    0.462\n    percent           0.516    0.051   10.070    0.000    1.771    0.723\n  Stress =~                                                             \n    interpers         1.000                               1.679    0.562\n    job               1.456    0.125   11.655    0.000    2.445    0.683\n  Depression =~                                                         \n    scl90d            1.000                               3.502    0.948\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Stress ~                                                              \n    Acculturation     0.068    0.021    3.241    0.001    0.139    0.139\n  Depression ~                                                          \n    educ             -0.095    0.032   -3.014    0.003   -0.027   -0.089\n    income           -0.096    0.030   -3.209    0.001   -0.027   -0.095\n    Stress            1.469    0.126   11.704    0.000    0.704    0.704\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .status ~~                                                             \n   .percent           1.665    0.307    5.424    0.000    1.665    0.336\n  Acculturation ~~                                                      \n    income            2.871    0.404    7.101    0.000    0.836    0.243\n    educ              2.469    0.382    6.455    0.000    0.719    0.220\n  educ ~~                                                               \n    income            2.135    0.365    5.852    0.000    2.135    0.190\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .scl90d            1.369                               1.369    0.100\n   .acculscl          1.157    1.115    1.038    0.299    1.157    0.089\n   .status            8.559    0.451   18.997    0.000    8.559    0.787\n   .percent           2.862    0.323    8.856    0.000    2.862    0.477\n   .interpers         6.112    0.357   17.099    0.000    6.112    0.684\n   .job               6.823    0.569   11.988    0.000    6.823    0.533\n    educ             10.682    0.482   22.170    0.000   10.682    1.000\n    income           11.822    0.533   22.170    0.000   11.822    1.000\n    Acculturation    11.790    1.256    9.386    0.000    1.000    1.000\n   .Stress            2.765    0.365    7.574    0.000    0.981    0.981\n   .Depression        6.036    0.591   10.210    0.000    0.492    0.492\n\nR-Square:\n                   Estimate\n    scl90d            0.900\n    acculscl          0.911\n    status            0.213\n    percent           0.523\n    interpers         0.316\n    job               0.467\n    Stress            0.019\n    Depression        0.508\n\n\n\n\nsemPaths2(shenSESnoComposite, layout = \"tree\", rotation = 2)\n\n\n\n\n\n\n\n\n\n\nCompare estimates\n\ndf1 &lt;- parameterEstimates(shenSEScomposite) |&gt; \n  mutate(est = est |&gt; round(3)) |&gt; \n  select(lhs, op, rhs, est)\ndf2 &lt;- parameterEstimates(shenSESnoComposite) |&gt; \n  mutate(est = est |&gt; round(3)) |&gt; \n  select(lhs, op, rhs, est)\n\n# join df1 and df2\nfull_join(df1, df2, by = c(\"lhs\", \"op\", \"rhs\"), suffix = c(\".composite\", \".noComposite\")) |&gt; print()\n\n             lhs op           rhs est.composite est.noComposite\n1            SES &lt;~          educ         1.000              NA\n2            SES &lt;~        income         1.013              NA\n3  Acculturation =~      acculscl         1.000           1.000\n4  Acculturation =~        status         0.444           0.444\n5  Acculturation =~       percent         0.516           0.516\n6         status ~~       percent         1.665           1.665\n7         Stress =~     interpers         1.000           1.000\n8         Stress =~           job         1.456           1.456\n9     Depression =~        scl90d         1.000           1.000\n10        scl90d ~~        scl90d         1.369           1.369\n11 Acculturation ~~        income         2.871           2.871\n12 Acculturation ~~          educ         2.469           2.469\n13          educ ~~        income         2.135           2.135\n14           SES ~~ Acculturation         0.000              NA\n15        Stress  ~ Acculturation         0.068           0.068\n16    Depression  ~           SES        -0.095              NA\n17    Depression  ~        Stress         1.469           1.469\n18      acculscl ~~      acculscl         1.157           1.157\n19        status ~~        status         8.559           8.559\n20       percent ~~       percent         2.862           2.862\n21     interpers ~~     interpers         6.112           6.112\n22           job ~~           job         6.823           6.823\n23          educ ~~          educ        10.682          10.682\n24        income ~~        income        11.822          11.822\n25           SES ~~           SES         0.000              NA\n26 Acculturation ~~ Acculturation        11.790          11.790\n27        Stress ~~        Stress         2.765           2.765\n28    Depression ~~    Depression         6.036           6.036\n29    Depression  ~          educ            NA          -0.095\n30    Depression  ~        income            NA          -0.096",
    "crumbs": [
      "Kline's",
      "Composite",
      "Composite: Formative"
    ]
  },
  {
    "objectID": "contents/Kline/kchap16-4.html",
    "href": "contents/Kline/kchap16-4.html",
    "title": "Chapter 16. Composite Models",
    "section": "",
    "text": "Load libraries\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(cSEM)\nlibrary(psych)",
    "crumbs": [
      "Kline's",
      "Composite",
      "Composite: HO"
    ]
  },
  {
    "objectID": "contents/Kline/kchap16-4.html#henselerogasawara-ho-specification-and-ml-analysis",
    "href": "contents/Kline/kchap16-4.html#henselerogasawara-ho-specification-and-ml-analysis",
    "title": "Chapter 16. Composite Models",
    "section": "Henseler–Ogasawara (HO) specification and ML analysis",
    "text": "Henseler–Ogasawara (HO) specification and ML analysis\n\nYu, X., Schuberth, F., & Henseler, J. (2023). Specifying composites in structural equation modeling: A refinement of the Henseler–Ogasawara specification. Statistical Analysis and Data Mining: The ASA Data Science Journal, 16(4), 348-357.\nSchuberth, F. (2021). The Henseler-Ogasawara specification of composites in structural equation modeling: A tutorial. Psychological Methods.\n\n\n\nSource: Fig 16.4 (p. 302)\n\n\n# input the correlations in lower diagnonal form\nshenLower.cor &lt;- '\n 1.00\n  .44 1.00\n  .69  .54 1.00\n  .21  .08  .16 1.00\n  .23  .15  .19  .19 1.00\n  .12  .08  .08  .08 -.03 1.00\n  .09  .06  .04  .01 -.02  .38 1.00\n  .03  .02 -.02 -.07 -.11  .37  .46 1.00 '\n\n# name the variables and convert to full correlation matrix\nshen.cor &lt;- lavaan::getCov(shenLower.cor, names = c(\"acculscl\", \"status\",\n \"percent\", \"educ\", \"income\", \"interpers\", \"job\", \"scl90d\"))\n \n# add the standard deviations and convert to covariances\nshen.cov &lt;- lavaan::cor2cov(shen.cor, sds = c(3.60,3.30,2.45,3.27,3.44,2.99,\n 3.58,3.70))\n\n\n# display correlations and covariances\nshen.cor |&gt; print()\n\n          acculscl status percent  educ income interpers   job scl90d\nacculscl      1.00   0.44    0.69  0.21   0.23      0.12  0.09   0.03\nstatus        0.44   1.00    0.54  0.08   0.15      0.08  0.06   0.02\npercent       0.69   0.54    1.00  0.16   0.19      0.08  0.04  -0.02\neduc          0.21   0.08    0.16  1.00   0.19      0.08  0.01  -0.07\nincome        0.23   0.15    0.19  0.19   1.00     -0.03 -0.02  -0.11\ninterpers     0.12   0.08    0.08  0.08  -0.03      1.00  0.38   0.37\njob           0.09   0.06    0.04  0.01  -0.02      0.38  1.00   0.46\nscl90d        0.03   0.02   -0.02 -0.07  -0.11      0.37  0.46   1.00\n\n\n\nshen.cov |&gt; print(digits = 2)\n\n          acculscl status percent  educ income interpers   job scl90d\nacculscl      13.0   5.23    6.09  2.47   2.85      1.29  1.16   0.40\nstatus         5.2  10.89    4.37  0.86   1.70      0.79  0.71   0.24\npercent        6.1   4.37    6.00  1.28   1.60      0.59  0.35  -0.18\neduc           2.5   0.86    1.28 10.69   2.14      0.78  0.12  -0.85\nincome         2.8   1.70    1.60  2.14  11.83     -0.31 -0.25  -1.40\ninterpers      1.3   0.79    0.59  0.78  -0.31      8.94  4.07   4.09\njob            1.2   0.71    0.35  0.12  -0.25      4.07 12.82   6.09\nscl90d         0.4   0.24   -0.18 -0.85  -1.40      4.09  6.09  13.69\n\n\n\nHO specification\n\nexcrescent variables, ex1 to ex4 error variances of all indicators fixed to zero\nstart values of zero are needed for two indicators of the acculturation composite for estimation to normally converge\n\n\nshenHO.model &lt;- '\n  # composites, excrescent variables, and indicators\n  # acculturation\n  Acculturation =~ acculscl+ start(0)*percent+ start(0)*status \n  ex1 =~ percent + acculscl + status\n  ex2 =~ status + acculscl + 0*percent\n  acculscl ~~ 0*acculscl\n    percent ~~ 0*percent\n  status ~~ 0*status\n\n  # SES\n  SES =~ educ + income\n  ex4 =~ income + educ\n  educ ~~ 0*educ\n  income ~~ 0*income\n  \n  # stress\n  Stress =~ interpers + job\n  ex3 =~ job + interpers\n  interpers ~~ 0*interpers\n  job ~~ 0*job\n  \n  # depression, fix indicator error variance\n  # to 1.369\n  Depression =~ scl90d\n  scl90d ~~ 1.369*scl90d \n  \n  # covariance between SES and acculturation\n  # is a free parameter\n  Acculturation ~~ SES\n  \n  # constrain covariances to zero\n  ex1 ~~ 0*Acculturation + 0*Stress + 0*SES + 0*Depression\n    + 0*ex2 + 0*ex3 + 0*ex4\n  ex2 ~~ 0*Acculturation + 0*Stress + 0*SES + 0*Depression\n    + 0*ex3 + 0*ex4\n  ex3 ~~ 0*Acculturation + 0*Stress + 0*SES + 0*Depression\n    + 0*ex4\n  ex4 ~~ 0*Acculturation + 0*Stress + 0*SES + 0*Depression\n  \n  # structural model\n  Stress ~ a*Acculturation\n  Depression ~ b*Stress + SES\n  \n  # define indirect effect\n  ab := a * b\n'\n\n\n# fit ho-specified model to data\n# estimates for excrescent variables have no interpretive value\n \nshenHO &lt;- lavaan::sem(model = shenHO.model, sample.cov = shen.cov,\n sample.nobs = 983)  \nlavaan::summary(shenHO, standardized = TRUE, fit.measures = TRUE,\n rsquare = TRUE) |&gt; print()\n\nlavaan 0.6.17 ended normally after 245 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n  Number of observations                           983\n\nModel Test User Model:\n                                                      \n  Test statistic                                19.334\n  Degrees of freedom                                15\n  P-value (Chi-square)                           0.199\n\nModel Test Baseline Model:\n\n  Test statistic                              1606.002\n  Degrees of freedom                                28\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.997\n  Tucker-Lewis Index (TLI)                       0.995\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -19670.378\n  Loglikelihood unrestricted model (H1)     -19660.711\n                                                      \n  Akaike (AIC)                               39382.757\n  Bayesian (BIC)                             39485.459\n  Sample-size adjusted Bayesian (SABIC)      39418.763\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.017\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.037\n  P-value H_0: RMSEA &lt;= 0.050                    0.999\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.022\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Acculturation =~                                                      \n    acculscl          1.000                               3.564    0.991\n    percent           0.519    0.051   10.074    0.000    1.849    0.755\n    status            0.507    0.086    5.870    0.000    1.805    0.547\n  ex1 =~                                                                \n    percent           1.000                               1.606    0.656\n    acculscl         -0.198    0.241   -0.820    0.412   -0.318   -0.088\n    status            0.397    0.207    1.920    0.055    0.637    0.193\n  ex2 =~                                                                \n    status            1.000                               2.686    0.814\n    acculscl         -0.140    0.149   -0.941    0.346   -0.376   -0.105\n    percent           0.000                               0.000    0.000\n  SES =~                                                                \n    educ              1.000                               2.379    0.728\n    income            1.173    0.196    5.969    0.000    2.790    0.811\n  ex4 =~                                                                \n    income            1.000                               2.009    0.584\n    educ             -1.115    0.261   -4.268    0.000   -2.241   -0.686\n  Stress =~                                                             \n    interpers         1.000                               2.233    0.747\n    job               1.440    0.122   11.798    0.000    3.216    0.899\n  ex3 =~                                                                \n    job               1.000                               1.570    0.439\n    interpers        -1.265    0.211   -5.989    0.000   -1.986   -0.665\n  Depression =~                                                         \n    scl90d            1.000                               3.501    0.948\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Stress ~                                                              \n    Acculturtn (a)    0.076    0.020    3.768    0.000    0.122    0.122\n  Depression ~                                                          \n    Stress     (b)    0.839    0.064   13.034    0.000    0.535    0.535\n    SES              -0.190    0.046   -4.119    0.000   -0.129   -0.129\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Acculturation ~~                                                      \n    SES               2.447    0.367    6.674    0.000    0.289    0.289\n    ex1               0.000                               0.000    0.000\n  ex1 ~~                                                                \n   .Stress            0.000                               0.000    0.000\n    SES               0.000                               0.000    0.000\n   .Depression        0.000                               0.000    0.000\n    ex2               0.000                               0.000    0.000\n    ex3               0.000                               0.000    0.000\n    ex4               0.000                               0.000    0.000\n  Acculturation ~~                                                      \n    ex2               0.000                               0.000    0.000\n  ex2 ~~                                                                \n   .Stress            0.000                               0.000    0.000\n    SES               0.000                               0.000    0.000\n   .Depression        0.000                               0.000    0.000\n    ex3               0.000                               0.000    0.000\n    ex4               0.000                               0.000    0.000\n  Acculturation ~~                                                      \n    ex3               0.000                               0.000    0.000\n .Stress ~~                                                             \n    ex3               0.000                               0.000    0.000\n  SES ~~                                                                \n    ex3               0.000                               0.000    0.000\n  ex3 ~~                                                                \n   .Depression        0.000                               0.000    0.000\n  ex4 ~~                                                                \n    ex3               0.000                               0.000    0.000\n  Acculturation ~~                                                      \n    ex4               0.000                               0.000    0.000\n  ex4 ~~                                                                \n   .Stress            0.000                               0.000    0.000\n  SES ~~                                                                \n    ex4               0.000                               0.000    0.000\n  ex4 ~~                                                                \n   .Depression        0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .acculscl          0.000                               0.000    0.000\n   .percent           0.000                               0.000    0.000\n   .status            0.000                               0.000    0.000\n   .educ              0.000                               0.000    0.000\n   .income            0.000                               0.000    0.000\n   .interpers         0.000                               0.000    0.000\n   .job               0.000                               0.000    0.000\n   .scl90d            1.369                               1.369    0.100\n    Acculturation    12.704    0.678   18.746    0.000    1.000    1.000\n    ex1               2.578    0.593    4.346    0.000    1.000    1.000\n    ex2               7.213    0.651   11.079    0.000    1.000    1.000\n    SES               5.660    1.105    5.123    0.000    1.000    1.000\n    ex4               4.037    1.087    3.712    0.000    1.000    1.000\n   .Stress            4.912    0.582    8.447    0.000    0.985    0.985\n    ex3               2.463    0.543    4.540    0.000    1.000    1.000\n   .Depression        8.603    0.450   19.126    0.000    0.702    0.702\n\nR-Square:\n                   Estimate\n    acculscl          1.000\n    percent           1.000\n    status            1.000\n    educ              1.000\n    income            1.000\n    interpers         1.000\n    job               1.000\n    scl90d            0.900\n    Stress            0.015\n    Depression        0.298\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    ab                0.064    0.017    3.770    0.000    0.065    0.065\n\n\n\n\n# predicted covariances\nlavaan::fitted(shenHO)\n\n# predicted correlations\nlavaan::lavInspect(shenHO, \"cor.lv\")\nlavaan::lavInspect(shenHO, \"cor.ov\")\n\n# residuals\nlavaan::residuals(shenHO, type = \"raw\")\nlavaan::residuals(shenHO, type = \"standardized.mplus\")\nlavaan::residuals(shenHO, type = \"cor.bollen\")",
    "crumbs": [
      "Kline's",
      "Composite",
      "Composite: HO"
    ]
  },
  {
    "objectID": "contents/chap12.html",
    "href": "contents/chap12.html",
    "title": "Chapter 12. Path Modeling",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)",
    "crumbs": [
      "Keith's",
      "Path Modeling"
    ]
  },
  {
    "objectID": "contents/chap12.html#연습문제",
    "href": "contents/chap12.html#연습문제",
    "title": "Chapter 12. Path Modeling",
    "section": "연습문제",
    "text": "연습문제\n\nTable 12.1에서 familiy background에 대해서도 동일한 분석을 수행해보세요.\n예전 salary 데이터에 대해서도 동일한 분석을 수행해보세요; 링크",
    "crumbs": [
      "Keith's",
      "Path Modeling"
    ]
  },
  {
    "objectID": "contents/chap16.html",
    "href": "contents/chap16.html",
    "title": "Chapter 15-16. Confirmatory Factor Analysis",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)",
    "crumbs": [
      "Keith's",
      "Factor Analysis"
    ]
  },
  {
    "objectID": "contents/chap16.html#errors",
    "href": "contents/chap16.html#errors",
    "title": "Chapter 15-16. Confirmatory Factor Analysis",
    "section": "Errors",
    "text": "Errors\nLinear model: \\(y = a \\cdot x + b + \\epsilon\\),  \\(\\epsilon\\): errors\n\n\\(E(Y|X=x_i) = a \\cdot x_i + b\\)  (\\(E\\): expectation, 기대값)\n\n\\(x_i\\)에 대해서 conditional mean이 선형함수로 결정됨을 가정\n\n\\(Var(Y|X=x_i) = \\sigma^2\\)\n\\(Y|X=x_i \\sim N(a \\cdot x_i + b, \\sigma^2)\\)\n\nErrors의 소스들:\n\nReducible error: 모형이 잡아내지 못한 신호; 영향을 미치지만 측정하지 않은 변수가 존재\nIrreducible error:\n\n측정 오차 (measurement error): ex. 성별, 젠더, 키, 온도, 강수량, 지능, 불쾌지수, …\nrandom processes: 물리적 세계의 불확실성 (stochastic vs. deterministic world)\n예를 들어, 동전을 4번 던질 때,\n\nH, H, T, H\nH, T, T, H\nT, T, H, H\nT, H, T, H\nT, T, T, H\n…\n\n이들은 보통 Gaussian 분포를 이루거나 가정: ex. 측정 오차들, 동전 앞면의 개수들, 키, 몸무게, IQ, …\n\n그 외에 자연스럽게 나타나는 분포들이 있음; Binomial, Poisson, Exponential, Gamma, Beta, …\n\n\nGaussian/Normal distribution\n\n랜덤한 값들의 합/평균들이 나타내는 분포\n\n측정 오차의 분포(error distribution)\n다양한 힘들의 상호작용으로 인한 분포\n\n분산이 유한한 분포 중에 정보 엔트로피가 최대(maximum entropy)인 분포\n중심극한정리(Central Limit Theorem)\n\n\\(X \\sim N(\\mu, \\sigma^2)\\), density function: \\(\\displaystyle f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\\)\n\\(X \\sim N(0, 1)\\): \\(\\displaystyle f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}x^2}\\), (standard normal distribution)\n예를 들어, 1000명의 사람들이 각자 16번의 동전 던지기를 시행하면서 좌우로 움직인다면,\n\n\n# A tibble: 1,000 x 16\n      V1    V2    V3    V4    V5    V6    V7    V8    V9   V10   V11   V12   V13\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1     1    -1    -1    -1    -1    -1     1    -1    -1    -1    -1    -1\n 2     1     1    -1     1     1     1    -1    -1     1    -1     1    -1     1\n 3    -1    -1     1    -1    -1     1    -1    -1     1    -1    -1    -1    -1\n 4    -1    -1     1    -1    -1     1     1     1    -1    -1     1    -1    -1\n 5     1     1     1     1    -1     1    -1     1     1    -1    -1    -1     1\n 6    -1     1    -1     1    -1    -1     1     1     1     1     1     1     1\n 7    -1     1    -1     1     1     1    -1     1     1     1     1    -1     1\n 8     1    -1    -1     1    -1    -1    -1     1    -1    -1    -1     1     1\n 9     1    -1     1    -1     1     1    -1    -1    -1     1    -1     1     1\n10    -1    -1     1     1    -1     1     1    -1    -1     1    -1     1     1\n# i 990 more rows\n# i 3 more variables: V14 &lt;dbl&gt;, V15 &lt;dbl&gt;, V16 &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nMesaurement errors\nReliability (신뢰도): 측정이 반복될 때, 얼마나 일관성 있는지\nReliability (신뢰도): \\(\\displaystyle \\rho = \\frac{V_{\\text{True}}}{V_{\\text{Observed}}} = \\frac{V_{\\text{True}}}{V_{\\text{True}} + V_{\\text{Error}}}\\)\n\n\nValidity (타당도): 측정이 실제로 측정하고자 하는 것을 얼마나 잘 측정하는지\n\n다면적인 측정을 통해 공통 요인을 추출해 타당도를 확보\n여러 측정도구를 사용해 타당도를 확보\nConvergent validity (수렴타당도): 동일한 구성개념을 측정하는 서로 다른 테스트들이 서로 높은 상관관계를 나타내는가?\nDiscriminant validity (차별타당도): 서로 다른 구성개념을 측정하는 테스트들이 서로 낮은 상관관계를 나타내는가?\n\n\n\n\n\n\n\n다음과 같이 독해력을 3가지로 측정하는 예를 생각하면,\n\n테스트 1: 참가자가 글을 읽고, 해당 글을 가장 잘 설명하는 그림을 4개의 선택지 중에서 선택.\n테스트 2: 참가자가 지시문을 읽고, 그 지시에 따라 행동을 실행 (예: “일어서서 테이블 주위를 걷고, 다시 앉으세요”).\n테스트 3: 참가자가 단어나 문장이 빠진 글을 읽고, 글의 의미를 바탕으로 빠진 단어나 문장을 채움.\n\n추가로 측정하는 능력 각각: unique variance\n\n읽은 내용을 시각적 이미지로 변환하는 능력.\n읽은 내용을 행동으로 옮기는 능력.\n자신의 지식에서 가장 적합한 단어나 문장을 선택해 글에 삽입하는 능력.\n\n\n\n\n\n\nFactor analysis (요인분석): indicators (지표들, manifest variable)의 분산을 공통 분산(common variance)과 고유 분산(unique variance)으로 분해\n\n(Common) Factors (요인): 구성개념을 나타내는 잠재변수에 대한 근사치로서(proxy) indicator들의 공통 분산으로 구성됨.\n고유 분산(unique variance) = 지표들이 갖는 고유/특정 분산(specific variance) + 측정오차(random measurement error)\n\n특정 분산: 공통 요인들로 설명되지 않는 분산\n측정오차: 측정 과정에서 발생하는 오차\n\n각 indicator들은 공통 요인과 고유 분산에 의해 결정/설명된다고 봄.\n\n공통 요인의 값을 구체적으로 구하기 어려움: “factor score indeterminacy”\n보통 요인의 개수를 결정하기 위해 탐색적 요인 분석(exploratory factor analysis, EFA)을 사용하며, 요인의 구조를 파악한 후 이를 확인하는 작업으로 확인적 요인 분석(confirmatory factor analysis, CFA)을 사용하지만, 현실적으로는 이 둘을 확실히 구분하기는 어려움.\n요인에 개수와 구조는 일반적으로 통계적 접근으로만 결정하기 어려움: 이론적 근거와 함께 고려\n\n\n예를 들어, 3-indicator, 1-factor 모형을 보면,\n\\(x_1 = \\lambda_{11} \\xi_1 + \\delta_1\\)\n\\(x_2 = \\lambda_{21} \\xi_1 + \\delta_2\\)\n\\(x_3 = \\lambda_{31} \\xi_1 + \\delta_3\\)\n(회귀계수 \\(\\lambda\\): factor loading, 요인부하량)\n가정: \\(COV(\\xi_1, \\delta_i) = 0\\),  \\(COV(\\delta_i, \\delta_j) = 0\\)\n따라서, \\(\\mathbf{\\Theta_{\\delta}} =\n\\begin{bmatrix}\nV(\\delta_1) \\\\\n0 & V(\\delta_2) \\\\\n0 & 0 & V(\\delta_3)\n\\end{bmatrix}\\)\n\n\n\n\nImplied covariance matrix:\n\\[\n\\Sigma(\\theta) = \\begin{bmatrix}\n\\lambda_{11}^2 V(\\xi_1) + V(\\delta_1)  \\\\\n\\lambda_{21} \\lambda_{11} V(\\xi_1) & \\lambda_{21}^2 V(\\xi_1) + V(\\delta_2) \\\\\n\\lambda_{31} \\lambda_{11} V(\\xi_1) & \\lambda_{31} \\lambda_{21} V(\\xi_1) & \\lambda_{31}^2 V(\\xi_1) + V(\\delta_3)\n\\end{bmatrix}\n\\]\nIdentified되려면, 적어도 한 개의 파라미터를 줄여야 함: 고정하거나 동등하게 설정\n\n\\(\\lambda_{11} = 1\\): unit loading identification (ULI)\n\n잠재변수가 지표와 동일한 단위를 갖게됨: \\(x_1 = 1 \\cdot \\xi_1 + \\delta_1\\)\nHave the same scale in the more limited sense that on average a one-unit shift of \\(\\xi_1\\) leads to a one unit shift in \\(x_1\\)\n잠재변수의 단위(metric)가 의미를 갖고 중요한 관심사인 경우\n\n\\(V(\\xi_1) = 1\\): unit variance identification (UVI)\n\n잠재변수가 표준화됨: 평균 0, 분산 1\n모든 요인들의 부하(factor loading)를 추정할 수 있고,\n공분산이 상관계수의 의미를 갖게됨.\n\n또는 예를 들어, 두 파라미터가 동일하도록 제약: \\(\\lambda_{11} = \\lambda_{21}\\)\n또는 effect coding: \\(\\displaystyle \\frac{\\lambda_{11} + \\lambda_{21} + \\lambda_{31}}{3} = 1\\); 요인의 분산이 모든 지표들에 의해 좀 더 안정적으로 추정됨.\n\n앞서 살펴본 homework와 grade의 관계에 대한 모형을 잠재변수를 통해 살펴보면,",
    "crumbs": [
      "Keith's",
      "Factor Analysis"
    ]
  },
  {
    "objectID": "contents/chap16.html#confirmatory-factor-analysis",
    "href": "contents/chap16.html#confirmatory-factor-analysis",
    "title": "Chapter 15-16. Confirmatory Factor Analysis",
    "section": "Confirmatory Factor Analysis",
    "text": "Confirmatory Factor Analysis\n예제: The Differential Ability Scales, Second Edition (DAS-II; Elliott, 2007)\n\nVerbal reason­ing (Verbal Ability)\nNonverbal, inductive reasoning (Nonverbal Reasoning)\nVisual–spatial reasoning (Spatial)\nShort-term memory (Memory)\n\n\n\n\nLoad the data\ndas2 &lt;- haven::read_sav(\"data/chap 16 CFA 1/das 2 cov.sav\")\ndas2cov &lt;- das2[1:12, 3:14] |&gt;\n  as.matrix() |&gt;\n  lav_matrix_vechr(diagonal = TRUE) |&gt;\n  getCov(names = c(\"wdss\", \"vsss\", \"sqss\", \"soss\", \"rpss\", \"rdss\", \"psss\", \"pcss\", \"nvss\", \"mass\", \"dfss\", \"dbss\"))\n\n\n\n\n\n\n\n\n제약 방식\n\n\n\nUnit-loading identification (ULI)\n\n첫번째 indicator의 loading을 1로 설정: default\n다른 indicator의 loading을 1로 하려면: NA 사용하여 free the first parameter\n\n'f1 =~ NA*x1 + 1*x2'\nUnit-variance identification (UVI)\n\nfactor의 분산을 1로 직접 제약\n\n'f1 =~ NA*x1 + x2 + x3\n f1 ~~ 1*f1'\n\nstandardized solutions: std.lv, std.all, std.nox\n\nstd.lv: latent variable만 표준화, 즉 위와 같이 factor의 분산을 1로 설정\nstd.all: latent와 indicator 모두 표준화\n\nsummary(model, standardized=\"TRUE\")  # 또는 type을 지정\nstandardizedSolution(fit, type=\"...\")\n\nEffect codings\n'f1 =~ a*x1 + b*x2 + c*x3\n  a + b + c == 3'  # constraint\n\n\nDAS-II 모형의 CFA:\n\ndas2_model &lt;- '\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss\n  Spatial =~ pcss + rdss + rpss\n  Memory =~ dfss + dbss + soss\n'\nfit &lt;- sem(das2_model, sample.cov = das2cov, sample.nobs = 800)\nsummary(fit, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 164 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        30\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               127.986\n  Degrees of freedom                                48\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              4335.087\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.981\n  Tucker-Lewis Index (TLI)                       0.974\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33708.567\n  Loglikelihood unrestricted model (H1)     -33644.574\n                                                      \n  Akaike (AIC)                               67477.134\n  Bayesian (BIC)                             67617.673\n  Sample-size adjusted Bayesian (SABIC)      67522.406\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.046\n  90 Percent confidence interval - lower         0.036\n  90 Percent confidence interval - upper         0.055\n  P-value H_0: RMSEA &lt;= 0.050                    0.762\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.027\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              1.000                               7.463    0.739\n    wdss              0.942    0.049   19.266    0.000    7.029    0.733\n    vsss              1.100    0.053   20.900    0.000    8.207    0.805\n  Nonverbal =~                                                          \n    psss              1.000                               5.570    0.541\n    mass              1.306    0.093   14.104    0.000    7.273    0.710\n    sqss              1.406    0.094   15.030    0.000    7.831    0.808\n  Spatial =~                                                            \n    pcss              1.000                               7.499    0.814\n    rdss              0.982    0.047   20.996    0.000    7.365    0.737\n    rpss              0.795    0.049   16.388    0.000    5.961    0.591\n  Memory =~                                                             \n    dfss              1.000                               7.387    0.669\n    dbss              1.035    0.058   17.961    0.000    7.643    0.754\n    soss              1.119    0.061   18.392    0.000    8.265    0.778\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal ~~                                                             \n    Nonverbal        33.464    3.077   10.875    0.000    0.805    0.805\n    Spatial          42.028    3.283   12.801    0.000    0.751    0.751\n    Memory           45.428    3.691   12.309    0.000    0.824    0.824\n  Nonverbal ~~                                                          \n    Spatial          37.267    3.202   11.638    0.000    0.892    0.892\n    Memory           35.230    3.285   10.724    0.000    0.856    0.856\n  Spatial ~~                                                            \n    Memory           44.873    3.539   12.680    0.000    0.810    0.810\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .nvss             46.169    2.923   15.793    0.000   46.169    0.453\n   .wdss             42.471    2.663   15.949    0.000   42.471    0.462\n   .vsss             36.520    2.709   13.482    0.000   36.520    0.352\n   .psss             74.838    4.000   18.711    0.000   74.838    0.707\n   .mass             51.973    3.120   16.657    0.000   51.973    0.496\n   .sqss             32.554    2.475   13.154    0.000   32.554    0.347\n   .pcss             28.663    2.240   12.794    0.000   28.663    0.338\n   .rdss             45.628    2.886   15.808    0.000   45.628    0.457\n   .rpss             66.343    3.650   18.177    0.000   66.343    0.651\n   .dfss             67.275    3.897   17.263    0.000   67.275    0.552\n   .dbss             44.461    2.880   15.440    0.000   44.461    0.432\n   .soss             44.555    3.050   14.609    0.000   44.555    0.395\n    Verbal           55.704    4.882   11.410    0.000    1.000    1.000\n    Nonverbal        31.029    4.002    7.754    0.000    1.000    1.000\n    Spatial          56.231    4.351   12.925    0.000    1.000    1.000\n    Memory           54.573    5.447   10.018    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    nvss              0.547\n    wdss              0.538\n    vsss              0.648\n    psss              0.293\n    mass              0.504\n    sqss              0.653\n    pcss              0.662\n    rdss              0.543\n    rpss              0.349\n    dfss              0.448\n    dbss              0.568\n    soss              0.605\n\n\n\n\n\n\n\n\n\n\nR squared\n\n\n\n이상적으로 요인이 지표들을 50% 이상 설명해 주기를 바람: \\(R^2 &gt; 0.5\\)\n좀 더 느슨하게는 average variance extracted (AVE)가 0.5 이상이면 좋음.\nReliability에 대한 참고: pp. 239-241, Kline(2023)\n\\(\\displaystyle\\omega = \\frac{(\\sum \\lambda_i)^2 \\cdot \\phi}{(\\sum \\lambda_i)^2 \\cdot \\phi + \\sum \\theta_{ii}}\\),   \\(\\phi\\): factor variance,   \\(\\theta_{ii}\\): error variance\nsemTools::AVE(fit)\n# Verbal Nonverbal   Spatial    Memory \n#  0.579     0.477     0.509     0.537 \n\n# Cronbach's alpha: uni-dimensional, tau-equivalent reliabiliy; factor loading을 동일하게 설정/가정\nsemTools::compRelSEM(fit, tau.eq = TRUE, obs.var = TRUE)\n# Verbal Nonverbal   Spatial    Memory \n#  0.804     0.713     0.754     0.776\n\n# Cofficient omega: a model-based alternative; permit covariances among the indicators\nsemTools::compRelSEM(fit)  # tau.eq = FALSE\n# Verbal Nonverbal   Spatial    Memory \n#  0.804     0.737     0.753     0.776 \n\n# Composite reliability\nsemTools::reliability(fit) # depricated\n#        Verbal Nonverbal Spatial Memory\n# alpha   0.804     0.713   0.754  0.776\n# omega   0.805     0.728   0.755  0.776\n# omega2  0.805     0.728   0.755  0.776\n# omega3  0.804     0.737   0.753  0.776\n# avevar  0.579     0.477   0.509  0.537\nCronbach’s alpha: psych::alpha()\n\n\n\nImplied Covariance/Correlation Matrix\n\n# implied covariance matrix\nfitted(fit) |&gt; print()\n\n$cov\n        nvss    wdss    vsss    psss    mass    sqss    pcss    rdss    rpss\nnvss 101.872                                                                \nwdss  52.464  91.885                                                        \nvsss  61.251  57.689 103.870                                                \npsss  33.464  31.518  36.797 105.867                                        \nmass  43.692  41.151  48.043  40.513 104.869                                \nsqss  47.046  44.310  51.731  43.623  56.957  93.882                        \npcss  42.028  39.584  46.213  37.267  48.658  52.393  84.894                \nrdss  41.280  38.879  45.390  36.604  47.792  51.460  55.230  99.875        \nrpss  33.407  31.465  36.734  29.623  38.678  41.647  44.698  43.902 101.872\ndfss  45.428  42.787  49.952  35.230  45.998  49.529  44.873  44.074  35.669\ndbss  46.998  44.265  51.679  36.448  47.588  51.241  46.423  45.597  36.902\nsoss  50.823  47.867  55.884  39.414  51.460  55.411  50.201  49.308  39.905\n        dfss    dbss    soss\nnvss                        \nwdss                        \nvsss                        \npsss                        \nmass                        \nsqss                        \npcss                        \nrdss                        \nrpss                        \ndfss 121.847                \ndbss  56.459 102.871        \nsoss  61.053  63.163 112.859\n\n\n\n\n# implied correlation matrix\ninspect(fit, \"cor.ov\") |&gt; print()  \n# cor.ov: observed variables only\n# cor.lv: latent variables only\n# cor.all: all variables\n\n      nvss  wdss  vsss  psss  mass  sqss  pcss  rdss  rpss  dfss  dbss  soss\nnvss 1.000                                                                  \nwdss 0.542 1.000                                                            \nvsss 0.595 0.591 1.000                                                      \npsss 0.322 0.320 0.351 1.000                                                \nmass 0.423 0.419 0.460 0.384 1.000                                          \nsqss 0.481 0.477 0.524 0.438 0.574 1.000                                    \npcss 0.452 0.448 0.492 0.393 0.516 0.587 1.000                              \nrdss 0.409 0.406 0.446 0.356 0.467 0.531 0.600 1.000                        \nrpss 0.328 0.325 0.357 0.285 0.374 0.426 0.481 0.435 1.000                  \ndfss 0.408 0.404 0.444 0.310 0.407 0.463 0.441 0.400 0.320 1.000            \ndbss 0.459 0.455 0.500 0.349 0.458 0.521 0.497 0.450 0.360 0.504 1.000      \nsoss 0.474 0.470 0.516 0.361 0.473 0.538 0.513 0.464 0.372 0.521 0.586 1.000\n\n\n\n\nResiduals\nresiduals(fit, type = \"cor.bentler\", summary = FALSE)\n\n\n\n\n\n\nResidual types\n\n\n\n\n“raw”: returns the raw (= unscaled) difference between the observed and the expected (model-implied) summary statistics.\n“cor”, or “cor.bollen”: the observed and model implied covariance matrices are first transformed to a correlation matrix (using cov2cor()), before the residuals are computed.\n“cor.bentler”: both the observed and model implied covariance matrices are rescaled by dividing the elements by the square roots of the corresponding variances of the observed covariance matrix.\n“normalized”: the residuals are divided by the square root of the asymptotic variance of the corresponding summary statistic (the variance estimate depends on the choice for the se argument). Unfortunately, the corresponding normalized residuals are not entirely correct, and this option is only available for historical interest.\n“standardized”: the residuals are divided by the square root of the asymptotic variance of these residuals. The resulting standardized residuals elements can be interpreted as z-scores.\n“standardized.mplus”: the residuals are divided by the square root of the asymptotic variance of these residuals. However, a simplified formula is used (see the Mplus reference below) which often results in negative estimates for the variances, resulting in many NA values for the standardized residuals.\n\n\n\n\n\n\n\n\n\nSummary options\n\n\n\nsummary: Logical. If TRUE, show various summaries of the (possibly scaled) residuals.\n\nWhen type = “raw”, we compute the RMR.\nWhen type = “cor.bentler”, we compute the SRMR.\nWhen type = “cor.bollen”, we compute the CRMR. An unbiased version of these summaries is also computed, as well as a standard error, a z-statistic and a p-value for the test of exact fit based on these summaries.\n\n\n\n\nresiduals(fit) |&gt; print()\n\n$type\n[1] \"raw\"\n\n$cov\n       nvss   wdss   vsss   psss   mass   sqss   pcss   rdss   rpss   dfss\nnvss  0.000                                                               \nwdss  1.468  0.000                                                        \nvsss -1.326  0.239  0.000                                                 \npsss  5.487  5.436  5.151  0.000                                          \nmass -3.742  0.796 -0.103 -1.562  0.000                                   \nsqss -3.101 -2.363  1.203 -4.672  2.968  0.000                            \npcss  4.914 -2.630  2.726  1.684 -1.716  1.540  0.000                     \nrdss  2.665 -7.918 -1.445  4.345 -6.843 -1.523  0.700  0.000              \nrpss  0.550 -3.500 -0.779  4.334  1.272  2.298 -3.749  4.038  0.000       \ndfss  4.509  1.158  1.983  1.724 -6.048 -2.588 -0.928  1.869 -2.710  0.000\ndbss -4.052 -2.318 -0.742  0.506  3.348  1.693  1.517  1.344  1.051 -0.529\nsoss  1.112  2.070 -0.953 -2.460  3.471 -1.478 -1.262 -1.368  0.045  0.869\n       dbss   soss\nnvss              \nwdss              \nvsss              \npsss              \nmass              \nsqss              \npcss              \nrdss              \nrpss              \ndfss              \ndbss  0.000       \nsoss -0.242  0.000\n\n\n\n\nresiduals(fit, type=\"standardized\") |&gt; print()\n\n$type\n[1] \"standardized\"\n\n$cov\n              nvss          wdss          vsss          psss          mass\nnvss         0.000                                                        \nwdss         1.319         0.000                                          \nvsss        -1.679         0.298         0.000                            \npsss         2.335         2.371         2.369         0.000              \nmass        -2.044         0.436        -0.063        -0.825 346956626.925\nsqss        -2.148        -1.658         0.961        -3.801         3.887\npcss         3.200        -1.869         2.113         1.010        -1.427\nrdss         1.391        -4.417        -0.875         2.023        -4.363\nrpss         0.240        -1.566        -0.369         1.729         0.606\ndfss         2.114         0.568         1.044         0.651        -2.861\ndbss        -2.430        -1.423        -0.512         0.236         1.953\nsoss         0.668         1.272        -0.674        -1.127         2.015\n              sqss          pcss          rdss          rpss          dfss\nnvss                                                                      \nwdss                                                                      \nvsss                                                                      \npsss                                                                      \nmass                                                                      \nsqss 121216123.901                                                        \npcss         1.778         0.000                                          \nrdss        -1.315         1.162  76639007.444                            \nrpss         1.418        -3.798         2.472 237262015.107              \ndfss        -1.546        -0.545         0.867        -1.052 162795134.700\ndbss         1.292         1.155         0.798         0.500        -0.373\nsoss        -1.177        -0.990        -0.817         0.021         0.635\n              dbss          soss\nnvss                            \nwdss                            \nvsss                            \npsss                            \nmass                            \nsqss                            \npcss                            \nrdss                            \nrpss                            \ndfss                            \ndbss  88848735.180              \nsoss        -0.275 185340208.945\n\n\n\n\nresiduals(fit, type = \"cor.bollen\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       nvss   wdss   vsss   psss   mass   sqss   pcss   rdss   rpss   dfss\nnvss  0.000                                                               \nwdss  0.015  0.000                                                        \nvsss -0.013  0.002  0.000                                                 \npsss  0.053  0.055  0.049  0.000                                          \nmass -0.036  0.008 -0.001 -0.015  0.000                                   \nsqss -0.032 -0.025  0.012 -0.047  0.030  0.000                            \npcss  0.053 -0.030  0.029  0.018 -0.018  0.017  0.000                     \nrdss  0.026 -0.083 -0.014  0.042 -0.067 -0.016  0.008  0.000              \nrpss  0.005 -0.036 -0.008  0.042  0.012  0.023 -0.040  0.040  0.000       \ndfss  0.040  0.011  0.018  0.015 -0.054 -0.024 -0.009  0.017 -0.024  0.000\ndbss -0.040 -0.024 -0.007  0.005  0.032  0.017  0.016  0.013  0.010 -0.005\nsoss  0.010  0.020 -0.009 -0.023  0.032 -0.014 -0.013 -0.013  0.000  0.007\n       dbss   soss\nnvss              \nwdss              \nvsss              \npsss              \nmass              \nsqss              \npcss              \nrdss              \nrpss              \ndfss              \ndbss  0.000       \nsoss -0.002  0.000\n\n\n\n\n# 필터링: 절대값이 0.05보다 작은 값은 NA로 대체\nresid_cor &lt;- residuals(fit, type = \"cor.bollen\")$cov\nresid_cor[abs(resid_cor) &lt; 0.05] &lt;- NA\nresid_cor |&gt; print()\n\n       nvss   wdss   vsss   psss   mass   sqss   pcss   rdss   rpss   dfss\nnvss      .                                                               \nwdss     NA      .                                                        \nvsss     NA     NA      .                                                 \npsss  0.053  0.055     NA      .                                          \nmass     NA     NA     NA     NA      .                                   \nsqss     NA     NA     NA     NA     NA      .                            \npcss  0.053     NA     NA     NA     NA     NA      .                     \nrdss     NA -0.083     NA     NA -0.067     NA     NA      .              \nrpss     NA     NA     NA     NA     NA     NA     NA     NA      .       \ndfss     NA     NA     NA     NA -0.054     NA     NA     NA     NA      .\ndbss     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA\nsoss     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA\n       dbss   soss\nnvss              \nwdss              \nvsss              \npsss              \nmass              \nsqss              \npcss              \nrdss              \nrpss              \ndfss              \ndbss      .       \nsoss     NA      .\n\n\n\n\nModification Indices\n파라미터를 추가로 추정하여 \\(\\chi^2\\) 값의 변화를 확인\n\nmodindices(fit, sort = TRUE, maximum.number = 8) |&gt; print()\n\n          lhs op  rhs     mi    epc sepc.lv sepc.all sepc.nox\n109      mass ~~ sqss 17.701 12.611  12.611    0.307    0.307\n48  Nonverbal =~ rdss 14.845 -1.191  -6.635   -0.664   -0.664\n54    Spatial =~ wdss 14.410 -0.320  -2.400   -0.250   -0.250\n57    Spatial =~ mass 13.497 -0.725  -5.436   -0.531   -0.531\n102      psss ~~ sqss 13.490 -8.793  -8.793   -0.178   -0.178\n35     Verbal =~ psss 12.725  0.396   2.956    0.287    0.287\n123      pcss ~~ rpss 12.675 -8.036  -8.036   -0.184   -0.184\n111      mass ~~ rdss 11.561 -7.250  -7.250   -0.149   -0.149\n\n\n\n# 필터링: MI &gt; 10 이상인 것만 출력\nmodindices(fit, sort = TRUE) |&gt; subset(mi &gt; 10) |&gt; print()\n\n          lhs op  rhs     mi    epc sepc.lv sepc.all sepc.nox\n109      mass ~~ sqss 17.701 12.611  12.611    0.307    0.307\n48  Nonverbal =~ rdss 14.845 -1.191  -6.635   -0.664   -0.664\n54    Spatial =~ wdss 14.410 -0.320  -2.400   -0.250   -0.250\n57    Spatial =~ mass 13.497 -0.725  -5.436   -0.531   -0.531\n102      psss ~~ sqss 13.490 -8.793  -8.793   -0.178   -0.178\n35     Verbal =~ psss 12.725  0.396   2.956    0.287    0.287\n123      pcss ~~ rpss 12.675 -8.036  -8.036   -0.184   -0.184\n111      mass ~~ rdss 11.561 -7.250  -7.250   -0.149   -0.149\n87       wdss ~~ rdss 11.198 -6.360  -6.360   -0.144   -0.144\n\n\n\n# 필터링: operator가 =~인 것, 즉 회귀계수만 출력\nmodindices(fit, sort = TRUE, maximum.number = 8) |&gt; subset(op == \"=~\") |&gt; print()\n\n         lhs op  rhs     mi    epc sepc.lv sepc.all sepc.nox\n48 Nonverbal =~ rdss 14.845 -1.191  -6.635   -0.664   -0.664\n54   Spatial =~ wdss 14.410 -0.320  -2.400   -0.250   -0.250\n57   Spatial =~ mass 13.497 -0.725  -5.436   -0.531   -0.531\n35    Verbal =~ psss 12.725  0.396   2.956    0.287    0.287",
    "crumbs": [
      "Keith's",
      "Factor Analysis"
    ]
  },
  {
    "objectID": "contents/chap16.html#higher-order-model",
    "href": "contents/chap16.html#higher-order-model",
    "title": "Chapter 15-16. Confirmatory Factor Analysis",
    "section": "Higher-order model",
    "text": "Higher-order model\n\n## Higher-order model\ndas2_model_higher &lt;- \"\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss\n  Spatial =~ pcss + rdss + rpss\n  Memory =~ dfss + dbss + soss\n\n  G =~ Verbal + Nonverbal + Spatial + Memory\n\"\n\nfit_higher &lt;- sem(das2_model_higher,\n  sample.cov = das2cov,\n  sample.nobs = 800,\n  sample.cov.rescale = FALSE\n)\n\nsummary(fit_higher, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 107 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        28\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               141.972\n  Degrees of freedom                                50\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              4335.087\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.978\n  Tucker-Lewis Index (TLI)                       0.972\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33721.564\n  Loglikelihood unrestricted model (H1)     -33650.578\n                                                      \n  Akaike (AIC)                               67499.128\n  Bayesian (BIC)                             67630.297\n  Sample-size adjusted Bayesian (SABIC)      67541.381\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.048\n  90 Percent confidence interval - lower         0.039\n  90 Percent confidence interval - upper         0.057\n  P-value H_0: RMSEA &lt;= 0.050                    0.626\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              1.000                               7.478    0.740\n    wdss              0.937    0.049   19.183    0.000    7.004    0.730\n    vsss              1.100    0.053   20.914    0.000    8.229    0.807\n  Nonverbal =~                                                          \n    psss              1.000                               5.562    0.540\n    mass              1.315    0.093   14.090    0.000    7.312    0.714\n    sqss              1.405    0.094   14.951    0.000    7.815    0.806\n  Spatial =~                                                            \n    pcss              1.000                               7.511    0.815\n    rdss              0.983    0.047   20.993    0.000    7.382    0.738\n    rpss              0.790    0.049   16.285    0.000    5.937    0.588\n  Memory =~                                                             \n    dfss              1.000                               7.349    0.665\n    dbss              1.047    0.058   17.904    0.000    7.695    0.758\n    soss              1.123    0.062   18.215    0.000    8.253    0.776\n  G =~                                                                  \n    Verbal            1.000                               0.858    0.858\n    Nonverbal         0.828    0.063   13.044    0.000    0.955    0.955\n    Spatial           1.057    0.060   17.475    0.000    0.903    0.903\n    Memory            1.046    0.069   15.141    0.000    0.913    0.913\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .nvss             46.075    2.929   15.728    0.000   46.075    0.452\n   .wdss             42.944    2.686   15.990    0.000   42.944    0.467\n   .vsss             36.291    2.718   13.352    0.000   36.291    0.349\n   .psss             75.069    4.014   18.702    0.000   75.069    0.708\n   .mass             51.529    3.115   16.543    0.000   51.529    0.491\n   .sqss             32.921    2.492   13.213    0.000   32.921    0.350\n   .pcss             28.590    2.250   12.706    0.000   28.590    0.336\n   .rdss             45.504    2.891   15.738    0.000   45.504    0.455\n   .rpss             66.754    3.671   18.187    0.000   66.754    0.654\n   .dfss             67.997    3.932   17.293    0.000   67.997    0.557\n   .dbss             43.783    2.871   15.248    0.000   43.783    0.425\n   .soss             44.890    3.070   14.620    0.000   44.890    0.397\n   .Verbal           14.760    2.029    7.274    0.000    0.264    0.264\n   .Nonverbal         2.701    1.031    2.619    0.009    0.087    0.087\n   .Spatial          10.425    1.925    5.415    0.000    0.185    0.185\n   .Memory            8.952    1.828    4.897    0.000    0.166    0.166\n    G                41.165    4.202    9.797    0.000    1.000    1.000\n\n\n\n\nlavResiduals(fit3, type = \"normalized\", zstat = F, summary = F) |&gt; print()\n\n$type\n[1] \"normalized\"\n\n$cov\n       nvss   wdss   vsss   psss   mass   sqss   pcss   rdss   rpss   dfss\nnvss  0.000                                                               \nwdss  0.363  0.000                                                        \nvsss -0.351  0.108  0.000                                                 \npsss  1.346  1.456  1.271  0.000                                          \nmass -0.615  0.605  0.385 -0.052  0.000                                   \nsqss -0.547 -0.318  0.643 -0.948  1.565  0.000                            \npcss  0.997 -1.052  0.418 -0.289 -0.905 -0.197  0.000                     \nrdss  0.486 -2.365 -0.535  0.530 -2.040 -0.827  1.570  0.000              \nrpss -0.317 -1.393 -0.661  0.376 -0.239 -0.125 -0.227  1.861  0.000       \ndfss  0.992  0.294  0.444  0.321 -1.139 -0.413 -0.458  0.352 -1.026  0.000\ndbss -1.069 -0.586 -0.166  0.050  1.150  0.685  0.192  0.263 -0.111 -0.130\nsoss  0.226  0.548 -0.207 -0.686  1.144 -0.095 -0.553 -0.413 -0.385  0.173\n       dbss   soss\nnvss              \nwdss              \nvsss              \npsss              \nmass              \nsqss              \npcss              \nrdss              \nrpss              \ndfss              \ndbss  0.000       \nsoss -0.040  0.000\n\n\n\n\nsemPlot::semPaths(fit_higher, what = \"est\", edge.label.cex = .8, fade = FALSE)\n\n\n\n\n\n\n\n\n\nBifactor model\n\n## Bifactor model\ndas2_model_bifactor &lt;- \"\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss\n  Spatial =~ pcss + rdss + rpss\n  Memory =~ dfss + dbss + soss\n\n  G =~ NA*nvss + 1*wdss + vsss + psss + mass + sqss + pcss + rdss + rpss + dfss + dbss + soss\n\n  mass ~~ 0*mass\n\n  # orthogonal factors (uncorrelated factors)\n  Verbal ~~ 0*Nonverbal\n  Verbal ~~ 0*Spatial\n  Verbal ~~ 0*Memory\n  Nonverbal ~~ 0*Spatial\n  Nonverbal ~~ 0*Memory\n  Spatial ~~ 0*Memory\n  G ~~ 0*Verbal\n  G ~~ 0*Nonverbal\n  G ~~ 0*Spatial\n  G ~~ 0*Memory\n\"\n\nfit_bif &lt;- sem(das2_model_bifactor,\n  sample.cov = das2cov,\n  sample.nobs = 800,\n  sample.cov.rescale = FALSE\n)\n\n# 또는 아래와 같이 uncorrelated factors로 설정\nfit_bif &lt;- sem(das2_model_bifactor,\n  sample.cov = das2cov,\n  sample.nobs = 800,\n  sample.cov.rescale = FALSE,\n  auto.cov.lv.x = FALSE\n) # uncorrelated factors\n\nsummary(fit_bif, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 594 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        35\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               108.487\n  Degrees of freedom                                43\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              4335.087\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.985\n  Tucker-Lewis Index (TLI)                       0.976\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33704.821\n  Loglikelihood unrestricted model (H1)     -33650.578\n                                                      \n  Akaike (AIC)                               67479.642\n  Bayesian (BIC)                             67643.604\n  Sample-size adjusted Bayesian (SABIC)      67532.459\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.044\n  90 Percent confidence interval - lower         0.033\n  90 Percent confidence interval - upper         0.054\n  P-value H_0: RMSEA &lt;= 0.050                    0.839\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.025\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              1.000                               3.675    0.364\n    wdss              1.189    0.210    5.661    0.000    4.371    0.456\n    vsss              0.999    0.162    6.159    0.000    3.670    0.360\n  Nonverbal =~                                                          \n    psss              1.000                               0.109    0.011\n    mass             70.326  219.176    0.321    0.748    7.646    0.746\n    sqss             11.347   35.165    0.323    0.747    1.234    0.127\n  Spatial =~                                                            \n    pcss              1.000                               1.687    0.183\n    rdss              3.889    2.449    1.588    0.112    6.562    0.656\n    rpss              1.110    0.273    4.070    0.000    1.873    0.185\n  Memory =~                                                             \n    dfss              1.000                               3.291    0.298\n    dbss              0.837    0.227    3.686    0.000    2.754    0.271\n    soss              1.136    0.350    3.247    0.001    3.738    0.352\n  G =~                                                                  \n    nvss              1.109    0.064   17.389    0.000    6.485    0.642\n    wdss              1.000                               5.850    0.610\n    vsss              1.226    0.065   18.772    0.000    7.172    0.703\n    psss              0.956    0.074   12.902    0.000    5.595    0.543\n    mass              1.166    0.078   14.989    0.000    6.822    0.666\n    sqss              1.267    0.076   16.629    0.000    7.412    0.765\n    pcss              1.180    0.072   16.387    0.000    6.900    0.748\n    rdss              1.113    0.075   14.795    0.000    6.511    0.651\n    rpss              0.937    0.073   12.815    0.000    5.484    0.543\n    dfss              1.137    0.082   13.930    0.000    6.653    0.602\n    dbss              1.206    0.078   15.540    0.000    7.055    0.695\n    soss              1.277    0.082   15.664    0.000    7.471    0.703\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal ~~                                                             \n    Nonverbal         0.000                               0.000    0.000\n    Spatial           0.000                               0.000    0.000\n    Memory            0.000                               0.000    0.000\n  Nonverbal ~~                                                          \n    Spatial           0.000                               0.000    0.000\n    Memory            0.000                               0.000    0.000\n  Spatial ~~                                                            \n    Memory            0.000                               0.000    0.000\n  Verbal ~~                                                             \n    G                 0.000                               0.000    0.000\n  Nonverbal ~~                                                          \n    G                 0.000                               0.000    0.000\n  Spatial ~~                                                            \n    G                 0.000                               0.000    0.000\n  Memory ~~                                                             \n    G                 0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .mass              0.000                               0.000    0.000\n   .nvss             46.440    3.259   14.248    0.000   46.440    0.455\n   .wdss             38.673    3.802   10.172    0.000   38.673    0.420\n   .vsss             39.087    3.019   12.945    0.000   39.087    0.376\n   .psss             74.685    3.977   18.778    0.000   74.685    0.705\n   .sqss             37.534    2.243   16.737    0.000   37.534    0.399\n   .pcss             34.543    2.497   13.834    0.000   34.543    0.406\n   .rdss             14.542   26.089    0.557    0.577   14.542    0.145\n   .rpss             68.417    3.959   17.282    0.000   68.417    0.671\n   .dfss             66.910    4.717   14.186    0.000   66.910    0.548\n   .dbss             45.638    3.274   13.938    0.000   45.638    0.443\n   .soss             43.221    4.804    8.998    0.000   43.221    0.382\n    Verbal           13.508    3.302    4.091    0.000    1.000    1.000\n    Nonverbal         0.012    0.074    0.160    0.873    1.000    1.000\n    Spatial           2.847    1.993    1.428    0.153    1.000    1.000\n    Memory           10.833    4.351    2.490    0.013    1.000    1.000\n    G                34.221    3.819    8.961    0.000    1.000    1.000\n\n\n\n\ninspect(fit_bif, what = \"est\") |&gt; print() # minus variance for mass\n\n$lambda\n     Verbal Nnvrbl Spatil Memory     G\nnvss  1.000  0.000  0.000  0.000 1.109\nwdss  1.189  0.000  0.000  0.000 1.000\nvsss  0.999  0.000  0.000  0.000 1.226\npsss  0.000  1.000  0.000  0.000 0.956\nmass  0.000 70.326  0.000  0.000 1.166\nsqss  0.000 11.347  0.000  0.000 1.267\npcss  0.000  0.000  1.000  0.000 1.180\nrdss  0.000  0.000  3.889  0.000 1.113\nrpss  0.000  0.000  1.110  0.000 0.937\ndfss  0.000  0.000  0.000  1.000 1.137\ndbss  0.000  0.000  0.000  0.837 1.206\nsoss  0.000  0.000  0.000  1.136 1.277\n\n$theta\n       nvss   wdss   vsss   psss   mass   sqss   pcss   rdss   rpss   dfss\nnvss 46.440                                                               \nwdss  0.000 38.673                                                        \nvsss  0.000  0.000 39.087                                                 \npsss  0.000  0.000  0.000 74.685                                          \nmass  0.000  0.000  0.000  0.000  0.000                                   \nsqss  0.000  0.000  0.000  0.000  0.000 37.534                            \npcss  0.000  0.000  0.000  0.000  0.000  0.000 34.543                     \nrdss  0.000  0.000  0.000  0.000  0.000  0.000  0.000 14.542              \nrpss  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 68.417       \ndfss  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 66.910\ndbss  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nsoss  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n       dbss   soss\nnvss              \nwdss              \nvsss              \npsss              \nmass              \nsqss              \npcss              \nrdss              \nrpss              \ndfss              \ndbss 45.638       \nsoss  0.000 43.221\n\n$psi\n          Verbal Nnvrbl Spatil Memory      G\nVerbal    13.508                            \nNonverbal  0.000  0.012                     \nSpatial    0.000  0.000  2.847              \nMemory     0.000  0.000  0.000 10.833       \nG          0.000  0.000  0.000  0.000 34.221\n\n\n\n\nparameterTable(fit_bif) |&gt; print()\n\n   id       lhs op       rhs user block group free ustart exo label plabel\n1   1    Verbal =~      nvss    1     1     1    0      1   0         .p1.\n2   2    Verbal =~      wdss    1     1     1    1     NA   0         .p2.\n3   3    Verbal =~      vsss    1     1     1    2     NA   0         .p3.\n4   4 Nonverbal =~      psss    1     1     1    0      1   0         .p4.\n5   5 Nonverbal =~      mass    1     1     1    3     NA   0         .p5.\n6   6 Nonverbal =~      sqss    1     1     1    4     NA   0         .p6.\n7   7   Spatial =~      pcss    1     1     1    0      1   0         .p7.\n8   8   Spatial =~      rdss    1     1     1    5     NA   0         .p8.\n9   9   Spatial =~      rpss    1     1     1    6     NA   0         .p9.\n10 10    Memory =~      dfss    1     1     1    0      1   0        .p10.\n11 11    Memory =~      dbss    1     1     1    7     NA   0        .p11.\n12 12    Memory =~      soss    1     1     1    8     NA   0        .p12.\n13 13         G =~      nvss    1     1     1    9     NA   0        .p13.\n14 14         G =~      wdss    1     1     1    0      1   0        .p14.\n15 15         G =~      vsss    1     1     1   10     NA   0        .p15.\n16 16         G =~      psss    1     1     1   11     NA   0        .p16.\n17 17         G =~      mass    1     1     1   12     NA   0        .p17.\n18 18         G =~      sqss    1     1     1   13     NA   0        .p18.\n19 19         G =~      pcss    1     1     1   14     NA   0        .p19.\n20 20         G =~      rdss    1     1     1   15     NA   0        .p20.\n21 21         G =~      rpss    1     1     1   16     NA   0        .p21.\n22 22         G =~      dfss    1     1     1   17     NA   0        .p22.\n23 23         G =~      dbss    1     1     1   18     NA   0        .p23.\n24 24         G =~      soss    1     1     1   19     NA   0        .p24.\n25 25      mass ~~      mass    1     1     1    0      0   0        .p25.\n26 26    Verbal ~~ Nonverbal    1     1     1    0      0   0        .p26.\n27 27    Verbal ~~   Spatial    1     1     1    0      0   0        .p27.\n28 28    Verbal ~~    Memory    1     1     1    0      0   0        .p28.\n29 29 Nonverbal ~~   Spatial    1     1     1    0      0   0        .p29.\n30 30 Nonverbal ~~    Memory    1     1     1    0      0   0        .p30.\n31 31   Spatial ~~    Memory    1     1     1    0      0   0        .p31.\n32 32    Verbal ~~         G    1     1     1    0      0   0        .p32.\n33 33 Nonverbal ~~         G    1     1     1    0      0   0        .p33.\n34 34   Spatial ~~         G    1     1     1    0      0   0        .p34.\n35 35    Memory ~~         G    1     1     1    0      0   0        .p35.\n36 36      nvss ~~      nvss    0     1     1   20     NA   0        .p36.\n37 37      wdss ~~      wdss    0     1     1   21     NA   0        .p37.\n38 38      vsss ~~      vsss    0     1     1   22     NA   0        .p38.\n39 39      psss ~~      psss    0     1     1   23     NA   0        .p39.\n40 40      sqss ~~      sqss    0     1     1   24     NA   0        .p40.\n41 41      pcss ~~      pcss    0     1     1   25     NA   0        .p41.\n42 42      rdss ~~      rdss    0     1     1   26     NA   0        .p42.\n43 43      rpss ~~      rpss    0     1     1   27     NA   0        .p43.\n44 44      dfss ~~      dfss    0     1     1   28     NA   0        .p44.\n45 45      dbss ~~      dbss    0     1     1   29     NA   0        .p45.\n46 46      soss ~~      soss    0     1     1   30     NA   0        .p46.\n47 47    Verbal ~~    Verbal    0     1     1   31     NA   0        .p47.\n48 48 Nonverbal ~~ Nonverbal    0     1     1   32     NA   0        .p48.\n49 49   Spatial ~~   Spatial    0     1     1   33     NA   0        .p49.\n50 50    Memory ~~    Memory    0     1     1   34     NA   0        .p50.\n51 51         G ~~         G    0     1     1   35     NA   0        .p51.\n    start    est      se\n1   1.000  1.000   0.000\n2   0.967  1.189   0.210\n3   1.074  0.999   0.162\n4   1.000  1.000   0.000\n5   1.538 70.326 219.176\n6   1.538 11.347  35.165\n7   1.000  1.000   0.000\n8   1.171  3.889   2.449\n9   0.857  1.110   0.273\n10  1.000  1.000   0.000\n11  1.016  0.837   0.227\n12  1.125  1.136   0.350\n13  1.000  1.109   0.064\n14  1.000  1.000   0.000\n15  1.056  1.226   0.065\n16  0.737  0.956   0.074\n17  0.852  1.166   0.078\n18  0.923  1.267   0.076\n19  0.868  1.180   0.072\n20  0.828  1.113   0.075\n21  0.676  0.937   0.073\n22  0.914  1.137   0.082\n23  0.922  1.206   0.078\n24  0.989  1.277   0.082\n25  0.000  0.000   0.000\n26  0.000  0.000   0.000\n27  0.000  0.000   0.000\n28  0.000  0.000   0.000\n29  0.000  0.000   0.000\n30  0.000  0.000   0.000\n31  0.000  0.000   0.000\n32  0.000  0.000   0.000\n33  0.000  0.000   0.000\n34  0.000  0.000   0.000\n35  0.000  0.000   0.000\n36 51.000 46.440   3.259\n37 46.000 38.673   3.802\n38 52.000 39.087   3.019\n39 53.000 74.685   3.977\n40 47.000 37.534   2.243\n41 42.500 34.543   2.497\n42 50.000 14.542  26.089\n43 51.000 68.417   3.959\n44 61.000 66.910   4.717\n45 51.500 45.638   3.274\n46 56.500 43.221   4.804\n47  0.050 13.508   3.302\n48  0.050  0.012   0.074\n49  0.050  2.847   1.993\n50  0.050 10.833   4.351\n51  0.050 34.221   3.819\n\n\n\n\nSingle-indicator model\n\n## Single-indicator model\ndas2_model_single &lt;- \"\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss\n  Spatial =~ pcss + rdss + rpss\n  Memory =~ 1*dfss\n\n  G =~ Verbal + Nonverbal + Spatial + Memory\n\n  dfss  ~~ 10.94*dfss\n\"\n\nfit_single &lt;- sem(das2_model_single,\n  sample.cov = das2cov,\n  sample.nobs = 800,\n  sample.cov.rescale = FALSE\n)\n\nsummary(fit_single, fit.measures = TRUE, standardized = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 114 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        23\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               115.280\n  Degrees of freedom                                32\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              3295.596\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.974\n  Tucker-Lewis Index (TLI)                       0.964\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -28212.815\n  Loglikelihood unrestricted model (H1)     -28155.175\n                                                      \n  Akaike (AIC)                               56471.630\n  Bayesian (BIC)                             56579.376\n  Sample-size adjusted Bayesian (SABIC)      56506.339\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.057\n  90 Percent confidence interval - lower         0.046\n  90 Percent confidence interval - upper         0.068\n  P-value H_0: RMSEA &lt;= 0.050                    0.142\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.032\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              1.000                               7.506    0.743\n    wdss              0.928    0.049   19.010    0.000    6.965    0.726\n    vsss              1.097    0.053   20.784    0.000    8.237    0.808\n  Nonverbal =~                                                          \n    psss              1.000                               5.635    0.547\n    mass              1.277    0.091   14.001    0.000    7.198    0.702\n    sqss              1.394    0.093   15.001    0.000    7.857    0.810\n  Spatial =~                                                            \n    pcss              1.000                               7.528    0.817\n    rdss              0.979    0.047   20.818    0.000    7.369    0.737\n    rpss              0.787    0.049   16.184    0.000    5.923    0.586\n  Memory =~                                                             \n    dfss              1.000                              10.539    0.954\n  G =~                                                                  \n    Verbal            1.000                               0.850    0.850\n    Nonverbal         0.843    0.066   12.848    0.000    0.954    0.954\n    Spatial           1.077    0.063   17.075    0.000    0.913    0.913\n    Memory            1.041    0.072   14.514    0.000    0.630    0.630\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .dfss             10.940                              10.940    0.090\n   .nvss             45.661    2.950   15.477    0.000   45.661    0.448\n   .wdss             43.487    2.730   15.929    0.000   43.487    0.473\n   .vsss             36.158    2.766   13.071    0.000   36.158    0.348\n   .psss             74.247    4.008   18.523    0.000   74.247    0.700\n   .mass             53.191    3.222   16.507    0.000   53.191    0.507\n   .sqss             32.264    2.570   12.553    0.000   32.264    0.343\n   .pcss             28.326    2.275   12.450    0.000   28.326    0.333\n   .rdss             45.694    2.918   15.660    0.000   45.694    0.457\n   .rpss             66.915    3.687   18.150    0.000   66.915    0.656\n   .Verbal           15.629    2.182    7.163    0.000    0.277    0.277\n   .Nonverbal         2.835    1.170    2.423    0.015    0.089    0.089\n   .Spatial           9.423    2.052    4.593    0.000    0.166    0.166\n   .Memory           66.929    4.294   15.586    0.000    0.603    0.603\n    G                40.710    4.237    9.609    0.000    1.000    1.000\n\n\n\n\ndas2_model_single2 &lt;- \"\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss\n  Spatial =~ pcss + rdss + rpss\n\n  G =~ Verbal + Nonverbal + Spatial + dfss\n\"\n\nfit_single2 &lt;- sem(das2_model_single2,\n  sample.cov = das2cov,\n  sample.nobs = 800,\n  sample.cov.rescale = FALSE\n)\n\nsummary(fit_single2, fit.measures = TRUE, standardized = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 97 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        23\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               115.280\n  Degrees of freedom                                32\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              3295.596\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.974\n  Tucker-Lewis Index (TLI)                       0.964\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -28212.815\n  Loglikelihood unrestricted model (H1)     -28155.175\n                                                      \n  Akaike (AIC)                               56471.630\n  Bayesian (BIC)                             56579.376\n  Sample-size adjusted Bayesian (SABIC)      56506.339\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.057\n  90 Percent confidence interval - lower         0.046\n  90 Percent confidence interval - upper         0.068\n  P-value H_0: RMSEA &lt;= 0.050                    0.142\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.032\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              1.000                               7.506    0.743\n    wdss              0.928    0.049   19.010    0.000    6.965    0.726\n    vsss              1.097    0.053   20.784    0.000    8.237    0.808\n  Nonverbal =~                                                          \n    psss              1.000                               5.635    0.547\n    mass              1.277    0.091   14.001    0.000    7.198    0.702\n    sqss              1.394    0.093   15.001    0.000    7.857    0.810\n  Spatial =~                                                            \n    pcss              1.000                               7.528    0.817\n    rdss              0.979    0.047   20.818    0.000    7.369    0.737\n    rpss              0.787    0.049   16.184    0.000    5.923    0.586\n  G =~                                                                  \n    Verbal            1.000                               0.850    0.850\n    Nonverbal         0.843    0.066   12.848    0.000    0.954    0.954\n    Spatial           1.077    0.063   17.075    0.000    0.913    0.913\n    dfss              1.041    0.072   14.514    0.000    6.643    0.601\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .nvss             45.661    2.950   15.477    0.000   45.661    0.448\n   .wdss             43.487    2.730   15.929    0.000   43.487    0.473\n   .vsss             36.158    2.766   13.071    0.000   36.158    0.348\n   .psss             74.247    4.008   18.523    0.000   74.247    0.700\n   .mass             53.191    3.222   16.507    0.000   53.191    0.507\n   .sqss             32.264    2.570   12.553    0.000   32.264    0.343\n   .pcss             28.326    2.275   12.450    0.000   28.326    0.333\n   .rdss             45.694    2.918   15.660    0.000   45.694    0.457\n   .rpss             66.915    3.687   18.150    0.000   66.915    0.656\n   .dfss             77.869    4.294   18.133    0.000   77.869    0.638\n   .Verbal           15.629    2.182    7.163    0.000    0.277    0.277\n   .Nonverbal         2.835    1.170    2.423    0.015    0.089    0.089\n   .Spatial           9.423    2.052    4.593    0.000    0.166    0.166\n    G                40.710    4.237    9.609    0.000    1.000    1.000",
    "crumbs": [
      "Keith's",
      "Factor Analysis"
    ]
  },
  {
    "objectID": "contents/chap19.html",
    "href": "contents/chap19.html",
    "title": "Chapter 19. Latent Means in SEM",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)\nlibrary(haven)\nhw_mean &lt;- read_sav(\"data/chap 19 latent means/Homework means.sav\")\nhw_mean |&gt; print()\n\n# A tibble: 1,000 x 15\n  bytxrstd  bytxmstd  bytxsstd  bytxhstd  parocc f1s36a2  ethnic  f2s25f2  eng92\n  &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;  &lt;dbl&gt; &lt;dbl+lb&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt; &lt;dbl&gt;\n1 44.2      52.6      44.5      53.9        27.4 1 [1 HO~ 1 [whi~  2 [1-3~  8   \n2 39.9      40.8      33.4      41.9        56.3 0 [NONE] 1 [whi~ NA        4.4 \n3 46.4      40.2      40.9      54          56.3 3 [4-6 ~ 1 [whi~ NA        3.25\n4 39.9      56.0      55.7      56.4        70.6 4 [7-9 ~ 1 [whi~  5 [10-~  5.5 \n5 52.7      47.9      44.8      41.8        56.3 3 [4-6 ~ 1 [whi~  5 [10-~  8.37\n6 42.3      61.9      53.5      47.0        70.6 3 [4-6 ~ 1 [whi~  1 [LES~  8.33\n# i 994 more rows\n# i 6 more variables: math92 &lt;dbl&gt;, sci92 &lt;dbl&gt;, soc92 &lt;dbl&gt;, Female &lt;dbl+lbl&gt;,\n#   byfaminc &lt;dbl&gt;, bypared &lt;dbl+lbl&gt;\nlibrary(lavaan)\nhw_model &lt;- \"\n  famback =~ parocc + byfaminc + bypared\n  prevach =~ bytxrstd + bytxmstd + bytxsstd + bytxhstd\n  hw =~ f2s25f2 + f1s36a2\n  grades =~ eng92 + math92 + sci92 + soc92\n\n  bytxrstd ~~ eng92\n  bytxmstd ~~ math92\n  bytxsstd ~~ sci92\n  bytxhstd ~~ soc92\n\n  prevach ~ famback\n  grades ~ prevach + hw\n  hw ~ prevach + famback\n\"\n\nsem_fit &lt;- sem(hw_model,\n  data = hw_mean,\n  mimic = \"mplus\"\n  # meanstructure = FALSE,\n)\n\nsummary(sem_fit,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 171 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        48\n\n  Number of observations                          1000\n  Number of missing patterns                        42\n\nModel Test User Model:\n                                                      \n  Test statistic                               113.358\n  Degrees of freedom                                56\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              7792.886\n  Degrees of freedom                                78\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.993\n  Tucker-Lewis Index (TLI)                       0.990\n                                                      \n  Robust Comparative Fit Index (CFI)             0.993\n  Robust Tucker-Lewis Index (TLI)                0.990\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -32104.831\n  Loglikelihood unrestricted model (H1)             NA\n                                                      \n  Akaike (AIC)                               64305.662\n  Bayesian (BIC)                             64541.235\n  Sample-size adjusted Bayesian (SABIC)      64388.784\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.032\n  90 Percent confidence interval - lower         0.023\n  90 Percent confidence interval - upper         0.040\n  P-value H_0: RMSEA &lt;= 0.050                    1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n                                                      \n  Robust RMSEA                                   0.032\n  90 Percent confidence interval - lower         0.023\n  90 Percent confidence interval - upper         0.041\n  P-value H_0: Robust RMSEA &lt;= 0.050             1.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.023\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  famback =~                                                            \n    parocc            1.000                              15.210    0.710\n    byfaminc          0.124    0.006   19.348    0.000    1.891    0.728\n    bypared           0.069    0.003   20.632    0.000    1.049    0.834\n  prevach =~                                                            \n    bytxrstd          1.000                               8.532    0.852\n    bytxmstd          0.988    0.030   32.651    0.000    8.432    0.853\n    bytxsstd          0.960    0.031   30.702    0.000    8.189    0.819\n    bytxhstd          0.942    0.030   31.597    0.000    8.039    0.830\n  hw =~                                                                 \n    f2s25f2           1.000                               1.171    0.592\n    f1s36a2           0.994    0.103    9.671    0.000    1.164    0.688\n  grades =~                                                             \n    eng92             1.000                               2.436    0.915\n    math92            0.876    0.025   34.958    0.000    2.135    0.812\n    sci92             0.961    0.023   41.780    0.000    2.341    0.884\n    soc92             1.049    0.023   45.416    0.000    2.555    0.908\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  prevach ~                                                             \n    famback           0.325    0.022   14.540    0.000    0.579    0.579\n  grades ~                                                              \n    prevach           0.140    0.011   12.973    0.000    0.490    0.490\n    hw                0.655    0.101    6.491    0.000    0.315    0.315\n  hw ~                                                                  \n    prevach           0.045    0.008    5.612    0.000    0.331    0.331\n    famback           0.018    0.005    3.986    0.000    0.235    0.235\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .bytxrstd ~~                                                           \n   .eng92             0.486    0.256    1.900    0.057    0.486    0.086\n .bytxmstd ~~                                                           \n   .math92            1.573    0.320    4.915    0.000    1.573    0.199\n .bytxsstd ~~                                                           \n   .sci92             0.459    0.294    1.563    0.118    0.459    0.064\n .bytxhstd ~~                                                           \n   .soc92             0.266    0.278    0.956    0.339    0.266    0.042\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc           51.388    0.679   75.669    0.000   51.388    2.400\n   .byfaminc          9.841    0.083  118.414    0.000    9.841    3.792\n   .bypared           3.128    0.040   78.568    0.000    3.128    2.485\n   .bytxrstd         51.257    0.320  160.386    0.000   51.257    5.120\n   .bytxmstd         51.493    0.316  163.072    0.000   51.493    5.206\n   .bytxsstd         51.179    0.320  160.149    0.000   51.179    5.117\n   .bytxhstd         51.373    0.310  165.931    0.000   51.373    5.302\n   .f2s25f2           3.280    0.066   49.942    0.000    3.280    1.659\n   .f1s36a2           2.481    0.055   45.366    0.000    2.481    1.465\n   .eng92             6.074    0.084   71.892    0.000    6.074    2.281\n   .math92            5.482    0.083   65.676    0.000    5.482    2.086\n   .sci92             5.770    0.084   68.653    0.000    5.770    2.177\n   .soc92             6.207    0.089   69.614    0.000    6.207    2.207\n    famback           0.000                               0.000    0.000\n   .prevach           0.000                               0.000    0.000\n   .hw                0.000                               0.000    0.000\n   .grades            0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .parocc          227.271   13.266   17.132    0.000  227.271    0.496\n   .byfaminc          3.161    0.198   15.973    0.000    3.161    0.469\n   .bypared           0.483    0.044   10.967    0.000    0.483    0.305\n   .bytxrstd         27.413    1.704   16.091    0.000   27.413    0.274\n   .bytxmstd         26.728    1.669   16.017    0.000   26.728    0.273\n   .bytxsstd         32.977    1.881   17.536    0.000   32.977    0.330\n   .bytxhstd         29.242    1.709   17.107    0.000   29.242    0.312\n   .f2s25f2           2.537    0.178   14.290    0.000    2.537    0.649\n   .f1s36a2           1.510    0.149   10.158    0.000    1.510    0.527\n   .eng92             1.155    0.080   14.479    0.000    1.155    0.163\n   .math92            2.349    0.122   19.249    0.000    2.349    0.340\n   .sci92             1.541    0.092   16.744    0.000    1.541    0.219\n   .soc92             1.383    0.091   15.197    0.000    1.383    0.175\n    famback         231.337   19.948   11.597    0.000    1.000    1.000\n   .prevach          48.379    3.317   14.583    0.000    0.665    0.665\n   .hw                1.021    0.154    6.644    0.000    0.745    0.745\n   .grades            3.063    0.200   15.324    0.000    0.516    0.516\nhotflash &lt;- read_sav(\"data/chap 19 latent means/hot flash simulated.sav\")\n\n# make a grouping variable as a factor\nhotflash &lt;- hotflash |&gt;\n  mutate(g = factor(Group, labels = c(\"Control\", \"Treatment\")))\n\nhotflash |&gt; print()\n\n# A tibble: 96 x 6\n  Group                       HF1   HF2  int1  int2 g        \n  &lt;dbl+lbl&gt;                 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n1 0 [Control]               33.4  32.7     64    38 Control  \n2 1 [Hypnosis Intervention] 25.5  12.7     52     5 Treatment\n3 1 [Hypnosis Intervention] 11.0   7.60     9     2 Treatment\n4 1 [Hypnosis Intervention]  8.05  4.05    13     6 Treatment\n5 0 [Control]               13.2   4.34    61    36 Control  \n6 0 [Control]                9.89  8.89    15    34 Control  \n# i 90 more rows\nhotflash_model &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre + Group\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\"\n\nsem_fit_hotflash &lt;- sem(hotflash_model,\n  data = hotflash,\n  mimic = \"mplus\"\n)\n\nsummary(sem_fit_hotflash,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 171 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        16\n\n  Number of observations                            96\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                                      \n  Test statistic                                 3.501\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.174\n\nModel Test Baseline Model:\n\n  Test statistic                               244.790\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.994\n  Tucker-Lewis Index (TLI)                       0.968\n                                                      \n  Robust Comparative Fit Index (CFI)             0.994\n  Robust Tucker-Lewis Index (TLI)                0.968\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1466.241\n  Loglikelihood unrestricted model (H1)             NA\n                                                      \n  Akaike (AIC)                                2964.482\n  Bayesian (BIC)                              3005.512\n  Sample-size adjusted Bayesian (SABIC)       2954.993\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.088\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.239\n  P-value H_0: RMSEA &lt;= 0.050                    0.245\n  P-value H_0: RMSEA &gt;= 0.080                    0.649\n                                                      \n  Robust RMSEA                                   0.088\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.239\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.245\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.649\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.066\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              18.708    0.932\n    HF1               0.260    0.116    2.247    0.025    4.855    0.439\n  hf_post =~                                                            \n    int2              1.000                              20.647    0.927\n    HF2               0.310    0.037    8.327    0.000    6.409    0.659\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre            0.487    0.196    2.482    0.013    0.441    0.441\n    Group           -28.615    3.207   -8.923    0.000   -1.386   -0.693\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              60.277   10.956    5.502    0.000   60.277    0.831\n .int1 ~~                                                               \n   .int2            -24.643   57.805   -0.426    0.670  -24.643   -0.405\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1             42.656    2.048   20.826    0.000   42.656    2.126\n   .HF1              15.737    1.128   13.949    0.000   15.737    1.424\n   .int2             40.870    2.369   17.253    0.000   40.870    1.834\n   .HF2              14.713    1.050   14.014    0.000   14.713    1.514\n    hf_pre            0.000                               0.000    0.000\n   .hf_post           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1             52.754  183.493    0.287    0.774   52.754    0.131\n   .HF1              98.604   18.048    5.464    0.000   98.604    0.807\n   .int2             70.301   52.454    1.340    0.180   70.301    0.142\n   .HF2              53.415    9.290    5.750    0.000   53.415    0.565\n    hf_pre          349.971  194.090    1.803    0.071    1.000    1.000\n   .hf_post         138.752   35.498    3.909    0.000    0.325    0.325\n# free covariance between pretest and group\nhotflash_model2 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre + Group\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\n  hf_pre ~~ Group  # Group: numeric type; 0, 1\n\"\n\nsem_fit_hotflash2 &lt;- sem(hotflash_model2, data = hotflash)\n\nsummary(sem_fit_hotflash2,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 340 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        14\n\n  Number of observations                            96\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.088\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.767\n\nModel Test Baseline Model:\n\n  Test statistic                               244.790\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.039\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1534.211\n  Loglikelihood unrestricted model (H1)             NA\n                                                      \n  Akaike (AIC)                                3096.421\n  Bayesian (BIC)                              3132.322\n  Sample-size adjusted Bayesian (SABIC)       3088.118\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.182\n  P-value H_0: RMSEA &lt;= 0.050                    0.792\n  P-value H_0: RMSEA &gt;= 0.080                    0.173\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.009\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              17.499    0.872\n    HF1               0.292    0.128    2.278    0.023    5.107    0.463\n  hf_post =~                                                            \n    int2              1.000                              21.358    0.923\n    HF2               0.325    0.042    7.811    0.000    6.946    0.693\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre            0.516    0.192    2.695    0.007    0.423    0.423\n    Group           -27.460    3.413   -8.047    0.000   -1.286   -0.643\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              58.842   10.816    5.440    0.000   58.842    0.834\n .int1 ~~                                                               \n   .int2            -10.175   53.441   -0.190    0.849  -10.175   -0.116\n  hf_pre ~~                                                             \n    Group            -1.871    1.031   -1.814    0.070   -0.107   -0.214\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1             96.414  154.371    0.625    0.532   96.414    0.239\n   .HF1              95.514   18.005    5.305    0.000   95.514    0.786\n   .int2             79.741   47.039    1.695    0.090   79.741    0.149\n   .HF2              52.114    9.151    5.695    0.000   52.114    0.519\n    Group             0.250    0.036    6.928    0.000    0.250    1.000\n    hf_pre          306.198  164.330    1.863    0.062    1.000    1.000\n   .hf_post         133.014   34.072    3.904    0.000    0.292    0.292\n# no effect of group on pretest\nhotflash_model3 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre + 0*Group\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\n  hf_pre ~~ Group  # Group: numeric type; 0, 1\n\"\n\nsem_fit_hotflash3 &lt;- sem(hotflash_model3, data = hotflash)\n\nsummary(sem_fit_hotflash3,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 425 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                            96\n\nModel Test User Model:\n                                                      \n  Test statistic                                16.249\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               244.790\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.939\n  Tucker-Lewis Index (TLI)                       0.697\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1542.291\n  Loglikelihood unrestricted model (H1)             NA\n                                                      \n  Akaike (AIC)                                3110.582\n  Bayesian (BIC)                              3143.918\n  Sample-size adjusted Bayesian (SABIC)       3102.872\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.272\n  90 Percent confidence interval - lower         0.160\n  90 Percent confidence interval - upper         0.402\n  P-value H_0: RMSEA &lt;= 0.050                    0.001\n  P-value H_0: RMSEA &gt;= 0.080                    0.996\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.113\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                               4.376    0.218\n    HF1               0.459    0.368    1.247    0.212    2.010    0.182\n  hf_post =~                                                            \n    int2              1.000                              19.936    0.859\n    HF2               0.345    0.053    6.447    0.000    6.875    0.682\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre            4.286    2.141    2.002    0.045    0.941    0.941\n    Group             0.000                               0.000    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              66.943   11.395    5.875    0.000   66.943    0.836\n .int1 ~~                                                               \n   .int2            119.973   36.912    3.250    0.001  119.973    0.515\n  hf_pre ~~                                                             \n    Group            -1.828    1.016   -1.800    0.072   -0.418   -0.835\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            383.573   56.158    6.830    0.000  383.573    0.952\n   .HF1             118.138   17.185    6.874    0.000  118.138    0.967\n   .int2            141.227   44.236    3.193    0.001  141.227    0.262\n   .HF2              54.278    9.412    5.767    0.000   54.278    0.534\n    Group             0.250    0.036    6.928    0.000    0.250    1.000\n    hf_pre           19.153   20.058    0.955    0.340    1.000    1.000\n   .hf_post          45.549   99.411    0.458    0.647    0.115    0.115\n# compare the two models\nlavTestLRT(sem_fit_hotflash2, sem_fit_hotflash3) |&gt; print()\n\n\nChi-Squared Difference Test\n\n                  Df    AIC    BIC   Chisq Chisq diff   RMSEA Df diff\nsem_fit_hotflash2  1 3096.4 3132.3  0.0881                           \nsem_fit_hotflash3  2 3110.6 3143.9 16.2486      16.16 0.39739       1\n                  Pr(&gt;Chisq)    \nsem_fit_hotflash2               \nsem_fit_hotflash3   5.82e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Keith's",
      "Latent Means"
    ]
  },
  {
    "objectID": "contents/chap19.html#multigroup-analysis",
    "href": "contents/chap19.html#multigroup-analysis",
    "title": "Chapter 19. Latent Means in SEM",
    "section": "Multigroup Analysis",
    "text": "Multigroup Analysis\n\n# multigroup analysis\nhotflash_model_mg &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\n  hf_pre ~ 0*1\n\"\n\nsem_fit_hotflash_mg &lt;- sem(hotflash_model_mg,\n  data = hotflash,\n  group = \"g\", # factor type\n  group.equal = c(\"loadings\", \"intercepts\")\n)\n\nsummary(sem_fit_hotflash_mg,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 326 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        31\n  Number of equality constraints                     6\n\n  Number of observations per group:                   \n    Control                                         48\n    Treatment                                       48\n\nModel Test User Model:\n                                                      \n  Test statistic                                 5.399\n  Degrees of freedom                                 3\n  P-value (Chi-square)                           0.145\n  Test statistic for each group:\n    Control                                      2.169\n    Treatment                                    3.229\n\nModel Test Baseline Model:\n\n  Test statistic                               182.941\n  Degrees of freedom                                12\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.986\n  Tucker-Lewis Index (TLI)                       0.944\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1425.733\n  Loglikelihood unrestricted model (H1)      -1423.033\n                                                      \n  Akaike (AIC)                                2901.465\n  Bayesian (BIC)                              2965.574\n  Sample-size adjusted Bayesian (SABIC)       2886.638\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.129\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.302\n  P-value H_0: RMSEA &lt;= 0.050                    0.185\n  P-value H_0: RMSEA &gt;= 0.080                    0.751\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.088\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Control]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              16.945    0.792\n    HF1     (.p2.)    0.388    0.115    3.388    0.001    6.573    0.607\n  hf_post =~                                                            \n    int2              1.000                              21.010    0.959\n    HF2     (.p4.)    0.312    0.037    8.433    0.000    6.559    0.587\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre            1.139    0.274    4.156    0.000    0.919    0.919\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              67.058   18.003    3.725    0.000   67.058    0.862\n .int1 ~~                                                               \n   .int2            -28.916   98.630   -0.293    0.769  -28.916   -0.355\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.15.)   42.049    2.027   20.742    0.000   42.049    1.966\n   .HF1     (.16.)   15.473    1.089   14.206    0.000   15.473    1.429\n   .int2    (.17.)   39.131    2.690   14.548    0.000   39.131    1.785\n   .HF2     (.18.)   14.044    1.248   11.256    0.000   14.044    1.258\n   .hf_post           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            170.287  124.805    1.364    0.172  170.287    0.372\n   .HF1              74.107   19.536    3.793    0.000   74.107    0.632\n   .int2             39.030  107.473    0.363    0.716   39.030    0.081\n   .HF2              81.674   19.666    4.153    0.000   81.674    0.655\n    hf_pre          287.136  148.917    1.928    0.054    1.000    1.000\n   .hf_post          68.950   49.956    1.380    0.168    0.156    0.156\n\n\nGroup 2 [Treatment]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              10.689    0.567\n    HF1     (.p2.)    0.388    0.115    3.388    0.001    4.146    0.384\n  hf_post =~                                                            \n    int2              1.000                               5.158    0.496\n    HF2     (.p4.)    0.312    0.037    8.433    0.000    1.610    0.308\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre           -0.456    0.459   -0.993    0.321   -0.944   -0.944\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              47.191   10.578    4.461    0.000   47.191    0.950\n .int1 ~~                                                               \n   .int2             44.760   37.540    1.192    0.233   44.760    0.319\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.15.)   42.049    2.027   20.742    0.000   42.049    2.230\n   .HF1     (.16.)   15.473    1.089   14.206    0.000   15.473    1.432\n   .int2    (.17.)   39.131    2.690   14.548    0.000   39.131    3.764\n   .HF2     (.18.)   14.044    1.248   11.256    0.000   14.044    2.684\n   .hf_post         -28.439    3.135   -9.071    0.000   -5.513   -5.513\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            241.229   74.588    3.234    0.001  241.229    0.679\n   .HF1              99.551   23.596    4.219    0.000   99.551    0.853\n   .int2             81.492   25.448    3.202    0.001   81.492    0.754\n   .HF2              24.780    5.610    4.417    0.000   24.780    0.905\n    hf_pre          114.255   70.087    1.630    0.103    1.000    1.000\n   .hf_post           2.901   52.283    0.055    0.956    0.109    0.109\n\n\n\n\nparameterTable(sem_fit_hotflash_mg) |&gt; print()\n\n   id     lhs op     rhs user block group free ustart exo label plabel   start\n1   1  hf_pre =~    int1    1     1     1    0      1   0         .p1.   1.000\n2   2  hf_pre =~     HF1    1     1     1    1     NA   0  .p2.   .p2.   0.388\n3   3 hf_post =~    int2    1     1     1    0      1   0         .p3.   1.000\n4   4 hf_post =~     HF2    1     1     1    2     NA   0  .p4.   .p4.   0.448\n5   5 hf_post  ~  hf_pre    1     1     1    3     NA   0         .p5.   0.000\n6   6     HF1 ~~     HF2    1     1     1    4     NA   0         .p6.   0.000\n7   7    int1 ~~    int2    1     1     1    5     NA   0         .p7.   0.000\n8   8  hf_pre ~1            1     1     1    0      0   0         .p8.   0.000\n9   9    int1 ~~    int1    0     1     1    6     NA   0         .p9. 224.087\n10 10     HF1 ~~     HF1    0     1     1    7     NA   0        .p10.  57.344\n11 11    int2 ~~    int2    0     1     1    8     NA   0        .p11. 233.573\n12 12     HF2 ~~     HF2    0     1     1    9     NA   0        .p12.  61.474\n13 13  hf_pre ~~  hf_pre    0     1     1   10     NA   0        .p13.   0.050\n14 14 hf_post ~~ hf_post    0     1     1   11     NA   0        .p14.   0.050\n15 15    int1 ~1            0     1     1   12     NA   0 .p15.  .p15.  46.312\n16 16     HF1 ~1            0     1     1   13     NA   0 .p16.  .p16.  17.077\n17 17    int2 ~1            0     1     1   14     NA   0 .p17.  .p17.  42.250\n18 18     HF2 ~1            0     1     1   15     NA   0 .p18.  .p18.  15.508\n19 19 hf_post ~1            0     1     1    0      0   0        .p19.   0.000\n20 20  hf_pre =~    int1    1     2     2    0      1   0        .p20.   1.000\n21 21  hf_pre =~     HF1    1     2     2   16     NA   0  .p2.  .p21.   0.275\n22 22 hf_post =~    int2    1     2     2    0      1   0        .p22.   1.000\n23 23 hf_post =~     HF2    1     2     2   17     NA   0  .p4.  .p23.   0.044\n24 24 hf_post  ~  hf_pre    1     2     2   18     NA   0        .p24.   0.000\n25 25     HF1 ~~     HF2    1     2     2   19     NA   0        .p25.   0.000\n26 26    int1 ~~    int2    1     2     2   20     NA   0        .p26.   0.000\n27 27  hf_pre ~1            1     2     2    0      0   0        .p27.   0.000\n28 28    int1 ~~    int1    0     2     2   21     NA   0        .p28. 165.271\n29 29     HF1 ~~     HF1    0     2     2   22     NA   0        .p29.  63.038\n30 30    int2 ~~    int2    0     2     2   23     NA   0        .p30.  56.430\n31 31     HF2 ~~     HF2    0     2     2   24     NA   0        .p31.  12.662\n32 32  hf_pre ~~  hf_pre    0     2     2   25     NA   0        .p32.   0.050\n33 33 hf_post ~~ hf_post    0     2     2   26     NA   0        .p33.   0.050\n34 34    int1 ~1            0     2     2   27     NA   0 .p15.  .p34.  39.000\n35 35     HF1 ~1            0     2     2   28     NA   0 .p16.  .p35.  14.396\n36 36    int2 ~1            0     2     2   29     NA   0 .p17.  .p36.  10.875\n37 37     HF2 ~1            0     2     2   30     NA   0 .p18.  .p37.   5.036\n38 38 hf_post ~1            0     2     2   31     NA   0        .p38.   0.000\n39 39    .p2. ==   .p21.    2     0     0    0     NA   0                0.000\n40 40    .p4. ==   .p23.    2     0     0    0     NA   0                0.000\n41 41   .p15. ==   .p34.    2     0     0    0     NA   0                0.000\n42 42   .p16. ==   .p35.    2     0     0    0     NA   0                0.000\n43 43   .p17. ==   .p36.    2     0     0    0     NA   0                0.000\n44 44   .p18. ==   .p37.    2     0     0    0     NA   0                0.000\n       est      se\n1    1.000   0.000\n2    0.388   0.115\n3    1.000   0.000\n4    0.312   0.037\n5    1.139   0.274\n6   67.058  18.003\n7  -28.916  98.630\n8    0.000   0.000\n9  170.287 124.805\n10  74.107  19.536\n11  39.030 107.473\n12  81.674  19.666\n13 287.136 148.917\n14  68.950  49.956\n15  42.049   2.027\n16  15.473   1.089\n17  39.131   2.690\n18  14.044   1.248\n19   0.000   0.000\n20   1.000   0.000\n21   0.388   0.115\n22   1.000   0.000\n23   0.312   0.037\n24  -0.456   0.459\n25  47.191  10.578\n26  44.760  37.540\n27   0.000   0.000\n28 241.229  74.588\n29  99.551  23.596\n30  81.492  25.448\n31  24.780   5.610\n32 114.255  70.087\n33   2.901  52.283\n34  42.049   2.027\n35  15.473   1.089\n36  39.131   2.690\n37  14.044   1.248\n38 -28.439   3.135\n39   0.000   0.000\n40   0.000   0.000\n41   0.000   0.000\n42   0.000   0.000\n43   0.000   0.000\n44   0.000   0.000\n\n\n\n# free pretest intercepts\nhotflash_model_mg2 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\"\nsem_fit_hotflash_mg2 &lt;- sem(hotflash_model_mg2,\n  data = hotflash,\n  group = \"g\", # factor type\n  group.equal = c(\"loadings\", \"intercepts\")\n)\n\nsummary(sem_fit_hotflash_mg2,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 316 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        32\n  Number of equality constraints                     6\n\n  Number of observations per group:                   \n    Control                                         48\n    Treatment                                       48\n\nModel Test User Model:\n                                                      \n  Test statistic                                 1.827\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.401\n  Test statistic for each group:\n    Control                                      0.069\n    Treatment                                    1.758\n\nModel Test Baseline Model:\n\n  Test statistic                               182.941\n  Degrees of freedom                                12\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.006\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1423.947\n  Loglikelihood unrestricted model (H1)      -1423.033\n                                                      \n  Akaike (AIC)                                2899.894\n  Bayesian (BIC)                              2966.567\n  Sample-size adjusted Bayesian (SABIC)       2884.473\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.278\n  P-value H_0: RMSEA &lt;= 0.050                    0.444\n  P-value H_0: RMSEA &gt;= 0.080                    0.495\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.041\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Control]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              15.824    0.753\n    HF1     (.p2.)    0.422    0.133    3.178    0.001    6.677    0.623\n  hf_post =~                                                            \n    int2              1.000                              19.820    0.914\n    HF2     (.p4.)    0.339    0.044    7.780    0.000    6.715    0.606\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre            1.148    0.272    4.227    0.000    0.917    0.917\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              63.289   17.987    3.519    0.000   63.289    0.858\n .int1 ~~                                                               \n   .int2             -1.590   92.867   -0.017    0.986   -1.590   -0.013\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1    (.14.)   46.160    2.931   15.749    0.000   46.160    2.197\n   .HF1     (.15.)   17.176    1.460   11.763    0.000   17.176    1.603\n   .int2    (.16.)   42.265    3.128   13.513    0.000   42.265    1.950\n   .HF2     (.17.)   15.587    1.547   10.077    0.000   15.587    1.407\n    hf_pre            0.000                               0.000    0.000\n   .hf_post           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            191.182  115.106    1.661    0.097  191.182    0.433\n   .HF1              70.168   19.782    3.547    0.000   70.168    0.611\n   .int2             77.023  100.443    0.767    0.443   77.023    0.164\n   .HF2              77.590   19.612    3.956    0.000   77.590    0.632\n    hf_pre          250.405  133.264    1.879    0.060    1.000    1.000\n   .hf_post          62.775   45.436    1.382    0.167    0.160    0.160\n\n\nGroup 2 [Treatment]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                               9.962    0.536\n    HF1     (.p2.)    0.422    0.133    3.178    0.001    4.203    0.390\n  hf_post =~                                                            \n    int2              1.000                               4.956    0.477\n    HF2     (.p4.)    0.339    0.044    7.780    0.000    1.679    0.321\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre           -0.480    0.493   -0.974    0.330   -0.964   -0.964\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              47.660   10.693    4.457    0.000   47.660    0.971\n .int1 ~~                                                               \n   .int2             40.639   35.830    1.134    0.257   40.639    0.283\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1    (.14.)   46.160    2.931   15.749    0.000   46.160    2.483\n   .HF1     (.15.)   17.176    1.460   11.763    0.000   17.176    1.595\n   .int2    (.16.)   42.265    3.128   13.513    0.000   42.265    4.065\n   .HF2     (.17.)   15.587    1.547   10.077    0.000   15.587    2.981\n    hf_pre           -6.970    3.653   -1.908    0.056   -0.700   -0.700\n   .hf_post         -34.716    5.690   -6.101    0.000   -7.004   -7.004\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            246.389   71.237    3.459    0.001  246.389    0.713\n   .HF1              98.344   23.936    4.109    0.000   98.344    0.848\n   .int2             83.545   24.502    3.410    0.001   83.545    0.773\n   .HF2              24.522    5.663    4.330    0.000   24.522    0.897\n    hf_pre           99.234   63.648    1.559    0.119    1.000    1.000\n   .hf_post           1.724   49.966    0.035    0.972    0.070    0.070\n\n\n\n\nlavTestLRT(sem_fit_hotflash_mg, sem_fit_hotflash_mg2) |&gt; print()\n\n\nChi-Squared Difference Test\n\n                     Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff\nsem_fit_hotflash_mg2  2 2899.9 2966.6 1.8271                           \nsem_fit_hotflash_mg   3 2901.5 2965.6 5.3986     3.5715 0.23146       1\n                     Pr(&gt;Chisq)  \nsem_fit_hotflash_mg2             \nsem_fit_hotflash_mg     0.05878 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# Test assumptions\nhotflash_model_mg3 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ c(a, a)*hf_pre\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\n  hf_pre ~ 0*1\n\"\n\nsem_fit_hotflash_mg3 &lt;- sem(hotflash_model_mg3,\n  data = hotflash,\n  group = \"g\", # factor type\n  group.equal = c(\n    \"loadings\", \"intercepts\",\n    \"residuals\", \"lv.variances\",\n    \"residual.covariances\"\n  )\n)\n\nsummary(sem_fit_hotflash_mg3,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 179 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        31\n  Number of equality constraints                    15\n\n  Number of observations per group:                   \n    Control                                         48\n    Treatment                                       48\n\nModel Test User Model:\n                                                      \n  Test statistic                                86.416\n  Degrees of freedom                                12\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    Control                                     28.363\n    Treatment                                   58.053\n\nModel Test Baseline Model:\n\n  Test statistic                               182.941\n  Degrees of freedom                                12\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.565\n  Tucker-Lewis Index (TLI)                       0.565\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1466.241\n  Loglikelihood unrestricted model (H1)      -1423.033\n                                                      \n  Akaike (AIC)                                2964.482\n  Bayesian (BIC)                              3005.512\n  Sample-size adjusted Bayesian (SABIC)       2954.993\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.359\n  90 Percent confidence interval - lower         0.290\n  90 Percent confidence interval - upper         0.433\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.534\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Control]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              18.708    0.932\n    HF1     (.p2.)    0.260    0.112    2.323    0.020    4.855    0.439\n  hf_post =~                                                            \n    int2              1.000                              14.886    0.871\n    HF2     (.p4.)    0.310    0.037    8.480    0.000    4.621    0.534\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre     (a)    0.487    0.195    2.500    0.012    0.611    0.611\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2     (.p6.)   60.277   10.820    5.571    0.000   60.277    0.831\n .int1 ~~                                                               \n   .int2    (.p7.)  -24.643   55.046   -0.448    0.654  -24.643   -0.405\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.15.)   42.656    2.048   20.826    0.000   42.656    2.126\n   .HF1     (.16.)   15.737    1.128   13.949    0.000   15.737    1.424\n   .int2    (.17.)   40.870    2.349   17.396    0.000   40.870    2.392\n   .HF2     (.18.)   14.713    1.044   14.088    0.000   14.713    1.702\n   .hf_post           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1    (.p9.)   52.755  179.207    0.294    0.768   52.755    0.131\n   .HF1     (.10.)   98.604   17.851    5.524    0.000   98.604    0.807\n   .int2    (.11.)   70.300   51.321    1.370    0.171   70.300    0.241\n   .HF2     (.12.)   53.415    9.107    5.865    0.000   53.415    0.714\n    hf_pre  (.13.)  349.971  190.043    1.842    0.066    1.000    1.000\n   .hf_post (.14.)  138.752   35.569    3.901    0.000    0.626    0.626\n\n\nGroup 2 [Treatment]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              18.708    0.932\n    HF1     (.p2.)    0.260    0.112    2.323    0.020    4.855    0.439\n  hf_post =~                                                            \n    int2              1.000                              14.886    0.871\n    HF2     (.p4.)    0.310    0.037    8.480    0.000    4.621    0.534\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre     (a)    0.487    0.195    2.500    0.012    0.611    0.611\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2     (.p6.)   60.277   10.820    5.571    0.000   60.277    0.831\n .int1 ~~                                                               \n   .int2    (.p7.)  -24.643   55.046   -0.448    0.654  -24.643   -0.405\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.15.)   42.656    2.048   20.826    0.000   42.656    2.126\n   .HF1     (.16.)   15.737    1.128   13.949    0.000   15.737    1.424\n   .int2    (.17.)   40.870    2.349   17.396    0.000   40.870    2.392\n   .HF2     (.18.)   14.713    1.044   14.088    0.000   14.713    1.702\n   .hf_post         -28.615    3.149   -9.087    0.000   -1.922   -1.922\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1    (.p9.)   52.755  179.207    0.294    0.768   52.755    0.131\n   .HF1     (.10.)   98.604   17.851    5.524    0.000   98.604    0.807\n   .int2    (.11.)   70.300   51.321    1.370    0.171   70.300    0.241\n   .HF2     (.12.)   53.415    9.107    5.865    0.000   53.415    0.714\n    hf_pre  (.13.)  349.971  190.043    1.842    0.066    1.000    1.000\n   .hf_post (.14.)  138.752   35.569    3.901    0.000    0.626    0.626\n\n\n\n\nlavTestLRT(sem_fit_hotflash_mg, sem_fit_hotflash_mg3) |&gt; print()\n\n\nChi-Squared Difference Test\n\n                     Df    AIC    BIC   Chisq Chisq diff  RMSEA Df diff\nsem_fit_hotflash_mg   3 2901.5 2965.6  5.3986                          \nsem_fit_hotflash_mg3 12 2964.5 3005.5 86.4155     81.017 0.4083       9\n                     Pr(&gt;Chisq)    \nsem_fit_hotflash_mg                \nsem_fit_hotflash_mg3  1.015e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# Slopes Vary\nhotflash_model_mg4 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ c(a, b)*hf_pre\n\n  HF1 ~~ HF2\n  int1 ~~ int2\n\n  hf_pre ~ 0*1\n\"\n\nsem_fit_hotflash_mg4 &lt;- sem(hotflash_model_mg4,\n  data = hotflash,\n  group = \"g\", # factor type\n  group.equal = c(\n    \"loadings\", \"intercepts\",\n    \"residuals\", \"lv.variances\",\n    \"residual.covariances\"\n  )\n)\n\nsummary(sem_fit_hotflash_mg4,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 158 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        31\n  Number of equality constraints                    14\n\n  Number of observations per group:                   \n    Control                                         48\n    Treatment                                       48\n\nModel Test User Model:\n                                                      \n  Test statistic                                45.194\n  Degrees of freedom                                11\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    Control                                     12.421\n    Treatment                                   32.772\n\nModel Test Baseline Model:\n\n  Test statistic                               182.941\n  Degrees of freedom                                12\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.800\n  Tucker-Lewis Index (TLI)                       0.782\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1445.630\n  Loglikelihood unrestricted model (H1)      -1423.033\n                                                      \n  Akaike (AIC)                                2925.260\n  Bayesian (BIC)                              2968.854\n  Sample-size adjusted Bayesian (SABIC)       2915.178\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.254\n  90 Percent confidence interval - lower         0.180\n  90 Percent confidence interval - upper         0.334\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.209\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Control]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              12.737    0.628\n    HF1     (.p2.)    0.454    0.089    5.088    0.000    5.789    0.560\n  hf_post =~                                                            \n    int2              1.000                              19.515    0.899\n    HF2     (.p4.)    0.334    0.033   10.242    0.000    6.513    0.683\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre     (a)    1.506    0.281    5.361    0.000    0.983    0.983\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2     (.p6.)   47.886    8.577    5.583    0.000   47.886    0.803\n .int1 ~~                                                               \n   .int2    (.p7.)   38.457   27.860    1.380    0.167   38.457    0.256\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.15.)   42.656    2.071   20.596    0.000   42.656    2.102\n   .HF1     (.16.)   15.737    1.055   14.916    0.000   15.737    1.522\n   .int2    (.17.)   38.862    2.653   14.651    0.000   38.862    1.790\n   .HF2     (.18.)   14.377    1.074   13.381    0.000   14.377    1.507\n   .hf_post           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1    (.p9.)  249.532   50.019    4.989    0.000  249.532    0.606\n   .HF1     (.10.)   73.335   12.582    5.828    0.000   73.335    0.686\n   .int2    (.11.)   90.406   28.265    3.199    0.001   90.406    0.192\n   .HF2     (.12.)   48.542    7.729    6.280    0.000   48.542    0.534\n    hf_pre  (.13.)  162.236   58.242    2.786    0.005    1.000    1.000\n   .hf_post (.14.)   12.657   30.309    0.418    0.676    0.033    0.033\n\n\nGroup 2 [Treatment]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              12.737    0.628\n    HF1     (.p2.)    0.454    0.089    5.088    0.000    5.789    0.560\n  hf_post =~                                                            \n    int2              1.000                               4.546    0.431\n    HF2     (.p4.)    0.334    0.033   10.242    0.000    1.517    0.213\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre     (b)   -0.222    0.189   -1.176    0.240   -0.623   -0.623\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2     (.p6.)   47.886    8.577    5.583    0.000   47.886    0.803\n .int1 ~~                                                               \n   .int2    (.p7.)   38.457   27.860    1.380    0.167   38.457    0.256\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.15.)   42.656    2.071   20.596    0.000   42.656    2.102\n   .HF1     (.16.)   15.737    1.055   14.916    0.000   15.737    1.522\n   .int2    (.17.)   38.862    2.653   14.651    0.000   38.862    3.687\n   .HF2     (.18.)   14.377    1.074   13.381    0.000   14.377    2.016\n   .hf_post         -27.918    3.030   -9.214    0.000   -6.141   -6.141\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1    (.p9.)  249.532   50.019    4.989    0.000  249.532    0.606\n   .HF1     (.10.)   73.335   12.582    5.828    0.000   73.335    0.686\n   .int2    (.11.)   90.406   28.265    3.199    0.001   90.406    0.814\n   .HF2     (.12.)   48.542    7.729    6.280    0.000   48.542    0.955\n    hf_pre  (.13.)  162.236   58.242    2.786    0.005    1.000    1.000\n   .hf_post (.14.)   12.657   30.309    0.418    0.676    0.612    0.612\n\n\n\n\nlavTestLRT(sem_fit_hotflash_mg3, sem_fit_hotflash_mg4) |&gt; print()\n\n\nChi-Squared Difference Test\n\n                     Df    AIC    BIC  Chisq Chisq diff  RMSEA Df diff\nsem_fit_hotflash_mg4 11 2925.3 2968.8 45.194                          \nsem_fit_hotflash_mg3 12 2964.5 3005.5 86.415     41.222 0.9154       1\n                     Pr(&gt;Chisq)    \nsem_fit_hotflash_mg4               \nsem_fit_hotflash_mg3  1.359e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# initial2: remove covariances between int1, int2\nhotflash_model_mg5 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre\n\n  HF1 ~~ HF2\n\n  hf_pre ~ 0*1\n\"\n\nsem_fit_hotflash_mg5 &lt;- sem(hotflash_model_mg5,\n  data = hotflash,\n  group = \"g\", # factor type\n  group.equal = c(\"loadings\", \"intercepts\")\n)\n\nsummary(sem_fit_hotflash_mg5,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 287 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        29\n  Number of equality constraints                     6\n\n  Number of observations per group:                   \n    Control                                         48\n    Treatment                                       48\n\nModel Test User Model:\n                                                      \n  Test statistic                                 7.074\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.215\n  Test statistic for each group:\n    Control                                      2.241\n    Treatment                                    4.834\n\nModel Test Baseline Model:\n\n  Test statistic                               182.941\n  Degrees of freedom                                12\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.988\n  Tucker-Lewis Index (TLI)                       0.971\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1426.571\n  Loglikelihood unrestricted model (H1)      -1423.033\n                                                      \n  Akaike (AIC)                                2899.141\n  Bayesian (BIC)                              2958.121\n  Sample-size adjusted Bayesian (SABIC)       2885.500\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.093\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.236\n  P-value H_0: RMSEA &lt;= 0.050                    0.277\n  P-value H_0: RMSEA &gt;= 0.080                    0.629\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.095\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Control]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              15.989    0.748\n    HF1     (.p2.)    0.416    0.097    4.289    0.000    6.650    0.620\n  hf_post =~                                                            \n    int2              1.000                              20.482    0.933\n    HF2     (.p4.)    0.320    0.035    9.104    0.000    6.552    0.593\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre            1.169    0.271    4.315    0.000    0.912    0.912\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              64.439   15.850    4.066    0.000   64.439    0.860\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.14.)   42.076    2.027   20.758    0.000   42.076    1.969\n   .HF1     (.15.)   15.623    1.097   14.241    0.000   15.623    1.457\n   .int2    (.16.)   38.970    2.680   14.541    0.000   38.970    1.774\n   .HF2     (.17.)   14.276    1.232   11.590    0.000   14.276    1.292\n   .hf_post           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            200.813   65.273    3.076    0.002  200.813    0.440\n   .HF1              70.829   16.758    4.226    0.000   70.829    0.616\n   .int2             62.914   75.243    0.836    0.403   62.914    0.130\n   .HF2              79.187   17.862    4.433    0.000   79.187    0.648\n    hf_pre          255.650   97.491    2.622    0.009    1.000    1.000\n   .hf_post          70.303   49.163    1.430    0.153    0.168    0.168\n\n\nGroup 2 [Treatment]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              11.989    0.641\n    HF1     (.p2.)    0.416    0.097    4.289    0.000    4.986    0.452\n  hf_post =~                                                            \n    int2              1.000                               6.661    0.627\n    HF2     (.p4.)    0.320    0.035    9.104    0.000    2.131    0.401\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre           -0.191    0.197   -0.972    0.331   -0.345   -0.345\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              46.821   10.332    4.532    0.000   46.821    0.977\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.14.)   42.076    2.027   20.758    0.000   42.076    2.249\n   .HF1     (.15.)   15.623    1.097   14.241    0.000   15.623    1.415\n   .int2    (.16.)   38.970    2.680   14.541    0.000   38.970    3.671\n   .HF2     (.17.)   14.276    1.232   11.590    0.000   14.276    2.687\n   .hf_post         -28.308    3.143   -9.006    0.000   -4.250   -4.250\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            206.182   65.952    3.126    0.002  206.182    0.589\n   .HF1              96.979   23.606    4.108    0.000   96.979    0.796\n   .int2             68.308   20.877    3.272    0.001   68.308    0.606\n   .HF2              23.690    5.445    4.351    0.000   23.690    0.839\n    hf_pre          143.736   60.058    2.393    0.017    1.000    1.000\n   .hf_post          39.097   22.874    1.709    0.087    0.881    0.881\n\n\n\n\n# No main effect\nhotflash_model_mg6 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ hf_pre\n\n  HF1 ~~ HF2\n\n  hf_pre ~ 0*1\n\n  # 또는 hf_post ~ 0*1\n\"\n\nsem_fit_hotflash_mg6 &lt;- sem(hotflash_model_mg6,\n  data = hotflash,\n  group = \"g\", # factor type\n  group.equal = c(\"loadings\", \"intercepts\", \"means\")\n)\n\nsummary(sem_fit_hotflash_mg6,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 255 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        28\n  Number of equality constraints                     6\n\n  Number of observations per group:                   \n    Control                                         48\n    Treatment                                       48\n\nModel Test User Model:\n                                                      \n  Test statistic                                68.745\n  Degrees of freedom                                 6\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    Control                                     54.194\n    Treatment                                   14.551\n\nModel Test Baseline Model:\n\n  Test statistic                               182.941\n  Degrees of freedom                                12\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.633\n  Tucker-Lewis Index (TLI)                       0.266\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1457.406\n  Loglikelihood unrestricted model (H1)      -1423.033\n                                                      \n  Akaike (AIC)                                2958.811\n  Bayesian (BIC)                              3015.227\n  Sample-size adjusted Bayesian (SABIC)       2945.763\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.467\n  90 Percent confidence interval - lower         0.371\n  90 Percent confidence interval - upper         0.569\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.524\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Control]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              19.218    0.800\n    HF1     (.p2.)    0.501    0.089    5.637    0.000    9.635    0.763\n  hf_post =~                                                            \n    int2              1.000                              33.875    0.970\n    HF2     (.p4.)    0.375    0.053    7.027    0.000   12.696    0.820\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre            1.637    0.274    5.967    0.000    0.929    0.929\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              63.008   17.384    3.625    0.000   63.008    0.871\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.14.)   34.183    1.786   19.142    0.000   34.183    1.423\n   .HF1     (.15.)   10.714    0.901   11.889    0.000   10.714    0.849\n   .int2    (.16.)   14.842    1.410   10.527    0.000   14.842    0.425\n   .HF2     (.17.)    4.945    0.627    7.891    0.000    4.945    0.319\n   .hf_post           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            208.016   61.528    3.381    0.001  208.016    0.360\n   .HF1              66.548   17.601    3.781    0.000   66.548    0.418\n   .int2             72.980  115.129    0.634    0.526   72.980    0.060\n   .HF2              78.723   22.701    3.468    0.001   78.723    0.328\n    hf_pre          369.314  117.973    3.131    0.002    1.000    1.000\n   .hf_post         157.820   80.531    1.960    0.050    0.138    0.138\n\n\nGroup 2 [Treatment]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              11.075    0.571\n    HF1     (.p2.)    0.501    0.089    5.637    0.000    5.553    0.494\n  hf_post =~                                                            \n    int2              1.000                               6.310    0.562\n    HF2     (.p4.)    0.375    0.053    7.027    0.000    2.365    0.441\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre           -0.378    0.254   -1.488    0.137   -0.664   -0.664\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              50.408   10.900    4.625    0.000   50.408    1.072\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.14.)   34.183    1.786   19.142    0.000   34.183    1.764\n   .HF1     (.15.)   10.714    0.901   11.889    0.000   10.714    0.954\n   .int2    (.16.)   14.842    1.410   10.527    0.000   14.842    1.322\n   .HF2     (.17.)    4.945    0.627    7.891    0.000    4.945    0.921\n   .hf_post           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            252.923   64.159    3.942    0.000  252.923    0.673\n   .HF1              95.269   24.474    3.893    0.000   95.269    0.755\n   .int2             86.205   22.612    3.812    0.000   86.205    0.684\n   .HF2              23.222    5.620    4.132    0.000   23.222    0.806\n    hf_pre          122.652   50.232    2.442    0.015    1.000    1.000\n   .hf_post          22.267   28.996    0.768    0.443    0.559    0.559\n\n\n\n\n# No slope difference: interaction\nhotflash_model_mg7 &lt;- \"\n  hf_pre =~ int1 + HF1\n  hf_post =~ int2 + HF2\n\n  hf_post ~ c(a, a)*hf_pre\n\n  HF1 ~~ HF2\n\n  hf_pre ~ 0*1\n\"\n\nsem_fit_hotflash_mg7 &lt;- sem(hotflash_model_mg7,\n  data = hotflash,\n  group = \"g\", # factor type\n  group.equal = c(\"loadings\", \"intercepts\")\n)\n\nsummary(sem_fit_hotflash_mg7,\n  remove.unused = FALSE, # keep the unused parameters\n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt; print()\n\nlavaan 0.6-18 ended normally after 226 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        29\n  Number of equality constraints                     7\n\n  Number of observations per group:                   \n    Control                                         48\n    Treatment                                       48\n\nModel Test User Model:\n                                                      \n  Test statistic                                22.975\n  Degrees of freedom                                 6\n  P-value (Chi-square)                           0.001\n  Test statistic for each group:\n    Control                                      2.791\n    Treatment                                   20.184\n\nModel Test Baseline Model:\n\n  Test statistic                               182.941\n  Degrees of freedom                                12\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.901\n  Tucker-Lewis Index (TLI)                       0.801\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1434.521\n  Loglikelihood unrestricted model (H1)      -1423.033\n                                                      \n  Akaike (AIC)                                2913.041\n  Bayesian (BIC)                              2969.457\n  Sample-size adjusted Bayesian (SABIC)       2899.993\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.243\n  90 Percent confidence interval - lower         0.143\n  90 Percent confidence interval - upper         0.352\n  P-value H_0: RMSEA &lt;= 0.050                    0.002\n  P-value H_0: RMSEA &gt;= 0.080                    0.994\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.113\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Control]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                              16.360    0.758\n    HF1     (.p2.)    0.395    0.093    4.244    0.000    6.469    0.607\n  hf_post =~                                                            \n    int2              1.000                              20.603    0.929\n    HF2     (.p4.)    0.319    0.038    8.466    0.000    6.573    0.595\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre     (a)    1.161    0.269    4.318    0.000    0.922    0.922\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              64.363   15.818    4.069    0.000   64.363    0.855\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                               0.000    0.000\n   .int1    (.14.)   41.568    1.970   21.100    0.000   41.568    1.927\n   .HF1     (.15.)   15.430    1.068   14.449    0.000   15.430    1.448\n   .int2    (.16.)   38.358    2.623   14.625    0.000   38.358    1.729\n   .HF2     (.17.)   14.122    1.209   11.680    0.000   14.122    1.278\n   .hf_post           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            197.742   66.985    2.952    0.003  197.742    0.425\n   .HF1              71.775   16.726    4.291    0.000   71.775    0.632\n   .int2             67.392   75.715    0.890    0.373   67.392    0.137\n   .HF2              78.925   17.807    4.432    0.000   78.925    0.646\n    hf_pre          267.659  101.065    2.648    0.008    1.000    1.000\n   .hf_post          63.824   48.810    1.308    0.191    0.150    0.150\n\n\nGroup 2 [Treatment]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_pre =~                                                             \n    int1              1.000                                  NA       NA\n    HF1     (.p2.)    0.395    0.093    4.244    0.000       NA       NA\n  hf_post =~                                                            \n    int2              1.000                               5.747    0.543\n    HF2     (.p4.)    0.319    0.038    8.466    0.000    1.833    0.353\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  hf_post ~                                                             \n    hf_pre     (a)    1.161    0.269    4.318    0.000       NA       NA\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .HF1 ~~                                                                \n   .HF2              47.574   10.825    4.395    0.000   47.574    0.859\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    hf_pre            0.000                                  NA       NA\n   .int1    (.14.)   41.568    1.970   21.100    0.000   41.568    2.232\n   .HF1     (.15.)   15.430    1.068   14.449    0.000   15.430    1.369\n   .int2    (.16.)   38.358    2.623   14.625    0.000   38.358    3.627\n   .HF2     (.17.)   14.122    1.209   11.680    0.000   14.122    2.722\n   .hf_post         -27.603    3.080   -8.961    0.000   -4.803   -4.803\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .int1            367.497   75.977    4.837    0.000  367.497    1.059\n   .HF1             130.206   26.200    4.970    0.000  130.206    1.025\n   .int2             78.807   23.605    3.339    0.001   78.807    0.705\n   .HF2              23.551    5.373    4.383    0.000   23.551    0.875\n    hf_pre          -20.543   19.456   -1.056    0.291       NA       NA\n   .hf_post          60.715   25.745    2.358    0.018    1.838    1.838",
    "crumbs": [
      "Keith's",
      "Latent Means"
    ]
  },
  {
    "objectID": "contents/cleaning.html",
    "href": "contents/cleaning.html",
    "title": "Cleaning",
    "section": "",
    "text": "select(), mutate(), filter(), rename() : 기본 tidyverse verbs\n\nrowSums(), rowMeans() : composite 변수들의 합 또는 평균을 구함\n\nfactor() : 카테고리 변수의 변환\n\n앞서 다운받은 데이터: altruism.csv 파일 링크\n\nlibrary(tidyverse)\n\n# import data\nhelping &lt;- read_csv(\"data/altruism.csv\")\nhelping |&gt; print()\n\n# A tibble: 120 x 12\n     id pho_1 pho_2 pho_3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n# i 114 more rows\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n\n\nrename()\n\nhelping |&gt;\n    rename(ph1 = pho_1, ph2 = pho_2, ph3 = pho_3) |&gt;\n    print()\n\n# A tibble: 120 x 12\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n# i 114 more rows\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n변형 후에는 꼭 변수에 assign!\n\nhelping &lt;-     # 원래 데이터에 overwrite\n    helping |&gt;\n    rename(ph1 = pho_1, ph2 = pho_2, ph3 = pho_3)\n\nhelping |&gt; print()\n\n# A tibble: 120 x 12\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n# i 114 more rows\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n\n\n\nrowSums(row, na.rm = TRUE) 함수를 이용하는 것이 직접 덧셈보다 더 적절함\nph1, ph2, ph3 세 문항을 더하려면,\n\n# 먼저 문항을 선택/확인\nhelping |&gt;\n  select(ph1:ph3) |&gt; # position!\n  print()\n\n# A tibble: 120 x 3\n    ph1   ph2   ph3\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    95    95    95\n2    58    62    NA\n3   100    50    50\n4    77    77    64\n5    NA    NA    NA\n6   100    75   100\n# i 114 more rows\n\n\n\nhelping |&gt;\n  select(ph1:ph3) |&gt;\n  rowSums(na.rm = TRUE) |&gt;\n  print()\n\n  [1] 285 120 200 218   0 275 257 178 256 189 215 226 209 246 159 197 205 225\n [19] 150 195  44   0   0 125 225 211 270 176 241 205   0 220  98  79 143 165\n [37]  49 294 300 292 101 285 208 230 255 150 299 188 208 205 138 267 187 300\n [55] 195 300 236  59 226 193 213 250  32 228 250 300 300 190 230 281 196 268\n [73] 240 250  39 233 211 198 199 234 300 215 240   9 261 209 281 201 270 255\n [91] 177 235 161   0 242 151 182 170   3 222 172 194 300 300 293 238 243 260\n[109] 197 294 280 195 255   1 162 278 176 262 300 164\n\n\n\nhelping[\"phone\"] &lt;-    # \"phone\"이라는 새로운 변수에 assign!\n  helping |&gt;\n  select(ph1:ph3) |&gt;\n  rowSums(na.rm = TRUE)\n\nhelping |&gt; print(width = Inf)\n\n# A tibble: 120 x 13\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n  emp_q26 phone\n    &lt;dbl&gt; &lt;dbl&gt;\n1      70   285\n2      59   120\n3     100   200\n4      69   218\n5      NA     0\n6      90   275\n# i 114 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\n다음과 같이 직접 더하는 것은 부적절\nhelping |&gt;\n  mutate(phone = ph1 + ph2 + ph3) \n#      id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n# 1     1    95    95    95     1  2004      80      NA      80      80      70\n# 2     2    58    62    NA     0  2003      62      58      59      57      56\n# 3     3   100    50    50    NA  2003      90      51      51      51      52\n# 4     4    77    77    64     1  2004      66      72      88      82      67\n# 5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n# 6     6   100    75   100     0  2004     100      60      70      55      70\n#   emp_q26 phone\n#     &lt;dbl&gt; &lt;dbl&gt;\n# 1      70   285\n# 2      59    NA\n# 3     100   200\n# 4      69   218\n# 5      NA    NA\n# 6      90   275\n# # … with 114 more rows\n\n\n\n\n\nrowMeans(row, na.rm = TRUE) 함수를 이용하는 것이 적절함\n\n# 먼저, 평균을 낼 문항을 선택/확인\nhelping |&gt;\n  select(emp_q20, emp_q22:emp_q26) |&gt;  # \":\" operator와 \",\" 섞어써도 무방\n  print()\n\n# A tibble: 120 x 6\n  emp_q20 emp_q22 emp_q23 emp_q24 emp_q25 emp_q26\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1      80      NA      80      80      70      70\n2      62      58      59      57      56      59\n3      90      51      51      51      52     100\n4      66      72      88      82      67      69\n5      NA      NA      NA      NA      NA      NA\n6     100      60      70      55      70      90\n# i 114 more rows\n\n\n\nhelping[\"persp\"] &lt;- helping |&gt;    # \"persp\"라는 새로운 변수에 assign!\n  select(emp_q20, emp_q22:emp_q26) |&gt;\n  rowMeans(na.rm = TRUE)\n\nhelping |&gt; print(width = Inf)\n\n# A tibble: 120 x 14\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n  emp_q26 phone persp\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      70   285  76  \n2      59   120  58.5\n3     100   200  65.8\n4      69   218  74  \n5      NA     0 NaN  \n6      90   275  74.2\n# i 114 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nTidyverse에서 row-wise operation을 하려면,\nhelping |&gt;\n    rowwise() |&gt;\n    mutate(persp = mean(c(emp_q20, c_across(emp_q22:emp_q26)), na.rm = TRUE)) |&gt;\n    ungroup()\n참고: column-wise operation\n\n\n\n\n\n\n표준화(standardize): scale(x) 함수를 이용\n중심화(center): scale(x, scale = FALSE) 함수를 이용\n\n\nhelping |&gt; \n    mutate(\n        phone_z = scale(phone) %&gt;% as.vector,  # scale()은 matrix로 반환; vector 변환 필요\n        persp_z = scale(persp) %&gt;% as.vector\n    ) |&gt;\n    print(n = 3, width = Inf)\n\n# A tibble: 120 x 16\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n  emp_q26 phone persp  phone_z persp_z\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1      70   285  76    1.05      0.104\n2      59   120  58.5 -1.00     -0.981\n3     100   200  65.8 -0.00643  -0.527\n# i 117 more rows\n\n\nacross() 함수를 이용하면 여러 변수를 한 번에 변환할 수 있음\n\nhelping |&gt; \n    mutate(across(.cols = c(phone, persp),\n                  .fns = ~(scale(.) %&gt;% as.vector),\n                  .names = \"{.col}_z\")) |&gt;  # 변수명을 일괄 변경: 변수명 + \"_z\"\n    print(n = 3, width = Inf)\n\n# A tibble: 120 x 16\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n  emp_q26 phone persp  phone_z persp_z\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1      70   285  76    1.05      0.104\n2      59   120  58.5 -1.00     -0.981\n3     100   200  65.8 -0.00643  -0.527\n# i 117 more rows\n\n\n\n\n\n카테고리 변수는 R의 factor 타입으로 바꾸어 분석하는 것이 유리함.\n간단한 연산은 직접 계산.\n\nhelping |&gt;\n  mutate(\n    sex = factor(sex, levels = c(0, 1), labels = c(\"male\", \"female\")),  # factor 타입의 변수로 변환\n    age = 2023 - age  # 출생년도로부터 나이 계산\n  ) |&gt;\n  print(width = Inf)\n\n# A tibble: 120 x 14\n     id   ph1   ph2   ph3 sex      age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95 female    19      80      NA      80      80      70\n2     2    58    62    NA male      20      62      58      59      57      56\n3     3   100    50    50 NA        20      90      51      51      51      52\n4     4    77    77    64 female    19      66      72      88      82      67\n5     5    NA    NA    NA NA        NA      NA      NA      NA      NA      NA\n6     6   100    75   100 male      19     100      60      70      55      70\n  emp_q26 phone persp\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      70   285  76  \n2      59   120  58.5\n3     100   200  65.8\n4      69   218  74  \n5      NA     0 NaN  \n6      90   275  74.2\n# i 114 more rows\n\n\n\n\n\nfilter()를 활용\n예를 들어, 5번째 행을 지우려면\n\nhelping |&gt;\n    filter(!id == 5) |&gt; # !는 not의 의미\n    print()\n\n# 다시 helping에 assign 해야 수정됨!\n\n# A tibble: 119 x 14\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     6   100    75   100     0  2004     100      60      70      55      70\n6     7    77    94    86     1  2004      91      93      85      91      73\n# i 113 more rows\n# i 3 more variables: emp_q26 &lt;dbl&gt;, phone &lt;dbl&gt;, persp &lt;dbl&gt;\n\n\n여러 행을 지우려면?\n%in% 응용\n\nhelping |&gt;\n    filter(!id %in% c(1, 3, 5)) |&gt; # !는 not의 의미\n    print()\n\n# A tibble: 117 x 14\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     2    58    62    NA     0  2003      62      58      59      57      56\n2     4    77    77    64     1  2004      66      72      88      82      67\n3     6   100    75   100     0  2004     100      60      70      55      70\n4     7    77    94    86     1  2004      91      93      85      91      73\n5     8    90    68    20     0  2004      67      66      31      67      63\n6     9   100    79    77     0  2003      61      51      30      51      51\n# i 111 more rows\n# i 3 more variables: emp_q26 &lt;dbl&gt;, phone &lt;dbl&gt;, persp &lt;dbl&gt;\n\n\n\n\n\nselect() 활용\nemp_q23, emp_q25 두 열을 삭제\n\nhelping |&gt;\n    select(-emp_q23, -emp_q25) |&gt;\n    print()\n\n# A tibble: 120 x 12\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q24 emp_q26 phone\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      70   285\n2     2    58    62    NA     0  2003      62      58      57      59   120\n3     3   100    50    50    NA  2003      90      51      51     100   200\n4     4    77    77    64     1  2004      66      72      82      69   218\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA     0\n6     6   100    75   100     0  2004     100      60      55      90   275\n# i 114 more rows\n# i 1 more variable: persp &lt;dbl&gt;\n\n\nemp_q23부터 emp_q26 열을 삭제 (위치의 의미로)\n\nhelping |&gt;\n    select(-(emp_q23:emp_q26)) |&gt;  # () 꼭 필요\n    print()\n\n# A tibble: 120 x 10\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 phone persp\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA   285  76  \n2     2    58    62    NA     0  2003      62      58   120  58.5\n3     3   100    50    50    NA  2003      90      51   200  65.8\n4     4    77    77    64     1  2004      66      72   218  74  \n5     5    NA    NA    NA    NA    NA      NA      NA     0 NaN  \n6     6   100    75   100     0  2004     100      60   275  74.2\n# i 114 more rows",
    "crumbs": [
      "R tutorial",
      "Cleaning"
    ]
  },
  {
    "objectID": "contents/cleaning.html#유용한-함수들",
    "href": "contents/cleaning.html#유용한-함수들",
    "title": "Cleaning",
    "section": "",
    "text": "select(), mutate(), filter(), rename() : 기본 tidyverse verbs\n\nrowSums(), rowMeans() : composite 변수들의 합 또는 평균을 구함\n\nfactor() : 카테고리 변수의 변환\n\n앞서 다운받은 데이터: altruism.csv 파일 링크\n\nlibrary(tidyverse)\n\n# import data\nhelping &lt;- read_csv(\"data/altruism.csv\")\nhelping |&gt; print()\n\n# A tibble: 120 x 12\n     id pho_1 pho_2 pho_3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n# i 114 more rows\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n\n\nrename()\n\nhelping |&gt;\n    rename(ph1 = pho_1, ph2 = pho_2, ph3 = pho_3) |&gt;\n    print()\n\n# A tibble: 120 x 12\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n# i 114 more rows\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n변형 후에는 꼭 변수에 assign!\n\nhelping &lt;-     # 원래 데이터에 overwrite\n    helping |&gt;\n    rename(ph1 = pho_1, ph2 = pho_2, ph3 = pho_3)\n\nhelping |&gt; print()\n\n# A tibble: 120 x 12\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n# i 114 more rows\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n\n\n\nrowSums(row, na.rm = TRUE) 함수를 이용하는 것이 직접 덧셈보다 더 적절함\nph1, ph2, ph3 세 문항을 더하려면,\n\n# 먼저 문항을 선택/확인\nhelping |&gt;\n  select(ph1:ph3) |&gt; # position!\n  print()\n\n# A tibble: 120 x 3\n    ph1   ph2   ph3\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    95    95    95\n2    58    62    NA\n3   100    50    50\n4    77    77    64\n5    NA    NA    NA\n6   100    75   100\n# i 114 more rows\n\n\n\nhelping |&gt;\n  select(ph1:ph3) |&gt;\n  rowSums(na.rm = TRUE) |&gt;\n  print()\n\n  [1] 285 120 200 218   0 275 257 178 256 189 215 226 209 246 159 197 205 225\n [19] 150 195  44   0   0 125 225 211 270 176 241 205   0 220  98  79 143 165\n [37]  49 294 300 292 101 285 208 230 255 150 299 188 208 205 138 267 187 300\n [55] 195 300 236  59 226 193 213 250  32 228 250 300 300 190 230 281 196 268\n [73] 240 250  39 233 211 198 199 234 300 215 240   9 261 209 281 201 270 255\n [91] 177 235 161   0 242 151 182 170   3 222 172 194 300 300 293 238 243 260\n[109] 197 294 280 195 255   1 162 278 176 262 300 164\n\n\n\nhelping[\"phone\"] &lt;-    # \"phone\"이라는 새로운 변수에 assign!\n  helping |&gt;\n  select(ph1:ph3) |&gt;\n  rowSums(na.rm = TRUE)\n\nhelping |&gt; print(width = Inf)\n\n# A tibble: 120 x 13\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n  emp_q26 phone\n    &lt;dbl&gt; &lt;dbl&gt;\n1      70   285\n2      59   120\n3     100   200\n4      69   218\n5      NA     0\n6      90   275\n# i 114 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\n다음과 같이 직접 더하는 것은 부적절\nhelping |&gt;\n  mutate(phone = ph1 + ph2 + ph3) \n#      id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n# 1     1    95    95    95     1  2004      80      NA      80      80      70\n# 2     2    58    62    NA     0  2003      62      58      59      57      56\n# 3     3   100    50    50    NA  2003      90      51      51      51      52\n# 4     4    77    77    64     1  2004      66      72      88      82      67\n# 5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n# 6     6   100    75   100     0  2004     100      60      70      55      70\n#   emp_q26 phone\n#     &lt;dbl&gt; &lt;dbl&gt;\n# 1      70   285\n# 2      59    NA\n# 3     100   200\n# 4      69   218\n# 5      NA    NA\n# 6      90   275\n# # … with 114 more rows\n\n\n\n\n\nrowMeans(row, na.rm = TRUE) 함수를 이용하는 것이 적절함\n\n# 먼저, 평균을 낼 문항을 선택/확인\nhelping |&gt;\n  select(emp_q20, emp_q22:emp_q26) |&gt;  # \":\" operator와 \",\" 섞어써도 무방\n  print()\n\n# A tibble: 120 x 6\n  emp_q20 emp_q22 emp_q23 emp_q24 emp_q25 emp_q26\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1      80      NA      80      80      70      70\n2      62      58      59      57      56      59\n3      90      51      51      51      52     100\n4      66      72      88      82      67      69\n5      NA      NA      NA      NA      NA      NA\n6     100      60      70      55      70      90\n# i 114 more rows\n\n\n\nhelping[\"persp\"] &lt;- helping |&gt;    # \"persp\"라는 새로운 변수에 assign!\n  select(emp_q20, emp_q22:emp_q26) |&gt;\n  rowMeans(na.rm = TRUE)\n\nhelping |&gt; print(width = Inf)\n\n# A tibble: 120 x 14\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA      NA\n6     6   100    75   100     0  2004     100      60      70      55      70\n  emp_q26 phone persp\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      70   285  76  \n2      59   120  58.5\n3     100   200  65.8\n4      69   218  74  \n5      NA     0 NaN  \n6      90   275  74.2\n# i 114 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nTidyverse에서 row-wise operation을 하려면,\nhelping |&gt;\n    rowwise() |&gt;\n    mutate(persp = mean(c(emp_q20, c_across(emp_q22:emp_q26)), na.rm = TRUE)) |&gt;\n    ungroup()\n참고: column-wise operation\n\n\n\n\n\n\n표준화(standardize): scale(x) 함수를 이용\n중심화(center): scale(x, scale = FALSE) 함수를 이용\n\n\nhelping |&gt; \n    mutate(\n        phone_z = scale(phone) %&gt;% as.vector,  # scale()은 matrix로 반환; vector 변환 필요\n        persp_z = scale(persp) %&gt;% as.vector\n    ) |&gt;\n    print(n = 3, width = Inf)\n\n# A tibble: 120 x 16\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n  emp_q26 phone persp  phone_z persp_z\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1      70   285  76    1.05      0.104\n2      59   120  58.5 -1.00     -0.981\n3     100   200  65.8 -0.00643  -0.527\n# i 117 more rows\n\n\nacross() 함수를 이용하면 여러 변수를 한 번에 변환할 수 있음\n\nhelping |&gt; \n    mutate(across(.cols = c(phone, persp),\n                  .fns = ~(scale(.) %&gt;% as.vector),\n                  .names = \"{.col}_z\")) |&gt;  # 변수명을 일괄 변경: 변수명 + \"_z\"\n    print(n = 3, width = Inf)\n\n# A tibble: 120 x 16\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n  emp_q26 phone persp  phone_z persp_z\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1      70   285  76    1.05      0.104\n2      59   120  58.5 -1.00     -0.981\n3     100   200  65.8 -0.00643  -0.527\n# i 117 more rows\n\n\n\n\n\n카테고리 변수는 R의 factor 타입으로 바꾸어 분석하는 것이 유리함.\n간단한 연산은 직접 계산.\n\nhelping |&gt;\n  mutate(\n    sex = factor(sex, levels = c(0, 1), labels = c(\"male\", \"female\")),  # factor 타입의 변수로 변환\n    age = 2023 - age  # 출생년도로부터 나이 계산\n  ) |&gt;\n  print(width = Inf)\n\n# A tibble: 120 x 14\n     id   ph1   ph2   ph3 sex      age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95 female    19      80      NA      80      80      70\n2     2    58    62    NA male      20      62      58      59      57      56\n3     3   100    50    50 NA        20      90      51      51      51      52\n4     4    77    77    64 female    19      66      72      88      82      67\n5     5    NA    NA    NA NA        NA      NA      NA      NA      NA      NA\n6     6   100    75   100 male      19     100      60      70      55      70\n  emp_q26 phone persp\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      70   285  76  \n2      59   120  58.5\n3     100   200  65.8\n4      69   218  74  \n5      NA     0 NaN  \n6      90   275  74.2\n# i 114 more rows\n\n\n\n\n\nfilter()를 활용\n예를 들어, 5번째 행을 지우려면\n\nhelping |&gt;\n    filter(!id == 5) |&gt; # !는 not의 의미\n    print()\n\n# 다시 helping에 assign 해야 수정됨!\n\n# A tibble: 119 x 14\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      80      70\n2     2    58    62    NA     0  2003      62      58      59      57      56\n3     3   100    50    50    NA  2003      90      51      51      51      52\n4     4    77    77    64     1  2004      66      72      88      82      67\n5     6   100    75   100     0  2004     100      60      70      55      70\n6     7    77    94    86     1  2004      91      93      85      91      73\n# i 113 more rows\n# i 3 more variables: emp_q26 &lt;dbl&gt;, phone &lt;dbl&gt;, persp &lt;dbl&gt;\n\n\n여러 행을 지우려면?\n%in% 응용\n\nhelping |&gt;\n    filter(!id %in% c(1, 3, 5)) |&gt; # !는 not의 의미\n    print()\n\n# A tibble: 117 x 14\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     2    58    62    NA     0  2003      62      58      59      57      56\n2     4    77    77    64     1  2004      66      72      88      82      67\n3     6   100    75   100     0  2004     100      60      70      55      70\n4     7    77    94    86     1  2004      91      93      85      91      73\n5     8    90    68    20     0  2004      67      66      31      67      63\n6     9   100    79    77     0  2003      61      51      30      51      51\n# i 111 more rows\n# i 3 more variables: emp_q26 &lt;dbl&gt;, phone &lt;dbl&gt;, persp &lt;dbl&gt;\n\n\n\n\n\nselect() 활용\nemp_q23, emp_q25 두 열을 삭제\n\nhelping |&gt;\n    select(-emp_q23, -emp_q25) |&gt;\n    print()\n\n# A tibble: 120 x 12\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 emp_q24 emp_q26 phone\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA      80      70   285\n2     2    58    62    NA     0  2003      62      58      57      59   120\n3     3   100    50    50    NA  2003      90      51      51     100   200\n4     4    77    77    64     1  2004      66      72      82      69   218\n5     5    NA    NA    NA    NA    NA      NA      NA      NA      NA     0\n6     6   100    75   100     0  2004     100      60      55      90   275\n# i 114 more rows\n# i 1 more variable: persp &lt;dbl&gt;\n\n\nemp_q23부터 emp_q26 열을 삭제 (위치의 의미로)\n\nhelping |&gt;\n    select(-(emp_q23:emp_q26)) |&gt;  # () 꼭 필요\n    print()\n\n# A tibble: 120 x 10\n     id   ph1   ph2   ph3   sex   age emp_q20 emp_q22 phone persp\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1    95    95    95     1  2004      80      NA   285  76  \n2     2    58    62    NA     0  2003      62      58   120  58.5\n3     3   100    50    50    NA  2003      90      51   200  65.8\n4     4    77    77    64     1  2004      66      72   218  74  \n5     5    NA    NA    NA    NA    NA      NA      NA     0 NaN  \n6     6   100    75   100     0  2004     100      60   275  74.2\n# i 114 more rows",
    "crumbs": [
      "R tutorial",
      "Cleaning"
    ]
  },
  {
    "objectID": "contents/cleaning.html#이상치-발견",
    "href": "contents/cleaning.html#이상치-발견",
    "title": "Cleaning",
    "section": "이상치 발견",
    "text": "이상치 발견\nOutliers을 찾는 방법은 다양하고 복잡한 테크닉을 요하기도 하는데, 앞으로 점차 익히게 될 것임\n예를 들어, age에 잘못 기입한 경우가 있는데\n\nhelping &lt;- read_csv(\"data/altruism.csv\")\n\nhelping |&gt;\n    ggplot(aes(x = age)) +\n    geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nage는 출생년도를 물어봤으나 다른 답을 한 경우들이 있음\n값은 2002 ~ 2004 사이가 정상이므로 filter()를 써서 확인해 볼 수 있음\n\nhelping |&gt;\n    filter(age &lt; 2002 | age &gt; 2004) |&gt;\n    print()\n\n# A tibble: 7 x 12\n     id pho_1 pho_2 pho_3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1    11    65    94    56     1   203      62      86      58      47      62\n2    21    17    10    17     1 20004       0       6       1       0       4\n3    43    90    88    30     1   507     100      78      62     100      78\n4    52   100    82    85     1   723      87      83      89     100      88\n5    59    76    86    64     0   709     100      93      67      94      79\n6   108    75   100    85     1  2005     100     100     100     100      97\n7   118    92    76    94     0  1108      55      51      51      60      53\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n값을 수정\n\n# 이상치에 대한 id를 우선 추출\nids_anomaly = helping |&gt;\n    filter(age &lt; 2002 | age &gt; 2004) |&gt;\n    pull(id)  # vector로 반환\nids_anomaly |&gt; print()\n\n[1]  11  21  43  52  59 108 118\n\n\n\n# 모두 NA로 변경하는 경우\nhelping |&gt;\n    # ifelse(조건, 참일 때 값, 거짓일 때 값)\n    mutate(\n        age = ifelse(age &gt; 2004, 2004, age),\n        age = ifelse(age &lt; 2002, NA, age)\n        # 대신, age = ifelse(age &gt; 2004, 2004, ifelse(age &lt; 2002, NA, age))\n        ) |&gt; \n    filter(id %in% ids_anomaly) |&gt; \n    print()\n\n# A tibble: 7 x 12\n     id pho_1 pho_2 pho_3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1    11    65    94    56     1    NA      62      86      58      47      62\n2    21    17    10    17     1  2004       0       6       1       0       4\n3    43    90    88    30     1    NA     100      78      62     100      78\n4    52   100    82    85     1    NA      87      83      89     100      88\n5    59    76    86    64     0    NA     100      93      67      94      79\n6   108    75   100    85     1  2004     100     100     100     100      97\n7   118    92    76    94     0    NA      55      51      51      60      53\n# i 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n\n# 직접 입력하는 방식\nhelping[helping$id %in% ids_anomaly, \"age\"] |&gt; print()\n\n# A tibble: 7 x 1\n    age\n  &lt;dbl&gt;\n1   203\n2 20004\n3   507\n4   723\n5   709\n6  2005\n7  1108\n\n\n\n# 직접 입력하는 방식\nhelping[helping$id %in% ids_anomaly, \"age\"] &lt;- c(NA, 2004, NA, NA, NA, 2005, NA)\nhelping |&gt;\n    filter(id %in% ids_anomaly) |&gt;\n    print()\n\n# A tibble: 7 x 12\n     id pho_1 pho_2 pho_3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1    11    65    94    56     1    NA      62      86      58      47      62\n2    21    17    10    17     1  2004       0       6       1       0       4\n3    43    90    88    30     1    NA     100      78      62     100      78\n4    52   100    82    85     1    NA      87      83      89     100      88\n5    59    76    86    64     0    NA     100      93      67      94      79\n6   108    75   100    85     1  2005     100     100     100     100      97\n7   118    92    76    94     0    NA      55      51      51      60      53\n# i 1 more variable: emp_q26 &lt;dbl&gt;",
    "crumbs": [
      "R tutorial",
      "Cleaning"
    ]
  },
  {
    "objectID": "contents/cleaning.html#샘플-r-script",
    "href": "contents/cleaning.html#샘플-r-script",
    "title": "Cleaning",
    "section": "샘플 R script",
    "text": "샘플 R script\n\nlibrary(tidyverse)\n\n# import data\nhelping &lt;- read_csv(\"data/altruism.csv\")\n\n# rename\nhelping &lt;- helping |&gt;\n  rename(ph1 = pho_1, ph2 = pho_2, ph3 = pho_3) \n\n# delete reponses\nhelping &lt;- helping |&gt;\n  filter(!id == 5)\n\n# scoring\nhelping[\"phone\"] &lt;- helping |&gt;\n  select(ph1:ph3) |&gt;\n  rowMeans(na.rm = TRUE)\n\nhelping[\"persp\"] &lt;- helping |&gt; \n  select(emp_q20, emp_q22, emp_q24:emp_q26) |&gt;\n  rowMeans(na.rm = TRUE)\n\n# substitute anolamies\nhelping &lt;- helping |&gt;\n  mutate(age = ifelse(age &gt; 2004, 2004, ifelse(age &lt; 2002, NA, age)))\n\n# factors and etc.\nhelping &lt;- helping |&gt;\n  mutate(\n    sex = factor(sex, levels = c(0, 1), labels = c(\"male\", \"female\")),\n    age = 2023 - age\n  )\n\n# select variables\nhelping &lt;- helping |&gt;\n  select(id, sex, age, phone, persp)\n\n정리된 파일로 분석 시작!\n\nhelping |&gt; print()\n\n# A tibble: 119 x 5\n     id sex      age phone persp\n  &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1 female    19  95    75  \n2     2 male      20  60    58.4\n3     3 NA        20  66.7  68.8\n4     4 female    19  72.7  71.2\n5     6 male      19  91.7  75  \n6     7 female    19  85.7  82.6\n# i 113 more rows",
    "crumbs": [
      "R tutorial",
      "Cleaning"
    ]
  },
  {
    "objectID": "contents/import.html",
    "href": "contents/import.html",
    "title": "Import",
    "section": "",
    "text": "자세한 데이터 import에 대해서는 링크",
    "crumbs": [
      "R tutorial",
      "Import"
    ]
  },
  {
    "objectID": "contents/import.html#text-files-csv",
    "href": "contents/import.html#text-files-csv",
    "title": "Import",
    "section": "Text files: csv",
    "text": "Text files: csv\nreadr 패키지(tidyverse에 포함)\nread_csv(), write_csv()\n\nR 기본 함수 read.csv()를 개선\n다양한 옵션은 ?read_csv, ?write_csv 참고\n\n\ncsv 파일 읽기\naltruism.csv 파일 링크\n\nlibrary(tidyverse)\n\nhelping &lt;- read_csv(\"data/altruism.csv\")\nhelping |&gt; print()\n\n# A tibble: 120 × 12\n      id pho_1 pho_2 pho_3   sex   age emp_q20 emp_q22 emp_q23 emp_q24 emp_q25\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1   250    95    95    95     1  2004      80      NA      80      80      70\n 2    32    58    62    NA     0  2003      62      58      59      57      56\n 3   109   100    50    50    NA  2003      90      51      51      51      52\n 4   209    77    77    64     1  2004      66      72      88      82      67\n 5    94    77    50    77     1  2003     100     100     100      51      78\n 6   260   100    75   100     0  2004     100      60      70      55      70\n 7   258    77    94    86     1  2004      91      93      85      91      73\n 8   244    90    68    20     0  2004      67      66      31      67      63\n 9   180   100    79    77     0  2003      61      51      30      51      51\n10   182    75    50    64     1  2003      80      80      70      65      70\n# … with 110 more rows, and 1 more variable: emp_q26 &lt;dbl&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nread_csv()의 자주 사용되는 옵션\nread_csv(\"data/file.csv\", skip = 2) # 첫 2절 스킵\nread_csv(\"data/file.csv\", na = \".\") # 결측치가 .으로 기록된 파일\n\n\n\n\ncsv 파일 쓰기\nwrite_csv(): 단, 쓰기를 하면서 변수 타입 소멸\n\nwrite_csv(helping, file=\"data/helping_new.csv\")",
    "crumbs": [
      "R tutorial",
      "Import"
    ]
  },
  {
    "objectID": "contents/import.html#excel-spreadsheets",
    "href": "contents/import.html#excel-spreadsheets",
    "title": "Import",
    "section": "Excel spreadsheets",
    "text": "Excel spreadsheets\nreadxl package\nread_excel(), read_xlsx(), read_xls()\n\n엑셀 파일 읽기\nstduents.xlsx 파일 링크\n\nlibrary(readxl) # install.packages(\"readxl\")\n\nstud &lt;- read_xlsx(\"data/students.xlsx\")\nstud |&gt; print()\n\n# A tibble: 1,000 × 93\n  stu_id sch_id sstratid   sex  race ethnic bys42a bys42b bys44a bys44b bys44c\n   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 124966   1249        1     2     4      1      3      4      2      4      4\n2 124972   1249        1     1     4      1      4      5      1      3      3\n3 175551   1755        1     2     3      0     NA      3      2      3      3\n4 180660   1806        1     1     4      1      2     NA      1      4      4\n5 180672   1806        1     2     4      1      2      3      1      4      3\n6 298885   2988        2     1     3      0      5      4      2      3      3\n# … with 994 more rows, and 82 more variables: bys44d &lt;dbl&gt;, bys44e &lt;dbl&gt;,\n#   bys44f &lt;dbl&gt;, bys44g &lt;dbl&gt;, bys44h &lt;dbl&gt;, bys44i &lt;dbl&gt;, bys44j &lt;dbl&gt;,\n#   bys44k &lt;dbl&gt;, bys44l &lt;dbl&gt;, bys44m &lt;dbl&gt;, bys48a &lt;dbl&gt;, bys48b &lt;dbl&gt;,\n#   bys79a &lt;dbl&gt;, byfamsiz &lt;dbl&gt;, famcomp &lt;dbl&gt;, bygrads &lt;dbl&gt;, byses &lt;dbl&gt;,\n#   byfaminc &lt;dbl&gt;, parocc &lt;dbl&gt;, bytxrstd &lt;dbl&gt;, bytxmstd &lt;dbl&gt;,\n#   bytxsstd &lt;dbl&gt;, bytxhstd &lt;dbl&gt;, bypared &lt;dbl&gt;, bytests &lt;dbl&gt;,\n#   par_inv &lt;dbl&gt;, f1s36a1 &lt;dbl&gt;, f1s36a2 &lt;dbl&gt;, f1s36b1 &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nNote\n\n\n\nSpecify sheet either by position or by name\nread_excel(\"salaries.xlsx\", sheet = 2) # The default is sheet = 1\nread_excel(\"salaries.xlsx\", sheet = \"personnel\")",
    "crumbs": [
      "R tutorial",
      "Import"
    ]
  },
  {
    "objectID": "contents/import.html#statistical-packages",
    "href": "contents/import.html#statistical-packages",
    "title": "Import",
    "section": "Statistical packages",
    "text": "Statistical packages\nSPSS의 데이터: read_sav()\nstudents-shorter.sav 파일 링크\n\nlibrary(haven) # install.packages(\"haven\")\n\nstud_spss &lt;- read_sav(\"data/students-shorter.sav\")\nstud_spss |&gt; print()\n\n# A tibble: 1,000 x 93\n   stu_id    sch_id   sstratid sex     race    ethnic  bys42a   bys42b   bys44a \n   &lt;dbl+lbl&gt; &lt;dbl+lb&gt; &lt;dbl+lb&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt; &lt;dbl+lb&gt; &lt;dbl+l&gt;\n 1 124966    1249     1        2 [Fem~ 4 [Whi~ 1 [whi~  3 [2-3~  4 [3-4~ 2 [Agr~\n 2 124972    1249     1        1 [Mal~ 4 [Whi~ 1 [whi~  4 [3-4~  5 [4-5~ 1 [Str~\n 3 175551    1755     1        2 [Fem~ 3 [Bla~ 0 [blk~ NA        3 [2-3~ 2 [Agr~\n 4 180660    1806     1        1 [Mal~ 4 [Whi~ 1 [whi~  2 [1-2~ NA       1 [Str~\n 5 180672    1806     1        2 [Fem~ 4 [Whi~ 1 [whi~  2 [1-2~  3 [2-3~ 1 [Str~\n 6 298885    2988     2        1 [Mal~ 3 [Bla~ 0 [blk~  5 [4-5~  4 [3-4~ 2 [Agr~\n 7 604419    6044     6        2 [Fem~ 4 [Whi~ 1 [whi~  4 [3-4~  5 [4-5~ 2 [Agr~\n 8 605355    6053     6        2 [Fem~ 4 [Whi~ 1 [whi~  2 [1-2~  3 [2-3~ 1 [Str~\n 9 605377    6053     6        2 [Fem~ 4 [Whi~ 1 [whi~  3 [2-3~  5 [4-5~ 2 [Agr~\n10 637529    6375     6        1 [Mal~ 3 [Bla~ 0 [blk~  5 [4-5~  6 [Ove~ 2 [Agr~\n# i 990 more rows\n# i 84 more variables: bys44b &lt;dbl+lbl&gt;, bys44c &lt;dbl+lbl&gt;, bys44d &lt;dbl+lbl&gt;,\n#   bys44e &lt;dbl+lbl&gt;, bys44f &lt;dbl+lbl&gt;, bys44g &lt;dbl+lbl&gt;, bys44h &lt;dbl+lbl&gt;,\n#   bys44i &lt;dbl+lbl&gt;, bys44j &lt;dbl+lbl&gt;, bys44k &lt;dbl+lbl&gt;, bys44l &lt;dbl+lbl&gt;,\n#   bys44m &lt;dbl+lbl&gt;, bys48a &lt;dbl+lbl&gt;, bys48b &lt;dbl+lbl&gt;, bys79a &lt;dbl+lbl&gt;,\n#   byfamsiz &lt;dbl+lbl&gt;, famcomp &lt;dbl+lbl&gt;, bygrads &lt;dbl+lbl&gt;, byses &lt;dbl+lbl&gt;,\n#   byfaminc &lt;dbl+lbl&gt;, parocc &lt;dbl&gt;, bytxrstd &lt;dbl+lbl&gt;, ...\n\n\n\nstud_spss |&gt;\n    select(ethnic) |&gt;\n    print()\n\n# A tibble: 1,000 x 1\n  ethnic            \n  &lt;dbl+lbl&gt;         \n1 1 [white-asian]   \n2 1 [white-asian]   \n3 0 [blk,namer,hisp]\n4 1 [white-asian]   \n5 1 [white-asian]   \n6 0 [blk,namer,hisp]\n# i 994 more rows\n\n\nlabelled 데이터 참고\n\ninstall.packages(\"labelled\")\nlibrary(labelled)\n\n\n# labelled 변수를 factor로 변환\nstud_spss |&gt;\n    unlabelled() |&gt;\n    print()\n\n# A tibble: 1,000 x 93\n  stu_id sch_id sstratid sex    race   ethnic bys42a bys42b bys44a bys44b bys44c\n   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; \n1 124966   1249        1 Female White~ white~ 2-3 h~ 3-4 h~ Agree  Stron~ Stron~\n2 124972   1249        1 Male   White~ white~ 3-4 h~ 4-5 h~ Stron~ Disag~ Disag~\n3 175551   1755        1 Female Black~ blk,n~ NA     2-3 h~ Agree  Disag~ Disag~\n4 180660   1806        1 Male   White~ white~ 1-2 h~ NA     Stron~ Stron~ Stron~\n5 180672   1806        1 Female White~ white~ 1-2 h~ 2-3 h~ Stron~ Stron~ Disag~\n6 298885   2988        2 Male   Black~ blk,n~ 4-5 h~ 3-4 h~ Agree  Disag~ Disag~\n# i 994 more rows\n# i 82 more variables: bys44d &lt;fct&gt;, bys44e &lt;fct&gt;, bys44f &lt;fct&gt;, bys44g &lt;fct&gt;,\n#   bys44h &lt;fct&gt;, bys44i &lt;fct&gt;, bys44j &lt;fct&gt;, bys44k &lt;fct&gt;, bys44l &lt;fct&gt;,\n#   bys44m &lt;fct&gt;, bys48a &lt;fct&gt;, bys48b &lt;fct&gt;, bys79a &lt;fct&gt;, byfamsiz &lt;fct&gt;,\n#   famcomp &lt;fct&gt;, bygrads &lt;dbl&gt;, byses &lt;dbl&gt;, byfaminc &lt;fct&gt;, parocc &lt;dbl&gt;,\n#   bytxrstd &lt;dbl&gt;, bytxmstd &lt;dbl&gt;, bytxsstd &lt;dbl&gt;, bytxhstd &lt;dbl&gt;,\n#   bypared &lt;fct&gt;, bytests &lt;dbl&gt;, par_inv &lt;dbl&gt;, f1s36a1 &lt;fct&gt;, ...\n\n\nLabels 제거하기\n\nstud &lt;- stud_spss |&gt;\n    remove_val_labels()\nstud |&gt; print()\n\n# A tibble: 1,000 x 93\n  stu_id sch_id sstratid   sex  race ethnic bys42a bys42b bys44a bys44b bys44c\n   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 124966   1249        1     2     4      1      3      4      2      4      4\n2 124972   1249        1     1     4      1      4      5      1      3      3\n3 175551   1755        1     2     3      0     NA      3      2      3      3\n4 180660   1806        1     1     4      1      2     NA      1      4      4\n5 180672   1806        1     2     4      1      2      3      1      4      3\n6 298885   2988        2     1     3      0      5      4      2      3      3\n# i 994 more rows\n# i 82 more variables: bys44d &lt;dbl&gt;, bys44e &lt;dbl&gt;, bys44f &lt;dbl&gt;, bys44g &lt;dbl&gt;,\n#   bys44h &lt;dbl&gt;, bys44i &lt;dbl&gt;, bys44j &lt;dbl&gt;, bys44k &lt;dbl&gt;, bys44l &lt;dbl&gt;,\n#   bys44m &lt;dbl&gt;, bys48a &lt;dbl&gt;, bys48b &lt;dbl&gt;, bys79a &lt;dbl&gt;, byfamsiz &lt;dbl&gt;,\n#   famcomp &lt;dbl&gt;, bygrads &lt;dbl&gt;, byses &lt;dbl&gt;, byfaminc &lt;dbl&gt;, parocc &lt;dbl&gt;,\n#   bytxrstd &lt;dbl&gt;, bytxmstd &lt;dbl&gt;, bytxsstd &lt;dbl&gt;, bytxhstd &lt;dbl&gt;,\n#   bypared &lt;dbl&gt;, bytests &lt;dbl&gt;, par_inv &lt;dbl&gt;, f1s36a1 &lt;dbl&gt;, ...",
    "crumbs": [
      "R tutorial",
      "Import"
    ]
  },
  {
    "objectID": "contents/ml.html",
    "href": "contents/ml.html",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "Load libraries\nlibrary(haven)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(manymome)\nsim1.csv 파일\nsim1 &lt;- read_csv(\"data/sim1.csv\")\nsim1 |&gt; print(n = 5)\n\n# A tibble: 30 x 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1  4.20\n2     1  7.51\n3     1  2.13\n4     2  8.99\n5     2 10.2 \n# i 25 more rows\nScatter plot of sim1 data\nsim1 |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Scatter plot of sim1 data\")",
    "crumbs": [
      "ML Estimation"
    ]
  },
  {
    "objectID": "contents/ml.html#ols-ordinary-least-squares-추정방식",
    "href": "contents/ml.html#ols-ordinary-least-squares-추정방식",
    "title": "Maximum Likelihood Estimation",
    "section": "OLS (Ordinary Least Squares) 추정방식",
    "text": "OLS (Ordinary Least Squares) 추정방식\n선형 모델 family인 \\(\\hat{y} = b_0 + b_1 x\\)을 세운 후\n잔차 \\(e_i = y_i - \\hat{y_i}\\)의 제곱의 합, 즉 \\(\\sum e_i^2\\)이 최소가 되도록하는 \\(b_0, b_1\\)을 추정하는 방법\n\nmod &lt;- lm(y ~ x, data = sim1) \nmod |&gt; coef() |&gt; print()\n\n(Intercept)           x \n   4.220822    2.051533 \n\n\n즉, OLS 방식에서 최선의 모형은 \\(\\hat{y} = 4.22 + 2.05x\\)\n이 모형의 예측값과 잔차를 보면,\n\n\nAdd predictions and residuals to sim1 data\nlibrary(modelr)\nsim1 &lt;- sim1 |&gt; \n  add_predictions(mod) |&gt; \n  add_residuals(mod)\nsim1 |&gt; print(n = 7)\n\n\n# A tibble: 30 x 4\n      x     y  pred  resid\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1  4.20  6.27 -2.07 \n2     1  7.51  6.27  1.24 \n3     1  2.13  6.27 -4.15 \n4     2  8.99  8.32  0.665\n5     2 10.2   8.32  1.92 \n6     2 11.3   8.32  2.97 \n7     3  7.36 10.4  -3.02 \n# i 23 more rows",
    "crumbs": [
      "ML Estimation"
    ]
  },
  {
    "objectID": "contents/ml.html#maximum-likelihood-estimation",
    "href": "contents/ml.html#maximum-likelihood-estimation",
    "title": "Maximum Likelihood Estimation",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\n데이터가 발생된 것으로 가정하는 분포를 고려했을 때,\n어떨때 주어진 데이터가 관측될 확률/가능도(likelihood)가 최대가 되겠는가로 접근하는 방식으로,\nX, Y의 관계와 확률분포를 함께 고려함.\n\n선형관계라면, 즉 \\(E(Y|X=x_i) = \\beta_0 + \\beta_1x_i\\)   (\\(E\\): expected value, 기대값)\n분포가 Gaussian이라면, 즉 \\(Y|(X=x_i) \\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2)\\)   (\\(\\sigma\\): 표준편차)\n\n\nLikelihood \\(L = \\displaystyle\\prod_{i=1}^{n}{P_i}\\)   (관측치들 독립일 때, product rule에 의해)\n분포가 Gaussian이라면(평균: \\(\\mu\\), 표준편차: \\(\\sigma\\)), 즉 \\(f(t) = \\displaystyle\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{(t-\\mu)^2}{2\\sigma^2}\\right)\\)라면\n\\(L = \\displaystyle\\prod_{i=1}^{n}{f(y_i, x_i)} = \\displaystyle\\prod_{i=1}^{n}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}\\right)}\\)\n이 때, 이 likelihood를 최대화하는 \\(\\beta_0, \\beta_1, \\sigma\\)를 찾는 것이 목표이며,\n이처럼 분포가 Gaussian라면, OLS estimation과 동일한 값을 얻음. (단, \\(\\sigma\\)는 bias가 존재)\n다른 분포를 가지더라도 동일하게 적용할 수 있음!\n\n즉, likelihood의 관점에서 주어진 데이터에 가장 근접하도록(likelihood가 최대가 되는) “분포의 구조”를 얻는 과정\n\n여러 편의를 위해, log likelihood를 최대화함.\n\n\n\n\n\n\nLog likelihood\n\n\n\n다음 두가지를 고려하면,\n\\(log(x*y) = log(x) + log(y)\\)\n\\(e^x * e^y = e^{x+y}\\)\n\\(log(L) = \\displaystyle\\sum_{i=1}^{n}{log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}\\right)\\right)} = \\displaystyle\\sum_{i=1}^{n}{-log(\\sqrt{2\\pi\\sigma^2}) - \\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}}\\)\n두 번째 항이 앞서 정의한 squared error와 동일함\n\n\n잔차에 대한 간단한 정보들\n\nlm(y ~ x, data = sim1) |&gt; summary() |&gt; print()\n\n\nCall:\nlm(formula = y ~ x, data = sim1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1469 -1.5197  0.1331  1.4670  4.6516 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.2208     0.8688   4.858 4.09e-05 ***\nx             2.0515     0.1400  14.651 1.17e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.203 on 28 degrees of freedom\nMultiple R-squared:  0.8846,    Adjusted R-squared:  0.8805 \nF-statistic: 214.7 on 1 and 28 DF,  p-value: 1.173e-14\n\n\n\n\n# 위의 모형의 잔차들에 대해 정규분포 함수의 값을 구하면,\nsigma &lt;- sd(sim1$resid)  # 잔차의 표준편차: 2.16\n# 좀 더 정확히는 샘플 수로 나누지 않고, 자유도로 나누어야 함\nsim1 &lt;- sim1 |&gt; \n  mutate(norm = dnorm(resid, mean = 0, sd = sigma))\nsim1 |&gt; print(n = 5)\n\n# A tibble: 30 x 5\n      x     y  pred  resid   norm\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1  4.20  6.27 -2.07  0.117 \n2     1  7.51  6.27  1.24  0.156 \n3     1  2.13  6.27 -4.15  0.0294\n4     2  8.99  8.32  0.665 0.176 \n5     2 10.2   8.32  1.92  0.124 \n# i 25 more rows\n\n\n\n\n\nGaussin distribution with mean 0 and sd 2.16\nx &lt;- seq(-10, 10, length = 100)\ny &lt;- dnorm(x, mean = 0, sd = sigma)\ntibble(x = x, y = y) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  labs(\n    title = \"Gaussian distribution with mean 0 and sd 2.16\",\n    x = \"Residuals\", y = \"Density\"\n  ) +\n  geom_bar(data = sim1, aes(x = resid, y = norm), stat = \"identity\", color = \"black\")\n\n\n\n\n\n\n\n\n\n즉, 위 그림에서 높이를 모두 곱하면, likelihood를 얻을 수 있으며,\nLog-likelihood는 log를 취해 더하면 되므로,\n\nlog(sim1$norm) |&gt; sum() |&gt; print()\n\n[1] -65.2347\n\n\n이 값은 lavaan 결과에서 보여지며, 모형 적합도 계산을 위한 기본적인 값으로 사용됨.\n\nfit &lt;- sem('y ~ x', data = sim1)\nsummary(fit, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-18 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         2\n\n  Number of observations                            30\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nModel Test Baseline Model:\n\n  Test statistic                                64.784\n  Degrees of freedom                                 1\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.000\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)                -65.226\n  Loglikelihood unrestricted model (H1)        -65.226\n                                                      \n  Akaike (AIC)                                 134.452\n  Bayesian (BIC)                               137.255\n  Sample-size adjusted Bayesian (SABIC)        131.028\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: RMSEA &lt;= 0.050                       NA\n  P-value H_0: RMSEA &gt;= 0.080                       NA\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  y ~                                                 \n    x                 2.052    0.135   15.166    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y                 4.529    1.169    3.873    0.000\n\n\n\nMaximum likelihood estimation의 결과는 분포가 Gaussian이라면 OLS와 동일하며,\n다른 분포를 가지는 데이터에 대해서도 동일한 원리로 (즉, likelihood를 최대로 하도록) 파라미터를 추정할 수 있음.\n대표적인 예가 logistic regression이며, 이 경우의 분포는 이항분포 또는 베르누이 분포를 가정하여 likelihood를 계산함.",
    "crumbs": [
      "ML Estimation"
    ]
  },
  {
    "objectID": "contents/ml.html#구조방정식-모형에서-분포에-대한-가정들",
    "href": "contents/ml.html#구조방정식-모형에서-분포에-대한-가정들",
    "title": "Maximum Likelihood Estimation",
    "section": "구조방정식 모형에서 분포에 대한 가정들",
    "text": "구조방정식 모형에서 분포에 대한 가정들\n관찰된 변수(measured variable)들이 multivariate normal distribution을 따른다는 가정을 함.\n이 때, 각 변수들은 정규분포를 따르게 되지만, 그 반대는 아님.\n\n\nSource: p. 150, Introduction to Statistical Learning with Applications in Python by G. James, D. Witten, T. Hastie, R. Tibshirani\n\n\n\n\n\n\n\nMultivariate Gaussian for 2-dimensional data\n\n\n\nMultivariate Gaussian 분포: \\(\\displaystyle f(\\mathbf{x}) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mu)^T\\Sigma^{-1}(\\mathbf{x}-\\mu)\\right)\\)\n\n\\(\\displaystyle \\mathbf{x}= \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\)\n\\(\\mu\\): \\(X_1\\)과 \\(X_2\\)의 평균 벡터: \\(\\displaystyle \\begin{bmatrix} \\mu_{1} \\\\ \\mu_{2} \\end{bmatrix}\\)\n\\(\\Sigma\\): 공분산 행렬(covariance matrix): \\(\\displaystyle \\begin{bmatrix} \\sigma_{1}^2 & \\sigma_{12} \\\\ \\sigma_{21} & \\sigma_{2}^2 \\end{bmatrix}\\)\n\n\n\n각 잠재변수(latent variable)들은 정규분포를 따른다고 가정\n\n정규성에 대한 검증이 필요하며,\n정규성을 따르지 않는 경우, robust estimation이나 bootstrap 방법을 사용할 수 있음.\n\n분포 뿐만 아니라 회귀에서의 모든 가정들이 SEM에서 동일하게 적용됨!",
    "crumbs": [
      "ML Estimation"
    ]
  },
  {
    "objectID": "contents/setup2.html",
    "href": "contents/setup2.html",
    "title": "Packages for SEM",
    "section": "",
    "text": "당분간 lavaan 버전 0.6-17을 사용\n\n# remove lavaan\nremove.packages(\"lavaan\")\n\n# install lavaan 0.6-17\ninstall.packages(\"https://cran.r-project.org/src/contrib/Archive/lavaan/lavaan_0.6-17.tar.gz\", repo = NULL, type = \"source\")\n\nsemTools\nmanymome\ntidySEM: graph_sem()\nlavaanExtra: nice_tidySEM()\nsemPlot: semPaths()",
    "crumbs": [
      "Packages for SEM"
    ]
  },
  {
    "objectID": "contents/visualize.html",
    "href": "contents/visualize.html",
    "title": "Visualize",
    "section": "",
    "text": "데이터 시각화는 탐색적 분석에 더 초점이 맞춰져 있음.\n\n소위 data mining이라고 부르는 데이터 내의 숨겨진 패턴을 찾고 분석하는 탐색적 분석은 전통적인 통계에서 discouraging되어 왔음.\n\n확률에 근거한 통계 이론은 데이터를 수집하기 전에 가설을 세우고 그 가설을 confirm하는 방식을 취함.\n\n논란의 여지가 있지만, 원칙적으로 가설에 근거해 수집한 자료가 가설과 일치하는지를 확인하는 작업에서는 자료를 두 번 이상 들여다 보지 않아야 함.\n\n그럼에도 불구하고, 탐색적 분석은 behind doors에서 이루어지거나 새로운 가설을 세우기 위한 방편으로 이용되었음.\n\n또한, 매우 엄격한 잣대를 적용하는 상황에서도 통계 이론의 특성으로 인해 기본적인 탐색적 분석은 반드시 선행되어야 함.\n\n연구 가설의 진위를 탐구할 때, 탐색적 분석에서 쉽게 빠질 수 있는 편향성(bias)는 항상 조심할 필요가 있고, 확신을 위해서는 새로이 자료를 수집해서 가설을 재검증할 필요가 있음.\n\n탐색적 분석을 위해서는 다양한 시각화 기술이 요하나, 일반적인 통계 분석을 위해서 필요로하는 최소한으로 제한하고자 함.\n또한, 복잡한 통계치를 살펴볼 때, 직접 시각화를 하기보다는 패키지가 알아서 시각화를 해주기 때문에 자세히 알지 못해도 무방함.\n좀 더 상세한 내용에 대해서는\n\nR for Data Science/Visualize\nggplot2 book\nggplot2 extensions\n통계치 표현: ggstatsplot, ggpubr\nData Visualization with R by Rob Kabacoff : 적절한 밸런스\nggplot2 cheatsheet : pdf 다운로드\n\n\n\n\n\n\n\nNote\n\n\n\n충분히 큰 데이터의 경우, 일정량의 데이터 가령 1/4을 따로 떼어놓고, 3/4만으로 탐색적 분석을 통해 모델을 만든 후, 따로 떼어놓은 1/4로 (가설)검증을 하는 cross-validation 방법이 있는데, machine leanring분야에서는 기본적인 process.\nCross-validation 방식에는 여러 변형들이 있음; e.g. 데이터를 4등분하여 각각 4번 위의 방식을 반복하여 합치는 방식, 3가지 (training, validation, test sets)로 나누어 분석",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#exploratory-vs.-confirmatory-analysis",
    "href": "contents/visualize.html#exploratory-vs.-confirmatory-analysis",
    "title": "Visualize",
    "section": "",
    "text": "데이터 시각화는 탐색적 분석에 더 초점이 맞춰져 있음.\n\n소위 data mining이라고 부르는 데이터 내의 숨겨진 패턴을 찾고 분석하는 탐색적 분석은 전통적인 통계에서 discouraging되어 왔음.\n\n확률에 근거한 통계 이론은 데이터를 수집하기 전에 가설을 세우고 그 가설을 confirm하는 방식을 취함.\n\n논란의 여지가 있지만, 원칙적으로 가설에 근거해 수집한 자료가 가설과 일치하는지를 확인하는 작업에서는 자료를 두 번 이상 들여다 보지 않아야 함.\n\n그럼에도 불구하고, 탐색적 분석은 behind doors에서 이루어지거나 새로운 가설을 세우기 위한 방편으로 이용되었음.\n\n또한, 매우 엄격한 잣대를 적용하는 상황에서도 통계 이론의 특성으로 인해 기본적인 탐색적 분석은 반드시 선행되어야 함.\n\n연구 가설의 진위를 탐구할 때, 탐색적 분석에서 쉽게 빠질 수 있는 편향성(bias)는 항상 조심할 필요가 있고, 확신을 위해서는 새로이 자료를 수집해서 가설을 재검증할 필요가 있음.\n\n탐색적 분석을 위해서는 다양한 시각화 기술이 요하나, 일반적인 통계 분석을 위해서 필요로하는 최소한으로 제한하고자 함.\n또한, 복잡한 통계치를 살펴볼 때, 직접 시각화를 하기보다는 패키지가 알아서 시각화를 해주기 때문에 자세히 알지 못해도 무방함.\n좀 더 상세한 내용에 대해서는\n\nR for Data Science/Visualize\nggplot2 book\nggplot2 extensions\n통계치 표현: ggstatsplot, ggpubr\nData Visualization with R by Rob Kabacoff : 적절한 밸런스\nggplot2 cheatsheet : pdf 다운로드\n\n\n\n\n\n\n\nNote\n\n\n\n충분히 큰 데이터의 경우, 일정량의 데이터 가령 1/4을 따로 떼어놓고, 3/4만으로 탐색적 분석을 통해 모델을 만든 후, 따로 떼어놓은 1/4로 (가설)검증을 하는 cross-validation 방법이 있는데, machine leanring분야에서는 기본적인 process.\nCross-validation 방식에는 여러 변형들이 있음; e.g. 데이터를 4등분하여 각각 4번 위의 방식을 반복하여 합치는 방식, 3가지 (training, validation, test sets)로 나누어 분석",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#basics",
    "href": "contents/visualize.html#basics",
    "title": "Visualize",
    "section": "Basics",
    "text": "Basics\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\n\npenguins |&gt;\n    print() # 무시\n\n# A tibble: 344 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA NA     2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# … with 338 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nVariabels:\n\nspecies: a penguin’s species (Adelie, Chinstrap, or Gentoo).\n\nflipper_length_mm: length of a penguin’s flipper, in millimeters.\n\nbody_mass_g: body mass of a penguin, in grams.\n\n더 자세한 사항은 ?penguins\nggplot을 이용한 시각화는 주로 3가지 성분으로 나뉨\n\ndata: 사용할 데이터\n\nmapping: data의 변수들을 어떤 특성에 mapping할 것인지 specify\n\ngeom: 어떤 시각화 개체(graphical objects)로 데이터를 표현할 것인지 specify\n\n\n# x, y축에 변수를 mapping\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n)\n\n\n\n\n\n\n\n\n\n# point로 데이터를 표시: scatterplot\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n#&gt; Warning: Removed 2 rows containing missing values (`geom_point()`).\n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n실제로 is.na()함수를 이용해 missing을 확인해보면,\npenguins |&gt;\n  select(species, flipper_length_mm, body_mass_g) |&gt;\n  filter(is.na(body_mass_g) | is.na(flipper_length_mm))  # true, false의 boolean type\n#&gt; # A tibble: 2 × 3\n#&gt;   species flipper_length_mm body_mass_g\n#&gt;   &lt;fct&gt;               &lt;int&gt;       &lt;int&gt;\n#&gt; 1 Adelie                 NA          NA\n#&gt; 2 Gentoo                 NA          NA\n\n\n\nAdding aesthetics and layers\n\n# spcies에 color (aesthetics)를 mapping\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point()\n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n위에서 species마다 다른 색을 입혀서 다른 패턴이 나타나는지 확인해 볼 수 있음\nggplot2는 + 기호로 연결하여 계속 layer를 추가할 수 있음.\n다음은 trendline 혹은 fitted line이라고 부르는 경향성을 확인해 볼 수 있는 라인의 layer를 추가함\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_smooth()`).”\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nData에 fitted curve를 구하는 방식에는 여러 방법이 있음\n\nLinear fit: 1차 함수형태인 직선으로 fit\nSmoothing fit\n\nPolynominal fit: n차 다항함수형태로 fit\nLoess/lowess: locally estimated/weighted scatterplot smoothing\nGAM: generalized additive model\nSpine: piece-wise polynominal regression\n\n\n나중에 좀 더 자세히 알아봄\n\n\nggplot2는 플랏의 대상에 다음과 같은 속성을 부여할 수 있음\ncolor, size, shape, fill, alpha\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species, shape = island)\n) +\n  geom_point() \n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n어떤 속성을 어떤 변수에 할당하는 것이 적절한지를 선택하는 것이 기술\n\n\n\n\nCategorical vs. continuous\ncolor와 같은 속성은 카테고리 변수가 좀 더 적절하나, 연속변수에서도 적용될 수 있음\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = bill_length_mm)\n) +\n  geom_point() \n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n반대로, x, y에 카테고리 변수를 mapping하여 scatterplot을 그리면 다음과 같은 overploting의 문제가 생김\n\nggplot(\n  data = penguins,\n  mapping = aes(x = species, y = body_mass_g, color = sex)\n) +\n  geom_point() \n\nWarning message:\n“Removed 11 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\n\nOverplotting\nOverplotting의 문제를 해결하는 방식은 주로\n\nalpha(투명도)를 조정하거나 랜덤하게 흐뜨려그리는 geom_jitter()를 사용\n\n애초에 겹치지 않게 그리는 방법도 있음: e.g. beeswarm plot\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = species, y = body_mass_g, color = sex)\n) +\n  geom_jitter(width = .2) # jitter의 정도: width, height\n\nWarning message:\n“Removed 11 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = species, y = body_mass_g, color = sex)\n) +\n  geom_jitter(width = .2, alpha = .5) # alpha: 투명도 0 ~ 1\n\nWarning message:\n“Removed 11 rows containing missing values (`geom_point()`).”",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#geometric-objects",
    "href": "contents/visualize.html#geometric-objects",
    "title": "Visualize",
    "section": "Geometric objects",
    "text": "Geometric objects\nggplot2는 40가지 넘는 geom objects를 제공함.\n주로 통계를 위해 쓰일 geom들은\n\ngeom_point, geom_smooth()\ngeom_boxplot()\ngeom_histogram(), geom_freqploy(), geom_density()\n\nGlobal vs. local mapping\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species)) + # color mapping은 geom_point에만 적용\n  geom_smooth() # 맨 위의 mapping에 있는 global mapping을 inherit\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_smooth()`).”\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_smooth(mapping = aes(linetype = sex), se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_smooth()`).”\n\n\n\n\n\n\n\n\n\n\nggplot(\n    data = penguins,\n    mapping = aes(x = bill_depth_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species)) +  # color mapping은 geom_point에만 \n  geom_smooth(method = lm)  # 맨 위의 mapping에 있는 global mapping을 inherit, method: fitted line의 종류\n\n\n\n\n\n\n\n\n\nggplot(\n    data = penguins,\n    mapping = aes(x = bill_depth_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species)) +\n  geom_smooth(method = lm) +  # 맨 위의 mapping에 있는 global mapping을 inherit\n  geom_smooth(mapping = aes(color = species), method = lm) # color mapping 추가\n\n\n\n\n\n\n\n\naes() 내부, 외부에서의 mapping\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n    geom_point(mapping = aes(color = species)) # aesthetic color에 변수를 mapping\n\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n    geom_point(color = \"skyblue\") + # geom의 color 속성에 색을 지정\n    geom_smooth(color = \"orangered\")",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#statistical-transformations",
    "href": "contents/visualize.html#statistical-transformations",
    "title": "Visualize",
    "section": "Statistical transformations",
    "text": "Statistical transformations\nggplot2는 편의를 위해 통계치를 구해 표시해주는데,\n경우에 따라 직접 통계치를 계산 후 새로 얻는 데이터로 그리는 것이 유리함\n\nDistribution\ngeom_histogram(), geom_freqploy(), geom_density()\n\n# y축에 표시되는 통계치들이 계산됨\nggplot(data = penguins, mapping = aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 100) # binwidth vs. bins\n\nggplot(data = penguins, mapping = aes(x = body_mass_g, colour = sex)) +\n  geom_freqpoly(binwidth = 100)\n\nggplot(data = penguins, mapping = aes(x = body_mass_g, colour = sex)) +\n  geom_density(bw = 100) # bw: band width\n\n\n\n\n\n\n\n\n\n\n\n\nBoxplot\nBoxplot은 분포에 대한 정보은 줄어드나, 카테고리별로 간결하게 비교되는 장점\nboxplot()\n\n\n\nggplot(data = penguins, mapping = aes(x = species, y = body_mass_g)) +\n    geom_boxplot()\n\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_boxplot()`).”\n\n\n\n\n\n\n\n\n\n\nggplot(data = penguins, mapping = aes(x = species, y = body_mass_g)) +\n    geom_boxplot() +\n    geom_jitter(alpha = .6)\n\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_boxplot()`).”\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\ncps &lt;- as_tibble(mosaicData::CPS85)\ncps |&gt;\n    filter(wage &lt; 30) |&gt; \n    ggplot(aes(x = as.factor(educ), y = wage)) +  # as.factor(): numeric을 factor로 변환\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\nggplot(data = penguins, mapping = aes(x = species, y = body_mass_g, fill = sex)) + # color는 box의 테두리 색, fill은 내부색\n  geom_boxplot()\n\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_boxplot()`).”\n\n\n\n\n\n\n\n\n\n\n\nBarplot\nBarplot은 여러방식으로 쓸 수 있는데, 문법이 조금 복잡하고, 수업에서 거의 사용하지 않을 예정이므로 웹사이트를 참조\nR for Data Science/Layers/Statistical transformations\n\nggplot(data = penguins) + \n  geom_bar(mapping = aes(x = species)) # 개수\n\n\n\n\n\n\n\n\n\n\nDiscretize\n연속 변수를 임의의 구간으로 나누어 카테고리처럼 적용하기 할 수 있음\ncut_width(), cut_number(), cut_interval()\n\ncut_width(): 구간의 길이를 정함\ncut_number(): 동일한 갯수의 관측값을 갖는 n개의 그룹\ncut_interval(): 동일한 길이의 n개의 그룹\n\n\nggplot(\n  data = penguins,\n  mapping = aes(\n      x = bill_length_mm, y = bill_depth_mm,\n      color = cut_interval(body_mass_g, 3) # body_mass_g의 값을 3개의 동일한 길이의 구간으로 나눔\n  )\n) +\n  geom_point() +\n  geom_smooth(se = FALSE, span = 1) # span: smoothing 정도 조절\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\nWarning message:\n“Removed 2 rows containing non-finite values (`stat_smooth()`).”\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#facets",
    "href": "contents/visualize.html#facets",
    "title": "Visualize",
    "section": "Facets",
    "text": "Facets\n카테고리 변수들이 지니는 카테고리들(레벨)로 나누어 그리기\nfacet_wrap(), facet_grid()\n\nggplot(data = penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n  geom_point() +\n  facet_wrap(~species) # species의 레벨로 나뉘어짐\n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\nfacet_wrap()은 레벨이 많아지면 다음의 facet_grid()와는 다르게 화면크기에 맞춰 다음 줄로 넘어감\n\nggplot(data = penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n  geom_point() +\n  facet_grid(sex ~ species)  # 행과 열에 각각 sex, species\n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\nggplot(\n  data = penguins, \n  aes(x = body_mass_g, y = flipper_length_mm, color = sex) # color 추가\n) +\n  geom_point(alpha = .6) +\n  facet_grid(island ~ species)  # 행과 열에 각각 sex, species\n\nWarning message:\n“Removed 11 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFacet과 color 중 어떤 방식으로 표현하는 것이 유리한가? 밸런스를 잘 선택!\n\n\nggplot(data = penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n  geom_point() +\n  facet_wrap(~species)\n\nggplot(data = penguins, aes(x = body_mass_g, y = flipper_length_mm, color = species)) +\n  geom_point()\n\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”\nWarning message:\n“Removed 2 rows containing missing values (`geom_point()`).”",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#labels",
    "href": "contents/visualize.html#labels",
    "title": "Visualize",
    "section": "Labels",
    "text": "Labels\nlabs() 안에 각 요소별로 지정\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = island)) +\n  geom_smooth() +\n  labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n    color = \"Species\", shape = \"Island\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n앞으로는 pipe operator와 함께, 축약 형태로\n\ndata = 대신 첫번째 argument 위치에 data frame이 위치\nmapping = 은 두번째 argument 위치에 aes()을 위치\n\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n은 다음과 같이\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n\n\nPipe operator로 다음과 연결될 수 있음\n\npenguins |&gt;\n    filter(!is.na(sex) & island != \"Torgersen\") |&gt;  # 성별이 missing이 아니고, Torgersen섬은 제외\n    ggplot(aes(x = body_mass_g, y = flipper_length_mm, color = sex)) +\n    geom_point() +\n    geom_smooth() +\n    facet_wrap(~island)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/visualize.html#examples",
    "href": "contents/visualize.html#examples",
    "title": "Visualize",
    "section": "Examples",
    "text": "Examples\n이전에 다뤘던 CPS85 데이터로 보면,\n\ncps &lt;- as_tibble(mosaicData::CPS85) # mosaicData package의 CPS85 데이터셋\ncps |&gt;\n   print() # 생략!\n\n# A tibble: 534 × 11\n   wage  educ race  sex   hispanic south married exper union   age sector  \n  &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;   \n1   9      10 W     M     NH       NS    Married    27 Not      43 const   \n2   5.5    12 W     M     NH       NS    Married    20 Not      38 sales   \n3   3.8    12 W     F     NH       NS    Single      4 Not      22 sales   \n4  10.5    12 W     F     NH       NS    Married    29 Not      47 clerical\n5  15      12 W     M     NH       NS    Married    40 Union    58 const   \n6   9      16 W     F     NH       NS    Married    27 Not      49 clerical\n# … with 528 more rows\n\n\n\ncps |&gt;\n    ggplot(aes(x = wage, color = married)) +\n    geom_freqpoly(binwidth=1)\n\n\n\n\n\n\n\n\n\ncps |&gt;\n    ggplot(aes(x = wage, color = married)) +\n    geom_freqpoly(binwidth = 1) +\n    facet_wrap(~sex)\n\n\n\n\n\n\n\n\n\ncps |&gt;\n  ggplot(aes(x = married, y = wage)) +\n  geom_boxplot(width = .2) +\n  geom_jitter(width = .2, alpha = .2, color = \"red\") +\n  scale_y_continuous(label = scales::label_dollar())  # y축 scale의 변경\n\n\n\n\n\n\n\n\n\ncps |&gt;\n  ggplot(aes(x = married, y = wage, fill = sex)) +\n  geom_boxplot()\n  \n\n\n\n\n\n\n\n\n\ncps |&gt;\n    filter(wage &lt; 30) |&gt; \n    ggplot(aes(x = sector, y = wage, fill = sex)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\ncps |&gt;\n    filter(wage &lt; 30) |&gt;\n    ggplot(aes(x = sector, y = wage, fill = sex)) +\n    geom_boxplot() +\n    facet_grid(married ~ .) \n\n\n\n\n\n\n\n\n\nplot &lt;- cps |&gt;\n  filter(wage &lt; 30) |&gt;\n  ggplot(aes(x = age, y = wage)) +\n  geom_point(alpha = .6) +\n  geom_smooth()\nplot\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n확대, 축소 혹은 제한된 범위에서 보려면 다음 2가지를 구분해야 함\ncoord_cartesian() vs. xlim() or ylim()\n\n\n\nplot + coord_cartesian(xlim = c(18, 40)) # zoom in\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nplot + xlim(18, 40) # data crop\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\nWarning message:\n“Removed 181 rows containing non-finite values (`stat_smooth()`).”\nWarning message:\n“Removed 181 rows containing missing values (`geom_point()`).”\n\n\n\n\n\n\n\n\n\n\ncps |&gt;\n    filter(wage &lt; 30 & sector %in% c(\"manag\", \"manuf\", \"prof\", \"sales\")) |&gt;\n    ggplot(aes(x = age, y = wage, color = sex)) +\n    geom_point() +\n    geom_smooth(se = FALSE, span = 1) +\n    facet_wrap(~sector)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "R tutorial",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/fit-index.html",
    "href": "contents/fit-index.html",
    "title": "Fit Indices",
    "section": "",
    "text": "다음에 설명하는 방법은 SEM의 보고 표준(Appelbaum et al., 2018)과 일치하며, 연구자가 과거보다 모델 적합도에 대해 더 많은 정보를 보고하도록 요구합니다:\n\n동시 추정 방법을 사용하는 경우 모델 카이제곱 을 자유도 및 p 값과 함께 보고합니다. 결과 변수가 이 분법적이고 이원 로지스틱 또는 확률 회귀 방법을 사용 하여 경로 공분산을 추정하는 경우와 같이 일부 분석에 서는 모델 카이제곱을 사용할 수 없는 경우도 있지만( 예는 Muthén과 Muthén(1998-2017, 3장) 참조), 이러한 경우는 예외적인 경우에 해당합니다.\n모델이 적합도 테스트에 실패하면 (a) 직접 그렇 게 말하고, 표본 크기에 관계없이 (b) 해당 모델을 10% 확률로 거부합니다. 다음으로, (c) 부적합의 크기와 가 능한 원인을 모두 진단합니다(국부적 적합도 검사). 그 근거는 실패를 설명하는 통계적으로 유의미하지만 약 간의 모델 데이터 불일치를 감지하는 것입니다. 이는 대규모 샘플에서 발생할 가능성이 가장 높습니다. 모델 을 거부하기로 한 초기 결정은 철회할 수 있지만, 관찰 된 모델-데이터 불일치가 실제로 중요하지 않은 이유에 대한 설명과 함께 국소 적합도 증거에 근거해서만 철회할 수 있습니다.\n모델이 정확한 적합도 테스트를 통과한 경우에도 로 컬 적합도를 검사해야 합니다. 그 이유는 통계적으로 유의 미하지는 않지만 모델에 의문을 제기할 만큼 큰 모델 데이 터 불일치를 감지하기 위해서입니다. 이는 작은 샘플에서 발생할 가능성이 높습니다. 지역 적합도에 대한 증거가 상 당한 불일치를 나타내는 경우, 카이제곱 테스트를 통과했 더라도 모델을 거부해야 합니다.\n본문에상관관계,표준화또는정규화된잔차등의 잔차 행렬을 보고합니다.원고를 작성합니다. 모델이 너무 커서 직접 설명하기 어려운 경우에는 (a) 부록 자료에 표를 제공하고 (b) 원고에 큰 잔류물의 위치 및 징후와 같은 잔류물의 패턴을 설명합니다. 모델이 어떻게 잘못 지정될 수 있는지 이해하는 데 진단적 가치가 있을 수 있는 패 턴을 찾습니다. 잔류에 대한 정보가 없는 결과 보고 는 불완전합니다. 안타깝게도 이 영역에서 불완전한 보고는 예외가 아니라 일반적입니다. 예를 들어, 조 직 관리 분야에서 발표된 144개의 SEM 연구를 검토 한 결과, 잔차가 언급된 연구는 약 17%에 불과했습 니다(Zhang et al., 2021).\n대략적인 적합도 지수 값을 보고하는 경우 이장의 앞부분에서 설명한 최소 집합에 대한 값을 포함하세요. 그러나 이러한 글로벌 적합도 통계에 대해 고정 또는 동적 임계값에만 의존하여 모델을 유지하는 것을 정당화하려고 해서는 안 됩니다. 특히 모델이 적합도 테스트에 실패하고 잔차의 패턴이 사소하지 않은 사양 오류를 시사하는 경우 특히 그렇습니다.\n초기 모델을 재특정하는 경우 그 근거를 설명 하세요. 또한 잔차와 같은 진단 통계가 재수정에서 어떤 역할을 했는지 설명해야 합니다. 즉, 모델에 대 한 수치 결과, 관련 이론 및 원래 모델에 대한 수정 사항 간의 연관성을 지적합니다(3장). 재조정된 모델 이 여전히 적합도 테스트에 실패하는 경우, 모델과 데이터의 불일치가 정말 미미하다는 것을 입증하고, 그렇지 않은 경우 모델에 대한 유의미한 공분산 증거 가 없다는 것을 입증하는 데 소홀히 한 것입니다.\n모델 적합도에 대한 통계적 증거는 중요하지만 모델 유지 여부를 결정하는 유일한 요소는 아닙니다. 예를 들어, 연구 문제를 고려할 때 매개변수 추정치 가 합리적이어야 합니다. 특정 데이터 집합에 잘 맞는 모델보다는 동일한 인과 프로세스에 의해 생성되는 향후의 데이터 집합에 잘 맞을 가능성이 합리적인 모델 을 선호해야 합니다. 이는 거의 모든 임의의 데이터에 잠재적으로 적합할 수 있는 복잡하고 과도하게 매개변수가 설정된 모델의 경우 특히 그렇습니다. 이러한 모델은 (1) 더 간결한 모델보다 위조 가능성이 적고 (2) 샘 플과 설정의 변화에 따라 생성될 가능성이 적습니다(Preacher et al., 2013).\n특정 모델을 유지하는 경우, 연구자는 해당 모델 이 동등하거나 거의 동등한 버전보다 선호되어야 하는 이유를 설명해야 합니다. 동일한 데이터를 정확히 또는 거의 비슷하게 설명합니 다. 이 단계는 통계보다 훨씬 더 논리적이며, 향후 연구 에서 심각한 경쟁 모델을 구별하기 위해 무엇을 할 수 있는지 설명하는 것도 포함됩니다. 동등하거나 거의 동 등한 모델에 대한 완전한 보고는 드물기 때문에, 양심 적인 독자는 이 문제를 해결함으로써 자신의 SEM 분 석을 실제로 구별할 수 있습니다. 동등한 버전의 구조 모델 생성 및 평가는 다음 장에서 다룹니다.\n모델이 유지되지 않는 경우 분석에서 테스트한 이론에 대한 의미를 설명하려면 학자로서의 기술이 필 요합니다. 결국, 유지 모델이 있든 없든 간에 진정한 영광은 최 선을 다해 논리적으로 끝까지 철저한 평가 프로세스를 따르는 데서 비롯됩니다. 시인 랄프 왈도 에머슨은 이 를 이렇게 표현했습니다: 잘한 일의 보상은 그것을 해 냈다는 것입니다(Mikis, 2012, 294쪽).\n\n결국, 유지 모델이 있든 없든 간에 진정한 영광은 최 선을 다해 논리적으로 끝까지 철저한 평가 프로세스를 따르는 데서 비롯됩니다. 시인 랄프 왈도 에머슨은 이 를 이렇게 표현했습니다: 잘한 일의 보상은 그것을 해 냈다는 것입니다(Mikis, 2012, 294쪽)."
  },
  {
    "objectID": "contents/fit-index.html#적합성-평가를-위한-권장-접근-방식",
    "href": "contents/fit-index.html#적합성-평가를-위한-권장-접근-방식",
    "title": "Fit Indices",
    "section": "",
    "text": "다음에 설명하는 방법은 SEM의 보고 표준(Appelbaum et al., 2018)과 일치하며, 연구자가 과거보다 모델 적합도에 대해 더 많은 정보를 보고하도록 요구합니다:\n\n동시 추정 방법을 사용하는 경우 모델 카이제곱 을 자유도 및 p 값과 함께 보고합니다. 결과 변수가 이 분법적이고 이원 로지스틱 또는 확률 회귀 방법을 사용 하여 경로 공분산을 추정하는 경우와 같이 일부 분석에 서는 모델 카이제곱을 사용할 수 없는 경우도 있지만( 예는 Muthén과 Muthén(1998-2017, 3장) 참조), 이러한 경우는 예외적인 경우에 해당합니다.\n모델이 적합도 테스트에 실패하면 (a) 직접 그렇 게 말하고, 표본 크기에 관계없이 (b) 해당 모델을 10% 확률로 거부합니다. 다음으로, (c) 부적합의 크기와 가 능한 원인을 모두 진단합니다(국부적 적합도 검사). 그 근거는 실패를 설명하는 통계적으로 유의미하지만 약 간의 모델 데이터 불일치를 감지하는 것입니다. 이는 대규모 샘플에서 발생할 가능성이 가장 높습니다. 모델 을 거부하기로 한 초기 결정은 철회할 수 있지만, 관찰 된 모델-데이터 불일치가 실제로 중요하지 않은 이유에 대한 설명과 함께 국소 적합도 증거에 근거해서만 철회할 수 있습니다.\n모델이 정확한 적합도 테스트를 통과한 경우에도 로 컬 적합도를 검사해야 합니다. 그 이유는 통계적으로 유의 미하지는 않지만 모델에 의문을 제기할 만큼 큰 모델 데이 터 불일치를 감지하기 위해서입니다. 이는 작은 샘플에서 발생할 가능성이 높습니다. 지역 적합도에 대한 증거가 상 당한 불일치를 나타내는 경우, 카이제곱 테스트를 통과했 더라도 모델을 거부해야 합니다.\n본문에상관관계,표준화또는정규화된잔차등의 잔차 행렬을 보고합니다.원고를 작성합니다. 모델이 너무 커서 직접 설명하기 어려운 경우에는 (a) 부록 자료에 표를 제공하고 (b) 원고에 큰 잔류물의 위치 및 징후와 같은 잔류물의 패턴을 설명합니다. 모델이 어떻게 잘못 지정될 수 있는지 이해하는 데 진단적 가치가 있을 수 있는 패 턴을 찾습니다. 잔류에 대한 정보가 없는 결과 보고 는 불완전합니다. 안타깝게도 이 영역에서 불완전한 보고는 예외가 아니라 일반적입니다. 예를 들어, 조 직 관리 분야에서 발표된 144개의 SEM 연구를 검토 한 결과, 잔차가 언급된 연구는 약 17%에 불과했습 니다(Zhang et al., 2021).\n대략적인 적합도 지수 값을 보고하는 경우 이장의 앞부분에서 설명한 최소 집합에 대한 값을 포함하세요. 그러나 이러한 글로벌 적합도 통계에 대해 고정 또는 동적 임계값에만 의존하여 모델을 유지하는 것을 정당화하려고 해서는 안 됩니다. 특히 모델이 적합도 테스트에 실패하고 잔차의 패턴이 사소하지 않은 사양 오류를 시사하는 경우 특히 그렇습니다.\n초기 모델을 재특정하는 경우 그 근거를 설명 하세요. 또한 잔차와 같은 진단 통계가 재수정에서 어떤 역할을 했는지 설명해야 합니다. 즉, 모델에 대 한 수치 결과, 관련 이론 및 원래 모델에 대한 수정 사항 간의 연관성을 지적합니다(3장). 재조정된 모델 이 여전히 적합도 테스트에 실패하는 경우, 모델과 데이터의 불일치가 정말 미미하다는 것을 입증하고, 그렇지 않은 경우 모델에 대한 유의미한 공분산 증거 가 없다는 것을 입증하는 데 소홀히 한 것입니다.\n모델 적합도에 대한 통계적 증거는 중요하지만 모델 유지 여부를 결정하는 유일한 요소는 아닙니다. 예를 들어, 연구 문제를 고려할 때 매개변수 추정치 가 합리적이어야 합니다. 특정 데이터 집합에 잘 맞는 모델보다는 동일한 인과 프로세스에 의해 생성되는 향후의 데이터 집합에 잘 맞을 가능성이 합리적인 모델 을 선호해야 합니다. 이는 거의 모든 임의의 데이터에 잠재적으로 적합할 수 있는 복잡하고 과도하게 매개변수가 설정된 모델의 경우 특히 그렇습니다. 이러한 모델은 (1) 더 간결한 모델보다 위조 가능성이 적고 (2) 샘 플과 설정의 변화에 따라 생성될 가능성이 적습니다(Preacher et al., 2013).\n특정 모델을 유지하는 경우, 연구자는 해당 모델 이 동등하거나 거의 동등한 버전보다 선호되어야 하는 이유를 설명해야 합니다. 동일한 데이터를 정확히 또는 거의 비슷하게 설명합니 다. 이 단계는 통계보다 훨씬 더 논리적이며, 향후 연구 에서 심각한 경쟁 모델을 구별하기 위해 무엇을 할 수 있는지 설명하는 것도 포함됩니다. 동등하거나 거의 동 등한 모델에 대한 완전한 보고는 드물기 때문에, 양심 적인 독자는 이 문제를 해결함으로써 자신의 SEM 분 석을 실제로 구별할 수 있습니다. 동등한 버전의 구조 모델 생성 및 평가는 다음 장에서 다룹니다.\n모델이 유지되지 않는 경우 분석에서 테스트한 이론에 대한 의미를 설명하려면 학자로서의 기술이 필 요합니다. 결국, 유지 모델이 있든 없든 간에 진정한 영광은 최 선을 다해 논리적으로 끝까지 철저한 평가 프로세스를 따르는 데서 비롯됩니다. 시인 랄프 왈도 에머슨은 이 를 이렇게 표현했습니다: 잘한 일의 보상은 그것을 해 냈다는 것입니다(Mikis, 2012, 294쪽).\n\n결국, 유지 모델이 있든 없든 간에 진정한 영광은 최 선을 다해 논리적으로 끝까지 철저한 평가 프로세스를 따르는 데서 비롯됩니다. 시인 랄프 왈도 에머슨은 이 를 이렇게 표현했습니다: 잘한 일의 보상은 그것을 해 냈다는 것입니다(Mikis, 2012, 294쪽)."
  },
  {
    "objectID": "contents/fit-index.html#잔여물-검사를-위한-팁",
    "href": "contents/fit-index.html#잔여물-검사를-위한-팁",
    "title": "Fit Indices",
    "section": "잔여물 검사를 위한 팁",
    "text": "잔여물 검사를 위한 팁\n잔차에 대해 보고하는 것이 중요하지만, 카이제곱 검정 결과 및 근사 적합도 지수 값과 마찬가지로 잔차의 크 기와 모델 오정의 유형 또는 양 사이에는 신뢰할 수 있 거나 신뢰할 수 있는 연관성이 없다는 것을 알아야 합 니다. 예를 들어, 상대적으로 작은 상관관계 잔차로 표 시되는 구체화 오류의 정도는 경미할 수도 있지만 심각 할 수도 있습니다. 한 가지 이유는 다음 장에서 정의하 는 수정 지수를 비롯한 잔차 및 기타 진단 통계의 값 자 체가 오특정에 의해 영향을 받기 때문입니다. 의학에 비유하자면, 특정 질병에 대한 진단 검사가 해당 질병 을 앓고 있는 환자에게는 정확도가 떨어질 수 있습니다 . 두 번째 이유는 모델 한 부분의 잘못된 지정이 모델의 다른 부분의 추정치를 왜곡하는 전체 추정에서의 오류전파입니다. 세 번째는 잔차는 동일하지만 인과 관계의 모순된 패턴을 갖는 동등 모델입니다. 그러나 우리는 일 반적으로 모형의 어느 부분이 잘못된 것인지 미리 알 수 없기 때문에 잔차가 우리에게 무엇을 알려주는지 정확 히 이해하기 어려울 수 있습니다. 잔차의 패턴을 검사하는 것이 때때로 도움이 될 수 있 습니다.rXY &gt;0인한쌍의변수X와Y가간접인과경로 로만 연결되어 있다고 가정해 보겠습니다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "강사: 조성균\nemail: sk.cho@snu.ac.kr\n수업시간: 월 7:00 ~ 9:50PM\n면담 시간: 수업 후\nWebsite: sem.modellings.art\n과제: Notice\n질문: Communicate/Ask",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#강의-정보",
    "href": "index.html#강의-정보",
    "title": "Welcome",
    "section": "",
    "text": "강사: 조성균\nemail: sk.cho@snu.ac.kr\n수업시간: 월 7:00 ~ 9:50PM\n면담 시간: 수업 후\nWebsite: sem.modellings.art\n과제: Notice\n질문: Communicate/Ask",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#강의-개요",
    "href": "index.html#강의-개요",
    "title": "Welcome",
    "section": "강의 개요",
    "text": "강의 개요\n심리통계에서 배운 회귀분석을 기반으로 하여, 잠재변수(latent variable)을 포함해 여러 변수들 간의 관계에 대한 가설에 대한 통계적 검증을 위한 구조모형을 다룹니다. 이는 확인적 요인 분석 (confirmatory factor analysis, CFA)과 경로분석 (path analysis)을 통합한 구조방정식모형 (Structural equation model, SEM)의 프레임워크를 포함하며, 인과관계의 대한 추론을 위해 SEM을 활용하는 방법과 그 한계에 대해 다룹니다. 본 수업에서는 R의 SEM 패키지들을 활용해 실습을 하며, 다양한 실제 사례를 통해 분석의 결과를 통계 이론의 이해를 바탕으로 올바로 해석할 수 있는 능력을 갖춥니다.\n\n교재 및 R 코드\n\n\n다중회귀분석과 구조방정식모형분석 - 다중회귀분석을 넘어 (3판), 2024 by Timothy Z. Keith (지은이), 노석준 (옮긴이)\n원제: Multiple Regression and Beyond: An Introduction to Multiple Regression and Structural Equation Modeling, Third Edition (2019)\n저자 웹사이트: https://tzkeith.com\n실습을 위한 참고 홈페이지\n\nR Cookbook for Structural Equation Modeling by Ge Jiang\nGithub repository\n\n\n\n\n\n\n\n참고 도서\n\n\nPrinciples and Practice of Structural Equation Modeling (5e), 2023 by Rex B. Kline\nR code 및 부가 자료 링크\nR 참고 도서\nR for Data Science (2e) by Hadley Wickham and Garrett Grolemund",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#수업-활동",
    "href": "index.html#수업-활동",
    "title": "Welcome",
    "section": "수업 활동",
    "text": "수업 활동\n출석 (10%), 일반과제 (30%), 중간고사 대체 과제 (30%), 기말고사 (30%)",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "contents/chap16.html#testing-competing-models",
    "href": "contents/chap16.html#testing-competing-models",
    "title": "Chapter 15-16. Confirmatory Factor Analysis",
    "section": "Testing Competing Models",
    "text": "Testing Competing Models\n\n\nCross-loadings\n\n\n## cross-loadings\ndas2_model_crossload &lt;- \"\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss\n  Spatial =~ pcss + rdss + rpss\n  Memory =~ dfss + dbss + soss + rdss\n\"\nfit_crossload &lt;- sem(das2_model_crossload, sample.cov = das2cov, sample.nobs = 800)\nsummary(fit_crossload, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 168 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        31\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               127.495\n  Degrees of freedom                                47\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              4335.087\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.981\n  Tucker-Lewis Index (TLI)                       0.974\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33708.321\n  Loglikelihood unrestricted model (H1)     -33644.574\n                                                      \n  Akaike (AIC)                               67478.643\n  Bayesian (BIC)                             67623.866\n  Sample-size adjusted Bayesian (SABIC)      67525.424\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.046\n  90 Percent confidence interval - lower         0.037\n  90 Percent confidence interval - upper         0.056\n  P-value H_0: RMSEA &lt;= 0.050                    0.725\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.027\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              1.000                               7.462    0.739\n    wdss              0.942    0.049   19.268    0.000    7.031    0.734\n    vsss              1.100    0.053   20.895    0.000    8.206    0.805\n  Nonverbal =~                                                          \n    psss              1.000                               5.572    0.542\n    mass              1.305    0.093   14.105    0.000    7.273    0.710\n    sqss              1.405    0.094   15.030    0.000    7.830    0.808\n  Spatial =~                                                            \n    pcss              1.000                               7.459    0.809\n    rdss              1.066    0.143    7.457    0.000    7.947    0.795\n    rpss              0.799    0.049   16.365    0.000    5.959    0.590\n  Memory =~                                                             \n    dfss              1.000                               7.378    0.668\n    dbss              1.035    0.058   17.941    0.000    7.638    0.753\n    soss              1.119    0.061   18.372    0.000    8.259    0.777\n    rdss             -0.085    0.134   -0.631    0.528   -0.625   -0.063\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal ~~                                                             \n    Nonverbal        33.468    3.077   10.876    0.000    0.805    0.805\n    Spatial          42.014    3.278   12.818    0.000    0.755    0.755\n    Memory           45.449    3.691   12.312    0.000    0.825    0.825\n  Nonverbal ~~                                                          \n    Spatial          37.148    3.196   11.624    0.000    0.894    0.894\n    Memory           35.272    3.288   10.729    0.000    0.858    0.858\n  Spatial ~~                                                            \n    Memory           45.113    3.565   12.654    0.000    0.820    0.820\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .nvss             46.191    2.924   15.797    0.000   46.191    0.453\n   .wdss             42.446    2.662   15.945    0.000   42.446    0.462\n   .vsss             36.524    2.709   13.484    0.000   36.524    0.352\n   .psss             74.823    3.999   18.708    0.000   74.823    0.707\n   .mass             51.970    3.121   16.652    0.000   51.970    0.496\n   .sqss             32.571    2.476   13.153    0.000   32.571    0.347\n   .pcss             29.264    2.475   11.822    0.000   29.264    0.345\n   .rdss             44.468    3.433   12.953    0.000   44.468    0.445\n   .rpss             66.362    3.654   18.160    0.000   66.362    0.651\n   .dfss             67.407    3.900   17.285    0.000   67.407    0.553\n   .dbss             44.528    2.879   15.466    0.000   44.528    0.433\n   .soss             44.648    3.049   14.643    0.000   44.648    0.396\n    Verbal           55.681    4.881   11.407    0.000    1.000    1.000\n    Nonverbal        31.044    4.003    7.755    0.000    1.000    1.000\n    Spatial          55.629    4.457   12.482    0.000    1.000    1.000\n    Memory           54.441    5.441   10.005    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    nvss              0.547\n    wdss              0.538\n    vsss              0.648\n    psss              0.293\n    mass              0.504\n    sqss              0.653\n    pcss              0.655\n    rdss              0.555\n    rpss              0.349\n    dfss              0.447\n    dbss              0.567\n    soss              0.604\n\n\n\n이전 모형과 비교하면,\n\n# Compare models\nlavTestLRT(fit, fit_crossload) |&gt; print()\n\n\nChi-Squared Difference Test\n\n              Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nfit_crossload 47 67479 67624 127.49                                    \nfit           48 67477 67618 127.99    0.49144     0       1     0.4833\n\n\n\nsemTools::compareFit(fit, fit_crossload) |&gt; summary()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n              Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nfit_crossload 47 67479 67624 127.49                                    \nfit           48 67477 67618 127.99    0.49144     0       1     0.4833\n\n####################### Model Fit Indices ###########################\n                        chisq df pvalue        rmsea          cfi          tli\nfit_crossload 127.495&lt;U+2020&gt; 47   .000        .046         .981         .974 \nfit                  127.986  48   .000 .046&lt;U+2020&gt; .981&lt;U+2020&gt; .974&lt;U+2020&gt;\n                      srmr               aic               bic\nfit_crossload .027&lt;U+2020&gt;        67478.643         67623.866 \nfit                  .027  67477.134&lt;U+2020&gt; 67617.673&lt;U+2020&gt;\n\n################## Differences in Fit Indices #######################\n                    df  rmsea cfi   tli srmr    aic    bic\nfit - fit_crossload  1 -0.001   0 0.001    0 -1.509 -6.193\n\n\n\n\n\n3-factor model\n\n\ndas2_model_3f &lt;- \"\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss + pcss + rdss + rpss\n  Memory =~ dfss + dbss + soss\n\"\nfit_3f &lt;- sem(das2_model_3f, sample.cov = das2cov, sample.nobs = 800)\nsummary(fit_3f, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 118 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        27\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               163.856\n  Degrees of freedom                                51\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              4335.087\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.974\n  Tucker-Lewis Index (TLI)                       0.966\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33726.502\n  Loglikelihood unrestricted model (H1)     -33644.574\n                                                      \n  Akaike (AIC)                               67507.004\n  Bayesian (BIC)                             67633.489\n  Sample-size adjusted Bayesian (SABIC)      67547.749\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.053\n  90 Percent confidence interval - lower         0.044\n  90 Percent confidence interval - upper         0.062\n  P-value H_0: RMSEA &lt;= 0.050                    0.304\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              1.000                               7.489    0.742\n    wdss              0.936    0.049   19.275    0.000    7.012    0.732\n    vsss              1.095    0.052   20.951    0.000    8.199    0.805\n  Nonverbal =~                                                          \n    psss              1.000                               5.579    0.542\n    mass              1.258    0.089   14.055    0.000    7.018    0.685\n    sqss              1.367    0.090   15.158    0.000    7.625    0.787\n    pcss              1.285    0.085   15.071    0.000    7.169    0.778\n    rdss              1.249    0.088   14.196    0.000    6.968    0.697\n    rpss              1.044    0.083   12.615    0.000    5.826    0.577\n  Memory =~                                                             \n    dfss              1.000                               7.398    0.670\n    dbss              1.033    0.057   17.972    0.000    7.638    0.753\n    soss              1.117    0.061   18.407    0.000    8.261    0.778\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal ~~                                                             \n    Nonverbal        33.675    3.044   11.063    0.000    0.806    0.806\n    Memory           45.656    3.701   12.335    0.000    0.824    0.824\n  Nonverbal ~~                                                          \n    Memory           35.599    3.266   10.899    0.000    0.862    0.862\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .nvss             45.794    2.913   15.720    0.000   45.794    0.450\n   .wdss             42.712    2.672   15.987    0.000   42.712    0.465\n   .vsss             36.642    2.713   13.506    0.000   36.642    0.353\n   .psss             74.738    3.945   18.945    0.000   74.738    0.706\n   .mass             55.615    3.135   17.740    0.000   55.615    0.530\n   .sqss             35.749    2.262   15.808    0.000   35.749    0.381\n   .pcss             33.501    2.087   16.049    0.000   33.501    0.395\n   .rdss             51.329    2.919   17.585    0.000   51.329    0.514\n   .rpss             67.931    3.626   18.733    0.000   67.931    0.667\n   .dfss             67.121    3.893   17.241    0.000   67.121    0.551\n   .dbss             44.529    2.884   15.443    0.000   44.529    0.433\n   .soss             44.616    3.054   14.611    0.000   44.616    0.395\n    Verbal           56.079    4.893   11.460    0.000    1.000    1.000\n    Nonverbal        31.130    3.956    7.868    0.000    1.000    1.000\n    Memory           54.727    5.454   10.034    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    nvss              0.550\n    wdss              0.535\n    vsss              0.647\n    psss              0.294\n    mass              0.470\n    sqss              0.619\n    pcss              0.605\n    rdss              0.486\n    rpss              0.333\n    dfss              0.449\n    dbss              0.567\n    soss              0.605\n\n\n\n\n# Compare models\nsemTools::compareFit(fit, fit_3f) |&gt; summary()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n       Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nfit    48 67477 67618 127.99                                          \nfit_3f 51 67507 67633 163.86      35.87 0.11703       3  7.978e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n####################### Model Fit Indices ###########################\n                 chisq df pvalue        rmsea          cfi          tli\nfit    127.986&lt;U+2020&gt; 48   .000 .046&lt;U+2020&gt; .981&lt;U+2020&gt; .974&lt;U+2020&gt;\nfit_3f        163.856  51   .000        .053         .974         .966 \n               srmr               aic               bic\nfit    .027&lt;U+2020&gt; 67477.134&lt;U+2020&gt; 67617.673&lt;U+2020&gt;\nfit_3f        .029         67507.004         67633.489 \n\n################## Differences in Fit Indices #######################\n             df rmsea    cfi    tli  srmr   aic    bic\nfit_3f - fit  3 0.007 -0.008 -0.008 0.002 29.87 15.816\n\n\n\n\n\nTransform 4-factor to 3-factor model with constraints (2-step)\n3-factor 모델이 초기 모델에 내포되어 있음을 확인할 수 있음.\n\ndas2_model_constrain &lt;- \"\n  # free the first paraemters\n  Verbal =~ NA*nvss + wdss + vsss\n  Nonverbal =~ NA*psss + mass + sqss\n  Spatial =~ NA*pcss + rdss + rpss\n  Memory =~ NA*dfss + dbss + soss\n\n  # unit variance indentification\n  Verbal ~~ 1*Verbal\n  Nonverbal ~~ 1*Nonverbal\n  Spatial ~~ 1*Spatial\n  Memory ~~ 1*Memory\n\n  # equality constraints\n  Spatial ~~ 1*Nonverbal\n\n  Nonverbal ~~ a*Verbal\n  Spatial ~~ a*Verbal\n  Nonverbal ~~ b*Memory\n  Spatial ~~ b*Memory\n\"\n\nfit_3f_2 &lt;- sem(das2_model_constrain, sample.cov = das2cov, sample.nobs = 800)\nsummary(fit_3f_2, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 30 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        29\n  Number of equality constraints                     2\n\n  Number of observations                           800\n\nModel Test User Model:\n                                                      \n  Test statistic                               163.856\n  Degrees of freedom                                51\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              4335.087\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.974\n  Tucker-Lewis Index (TLI)                       0.966\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -33726.502\n  Loglikelihood unrestricted model (H1)     -33644.574\n                                                      \n  Akaike (AIC)                               67507.004\n  Bayesian (BIC)                             67633.489\n  Sample-size adjusted Bayesian (SABIC)      67547.749\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.053\n  90 Percent confidence interval - lower         0.044\n  90 Percent confidence interval - upper         0.062\n  P-value H_0: RMSEA &lt;= 0.050                    0.304\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal =~                                                             \n    nvss              7.489    0.327   22.920    0.000    7.489    0.742\n    wdss              7.012    0.312   22.489    0.000    7.012    0.732\n    vsss              8.199    0.320   25.584    0.000    8.199    0.805\n  Nonverbal =~                                                          \n    psss              5.579    0.355   15.736    0.000    5.579    0.542\n    mass              7.018    0.333   21.071    0.000    7.018    0.685\n    sqss              7.625    0.299   25.518    0.000    7.625    0.787\n  Spatial =~                                                            \n    pcss              7.169    0.286   25.104    0.000    7.169    0.778\n    rdss              6.968    0.323   21.558    0.000    6.968    0.697\n    rpss              5.826    0.344   16.960    0.000    5.826    0.577\n  Memory =~                                                             \n    dfss              7.398    0.369   20.068    0.000    7.398    0.670\n    dbss              7.638    0.326   23.404    0.000    7.638    0.753\n    soss              8.261    0.338   24.429    0.000    8.261    0.778\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal ~~                                                             \n    Nonverbal  (a)    0.806    0.021   38.739    0.000    0.806    0.806\n    Spatial    (a)    0.806    0.021   38.739    0.000    0.806    0.806\n  Nonverbal ~~                                                          \n    Memory     (b)    0.862    0.019   45.886    0.000    0.862    0.862\n  Spatial ~~                                                            \n    Memory     (b)    0.862    0.019   45.886    0.000    0.862    0.862\n  Nonverbal ~~                                                          \n    Spatial           1.000                               1.000    1.000\n  Verbal ~~                                                             \n    Memory            0.824    0.022   36.764    0.000    0.824    0.824\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Verbal            1.000                               1.000    1.000\n    Nonverbal         1.000                               1.000    1.000\n    Spatial           1.000                               1.000    1.000\n    Memory            1.000                               1.000    1.000\n   .nvss             45.794    2.913   15.720    0.000   45.794    0.450\n   .wdss             42.712    2.672   15.987    0.000   42.712    0.465\n   .vsss             36.642    2.713   13.506    0.000   36.642    0.353\n   .psss             74.738    3.945   18.945    0.000   74.738    0.706\n   .mass             55.615    3.135   17.740    0.000   55.615    0.530\n   .sqss             35.749    2.262   15.808    0.000   35.749    0.381\n   .pcss             33.501    2.087   16.049    0.000   33.501    0.395\n   .rdss             51.329    2.919   17.585    0.000   51.329    0.514\n   .rpss             67.931    3.626   18.733    0.000   67.931    0.667\n   .dfss             67.121    3.893   17.241    0.000   67.121    0.551\n   .dbss             44.529    2.884   15.443    0.000   44.529    0.433\n   .soss             44.616    3.054   14.611    0.000   44.616    0.395\n\n\n\n\n# Compare models\nlavTestLRT(fit_3f, fit_3f_2) |&gt; print()\n\n\nChi-Squared Difference Test\n\n         Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nfit_3f   51 67507 67633 163.86                                    \nfit_3f_2 51 67507 67633 163.86  1.387e-09     0       0           \n\n\n\n\nModification indices for 3-factor model\n\nmodindices(fit_3f, sort = TRUE, maximum.number = 8) |&gt; print()\n\n       lhs op  rhs     mi     epc sepc.lv sepc.all sepc.nox\n93    mass ~~ sqss 26.473  10.256  10.256    0.230    0.230\n106   pcss ~~ rdss 26.463   9.511   9.511    0.229    0.229\n95    mass ~~ rdss 24.101 -10.817 -10.817   -0.202   -0.202\n71    wdss ~~ rdss 15.597  -7.636  -7.636   -0.163   -0.163\n111   rdss ~~ rpss 15.509   9.200   9.200    0.156    0.156\n31  Verbal =~ psss  9.941   0.323   2.420    0.235    0.235\n94    mass ~~ pcss  7.622  -5.266  -5.266   -0.122   -0.122\n99    mass ~~ soss  7.343   5.784   5.784    0.116    0.116\n\n\n\n가령, mass와 rdss의 잔차/오차분산 간의 상관관계를 추정하면,\n\ndas2_model_3f_modi &lt;- \"\n  Verbal =~ nvss + wdss + vsss\n  Nonverbal =~ psss + mass + sqss + pcss + rdss + rpss\n  Memory =~ dfss + dbss + soss\n\n  mass ~~ rdss\n\"\nfit_3f_modi &lt;- sem(das2_model_3f_modi, sample.cov = das2cov, sample.nobs = 800)\nparameterEstimates(fit_3f_modi, standardized = \"std.all\") |&gt; subset(lhs == \"mass\" | lhs == \"rdss\") |&gt; print()\n\n    lhs op  rhs     est    se      z pvalue ci.lower ci.upper std.all\n13 mass ~~ rdss -10.954 2.110 -5.192      0  -15.089   -6.818  -0.217\n18 mass ~~ mass  52.562 3.080 17.066      0   46.526   58.599   0.501\n21 rdss ~~ rdss  48.374 2.864 16.889      0   42.760   53.987   0.484\n\n\n\nanova(fit_3f, fit_3f_modi) |&gt; print()\n\n\nChi-Squared Difference Test\n\n            Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nfit_3f_modi 50 67483 67614 138.09                                          \nfit_3f      51 67507 67633 163.86     25.764 0.17594       1  3.858e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nResiduals for 3-factor model\n\nresiduals(fit_3f, \"cor\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       nvss   wdss   vsss   psss   mass   sqss   pcss   rdss   rpss   dfss\nnvss  0.000                                                               \nwdss  0.015  0.000                                                        \nvsss -0.014  0.004  0.000                                                 \npsss  0.051  0.055  0.048  0.000                                          \nmass -0.023  0.023  0.015 -0.002  0.000                                   \nsqss -0.021 -0.012  0.026 -0.036  0.065  0.000                            \npcss  0.039 -0.040  0.017 -0.011 -0.036 -0.008  0.000                     \nrdss  0.019 -0.088 -0.021  0.020 -0.078 -0.033  0.065  0.000              \nrpss -0.012 -0.051 -0.025  0.014 -0.009 -0.005 -0.009  0.073  0.000       \ndfss  0.038  0.011  0.017  0.012 -0.043 -0.016 -0.018  0.013 -0.038  0.000\ndbss -0.041 -0.023 -0.007  0.002  0.045  0.028  0.008  0.010 -0.004 -0.005\nsoss  0.009  0.022 -0.008 -0.026  0.045 -0.004 -0.022 -0.016 -0.015  0.007\n       dbss   soss\nnvss              \nwdss              \nvsss              \npsss              \nmass              \nsqss              \npcss              \nrdss              \nrpss              \ndfss              \ndbss  0.000       \nsoss -0.002  0.000\n\n\n\n\nresiduals(fit_3f_modi, \"cor\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       nvss   wdss   vsss   psss   mass   sqss   pcss   rdss   rpss   dfss\nnvss  0.000                                                               \nwdss  0.014  0.000                                                        \nvsss -0.014  0.004  0.000                                                 \npsss  0.056  0.060  0.054  0.000                                          \nmass -0.031  0.016  0.007 -0.012  0.000                                   \nsqss -0.013 -0.005  0.035 -0.032  0.051  0.000                            \npcss  0.047 -0.033  0.025 -0.007 -0.050 -0.003  0.000                     \nrdss  0.012 -0.095 -0.028  0.010  0.000 -0.047  0.051  0.000              \nrpss -0.008 -0.048 -0.021  0.014 -0.022 -0.004 -0.008  0.060  0.000       \ndfss  0.039  0.012  0.018  0.016 -0.052 -0.010 -0.012  0.005 -0.036  0.000\ndbss -0.041 -0.023 -0.006  0.006  0.035  0.033  0.013  0.000 -0.003 -0.005\nsoss  0.008  0.021 -0.008 -0.022  0.034  0.002 -0.017 -0.027 -0.013  0.007\n       dbss   soss\nnvss              \nwdss              \nvsss              \npsss              \nmass              \nsqss              \npcss              \nrdss              \nrpss              \ndfss              \ndbss  0.000       \nsoss -0.002  0.000",
    "crumbs": [
      "Keith's",
      "Factor Analysis"
    ]
  }
]